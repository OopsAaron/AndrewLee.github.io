<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>思建的NLP之旅</title>
  
  <subtitle>沉淀自己</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-11-30T13:08:58.435Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>李思建</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>2020-11-26-ipad做网页笔记方法</title>
    <link href="http://yoursite.com/2020/11/26/2020-11-26-ipad%E5%81%9A%E7%BD%91%E9%A1%B5%E7%AC%94%E8%AE%B0%E6%96%B9%E6%B3%95/"/>
    <id>http://yoursite.com/2020/11/26/2020-11-26-ipad%E5%81%9A%E7%BD%91%E9%A1%B5%E7%AC%94%E8%AE%B0%E6%96%B9%E6%B3%95/</id>
    <published>2020-11-26T07:02:37.000Z</published>
    <updated>2020-11-30T13:08:58.435Z</updated>
    
    <content type="html"><![CDATA[<h4 id="问题">问题</h4><p>想将网页内容导出pdf，用ipad做笔记。但是在用chrome或者dge进行打印成pdf时因为网页本身的顶部的tab总会掩盖掉一部分网页内容</p><h4 id="解决">解决</h4><p>最终找到一个好方法</p><p>用Safari打开网页，然后在网页右上角点击分享按钮，会出现标记样式。点击就可以在网页端做笔记。</p><p><img src="https://i.loli.net/2020/11/26/HpRy5toOr1fcUPe.png"></p><p>不过毕竟在goodnotes做笔记习惯了，所以可以用goodnotes打开，然后就自动导入pdf到goodnotes中，就可以愉快做笔记了</p><p>虽然导入的pdf字体有点小，不过好在不会遮挡住任何网页内容。这是我想到的最合适我的方法了</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;问题&quot;&gt;问题&lt;/h4&gt;
&lt;p&gt;想将网页内容导出pdf，用ipad做笔记。但是在用chrome或者dge进行打印成pdf时因为网页本身的顶部的tab总会掩盖掉一部分网页内容&lt;/p&gt;
&lt;h4 id=&quot;解决&quot;&gt;解决&lt;/h4&gt;
&lt;p&gt;最终找到一个好方法&lt;/p&gt;
&lt;p&gt;用S
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Practices阅读笔记</title>
    <link href="http://yoursite.com/2020/11/26/2020-11-26-Deep%20Learning%20for%20NLP%20Best%20Practices%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2020/11/26/2020-11-26-Deep%20Learning%20for%20NLP%20Best%20Practices%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</id>
    <published>2020-11-26T06:20:14.000Z</published>
    <updated>2020-11-26T07:18:35.534Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言">前言</h3><p>博客地址：<a href="http://ruder.io/deep-learning-nlp-best-practices/index.html" target="_blank" rel="noopener" class="uri">http://ruder.io/deep-learning-nlp-best-practices/index.html</a></p><h3 id="正文">正文</h3><p>本文尝试将阅读上述博客之后的得到的一些tips记录起来</p><h3 id="参考">参考</h3><blockquote><p><a href="https://ruder.io/deep-learning-nlp-best-practices/index.html" target="_blank" rel="noopener" class="uri">https://ruder.io/deep-learning-nlp-best-practices/index.html</a></p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      本篇博客写的不错，会在实践中给出很多指导
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-11-24-transformer优化-temp</title>
    <link href="http://yoursite.com/2020/11/24/2020-11-24-transformer%E4%BC%98%E5%8C%96-temp/"/>
    <id>http://yoursite.com/2020/11/24/2020-11-24-transformer%E4%BC%98%E5%8C%96-temp/</id>
    <published>2020-11-24T11:48:40.000Z</published>
    <updated>2020-12-01T11:50:50.484Z</updated>
    
    <content type="html"><![CDATA[<p>想法：</p><p>总体来说，基于原始的transformer模型，在这个基础上综合增加其它论文的创新点，以及最好能够有自己的创新点</p><p>思路有两个：</p><ol type="1"><li>通过现有的模型，去解决没有解决过的任务。比如NLP的模型去解决CV的任务</li><li>在现有的模型中去添加自己的想法，使得有更好的性能 ，如transformer-XL对transformer的改进</li></ol><p>一些想法：</p><ol type="1"><li><p>一些在bert上进行优化的模型是否可以运用到修改transformer的attention结构中。例如ConvBERT,AlBERT</p><p>ALBERT相比原始BERT其实更适合浅层Transformer，也可以作为之后的尝试方向。</p></li><li><p>降低transformer模型的计算量，已经存在的模型，模型剪枝、量化、蒸馏等精简技术，又或者修改 Attention 结构</p></li><li><p>解决自回归的问题，之前读过teacher-forcing 模型来进行改进</p></li><li><p>之前改进的模型如果用在图像领域，可以用在文本领域，如果这样做的话，是否有创新性呢？？</p></li><li><p>感觉最近的论文对于transformer的稀疏性研究挺多 ，是否可以借鉴几篇 。 对transformer进行优化</p></li><li><p>将transformer中的部分结构换成LSTM是否会效果好一些呢 ？ 结合attention和LSTM</p></li><li><p>重点基于transformer-XL和Reformer这些关注点高的论文</p></li><li><p>从经典论文引用的论文中去找到新方法去结合自己的模型</p></li><li><p>BERT以及其它的模型，可以不用这么多层数，不用这么多的数据集，去设计不同的自监督任务去改进模型。</p><p>可以像用word2vec一样使用这些pretrain模型，然后去专注task相关的研究（在你能接受的数据和计算资源范围内。这样是否可以呢 ？？</p><p>比如只有6层的BERT-base中，MLM策略是采用的自己设计的，或者融合其它模型的MLM策略，与BERT-base进行比较，发现效果有所提升，是否就可以发表论文？？</p><p>因为设计的自监督任务必须要与下游任务尽量相关的，这样才能在预训练阶段学到更多东西</p></li><li><p>对于一些优化器或者激活函数或者norm函数进行优化，（使用最新模型使用的优化器）可能会对模型效果有所提升</p></li><li><p>有空读读word2vec的内容，从之前的模型中去结合形成创新点</p></li><li><p>网上是否有综合了transformer-base模型的github项目。类似于hugging face对于BERT-base模型</p></li><li><p>将decoder的自回归变成非自回归形式的。参考论文&lt;Deep Encoder, Shallow Decoder: Reevaluating the Speed-Quality Tradeoff in Machine Translation&gt; 2020年论文，无代码</p></li><li><p>exposure bias问题的解决，参考<scheduled sampling="" for="" transformers="">，8.1分享。有torch代码（非官方）</scheduled></p></li><li><p>将一些部分设置为adaptive 动态的。</p></li></ol><p>总体就是以一个模型（如transformer-XL）为基础，在上面增加其它的思想，增加创新点</p><p>计划：</p><p>transformer-XL可以增强transformer处理长文本的能力，但是同时它的复杂度依然是n2，所以以transformer-XL为基础，来优化结构，降低复杂度，是可以解决的一个思路，同时如果能够解决exposure bias问题更好</p><p>以transformer-XL为基础进行改进，有pytorch代码。解决机器翻译问题，或者看邱锡鹏教授实验室的研究方向</p><p><a href="https://github.com/kimiyoung/transformer-xl" target="_blank" rel="noopener" class="uri">https://github.com/kimiyoung/transformer-xl</a></p><p>对transformer-XL进行压缩，减少memory。 或者参考XLNET， 也是结合transformer-xl</p><p>结合BERT的一些思想， 或者其它的压缩模型思想</p><p>线性核transformer进行结合 ， paperweekly 苏剑林的提议， 结合到transformer-xl中</p><p>将层数降低比较是否可以？</p><p>今年ICLR2020已经有一些工作对天transformer-XL进行改进 。借鉴查看</p><hr><p>疑问：</p><p>如果用bert去做图像的任务，会有以下问题</p><ol type="1"><li><p>BERT 训练需要的资源多，可能会训练的周期长。解决方法：将参数量减少，层数减少</p></li><li><p>bert 主要是进行自监督，也就是处理的是没有标签的数据。这样的数据在nlp很多，但是在图像上很多是有标签的</p></li></ol><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      transformer模型改进优化总结
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-11-20-论文分享</title>
    <link href="http://yoursite.com/2020/11/20/2020-11-20-%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"/>
    <id>http://yoursite.com/2020/11/20/2020-11-20-%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/</id>
    <published>2020-11-20T02:46:05.000Z</published>
    <updated>2020-11-26T06:16:10.478Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2020/11/20/ict1zGoAVsCSgbJ.png"></p><blockquote><p>NeurIPS 2020</p><p>code url (official tf) : <a href="https://github.com/yitu-opensource/ConvBert" target="_blank" rel="noopener" class="uri">https://github.com/yitu-opensource/ConvBert</a></p></blockquote><h3 id="背景">背景</h3><p>本文是国内的依图科技发表在NeurlPS 2020 上的一篇论文，今年的NeurlPS 2020 将在12月份温哥华举办，全球仅接受了1900篇论文，所以接受的论文很值得去阅读了。</p><p>本文是从模型基本的attention结构入手去改进BERT模型</p><p>本文的Abstract如下：</p><p><img src="https://i.loli.net/2020/11/20/Rb6es4qz9fjDcCd.png"></p><h3 id="问题">问题</h3><p>BERT-family模型现在不断刷新NLP领域，但是BERT模型的基础attention块也严重依赖全局的自我注意力块，并且消耗大量的memory以及计算资源。</p><p>降低BERT对于计算资源的利用是目前研究的问题</p><h3 id="解决">解决</h3><p>有很多工作从改进预训练任务或者利用知识蒸馏的方法优化模型的训练，但是少有改进attention模型结构（backbone architecture）的工作。</p><p>依图发现一些注意力头虽然是全局视角，但是只能关注到局部依赖，这样就存在一些计算冗余。</p><p>依图研发团队从这种模型结构本身的冗余出发，提出了一种基于跨度的动态卷积（span-based dynamic convolution）去代替<strong>一部分</strong>原有的attention去直接建模局部依赖。 （注意是一部分，而不是全部）</p><p>通过这种span-based dynamic convolution 和剩余的 self-attention heads 一起使得全局和局部学习更高效。</p><p>也就是将卷积整合到self-attention中去形成一个mixed attention mechanism ，以此来结合这二者的优势</p><h3 id="贡献">贡献</h3><p>提出了ConvBERT，通过全新的注意力模块，仅用 1/10 的训练时间和 1/6 的参数就获得了跟 BERT模型一样的精度。</p><p>具体如下：</p><p><img src="https://i.loli.net/2020/11/20/5GKrsQzLR7w43Bg.png"></p><h3 id="模型">模型</h3><h4 id="self-attention-dynamic-convolution-span-based-dynamic-convolution-的不同">Self-attention &amp; Dynamic convolution &amp; Span-based dynamic convolution 的不同</h4><p>其中可以认为attention weight = convolution kernel （都是代表的关联度的强弱）</p><p><img src="https://i.loli.net/2020/11/20/NDqxszOCaG4T5Ub.png"></p><p>在前人的Dynamic convolution基础上，基于span改进得到了span-based dynamic convolution。不只是接受单一的token，而是接受一段token的span来产生更多的自适应的卷积核，这样可以解决Dynamic convolution存在的问题，<strong>可以达到使得不同上下文的同一个token能够产生不同的卷积核。</strong>具体如下：</p><p><img src="https://i.loli.net/2020/11/20/ipIFCS5nBwW71xm.png"></p><h4 id="bottleneck-structure-grouped-linear">bottleneck structure &amp; grouped linear</h4><p>除了对于最基本的attention结构进行改动之外，本文还在另外两处对BERT进行了修改，即bottleneck structure &amp; grouped linear</p><p>bottleneck structure ： 通过将输入token嵌入到低维空间中来减少不必要的head的数量 （降维来减少head），以此来减少冗余并且提高效率</p><p>grouped linear ： 因为一般的前馈网络层（FFN）的维度会是输入输出维度的4倍，维度较高，这样就消耗了很多计算量，于是采用了groupedlinear operator 操作，在降低参数的同时也没有降低表示能力。</p><p>具体如下：</p><p><img src="https://i.loli.net/2020/11/20/glMfEBvmbS7arkX.png"></p><blockquote><p>注：</p><p>感觉grouped linear 和reformer模型中的FFN层的分块类似</p></blockquote><p>BERT和ConvBERT模型的attention map（attention map 可以理解成词与词之间的关系）的不同如下图所示。大多注意力主要集中在对角线，即主要学习到的是局部的注意力。这就意味着其中存在着冗余，也就是说很多 attention map 中远距离关系值是没有必要计算的。</p><p>ConvBERT的attention map不再过多关注局部的关系，这正是卷积模块减少冗余的作用体现</p><p><img src="https://i.loli.net/2020/11/20/iZTbkydfSw3zNJc.png"></p><p>很多head是不必要存在的，于是采用卷积操作来捕获局部依赖。原文如下：</p><p><img src="https://i.loli.net/2020/11/20/z1XMwN9E4quKpUJ.png"></p><h4 id="light-weight-dynamic-convolution">Light-weight &amp; dynamic convolution</h4><p>这两个模型都是Facebook AI Research发表在ICLR 2019上的论文中提出的，<a href="https://openreview.net/pdf?id=SkVhlh09tX" target="_blank" rel="noopener">原文地址</a></p><p><img src="https://i.loli.net/2020/11/20/MprAVtU3zRQFyfe.png"></p><p>模型基本如下：</p><p>先是引出了Light-weight convolution的运算操作：</p><p><img src="https://i.loli.net/2020/11/20/hP6dy915jSEMoLi.png"></p><p>其中X∈R^n×d 为输入的特征，而W∈R^k 则是卷积核(相当于是加权)，k 为卷积核的大小。轻量卷积的作用是将输入的每个词对应的特征附近的 k 个特征加权平均生成输出。</p><p>在此基础上，动态卷积（dynamic convolution ）可以写作</p><p><img src="https://i.loli.net/2020/11/20/ayimwIMseuzNbjA.png"></p><p>此处卷积核是由对应的词的特征经过线性变换和 softmax 之后产生的。</p><h4 id="span-based-dynamic-convolution">Span-based dynamic convolution</h4><p>相比于动态卷积，Span-based dynamic convolution 依赖的不是单一的token，而是local context</p><p><img src="https://i.loli.net/2020/11/20/JUOTimGSQa9o73E.png"></p><p>输入 X 先经过线性变换生成Q和V，同时经过卷积生成基于跨度的K_s，由Q⊙K_s经过线性变换以及softmax来产生卷积核与V进一步做轻量卷积，从而得到终的输出。</p><p>具体如下：</p><p><img src="https://i.loli.net/2020/11/20/FcnEjJLkDgT27Z9.png"></p><p>三者的不不同如下图</p><p><img src="https://i.loli.net/2020/11/20/Ev7wzpkdQsy3KRj.png"></p><h4 id="convbert模型的总体架构">ConvBERT模型的总体架构</h4><p>粉色背景的是span-based dynamic convolution。卷积和attention共享query但是有不同k，去分别生成attention map 和 convolution kernel （都是在softmax之后得到的），以此来关注局部和全局的依赖。</p><p><img src="https://i.loli.net/2020/11/20/vGYPtVUA1uq67Ek.png"></p><p>总体表示如下：</p><p><img src="https://i.loli.net/2020/11/20/aq3tfTYKN2xd9wg.png"></p><h3 id="实验">实验</h3><p>本文做了挺多的实验，并且达到了SOTA水平</p><p>预训练数据集：OpenWebText</p><p>测试数据集： GLUE &amp; SQuAD</p><h4 id="消融实验">消融实验</h4><ol type="1"><li>本文对于attention的三个创新点进行消融，具体实验结果如下：</li></ol><p><img src="https://i.loli.net/2020/11/20/BNCuJeG3OcKygZq.png"></p><ol start="2" type="1"><li>除此之外，本文还研究了核的大小对GLUE score的影响，当核大小=9时效果最好，本文的模型也采用了这种设计</li></ol><p><img src="https://i.loli.net/2020/11/20/iCw8nOuU2pjFHb9.png"></p><ol start="3" type="1"><li><p>本文将提出的Span-based Dynamic 与 经典的卷积进行对比实验，结果如下：</p><p><img src="https://i.loli.net/2020/11/20/OetQPKzr1bGpxuD.png"></p></li></ol><h4 id="综合实验">综合实验</h4><p>将ConvBERT和BERT还有ELECTRA在相似大小的情况下进行比较</p><p>在GLUE测试集上比较</p><p><img src="https://i.loli.net/2020/11/20/oQUFRbn97z4dLZa.png"></p><p>在SQuAD测试集上比较</p><p><img src="https://i.loli.net/2020/11/20/MtqUbFjgaROc5lf.png"></p><h3 id="总结">总结</h3><p>之前看过韩松实验室的Lite transformer模型，主要是对transformer模型的压缩。Lite transformer也是观察到transformer模型中的attention存在着冗余，于是也是提出了CNN结构结合self-attenion来一起学习全局和局部依赖。只是与本文不同的是，Lite transformer是通过将输入进行分流，一部分流入CNN块，来建模输入序列中的局部关系依赖，一部分流入self-attention块来迫使self-attention建模全局关系，以此减少了self-attention中的O(n2)复杂度，从而达到压缩的目的。</p><p>而本文是span-based dynamic convolution和self-attention的融合来改进attention机制，感觉是从根本上解决attention的固有缺点，并且效果也达到了SOTA水平，本文更优质一些</p><p>不过现在paper with code中除了官方给出的code，没有其他人贡献code，随着这篇论文被更多科研人员知道并使用改进，也会贡献更多的code吧，更希望hugging face 能够将本模型融入到github中，为NLP做出更多贡献</p><h3 id="参考">参考</h3><blockquote><p><a href="https://zhuanlan.zhihu.com/p/60482693" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/60482693</a></p><p><a href="https://www.jiqizhixin.com/articles/2020-11-12-5" target="_blank" rel="noopener" class="uri">https://www.jiqizhixin.com/articles/2020-11-12-5</a></p></blockquote><hr><p>自面世以来，Transformer 模型已经在多个领域取得了 SOTA 结果，包括自然语言处理、图像处理甚至是音乐处理。众所周知，Transformer 架构的核心是注意力模块，它计算输入序列中所有位置对的相似度得分。然而，随着输入序列长度的增加，注意力机制本身的问题也越来越突出，因为它需要二次方的计算时间来产生所有的相似度得分，用来存储这些得分的内存大小也是如此。</p><p>针对那些需要长距离注意力的应用，部分研究者已经提出了一些速度快、空间利用率高的方法，其中比较普遍的方法是稀疏注意力。</p><p><img src="E:\myBlog\source_posts\v2-6bc976426f0f0c68f43f26abe7500836_720w.jpg" alt="img">标准的稀疏化技术。</p><p>然而，稀疏注意力方法也有一些局限。首先，它们需要高效的稀疏矩阵乘法运算，但这并不是所有加速器都能做到的；其次，它们通常不能为自己的表示能力提供严格的理论保证；再者，它们主要针对 Transformer 模型和生成预训练进行优化；最后，它们通常会堆更多的注意力层来补偿稀疏表示，这使其很难与其他预训练好的模型一起使用，需要重新训练，消耗大量能源。</p><p>此外，稀疏注意力机制通常不足以解决常规注意力方法应用时所面临的所有问题，如指针网络。还有一些运算是无法稀疏化的，比如常用的 softmax 运算。</p><p>为了解决这些问题，来自谷歌、剑桥大学、DeepMind、阿兰 · 图灵研究所的研究者提出了一种新的 Transformer 架构——Performer。它的<strong>注意力机制能够线性扩展，因此能够在处理长序列的同时缩短训练时间</strong>。这点在 ImageNet64 等图像数据集和 PG-19 文本数据集等序列的处理过程中都非常有用。</p><p><img src="E:\myBlog\source_posts\v2-733efa2ccadc511ca629f3e72c7d84aa_720w.jpg" alt="img"></p><p>论文链接：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2009.14794.pdf">https://arxiv.org/pdf/2009.14794.pdf</a></p><p>Performer 使用一个高效的（线性）广义注意力框架（generalized attention framework），允许基于不同相似性度量（核）的一类广泛的注意力机制。该框架通过谷歌的新算法 FAVOR+（ Fast Attention Via Positive Orthogonal Random Features）来实现，后者能够提供注意力机制的可扩展低方差、无偏估计，这可以通过随机特征图分解（常规 softmax-attention）来表达。该方法在保持线性空间和时间复杂度的同时准确率也很有保证，也可以应用到独立的 softmax 运算。此外，该方法还可以和可逆层等其他技术进行互操作。</p><p>研究者表示，他们相信该研究为注意力、Transformer 架构和核方法提供了一种新的思维方式。</p><p>代码地址：<a href="https://link.zhihu.com/?target=https%3A//github.com/google-research/google-research/tree/master/performer">https://github.com/google-research/google-research/tree/master/performer</a></p><p>论文公布之后，Youtube 知名深度学习频道 Yannic Kilcher 对该文章进行了解读。</p><p><strong>广义的注意力机制</strong></p><p>在以往的注意力机制中，分别对应矩阵行与列的 query 和 key 输入相乘，通过 softmax 计算形成一个注意力矩阵，以存储相似度系数。值得注意的是，这种方法不能将 query-key 生成结果传递给非线性 softmax 计算之后再将其分解为原始的 query 和 key。然而，将注意力矩阵分解为原始 query 和 key 的随机非线性函数的乘积是可以的，即所谓的随机特征（random feature），这样就可以更加高效地对相似度信息进行编码。</p><p><img src="E:\myBlog\source_posts\v2-36bfb8693fd72033970bd83762575b3e_720w.jpg" alt="img"></p><p><em>标准注意力矩阵包括每一对 entry 的相似度系数，由 query 和 key 上的 softmax 计算组成，表示为 q 和 k。</em></p><p>常规的 softmax 注意力可以看作是由指数函数和高斯投影定义的非线性函数的一个特例。在这里我们也可以反向推理，首先实现一些更广义的非线性函数，隐式定义 query-key 结果中其他类型的相似性度量或核函数。研究者基于早期的核方法（kernel method），将其定义为广义注意力（generalized attention）。尽管对于大多核函数来说，闭式解并不存在，但这一机制仍然可以应用，因为它并不依赖于闭式解。</p><p>该研究首次证明了，任意注意力矩阵都可以通过随机特征在下游 Transformer 应用中实现有效地近似。实现这一点的的新机制是使用正随机特征，即原始 query 和 key 的正直非线性函数，这对于避免训练过程中的不稳定性至关重要，并实现了对常规 softmax 注意力的更准确近似。</p><p><strong>新算法 FAVOR+：通过矩阵相关性实现快速注意力</strong></p><p>上文描述的分解允许我们以线性而非二次内存复杂度的方式存储隐式注意力矩阵。我们还可以通过分解获得一个线性时间注意力机制。虽然在分解注意力矩阵之后，原始注意力机制与具有值输入的存储注意力矩阵相乘以获得最终结果，我们可以重新排列矩阵乘法以近似常规注意力机制的结果，并且不需要显式地构建二次方大小的注意力矩阵。最终生成了新算法 FAVOR+。</p><p><img src="E:\myBlog\source_posts\v2-131935e1ce0b0a5fb4fd1a0c6e2bc6f6_720w.jpg" alt="img"></p><p><em>左：标准注意力模块计算，其中通过执行带有矩阵 A 和值张量 V 的矩阵乘法来计算最终的预期结果；右：通过解耦低秩分解 A 中使用的矩阵 Q′和 K′以及按照虚线框中指示的顺序执行矩阵乘法，研究者获得了一个线性注意力矩阵，同时不用显式地构建 A 或其近似。</em></p><p>上述分析与双向注意力（即非因果注意力）相关，其中没有 past 和 future 的概念。对于输入序列中没有注意前后 token 的单向（即因果）注意力而言，研究者稍微修改方法以使用前缀和计算（prefix-sum computation），它们只存储矩阵计算的运行总数，而不存储显式的下三角常规注意力矩阵。</p><p><embed src="E:\myBlog\source_posts\v2-7698eb01869e11a5042e8f1742497f44_b.webp"></p><p><em>左：标准单向注意力需要 mask 注意力矩阵以获得其下三角部分；右：LHS 上的无偏近似可以通过前缀和获得，其中用于 key 和值向量的随机特征图的外积（outer-product）前缀和实现动态构建，并通过 query 随机特征向量进行左乘计算，以在最终矩阵中获得新行（new row）。</em></p><p><strong>性能</strong></p><p>研究者首先对 Performer 的空间和时间复杂度进行基准测试，结果表明，注意力的加速比和内存减少在实证的角度上近乎最优，也就是说，这非常接近在模型中根本不使用注意力机制的情况。</p><p><img src="E:\myBlog\source_posts\v2-e01a97ee8d354814ddf45830642aa026_720w.jpg" alt="img"></p><p><em>在以时间（T）和长度（L）为度量的双对数坐标轴中，常规 Transformer 模型的双向 timing。</em></p><p>研究者进一步证明，使用无偏 softmax 近似，该 Performer 模型在稍微进行微调之后可以向后兼容预训练 Transformer 模型，从而在提升推理速度的同时降低能耗，并且不需要从头训练预先存在的模型。</p><p><img src="E:\myBlog\source_posts\v2-d2a50f3dd7c0edebc63b98ed17329fde_720w.jpg" alt="img"></p><p><em>在 One Billion Word Benchmark (LM1B) 数据集上，研究者将原始预训练 Transformer 的权重迁移至 Performer 模型，使得初始非零准确度为 0.07（橙色虚线）。但在微调之后，Performer 的准确度在很少的梯度步数之后迅速恢复。</em></p><p><strong>应用示例：蛋白质建模</strong></p><p>蛋白质具有复杂的 3D 结构，是生命必不可少的拥有特定功能的大分子。和单词一样，蛋白质可以被看做线性序列，每个字符代表一种氨基酸。将 Transformers 应用于大型未标记的蛋白质序列语料库，生成的模型可用于精确预测折叠功能大分子。正如该研究理论结果所预测的那样，Performer-ReLU 在蛋白质序列数据建模方面表现良好，而 Performer-Softmax 与 Transformer 性能相媲美。</p><p><img src="E:\myBlog\source_posts\v2-4503e75224c3c3d47e6af8c4452b07e2_720w.jpg" alt="img"></p><p><em>Performer 在蛋白质序列建模时的性能。</em></p><p>下面可视化一个蛋白质 Performer 模型，该模型使用基于 ReLU 的近似注意力机制进行训练。研究者发现，Performer 的密集注意力近似有可能捕捉到跨多个蛋白质序列的全局相互作用。作为概念的证明，研究者在串联蛋白长序列上训练模型，这使得常规的 Transformer 模型内存过载。但由于具有良好的空间利用效率，Performer 不会出现这一问题。</p><p><img src="E:\myBlog\source_posts\v2-88300c475bac1e929f60c17cfb66b99b_720w.jpg" alt="img"></p><p><em>左：从注意力权重估计氨基酸相似性矩阵。该模型可以识别高度相似的氨基酸对，例如 (D,E) 和 (F,Y)。</em></p><p><img src="E:\myBlog\source_posts\v2-8c549a83a85d004d34937b39755de8db_720w.jpg" alt="img"></p><p><em>Performer 和 Transformer 在长度为 8192 的蛋白质序列上的性能。</em></p><p>随着 Transformer 的频繁跨界，越来越多的研究者开始关注其内存占用和计算效率的问题，比如机器之心前段时间介绍的《<a href="https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3MzI4MjgzMw%3D%3D%26mid%3D2650800087%26idx%3D2%26sn%3Dc0c347ae188499fffa0db62c75a0ba2a%26scene%3D21%23wechat_redirect">抛弃注意力，比 EfficientNet 快 3.5 倍，类 Transformer 新模型跨界视觉任务实现新 SOTA</a>》。在那篇文章中，研究者提出了一种名为「lambda」的层，这些层提供了一种捕获输入和一组结构化上下文元素之间长程交互的通用框架。类似的改进还在不断涌现，我们也将持续关注。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      记录值得分享的论文;
《ConvBERT》: 对BERT中attention结构的改进

    
    </summary>
    
    
      <category term="论文分享" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-11-19-论文分享</title>
    <link href="http://yoursite.com/2020/11/19/2020-11-19-%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"/>
    <id>http://yoursite.com/2020/11/19/2020-11-19-%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/</id>
    <published>2020-11-19T08:43:49.000Z</published>
    <updated>2020-12-07T02:33:52.288Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2020/11/19/9cQSCTG7V4IPsej.png"></p><blockquote><p>ICLR 2021 (under review)</p><p>code url (official torch) : <a href="https://github.com/lucidrains/lambda-networks" target="_blank" rel="noopener" class="uri">https://github.com/lucidrains/lambda-networks</a></p><p>code url (unofficial torch):<a href="https://github.com/leaderj1001/LambdaNetworks" target="_blank" rel="noopener" class="uri">https://github.com/leaderj1001/LambdaNetworks</a></p></blockquote><h3 id="背景">背景</h3><p>transformer的注意力机制在建模长依赖关系时有优势，但是同时也存在一定的缺点，就是需要去建立expensive attention maps (Q*K ) , 复杂度是O(n2)，而且很消耗内存，这样就限制了transformer来处理更长的序列或者是多维的输入，比如图像。</p><h3 id="问题">问题</h3><p>怎样去解决attention带来的资源的消耗以及无法处理多维度的结构化的上下文信息（图像）</p><h3 id="解决">解决</h3><p>提出了 lambda layer，通过将上下文转化为线性函数（lambda）来捕获关联性，输入不同这些线性函数也是不同的，以此来避免了attention maps，这样可以应用到长序列或者高分辨率的图象上。</p><p>由多个lambda layer组成的神经网络叫做Lambda Networks ，并且认为<strong>lambda layer可以有效替代attention机制</strong></p><p>优点： 计算简单且高效（small memory cost），且在流行数据集上表现不错</p><p><img src="https://i.loli.net/2020/11/19/PZfBu3tOpUmvQn7.png"></p><h4 id="lambda-layer-的大体介绍">lambda layer 的大体介绍</h4><p><img src="https://i.loli.net/2020/11/19/CNXsc9DRwtBaJj5.png"></p><h3 id="模型">模型</h3><h4 id="模型架构">模型架构</h4><p><img src="https://i.loli.net/2020/11/19/NDaA9MWiZ2rjsBJ.png"></p><p>图中每个pixel（query）都可以类比于NLP中的一个token，要attention to local context （pixel 所在的框)，如中间图所示，如果是attention机制，那么attent to每一个pixel，就会形成很大的attention map，而这只是一个pixel所形成的attention map ，当框在图像上滑动，计算每一个pixel的时候，内存消耗是巨大的。</p><p>图右采取的是 lambda函数的思想，也就是对于每一个pixel（query），都会计算一个线性的lambda函数，同时也是个矩阵，再和query相乘，就得到了此query对应的输出向量y。由于是线性的，所以消耗资源很少，可以处理高分辨率的图像</p><p>lambda layer 可以捕获全局的或者局部的关系</p><p><img src="https://i.loli.net/2020/11/19/EAaSbCwmdPeYO3y.png"></p><p>lambda network中对于各种参数的定义，基本和transformer一致</p><p><img src="https://i.loli.net/2020/11/19/jn9coamK7Ns46Qy.png"></p><blockquote><p>注：</p><p>C和X一般是一样的东西</p><p>n : local context pixels （ n = 225*225 ）</p></blockquote><p><img src="https://i.loli.net/2020/11/19/pD6N5er9HnLdOI3.png"></p><blockquote><p>lambda layer 主要数据流向：</p><p>input: X,C -&gt; 生成线性函数，应用到对应的query中</p><p>output: Y</p></blockquote><p>具体流程如下：</p><p>传统的self-attention</p><p><img src="https://i.loli.net/2020/11/19/NzglXkjLWhxB745.png"></p><p>lambda network attention</p><p><img src="https://i.loli.net/2020/11/19/s3GpwT5IAKYohzc.png"></p><p>不太懂key和自己做attention有什么意义呢？</p><p>lambda layer 中涉及的公式</p><p><img src="https://i.loli.net/2020/11/19/sCkquJxQLDgX9wd.png"></p><p><img src="https://i.loli.net/2020/11/19/t8HSnIKipmzWuLs.png" alt="image-20201119175615608"></p><h3 id="实验">实验</h3><p><img src="https://i.loli.net/2020/11/19/LwR5yKYzBUrHqAW.png"></p><h3 id="代码">代码</h3><p>代码实际上实现的很简单，在官方给出的代码中只是写了lambda layer的实现，于是我又在paper with code上了另一个torch实现的模型代码（数据集用的是CIFAR10），实现的是Lambda ResNets。就是在ResNets的框架下，在每一层使用了lambda layer进行计算</p><p>代码中应用的<code>einsum函数</code>很简洁方便并且还高效，可以表示点积、外积、转置、矩阵-向量乘法、矩阵-矩阵乘法等一些常见的矩阵运算，推荐！</p><h3 id="总结">总结</h3><p>感觉论文写的很不容易阅读，也算看过不少transformer的论文了，但是没有见过这篇论文这样的拗口的表达方式。明明很简单的概念，按照transformer类似表达就可以了，非要整出来一套奇怪的符号。作者是一个匿名作者，但是我估计可能是别的专业转行过来深度学习的</p><p>总体来说，这篇论文的写作和表达方式上对于后来的研究者来说很不友好！不过既然模型效果不错，那么在用attention处理图像问题的时候，可以考虑借鉴本文的思路</p><h3 id="pdf">PDF</h3><div class="pdfobject-container" data-target="\source\_posts\papers\LAMBDANETWORKS.pdf" data-height="500px"></div><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      记录值得分享的论文;
《Lambda network》: 提出了 lambda layer，通过将上下文转化为线性函数（lambda）来捕获关联性，以此避免attention maps，这样可以应用到长序列或者高分辨率的图象上。

    
    </summary>
    
    
      <category term="论文分享" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-11-16-使用预训练模型</title>
    <link href="http://yoursite.com/2020/11/16/2020-11-16-%E4%BD%BF%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"/>
    <id>http://yoursite.com/2020/11/16/2020-11-16-%E4%BD%BF%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/</id>
    <published>2020-11-16T11:02:19.000Z</published>
    <updated>2020-11-19T08:42:42.803Z</updated>
    
    <content type="html"><![CDATA[<h4 id="预训练模型">预训练模型</h4><blockquote><p><strong>Hugging face 简介</strong></p></blockquote><p>Hugging face 是一家总部位于纽约的聊天机器人初创服务商，开发的应用在青少年中颇受欢迎，相比于其他公司，Hugging Face更加注重产品带来的情感以及环境因素。官网链接在此 <a href="https://link.zhihu.com/?target=https%3A//huggingface.co/">https://huggingface.co/</a> 。</p><p>但更令它广为人知的是Hugging Face专注于NLP技术，拥有大型的开源社区。尤其是在github上开源的自然语言处理，预训练模型库 Transformers，已被下载超过一百万次，github上超过<strong>24000</strong>个star。Transformers 提供了NLP领域大量state-of-art的 预训练语言模型结构的模型和调用框架。以下是repo的链接（<a href="https://link.zhihu.com/?target=https%3A//github.com/huggingface/transformers">https://github.com/huggingface/transformers</a>）</p><p>这个库最初的名称是<strong>pytorch-pretrained-bert</strong>，它随着BERT一起应运而生。Google2018年10月底在 <a href="https://link.zhihu.com/?target=https%3A//github.com/google-research/bert">https://github.com/google-research/bert</a> 开源了BERT的tensorflow实现。当时，BERT以其强劲的性能，引起NLPer的广泛关注。几乎与此同时，pytorch-pretrained-bert也开始了它的第一次提交。pytorch-pretrained-bert 用当时已有大量支持者的pytorch框架复现了BERT的性能，并提供预训练模型的下载，使没有足够算力的开发者们也能够在几分钟内就实现 state-of-art-fine-tuning。</p><p><img src="E:\myBlog\source_posts\v2-43d31689c936d9f721eed2a2ccd51d7a_720w.png" alt="img"></p><p>因为pytorch框架的友好，BERT的强大，以及pytorch-pretrained-bert的简单易用，使这个repo也是受到大家的喜爱，不到10天就突破了1000个star。在2018年11月17日，repo就实现了BERT的基本功能，发布了版本0.1.2。接下来他们也没闲着，又开始将GPT等模型也往repo上搬。在2019年2月11日release的 0.5.0版本中，已经添加上了OpenAI GPT模型，以及Google的TransformerXL。</p><p>直到2019年7月16日，在repo上已经有了包括BERT，GPT，GPT-2，Transformer-XL，XLNET，XLM在内六个预训练语言模型，这时候名字再叫pytorch-pretrained-bert就不合适了，于是改成了pytorch-transformers，势力范围扩大了不少。这还没完！2019年6月Tensorflow2的beta版发布，Huggingface也闻风而动。为了立于不败之地，又实现了TensorFlow 2.0和PyTorch模型之间的深层互操作性，可以在TF2.0/PyTorch框架之间随意迁移模型。在2019年9月也发布了2.0.0版本，同时正式更名为 transformers 。到目前为止，transformers 提供了超过100种语言的，32种预训练语言模型，简单，强大，高性能，是新手入门的不二选择。</p><blockquote><p><strong>Transfromers中BERT简单运用</strong></p></blockquote><p>前几期里，一直在分享论文的阅读心得，虽然不是第一次看，但不知道大家是不是和我一样又有所收获。本期我们一起来看看如何使用Transformers包实现简单的BERT模型调用。</p><p>安装过程不再赘述，比如安装2.2.0版本 pip install transformers==2.2.0 即可，让我们看看如何调用BERT。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertModel, BertTokenizer</span><br><span class="line"><span class="comment"># 这里我们调用bert-base模型，同时模型的词典经过小写处理</span></span><br><span class="line">model_name = <span class="string">'bert-base-uncased'</span></span><br><span class="line"><span class="comment"># 读取模型对应的tokenizer</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(model_name)</span><br><span class="line"><span class="comment"># 载入模型</span></span><br><span class="line">model = BertModel.from_pretrained(model_name)</span><br><span class="line"><span class="comment"># 输入文本</span></span><br><span class="line">input_text = <span class="string">"Here is some text to encode"</span></span><br><span class="line"><span class="comment"># 通过tokenizer把文本变成 token_id</span></span><br><span class="line">input_ids = tokenizer.encode(input_text, add_special_tokens=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># input_ids: [101, 2182, 2003, 2070, 3793, 2000, 4372, 16044, 102]</span></span><br><span class="line">input_ids = torch.tensor([input_ids])</span><br><span class="line"><span class="comment"># 获得BERT模型最后一个隐层结果</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    last_hidden_states = model(input_ids)[<span class="number">0</span>]  <span class="comment"># Models outputs are now tuples</span></span><br><span class="line"></span><br><span class="line"><span class="string">""" tensor([[[-0.0549,  0.1053, -0.1065,  ..., -0.3550,  0.0686,  0.6506],</span></span><br><span class="line"><span class="string">         [-0.5759, -0.3650, -0.1383,  ..., -0.6782,  0.2092, -0.1639],</span></span><br><span class="line"><span class="string">         [-0.1641, -0.5597,  0.0150,  ..., -0.1603, -0.1346,  0.6216],</span></span><br><span class="line"><span class="string">         ...,</span></span><br><span class="line"><span class="string">         [ 0.2448,  0.1254,  0.1587,  ..., -0.2749, -0.1163,  0.8809],</span></span><br><span class="line"><span class="string">         [ 0.0481,  0.4950, -0.2827,  ..., -0.6097, -0.1212,  0.2527],</span></span><br><span class="line"><span class="string">         [ 0.9046,  0.2137, -0.5897,  ...,  0.3040, -0.6172, -0.1950]]]) </span></span><br><span class="line"><span class="string">shape: (1, 9, 768)     </span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><p>可以看到，包括import在内的不到十行代码，我们就实现了读取一个预训练过的BERT模型，来encode我们指定的一个文本，对文本的每一个token生成768维的向量。如果是二分类任务，我们接下来就可以把第一个token也就是[CLS]的768维向量，接一个linear层，预测出分类的logits，或者根据标签进行训练。</p><p>如果你想在一些NLP常用数据集上复现BERT的效果，Transformers上也有现成的代码和方法，只要把数据配置好，运行命令即可，而且finetune的任务可以根据你的需要切换，非常方便。</p><p><img src="E:\myBlog\source_posts\v2-e26fa3a9005015c5cd6fa9ddedc6b2bd_720w.jpg" alt="img"></p><blockquote><p><strong>BERT configuration</strong></p></blockquote><p>接下来，我们进一步看下Transformers的源码，我们首先进入代码的路径src/transformers 下，其中有很多的python代码文件。</p><p><img src="E:\myBlog\source_posts\v2-3f94e3668496b5fe2275407e5cbbd440_720w.jpg" alt="img"></p><p>以 <strong>configuration</strong> 开头的都是各个模型的配置代码，比如 configuration_bert.py。在这个文件里我们能够看到，主要是一个继承自 PretrainedConfig 的类 BertConfig的定义，以及不同BERT模型的config文件的下载路径，下方显示前三个。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">BERT_PRETRAINED_CONFIG_ARCHIVE_MAP = {</span><br><span class="line">    <span class="string">"bert-base-uncased"</span>: <span class="string">"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json"</span>,</span><br><span class="line">    <span class="string">"bert-large-uncased"</span>: <span class="string">"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-config.json"</span>,</span><br><span class="line">    <span class="string">"bert-base-cased"</span>: <span class="string">"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json"</span>,</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>我们打开第一个的链接，就能下载到bert-base-uncased的模型的配置，其中包括dropout, hidden_size, num_hidden_layers, vocab_size 等等。比如bert-base-uncased的配置它是12层的，词典大小30522等等，甚至可以在config里利用output_hidden_states配置是否输出所有hidden_state。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">{</span><br><span class="line">  <span class="string">"architectures"</span>: [</span><br><span class="line">    <span class="string">"BertForMaskedLM"</span></span><br><span class="line">  ],</span><br><span class="line">  <span class="string">"attention_probs_dropout_prob"</span>: <span class="number">0.1</span>,</span><br><span class="line">  <span class="string">"hidden_act"</span>: <span class="string">"gelu"</span>,</span><br><span class="line">  <span class="string">"hidden_dropout_prob"</span>: <span class="number">0.1</span>,</span><br><span class="line">  <span class="string">"hidden_size"</span>: <span class="number">768</span>,</span><br><span class="line">  <span class="string">"initializer_range"</span>: <span class="number">0.02</span>,</span><br><span class="line">  <span class="string">"intermediate_size"</span>: <span class="number">3072</span>,</span><br><span class="line">  <span class="string">"max_position_embeddings"</span>: <span class="number">512</span>,</span><br><span class="line">  <span class="string">"num_attention_heads"</span>: <span class="number">12</span>,</span><br><span class="line">  <span class="string">"num_hidden_layers"</span>: <span class="number">12</span>,</span><br><span class="line">  <span class="string">"type_vocab_size"</span>: <span class="number">2</span>,</span><br><span class="line">  <span class="string">"vocab_size"</span>: <span class="number">30522</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><blockquote><p><strong>BERT tokenization</strong></p></blockquote><p>以<strong>tokenization</strong>开头的都是跟vocab有关的代码，比如在 tokenization_bert.py 中有函数如whitespace_tokenize，还有不同的tokenizer的类。同时也有各个模型对应的vocab.txt。从第一个链接进去就是bert-base-uncased的词典，这里面有30522个词，对应着config里面的vocab_size。</p><p>其中，第0个token是[pad]，第101个token是[CLS]，第102个token是[SEP]，所以之前我们encode得到的 [101, 2182, 2003, 2070, 3793, 2000, 4372, 16044, 102] ，其实tokenize后convert前的token就是 ['[CLS]', 'here', 'is', 'some', 'text', 'to', 'en', '##code', '[SEP]']，经过之前BERT论文的介绍，大家应该都比较熟悉了。</p><blockquote><p>其中值得一提的是，BERT的vocab预留了不少unused token，如果我们会在文本中使用特殊字符，在vocab中没有，这时候就可以通过替换vacab中的unused token，实现对新的token的embedding进行训练。</p></blockquote><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">PRETRAINED_VOCAB_FILES_MAP = {</span><br><span class="line">    <span class="string">"vocab_file"</span>: {</span><br><span class="line">        <span class="string">"bert-base-uncased"</span>: <span class="string">"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt"</span>,</span><br><span class="line">        <span class="string">"bert-large-uncased"</span>: <span class="string">"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt"</span>,</span><br><span class="line">        <span class="string">"bert-base-cased"</span>: <span class="string">"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt"</span>,</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><blockquote><p><strong>BERT modeling</strong></p></blockquote><p>以modeling开头的就是我们最关心的模型代码，比如 modeling_bert.py。同样的，文件中有<strong>许多不同的预训练模型</strong>以供下载，我们可以按需获取。</p><p>代码中我们可以重点关注BertModel类，它就是BERT模型的基本代码。我们可以看到它的类定义中，由embedding，encoder，pooler组成，forward时顺序经过三个模块，输出output。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertModel</span><span class="params">(BertPreTrainedModel)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        super().__init__(config)</span><br><span class="line">        self.config = config</span><br><span class="line"></span><br><span class="line">        self.embeddings = BertEmbeddings(config)</span><br><span class="line">        self.encoder = BertEncoder(config)</span><br><span class="line">        self.pooler = BertPooler(config)</span><br><span class="line"></span><br><span class="line">        self.init_weights()</span><br><span class="line">        </span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        self, input_ids=None, attention_mask=None, token_type_ids=None,</span></span></span><br><span class="line"><span class="function"><span class="params">        position_ids=None, head_mask=None, inputs_embeds=None,</span></span></span><br><span class="line"><span class="function"><span class="params">        encoder_hidden_states=None, encoder_attention_mask=None,</span></span></span><br><span class="line"><span class="function"><span class="params">    )</span>:</span></span><br><span class="line">    <span class="string">""" 省略部分代码 """</span></span><br><span class="line">    </span><br><span class="line">        embedding_output = self.embeddings(</span><br><span class="line">            input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds</span><br><span class="line">        )</span><br><span class="line">        encoder_outputs = self.encoder(</span><br><span class="line">            embedding_output,</span><br><span class="line">            attention_mask=extended_attention_mask,</span><br><span class="line">            head_mask=head_mask,</span><br><span class="line">            encoder_hidden_states=encoder_hidden_states,</span><br><span class="line">            encoder_attention_mask=encoder_extended_attention_mask,</span><br><span class="line">        )</span><br><span class="line">        sequence_output = encoder_outputs[<span class="number">0</span>]</span><br><span class="line">        pooled_output = self.pooler(sequence_output)</span><br><span class="line"></span><br><span class="line">        outputs = (sequence_output, pooled_output,) + encoder_outputs[</span><br><span class="line">            <span class="number">1</span>:</span><br><span class="line">        ]  <span class="comment"># add hidden_states and attentions if they are here</span></span><br><span class="line">        <span class="keyword">return</span> outputs  <span class="comment"># sequence_output, pooled_output, (hidden_states), (attentions)</span></span><br></pre></td></tr></tbody></table></figure><p>BertEmbeddings这个类中可以清楚的看到，embedding由三种embedding相加得到，经过layernorm 和 dropout后输出。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=<span class="number">0</span>)</span><br><span class="line">        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)</span><br><span class="line">        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)</span><br><span class="line">        <span class="comment"># self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load</span></span><br><span class="line">        <span class="comment"># any TensorFlow checkpoint file</span></span><br><span class="line">        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)</span><br><span class="line">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None)</span>:</span></span><br><span class="line">        <span class="string">""" 省略 embedding生成过程 """</span></span><br><span class="line">          </span><br><span class="line">        embeddings = inputs_embeds + position_embeddings + token_type_embeddings</span><br><span class="line">        embeddings = self.LayerNorm(embeddings)</span><br><span class="line">        embeddings = self.dropout(embeddings)</span><br><span class="line">        <span class="keyword">return</span> embeddings</span><br></pre></td></tr></tbody></table></figure><p>BertEncoder主要将embedding的输出，逐个经过每一层Bertlayer的处理，得到各层hidden_state，再根据config的参数，来决定最后是否所有的hidden_state都要输出，BertLayer的内容展开的话，篇幅过长，读者感兴趣可以自己一探究竟。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertEncoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.output_attentions = config.output_attentions</span><br><span class="line">        self.output_hidden_states = config.output_hidden_states</span><br><span class="line">        self.layer = nn.ModuleList([BertLayer(config) <span class="keyword">for</span> _ <span class="keyword">in</span> range(config.num_hidden_layers)])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        self,</span></span></span><br><span class="line"><span class="function"><span class="params">        hidden_states,</span></span></span><br><span class="line"><span class="function"><span class="params">        attention_mask=None,</span></span></span><br><span class="line"><span class="function"><span class="params">        head_mask=None,</span></span></span><br><span class="line"><span class="function"><span class="params">        encoder_hidden_states=None,</span></span></span><br><span class="line"><span class="function"><span class="params">        encoder_attention_mask=None,</span></span></span><br><span class="line"><span class="function"><span class="params">    )</span>:</span></span><br><span class="line">        all_hidden_states = ()</span><br><span class="line">        all_attentions = ()</span><br><span class="line">        <span class="keyword">for</span> i, layer_module <span class="keyword">in</span> enumerate(self.layer):</span><br><span class="line">            <span class="keyword">if</span> self.output_hidden_states:</span><br><span class="line">                all_hidden_states = all_hidden_states + (hidden_states,)</span><br><span class="line"></span><br><span class="line">            layer_outputs = layer_module(</span><br><span class="line">                hidden_states, attention_mask, head_mask[i], encoder_hidden_states, encoder_attention_mask</span><br><span class="line">            )</span><br><span class="line">            hidden_states = layer_outputs[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> self.output_attentions:</span><br><span class="line">                all_attentions = all_attentions + (layer_outputs[<span class="number">1</span>],)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Add last layer</span></span><br><span class="line">        <span class="keyword">if</span> self.output_hidden_states:</span><br><span class="line">            all_hidden_states = all_hidden_states + (hidden_states,)</span><br><span class="line"></span><br><span class="line">        outputs = (hidden_states,)</span><br><span class="line">        <span class="keyword">if</span> self.output_hidden_states:</span><br><span class="line">            outputs = outputs + (all_hidden_states,)</span><br><span class="line">        <span class="keyword">if</span> self.output_attentions:</span><br><span class="line">            outputs = outputs + (all_attentions,)</span><br><span class="line">        <span class="keyword">return</span> outputs  <span class="comment"># last-layer hidden state, (all hidden states), (all attentions)</span></span><br></pre></td></tr></tbody></table></figure><p>Bertpooler 其实就是将BERT的[CLS]的hidden_state 取出，经过一层DNN和Tanh计算后输出。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertPooler</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.dense = nn.Linear(config.hidden_size, config.hidden_size)</span><br><span class="line">        self.activation = nn.Tanh()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, hidden_states)</span>:</span></span><br><span class="line">        <span class="comment"># We "pool" the model by simply taking the hidden state corresponding</span></span><br><span class="line">        <span class="comment"># to the first token.</span></span><br><span class="line">        first_token_tensor = hidden_states[:, <span class="number">0</span>]</span><br><span class="line">        pooled_output = self.dense(first_token_tensor)</span><br><span class="line">        pooled_output = self.activation(pooled_output)</span><br><span class="line">        <span class="keyword">return</span> pooled_output</span><br></pre></td></tr></tbody></table></figure><p>在这个文件中还有上述基础的BertModel的进一步的变化，比如BertForMaskedLM，BertForNextSentencePrediction这些是Bert加了预训练头的模型，还有BertForSequenceClassification， BertForQuestionAnswering 这些加上了特定任务头的模型。</p><ol type="1"><li>"Here is some text to encode"加上CLS和SEP也才八个,因为Bert的tokenizer使用了wordpiece 算法，</li></ol><p>这句话在tokenize了以后是下面这样的['here', 'is', 'some', 'text', 'to', 'en', '##code'],加上[CLS]和[SEP]就变成了9个token。</p><ol start="2" type="1"><li><p>with torch.no_grad(): last_hidden_states = model(input_ids)[0]</p><p>这里的[0]， 原因是模型输出是 seq_output, pooled_output，这里取的是seq_output。</p><p>sequence_output = encoder_outputs[0]</p><p>这边的[0]是因为 取得是模型最后一层的state（因为在元组里，最后一层的state放在最上面了）</p><p>first_token_tensor = hidden_states[:, 0]</p><p>pooler里面这边的0是因为 想要取的是 整个序列第一个token也就是[CLS] token的state</p></li></ol><h4 id="基本原理">基本原理</h4><p>使用的基本原理也非常简单，from_pretrained的参数pretrained_model_name_or_path，可以接受的参数有几种，short-cut name（缩写名称，类似于gpt2这种）、identifier name（类似于microsoft/DialoGPT-small这种）、文件夹、文件。</p><p><strong>对于short-cut name或identifier name，这种情况下，本地有文件，可以使用本地的，本地没有文件，则下载。</strong>(上面的例子就是没有下载文件，直接用的是short-cut name)。一些常用的short-cut name，可以从这个链接查看：<a href="https://link.zhihu.com/?target=https%3A//huggingface.co/transformers/pretrained_models.html">https://huggingface.co/transformers/pretrained_models.html</a>。</p><p>对于文件夹，则会从文件夹中找vocab.json、pytorch_model.bin、tf_model.h5、merges.txt、special_tokens_map.json、added_tokens.json、tokenizer_config.json、sentencepiece.bpe.model等进行加载。所以这也是为什么下载的时候，一定要保证这些名称是这几个，不能变。</p><p>对于文件，则会直接加载文件。</p><p>官方给的样例，通常都是short-cut name，这里操作就是替换成下载好文件的文件夹。至此，我们完成了模型、词典等各种文件的本地加载。</p><p>具体操作如下：</p><blockquote><p>模型库</p></blockquote><p>官网的模型库的地址如下：<a href="https://link.zhihu.com/?target=https%3A//huggingface.co/models">https://huggingface.co/models</a></p><p><img src="E:\myBlog\source_posts\v2-b06d68ff284848b750e38eb5a450e661_720w.jpg" alt="img"></p><blockquote><p>使用模型</p></blockquote><p>首先需要安装<code>transformers</code>库，使用以下命令安装：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install transformers</span><br></pre></td></tr></tbody></table></figure><p>接下来在代码中调用<code>AutoTokenizer.from_pretrained</code>和<code>AutoModel.from_pretrained</code>即可例如：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> *</span><br><span class="line">model_name = <span class="string">'hfl/chinese-xlnet-base'</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_name)</span><br><span class="line">model = AutoModel.from_pretrained(model_name)</span><br></pre></td></tr></tbody></table></figure><p>运行后系统会自动下载相关的模型文件并存放在电脑中：</p><p><img src="E:\myBlog\source_posts\v2-550ec0db9dbec7dd23afd33c925c019f_720w.jpg" alt="img"></p><p>使用Windows模型保存的路径在<code>C:\Users\[用户名]\.cache\torch\transformers\</code>目录下，根据模型的不同下载的东西也不相同</p><p>使用Linux模型保存的路径在<code>~/.cache/torch/transformers/</code>目录下</p><p><img src="E:\myBlog\source_posts\v2-a9751b1feebbfabc5615ce594dad816f_720w.jpg" alt="img"></p><blockquote><p>存在的问题</p></blockquote><blockquote><p><code>这些前提是你的电脑有网络可以直接使用代码下载相应的模型文件，但是问题是有些机器是没有外网连接权限或者下载速度非常慢。</code></p><p><code>这时候就需要把模型文件下载后在导入代码中，还是以刚才的hfl/chinese-xlnet-base模型为例，直接在官网搜索模型，点击进入模型的详情界面</code></p></blockquote><p><img src="E:\myBlog\source_posts\v2-6c04926b05ec59131e7bc3e018d1255a_720w.jpg" alt="img"></p><p>在界面中找到<code>List all files in model</code></p><p><img src="E:\myBlog\source_posts\v2-46b282a4b5ff9a9fe445e95d403f2c07_720w.jpg" alt="img"></p><p>把弹窗内的文件全部下载下来</p><p><img src="E:\myBlog\source_posts\v2-09353001fd2ab6a1ffbb210f675381dc_720w.jpg" alt="img"></p><p>我们假设文件保存在<code>E:\models\hfl\chinese-xlnet-base\</code>目录下</p><p><img src="E:\myBlog\source_posts\v2-b6a23972b45d1b9cc88701a55bcc159a_720w.jpg" alt="img"></p><p>我们只需要把<code>model_name</code>修改为下载的文件夹即可</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> *</span><br><span class="line">model_name = <span class="string">'E:/models/hfl/chinese-xlnet-base/'</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_name)</span><br><span class="line">model = AutoModel.from_pretrained(model_name)</span><br></pre></td></tr></tbody></table></figure><h3 id="参考">参考</h3><blockquote><p>使用transformers预训练模型</p><p><span class="github-emoji" style="display:inline;vertical-align:middle;color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8">⭐</span> <a href="https://zhuanlan.zhihu.com/p/120315111" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/120315111</a></p><p><span class="github-emoji" style="display:inline;vertical-align:middle;color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8">⭐</span><a href="https://zhuanlan.zhihu.com/p/147144376" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/147144376</a></p><p><a href="https://zhuanlan.zhihu.com/p/274509234" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/274509234</a></p><p>关于hugging face/transformers的介绍</p><p><a href="https://zhuanlan.zhihu.com/p/141527015" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/141527015</a></p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      在NLP领域越来越依赖预训练模型，那么如何高效使用已经预训练好的模型到自己的模型中呢？本文参考其它博客，做个总结
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-11-12-从attention到BERT-family</title>
    <link href="http://yoursite.com/2020/11/12/2020-11-12-%E4%BB%8Eattention%E5%88%B0BERT-family/"/>
    <id>http://yoursite.com/2020/11/12/2020-11-12-%E4%BB%8Eattention%E5%88%B0BERT-family/</id>
    <published>2020-11-12T04:54:33.000Z</published>
    <updated>2020-11-14T12:29:50.995Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言">前言</h3><p>做NLP领域的应该没人不知道BERT，之前开组会的时候讲到过BERT的一篇改进论文，导师建议对transformer、BERT以及预训练模型发展做个总结综述，作为下次汇报的内容。于是这几天在网上参考了一些博客，还有大佬邱锡鹏老师和刘群老师关于预训练模型的报告，以及结合自己读的一些相关论文，对整个模型的发展演变梳理了一下，汇总如下</p><h3 id="attention以及self-attention">attention以及self-attention</h3><h4 id="提出">提出</h4><p>借鉴了人类的注意力机制</p><p><img src="E:\myBlog\source_posts\516P2JBYrgOToyU.jpg" alt="人类的视觉注意力"></p><p>人类视觉通过快速扫描全局图像，获得注意力焦点，而后对这一区域投入更多注意力资源，以获取更多所需要关注目标的细节信息，而抑制其他无用信息。这是人类利用有限的注意力资源从大量信息中快速筛选出高价值信息的手段</p><p>深度学习中的注意力机制从本质上讲和人类的选择性视觉注意力机制类似，核心目标也是从众多信息中选择出对当前任务目标更关键的信息。</p><h4 id="encoder-decoder框架">Encoder-Decoder框架</h4><p>一般的seq2seq模型</p><p><img src="E:\myBlog\source_posts\ohBVdz1KDn4ZcOb.png" alt="抽象的文本处理领域的Encoder-Decoder框架"></p><p>对于句子对&lt;Source,Target&gt;，我们的目标是给定输入句子Source，期待通过Encoder-Decoder框架来生成目标句子Target。Source和Target分别由各自的单词序列构成：</p><p><img src="E:\myBlog\source_posts\qrCwtas6iRVKYbA.png" alt="img"></p><p>Encoder对输入句子Source进行编码，将输入句子通过非线性变换转化为中间语义表示C：</p><p><img src="E:\myBlog\source_posts\M7EXx2KPgeHFQhL.png" alt="img"></p><p>对于解码器Decoder来说，其任务是根据句子Source的中间语义表示C和之前已经生成的历史信息 y1,y2,y3...yi-1来生成i时刻要生成的单词yi：</p><p><img src="E:\myBlog\source_posts\6uHkShXDNKOlBQ3.png" alt="img"></p><p>每个yi都依次这么产生，那么看起来就是整个系统根据输入句子Source生成了目标句子Target。机器翻译、文本摘要、问答系统都是encoder-decoder框架。</p><h4 id="问题">问题</h4><p><img src="E:\myBlog\source_posts\KNWS6Ax1uI285cH.png" alt="img"></p><p>在生成目标句子的单词时，不论生成哪个单词，它们使用的输入句子Source的语义编码C都是一样的，没有任何区别。</p><p>而语义编码C是由句子Source的每个单词经过Encoder编码产生的，这意味着不论是生成哪个单词，y1,y2还是y3，其实句子Source中任意单词对生成某个目标单词yi来说影响力都是相同的，这是为何说这个模型没有体现出注意力的缘由。这类似于人类看到眼前的画面，但是眼中却没有注意焦点</p><p>没有引入注意力的模型在输入句子比较短的时候问题不大，但是如果输入句子比较长，此时所有语义完全通过一个中间语义向量来表示，单词自身的信息已经消失，可想而知会丢失很多细节信息，这也是为何要引入注意力模型的重要原因。</p><h4 id="attention">attention</h4><p>目标句子中的每个单词都应该学会其对应的源语句子中单词的注意力分配概率信息。这意味着在生成每个单词yi的时候，原先都是相同的中间语义表示C会被替换成根据当前生成单词而不断变化的Ci（C是动态的，根据生成次词的不同，C也是不同的）。</p><p><strong>即由固定的中间语义表示C换成了根据当前输出单词来调整成加入注意力模型的变化的Ci</strong>。</p><p><img src="E:\myBlog\source_posts\oe6E7C9sUHrVuNZ.png" alt="引入注意力模型的Encoder-Decoder框架"></p><p>即生成目标句子单词的过程成了下面的形式：</p><p><img src="E:\myBlog\source_posts\q8ZFEu4BSI1lo9z.png" alt="img"></p><p>而每个Ci可能对应着不同的源语句子单词的注意力分配概率分布</p><p><img src="E:\myBlog\source_posts\TBg8KZhoyiOlUst.png" alt="img"></p><p>实际中，Tom、chase、jerry都是被编码成512维的向量，所以权重相加之后Ctom应该也是一个向量</p><p>其中，f2函数代表Encoder对输入英文单词的某种变换函数，比如如果Encoder是用的RNN模型的话，这个f2函数的结果往往是某个时刻输入xi后隐层节点的状态值；g代表Encoder根据单词的中间表示合成整个句子中间语义表示的变换函数，一般的做法中，g函数就是对构成元素加权求和，即下列公式：</p><p><img src="E:\myBlog\source_posts\W9SnjkNw3BVasdc.png" alt="img"></p><p>其中，Lx代表输入句子Source的长度，aij代表在Target输出第i个单词时Source输入句子中第j个单词的注意力分配系数，而hj则是Source输入句子中第j个单词的语义编码。假设下标i就是上面例子所说的“ 汤姆” ，那么Lx就是3，h1=f(“Tom”)，h2=f(“Chase”),h3=f(“Jerry”)分别是输入句子每个单词的语义编码，对应的注意力模型权值则分别是0.6,0.2,0.2，所以g函数本质上就是个加权求和函数。如果形象表示的话，翻译中文单词“汤姆”的时候，数学公式对应的中间语义表示Ci的形成过程类似图4。</p><p><img src="E:\myBlog\source_posts\LRJZMaUbN9VuxGr.png" alt="Attention的形成过程"></p><p>如何得到单词注意力分配概率Ｃ呢？</p><p>为了便于说明，我们假设对图2的非Attention模型的Encoder-Decoder框架进行细化，Encoder采用RNN模型，Decoder也采用RNN模型，这是比较常见的一种模型配置，则图2的框架转换为图5。</p><p><img src="E:\myBlog\source_posts\ebTh6NzIdlVG1L2.png" alt="RNN作为具体模型的Encoder-Decoder框架"></p><p>那么用下图可以较为便捷地说明注意力分配概率分布值的通用计算过程。</p><p><img src="E:\myBlog\source_posts\TEXoxdzgrHOuiMQ.jpg" alt="注意力分配概率计算"></p><p><span class="github-emoji" style="display:inline;vertical-align:middle;color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8">⭐</span> 利用的是i-1时刻的隐状态（作为query）去和h1，h2，h3求相似性</p><p>对于采用RNN的Decoder来说，在时刻i，如果要生成yi单词，我们是可以知道Target在生成Yi之前的时刻i-1时，隐层节点i-1时刻的输出值Hi-1的，而我们的目的是要计算<strong>生成Yi时</strong>输入句子中的单词对Yi来说的注意力分配概率分布，那么<strong>可以用Target输出句子i-1时刻的隐层节点状态Hi-1去一一和输入句子Source中每个单词对应的RNN隐层节点状态hj进行对比</strong>，然后函数F的输出经过Softmax进行归一化就得到了符合概率分布取值区间的注意力分配概率分布数值。</p><h4 id="attention机制的本质思想">Attention机制的本质思想</h4><p><img src="E:\myBlog\source_posts\y3BToK96rsgUZfF.png" alt="Attention机制的本质思想"></p><p>我们可以这样来看待Attention机制：将Source中的构成元素想象成是由一系列的&lt;Key,Value&gt;数据对构成，此时给定Target中的某个元素Query，通过计算Query和各个Key的相似性或者相关性，得到每个Key对应Value的权重系数，然后对Value进行加权求和，即得到了最终的Attention数值。</p><p>所以本质上Attention机制是对Source中元素的Value值进行加权求和，而Query和Key用来计算对应Value的权重系数。即可以将其本质思想改写为如下公式：</p><p><img src="E:\myBlog\source_posts\ipGlzuFcmS8n2VR.png" alt="img"></p><p>其中，Lx=||Source||代表Source的长度，公式含义即如上所述。上文所举的机器翻译的例子里，因为在计算Attention的过程中，Source中的Key和Value合二为一，指向的是同一个东西，也即输入句子中每个单词对应的语义编码，所以可能不容易看出这种能够体现本质思想的结构。</p><p>当然，从概念上理解，把Attention仍然理解为从大量信息中有选择地筛选出少量重要信息并聚焦到这些重要信息上，忽略大多不重要的信息，这种思路仍然成立。聚焦的过程体现在权重系数的计算上，权重越大越聚焦于其对应的Value值上，即权重代表了信息的重要性，而Value是其对应的信息。</p><h4 id="计算过程">计算过程</h4><p>至于Attention机制的具体计算过程，如果对目前大多数方法进行抽象的话，可以将其归纳为两个过程：第一个过程是根据Query和Key计算权重系数，第二个过程根据权重系数对Value进行加权求和。而第一个过程又可以细分为两个阶段：第一个阶段根据Query和Key计算两者的相似性或者相关性；第二个阶段对第一阶段的原始分值进行归一化处理；这样，可以将Attention的计算过程抽象为如图10展示的三个阶段。</p><p><img src="E:\myBlog\source_posts\tzO9fhXUeBblmVG.png" alt="三阶段计算Attention过程"></p><p>在第一个阶段，可以引入不同的函数和计算机制，根据Query和某个Key_i，计算两者的相似性或者相关性，最常见的方法包括：求两者的向量点积、求两者的向量Cosine相似性或者通过再引入额外的神经网络来求值，即如下方式：</p><p><img src="E:\myBlog\source_posts\9xpPOa7ohFf3u1d.png" alt="img"></p><p>第一阶段产生的分值根据具体产生的方法不同其数值取值范围也不一样，第二阶段引入类似SoftMax的计算方式对第一阶段的得分进行数值转换，一方面可以进行归一化，将原始计算分值整理成所有元素权重之和为1的概率分布；另一方面也可以通过SoftMax的内在机制更加突出重要元素的权重。即一般采用如下公式计算：</p><p><img src="E:\myBlog\source_posts\XFW5tcSGjqBnIyN.png" alt="img"></p><p>第二阶段的计算结果a_i即为value_i对应的权重系数，然后进行加权求和即可得到Attention数值：</p><p><img src="E:\myBlog\source_posts\soa1M9LIPGi3krC.png" alt="img"></p><p>通过如上三个阶段的计算，即可求出针对Query的Attention数值，目前绝大多数具体的注意力机制计算方法都符合上述的三阶段抽象计算过程。</p><h4 id="self-attention模型">Self Attention模型</h4><p>在attention机制中，query是来自外在的张量，而Self Attention的query则是自己本身</p><p>但是Self Attention在计算过程中会直接将句子中任意两个单词的联系通过一个计算步骤直接联系起来，所以远距离依赖特征之间的距离被极大缩短，有利于有效地利用这些特征。除此外，Self Attention对于增加计算的并行性也有直接帮助作用。这是为何Self Attention逐渐被广泛使用的主要原因。</p><h3 id="transformer">transformer</h3><p>完全基于self-attention的模型</p><p><img src="E:\myBlog\source_posts\image-20201113233224742.png" alt="image-20201113233224742"></p><h3 id="transformer的改进模型">transformer的改进模型</h3><h3 id="预训练模型">预训练模型</h3><h3 id="bert">BERT</h3><h3 id="bert的改进模型">BERT的改进模型</h3><h3 id="总结">总结</h3><hr><h4 id="总结-1">总结</h4><p>Transformer相比LSTM的优点</p><ol type="1"><li><p><strong>完全的并行计算</strong>，Transformer的attention和feed-forward，均可以并行计算。而LSTM则依赖上一时刻，必须串行</p></li><li><p><strong>减少长程依赖</strong>，利用self-attention将每个字之间距离缩短为1，大大缓解了长距离依赖问题</p></li><li><p><strong>提高网络深度</strong>。由于大大缓解了长程依赖梯度衰减问题，Transformer网络可以很深，基于Transformer的BERT甚至可以做到24层。而LSTM一般只有2层或者4层。网络越深，高阶特征捕获能力越好，模型performance也可以越高。</p><p>缺点：</p></li><li><p>文本长度很长时，比如篇章级别，<strong>计算量爆炸</strong>。self-attention的计算量为O(n^2), n为文本长度。</p></li><li><p>Transformer参数量较大，在大规模数据集上，效果远好于LSTM。但在<strong>小规模数据集</strong>上，如果不是利用pretrain models，效果不一定有LSTM好。</p></li></ol><h3 id="transformer家族2----编码长度优化transformer-xllongformer">🚀Transformer家族2 -- 编码长度优化（Transformer-XL、Longformer）</h3><h4 id="背景">1 背景</h4><ol type="1"><li>Transformer利用self-attention机制进行建模，使得任何两个位置token距离都为1。如果没有内存和算力的限制，Transformer理论上可以编码无限长的文本。但由于attention计算量十分大，而且计算复杂度和序列长度为O(n^2)关系，导致序列长度增加，内存和计算量消耗飞快增加。实际中由于内存和算力有限，一般只能编码一定长度，例如512。</li></ol><p>为了提升模型的长程编码能力，从而提升模型在长文本，特别是document-level语料上的效果，我们必须对Transformer编码长度进行优化。</p><h4 id="transformer-xl">2 Transformer-XL</h4><p><img src="E:\myBlog\source_posts\ctFMhR8IEdPVSZ1.png" alt="在这里插入图片描述"> 论文信息：2019年01月，谷歌 &amp; CMU，ACL 2019 论文地址 https://arxiv.org/abs/1901.02860 代码和模型地址 https://github.com/kimiyoung/transformer-xl</p><h5 id="为什么需要transformer-xl">2.1 为什么需要Transformer-XL</h5><p>为了解决长文本编码问题，原版Transformer采用了固定编码长度的方案，例如512个token。将长文本按照固定长度，切分为多个segment。每个segment内部单独编码，segment之间不产生交互信息。这种方式的问题如下</p><ol type="1"><li>模型无法建模超过固定编码长度的文本</li><li>segment之间没有交互信息，导致了文本碎片化。长语句的编码效果有待提升。</li><li>predict阶段，decoder每生成一个字，就往后挪一个，没有重复利用之前信息，导致计算量爆炸</li></ol><p>train和evaluate过程如下<img src="E:\myBlog\source_posts\8YW7R4Q5J6zbmD1.png" alt="在这里插入图片描述"></p><h5 id="实现方法">2.2 实现方法</h5><h6 id="segment-level-recurrence-with-state-reuse-片段级递归和信息复用">2.2.1 Segment-Level Recurrence with State Reuse 片段级递归和信息复用</h6><p>Transformer-XL在编码后一个segment时，将前一个segment的隐层缓存下来。后一个segment的self-attention计算，会使用到前一个segment的隐层。后一个segment的第n+1层，对前一个segment的第n层隐层进行融合。故最大编码长度理论上为O(N × L)。</p><p>在预测阶段，由于对segment隐层使用了缓存，故每预测一个词，不需要重新对之前的文本进行计算。大大提升了预测速度，最大可达到原始Transformer的1800倍。如下图所示 <img src="E:\myBlog\source_posts\KmazXviordxyZuw.png" alt="在这里插入图片描述"></p><h6 id="relative-positional-encodings-相对位置编码">2.2.2 Relative Positional Encodings 相对位置编码</h6><p>segment递归中有个比较大的问题，就是如何区分不同segment中的相同位置。如果采用原版Transformer中的绝对编码方案，两者是无法区分的。如下 <img src="E:\myBlog\source_posts\38l7XhVjZnqyuBe.png" alt="gh">不同segment中的相同位置，其position encoding会相同。这显然是有问题的。Transformer-XL将绝对位置编码改为了q和k之间的相对位置编码，代表了两个token之间的相对位置。从语义上讲，是make sense的。我们来看看具体实现方式。</p><p>绝对位置编码的attention计算如下 <img src="E:\myBlog\source_posts\IKCVE7riwaQ9Zl1.png" alt="在这里插入图片描述">分为四部分</p><ol type="1"><li>query的token encoding和 key的token encoding，之间的关联信息</li><li>query的token encoding和 key的position encoding，之间的关联信息。Uj为绝对位置j的编码向量</li><li>query的position encoding和 key的token encoding，之间的关联信息。Ui为绝对位置i的编码向量</li><li>query的position encoding和 key的position encoding，之间的关联信息</li></ol><p>而采用相对位置编码后，attention计算如下 <img src="E:\myBlog\source_posts\Id9fV3bxSnmHyu2.png" alt="在这里插入图片描述">同样包含四部分，仍然为二者token encoding和position encoding之间的关联关系。区别在于</p><ol type="1"><li>Ri-j为i和j之间相对位置编码，其中R为相对位置编码矩阵</li><li>u和v为query的位置编码，采用一个固定向量。因为采用相对位置编码后，无法对单个绝对位置进行编码了。文中称为global content bias，和global positional bias</li></ol><p>也有其他文章，采用了不同的相对位置编码方案。比如"Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.2018. Self-attention with relative position representations. arXiv preprint arXiv:1803.02155." 中只有a和b两部分，丢掉了c和d。Transformer-XL对两种方案进行了对比实验，证明前一种好。</p><h5 id="实验结果">2.3 实验结果</h5><h6 id="长文本编码效果">长文本编码效果</h6><p><img src="E:\myBlog\source_posts\IyoR924J3BQWrOu.png" alt="在这里插入图片描述"> 在WikiText-103上的实验结果。WikiText-103包含词级别的超长文本，平均每篇文章长度为3.6K token。利用它可以验证模型的长文本编码能力。实验结果表明Transformer-XL large的PPL最低，效果最好。同时作者在One Billion Word、enwik8、text8上均进行了实验，都表明Transformer-XL效果最好。</p><h6 id="有效编码长度">有效编码长度</h6><p><img src="E:\myBlog\source_posts\oKQv3akDcXWOZ7B.png" alt="在这里插入图片描述"> 模型可编码的有效长度如上，r为top-r难度样本上的表现。Transformer-XL比RNN长80%，比Transformer长450%。证明Transformer-XL可编码长度最长，长程捕获能力最强。</p><h6 id="预测速度">预测速度</h6><p><img src="E:\myBlog\source_posts\g2TyZhWtRSEA7pB.png" alt="在这里插入图片描述"> 在不同segment长度下，模型预测速度的对比。和原始Transformer相比，预测速度最大可以提升1874倍。</p><h6 id="消融分析">消融分析</h6><p>文章对片段级递归和相对位置编码两个Method进行了消融分析，如下 <img src="E:\myBlog\source_posts\4EeY78JcQojSuhD.png" alt="在这里插入图片描述"> 两个改进点均有作用，其中片段级递归作用更大。</p><h4 id="longformer">3 Longformer</h4><p><img src="E:\myBlog\source_posts\KhcS2VIyvNEAwrg.png" alt="在这里插入图片描述"> 论文信息：2020年04月，allenai 论文地址 https://arxiv.org/abs/2004.05150 代码和模型地址 https://github.com/allenai/longformer</p><h5 id="改进方法">3.1 改进方法</h5><h6 id="attention稀疏化">3.1.1 attention稀疏化</h6><p>Transformer不能捕获长距离信息，本质原因还是因为计算量过大导致的。那我们通过降低attention计算量，是不是就可以提升长距离编码能力呢。答案是肯定的，LongFormer提出了三种attention稀疏化的方法，来降低计算量。 <img src="E:\myBlog\source_posts\hDmpknyqTSIUEW7.png" alt="在这里插入图片描述"> a是原始的全连接方式的attention。后面三种为文章使用的稀疏attention。</p><ol type="1"><li>Sliding Window attention。滑窗方式的attention。假设序列长度为n，滑窗大小w，则每个位置的attention只和滑窗范围内的token进行计算，复杂度从O(n^2)变为了O(n * w)。当w &lt;&lt; n时，可以大大降低计算量。</li><li>Dilated Sliding Window attention。受到空洞卷积的启发，提出了空洞滑窗attention。看下面这张图就明白了。</li></ol><p><img src="E:\myBlog\source_posts\ZwAy1H3kB7SjIP5.png" alt="在这里插入图片描述"></p><ol type="1"><li>Global Attention + sliding window。某些关键位置采用全局attention，这些位置的attention和所有token进行计算。而其他相对不关键的位置则采用滑窗attention。那什么叫关键位置呢？作者举例，分类问题中[CLS]为关键位置，需要计算全局attention。QA中question每个位置均为关键位置，同样计算全局attention。</li></ol><h6 id="tensor-virtual-machine-tvm">3.1.2 Tensor Virtual Machine (TVM)</h6><p>作者使用了TVM构建CUDA kernel，加快了longformer的速度，并降低了显存需求。这个又是另一个模型加速方面的话题，我们就先不展开了。</p><h5 id="实验结果-1">3.2 实验结果</h5><h6 id="大小模型效果">大小模型效果</h6><p><img src="E:\myBlog\source_posts\ZThJDNXByQp2Lod.png" alt="在这里插入图片描述">作者在大小模型上均实验了LongFormer的效果。小模型为12 layers，512 hidden。大模型为30 layers，512 hidden。在text8和enwik8数据集上。小模型达到了SOTA。大模型比18层的Transformer-XL好，虽然不如Adaptive-span-Transformer和Compressive，但胜在可以pretrain-finetune</p><h6 id="消融分析-1">消融分析</h6><p><img src="E:\myBlog\source_posts\pMCoW2ZLv6gq3j4.png" alt="在这里插入图片描述">消融分析中，可以发现</p><ol type="1"><li>Dilation空洞，有一定的收益</li><li>top layer滑窗大小比bottom layer大时，效果好一些。这个也是make sense的。因为top layer捕获高维语义，关联信息距离相对较远，窗口应该尽量大一些。</li></ol><h6 id="语料长度">语料长度</h6><p><img src="E:\myBlog\source_posts\IjGfH7Mi1LK2hsC.png" alt="在这里插入图片描述"> 从上表中我们发现，语料都是特别长的长文本。LongFormer真的是document级别的Transformer。</p><h6 id="下游任务finetune效果">下游任务finetune效果</h6><p><img src="E:\myBlog\source_posts\JdZUrh6L3QFBHIn.png" alt="在这里插入图片描述"> <img src="E:\myBlog\source_posts\WlOZR3iaJSA2nIU.png" alt="在这里插入图片描述"> 第一个table为RoBERTa和LongFormer在问答、指代消解、分类任务中的对比。第二个table为这几个任务数据集平均文本长度。每项任务都是超出RoBERTa，当文本长度大于512时，performance提升特别明显。更加说明了长程捕获能力在NLP中的重要性。</p><h3 id="transformer家族3----计算效率优化adaptive-spanreformerlite-transformer">🚀Transformer家族3 -- 计算效率优化（Adaptive-Span、Reformer、Lite-Transformer）</h3><h4 id="背景-1">1 背景</h4><p>上文我们从编码长度优化的角度，分析了如何对Transformer进行优化。Transformer-XL、LongFormer等模型，通过片段递归和attention稀疏化等方法，将长文本编码能力提升到了很高的高度。基本已经克服了Transformer长文本捕获能力偏弱的问题，使得下游任务模型performance得到了较大提升，特别是文本较长（大于512）的任务上。</p><p>但Transformer计算量和内存消耗过大的问题，还亟待解决。事实上，Transformer-XL、LongFormer已经大大降低了内存和算力消耗。毕竟Transformer之所以长距离编码能力偏弱，就是因为其计算量是序列长度的平方关系，对算力需求过大，导致当前GPU/TPU不能满足需求。编码长度优化和计算量优化，二者是相辅相成的。但着眼于论文的出发点，我们还是分为两个不同的章节进行分析。毕竟总不能所有模型都放在一个章节吧（_）。</p><p>本文我们带来Adaptive-Span Transformer、Reformer、Lite-Transformer等几篇文章</p><h4 id="adaptive-span-transformer">2 Adaptive-Span Transformer</h4><p><img src="E:\myBlog\source_posts\SZIDO7KcoGHqvhF.png" alt="在这里插入图片描述"> 论文信息：2019年5月，FaceBook，ACL2019 论文地址 https://arxiv.org/pdf/1905.07799.pdf 代码和模型地址 https://github.com/facebookresearch/adaptive-span</p><h5 id="为什么需要adaptive-span">2.1 为什么需要Adaptive-Span</h5><p>之前Transformer-XL将长文本编码能力提升到了较高高度，但是否每个layer的每个head，都需要这么长的attention呢？尽管使用了多种优化手段，长距离attention毕竟还是需要较大的内存和算力。研究发现，大部分head只需要50左右的attention长度，只有少部分head需要较长的attention。这个是make sense的，大部分token只和它附近的token有关联。如下图 <img src="E:\myBlog\source_posts\nOstVdScKG68CDz.png" alt="在这里插入图片描述">我们是否可以实现attention span长度的自适应呢？让不同的layer的不同的head，自己去学习自己的attention span长度呢？Adaptive-Span Transformer给出了肯定答案。</p><h5 id="实现方案">2.2 实现方案</h5><p>文章设定每个attention head内的token计算，都使用同一个span长度。我们就可以利用attention mask来实现自适应span。对每个head都添加一个attention mask，mask为0的位置不进行attention计算。文章设计的mask函数如下 <img src="E:\myBlog\source_posts\yRVqrjfJPgBKlIk.png" alt="在这里插入图片描述"> R为超参，控制曲线平滑度。其为单调递减函数，如下图。 <img src="E:\myBlog\source_posts\hvoFmDIp1BjgV5f.png" alt="在这里插入图片描述"></p><h5 id="实验结果-2">2.3 实验结果</h5><p><img src="E:\myBlog\source_posts\EDsBxfipL4HcOwF.png" alt="在这里插入图片描述"> 和Transformer家族其他很多模型一样，Adaptive-span也在字符级别的语言模型上进行了验证，数据集为text8。如上，Transformer注意力长度固定为512，结论如下</p><ol type="1"><li>Transformer-XL长程编码能力确实很强，平均span可达3800。</li><li>注意力长度确实不需要总那么长，Adaptive-Span大模型上，平均长度只有245</li><li>Adaptive-Span在算力需求很小（只有XL的1/3）的情况下，效果可以达到SOTA。</li></ol><p><img src="E:\myBlog\source_posts\IDjAR2ucfELsegw.png" alt="在这里插入图片描述">上面是在enwik8上的结果。Adaptive-Span又一次在算力很小的情况下，达到了最优效果。值得注意的是，64层的Transformer居然需要120G的计算量，又一次证明了原版Transformer是多么的吃计算资源。另外Transformer-XL在节省计算资源上，其实也算可圈可点。</p><h4 id="reformer">3 Reformer</h4><p><img src="E:\myBlog\source_posts\G8AVxR7fuCZdPHn.png" alt="在这里插入图片描述"> 论文信息：2020年1月，谷歌，ICLR2020 论文地址 https://arxiv.org/abs/2001.04451 代码和模型地址 https://github.com/google/trax/tree/master/trax/models/reformer</p><h5 id="为什么需要reformer">3.1 为什么需要Reformer</h5><p>Transformer内存和计算量消耗大的问题，一直以来广为诟病，并导致其一直不能在长文本上进行应用。（BERT、RoBERTa均设置最大长度为512）。Reformer认为Transformer有三大问题</p><ol type="1"><li>attention层计算量和序列长度为平方关系，导致无法进行长距离编码</li><li>内存占用和模型层数呈N倍关系，导致加深Transformer层数，消耗的内存特别大</li><li>feed-forward的dff比隐层dmodel一般大很多，导致FF层占用的内存特别大</li></ol><p>针对这几个问题，Reformer创新性的提出了三点改进方案</p><ol type="1"><li>LOCALITY-SENSITIVE HASHING 局部敏感hash，使得计算量从 O(L^2)降低为O(L log L) ,L为序列长度</li><li>Reversible Transformer 可逆Transformer，使得N层layers内存消耗变为只需要一层，从而使得模型加深不会受内存限制。</li><li>Feed-forward Chunking 分块全连接，大大降低了feed-forward层的内存消耗。</li></ol><p>Reformer是Transformer家族中最为关键的几个模型之一（去掉之一貌似都可以，顶多Transformer-XL不答应），其创新新也特别新颖，很多思想值得我们深入思考和借鉴。其效果也是特别明显，大大提高了内存和计算资源效率，编码长度可达64k。下面针对它的三点改进方案进行分析，有点难懂哦。</p><h5 id="实现方案-1">3.2 实现方案</h5><h6 id="locality-sensitive-hashing-局部敏感hash">3.2.1 LOCALITY-SENSITIVE HASHING 局部敏感hash</h6><p>局部敏感hash有点难懂，Reformer针对Transformer结构进行了深度灵魂拷问</p><h6 id="query和key必须用两套吗">Query和Key必须用两套吗</h6><p>Transformer主体结构为attention，原版attention计算方法如下 <img src="E:\myBlog\source_posts\Z1fYDpO6BnLyHj4.png" alt="在这里插入图片描述"> 每个token，利用其query向量，和其他token的key向量进行点乘，从而代表两个token之间的相关性。归一化后，利用得到的相关性权重，对每个token的value向量进行加权求和。首先一个问题就是，query和key向量可以是同一套吗？我们可否利用key向量去和其他token的key计算相关性呢？</p><p>为此文章进行实验分析，证明是可行的。个人认为这一点也是make sense的。 <img src="E:\myBlog\source_posts\YyTZDG1Bohkswit.png" alt="在这里插入图片描述">在文本和图像上，Q=K的attention，和普通attention，效果差别不大。</p><h6 id="必须和每个token计算相关性吗">必须和每个token计算相关性吗</h6><p>原版attention中，一个token必须和序列中其他所有token计算相关性，导致计算量随序列长度呈平方关系增长，大大制约了可编码最大长度。那必须和每个token计算相关性吗？其实之前Adaptive-Span Transformer也深度拷问过这个话题。它得出的结论是，对于大部分layer的multi-head，长度50范围内进行attention就已经足够了。不过Adaptive-Span采取的方法还是简单粗暴了一点，它约定每个head的attention span长度是固定的，并且attention span为当前token附近的其他token。</p><p>Adaptive-Span Transformer的这种方法显然还是没有抓住Attention计算冗余的痛点。Attention本质是加权求和，权重为两个token间的相关性。最终结果取决于较大的topk权重，其他权值较小的基本就是炮灰。并且softmax归一化更是加剧了这一点。小者更小，大者更大。为了减少计算冗余，我们可以只对相关性大的其他token的key向量计算Attention。</p><h6 id="怎么找到相关性大的向量呢">怎么找到相关性大的向量呢</h6><p>我们现在要从序列中找到与本token相关性最大的token，也就是当前key向量与哪些key向量相关性大。极端例子，如果两个向量完全相同，他们的相关性是最高的。确定两个高维向量的相关性确实比较困难，好在我们可以利用向量Hash来计算。</p><p>Reformer采用了局部敏感hash。我们让两个key向量在随机向量上投影，将它们划分到投影区间内。 <img src="E:\myBlog\source_posts\gSHlhWRaoqfk9pN.png" alt="在这里插入图片描述"> 如图所示，划分了四个区间（4个桶bucket），进行了三次Hash。第一次Hash，使得上面两个向量分别归入bucket0和bucket3中，下面两个向量都归入bucket0。第二次Hash，上面两个向量和下面两个，均归入到bucket2中了。我们可以发现</p><ol type="1"><li>相似的向量，也就是相关性大的，容易归入到一个bucket中</li><li>局部敏感Hash还是有一定的错误率的，我们可以利用多轮Hash来缓解。这也是Reformer的做法，它采取了4轮和8轮的Hash。</li></ol><h6 id="整个流程">整个流程</h6><p>经过局部敏感Hash后，我们可以将相关性大的key归入同一个bucket中。这样只用在bucket内进行普通Attention即可，大大降低了计算冗余度。为了实现并行计算，考虑到每个bucket包含的向量数目可能不同，实际处理中需要多看一个bucket。整个流程如下 <img src="E:\myBlog\source_posts\ZS5dCT8nVcaiob4.png" alt="在这里插入图片描述"></p><ol type="1"><li>让query等于key</li><li>局部敏感Hash（LSH）分桶。上图同一颜色的为同一个桶，共4个桶</li><li>桶排序，将相同的桶放在一起</li><li>为了实现并行计算，将所有桶分块（chunk），每个chunk大小相同</li><li>桶内计算Attention，由于之前做了分块操作，所以需要多看一个块。</li></ol><h6 id="多轮lsh">多轮LSH</h6><p>为了减少分桶错误率，文章采用了多次分桶，计算LSH Attention，Multi-round LSH attention。可以提升整体准确率。如下表。<img src="E:\myBlog\source_posts\L4BDoiwTfaRjInv.png" alt="在这里插入图片描述"></p><h6 id="reversible-transformer-可逆transformer">3.2.2 REVERSIBLE TRANSFORMER 可逆Transformer</h6><p>LSH局部敏感Hash确实比较难理解，可逆Transformer相对好懂一些。这个方案是为了解决Transformer内存占用量，随layers层数线性增长的问题。为什么会线性增长呢？原因是反向传播中，梯度会从top layer向bottom layer传播，所以必须保存住每一层的Q K V向量，也就导致N层就需要N套Q K V。</p><p>那有没有办法不保存每一层的Q K V呢？可逆Transformer正是这个思路。它利用时间换空间的思想，只保留一层的向量，反向传播时，实时计算出之前层的向量。所以叫做Reversible。Reformer每一层分为两部分，x1和x2。输出也两部分，y1和y2。计算如下</p><p><img src="E:\myBlog\source_posts\gKZT1mRFhEj48nU.png" alt="image-20201028214552153"></p><p>采用可逆残差连接后，模型效果基本没有下降。这也是make sense的，毕竟可逆是从计算角度来解决问题的，对模型本身没有改变。 <img src="E:\myBlog\source_posts\hElWr4uHtUomVYG.png" alt="在这里插入图片描述"></p><h6 id="feed-forward-chunking-ff层分块">3.2.3 Feed-Forward chunking FF层分块</h6><p>针对fead-forward层内存消耗过大的问题，Reformer也给出了解决方案，就是FF层分块。如下 <img src="E:\myBlog\source_posts\rpzwVXOSUavEs18.png" alt="在这里插入图片描述"></p><h5 id="实验结果-3">3.3 实验结果</h5><h6 id="内存和时间复杂度">内存和时间复杂度</h6><p>Reformer三个创新点，大大降低了内存和时间复杂度，消融分析如下 <img src="E:\myBlog\source_posts\5ixhnpvKeQHrBw1.png" alt="在这里插入图片描述"></p><h6 id="模型效果">模型效果</h6><p>如下为在机器翻译上的效果。Reformer减少了算力消耗，同时也大大增加了长文本编码能力，故模型效果也得到了提升。如下。 <img src="E:\myBlog\source_posts\8EtHVoBsfYxnmhO.png" alt="在这里插入图片描述"></p><h4 id="lite-transformer">4 Lite Transformer</h4><p><img src="E:\myBlog\source_posts\9UfkunTVEpL5XBG.png" alt="在这里插入图片描述"> 论文信息：2020年4月，MIT &amp; 上海交大，ICLR2020 论文地址 https://arxiv.org/abs/2004.11886 代码和模型地址 https://github.com/mit-han-lab/lite-transformer</p><h5 id="为什么要做lite-transformer">4.1 为什么要做Lite Transformer</h5><p>主要出发点仍然是Transformer计算量太大，计算冗余过多的问题。跟Adaptive-Span Transformer和Reformer想法一样，Lite Transformer也觉得没必要做Full Attention，很多Attention连接是冗余的。不一样的是，它通过压缩Attention通道的方式实现，将多头减少了一半。与Base Transformer相比，计算量减少了2.5倍。并且文章使用了量化和剪枝技术，使得模型体积减小了18.2倍。</p><h5 id="实现方案-2">4.2 实现方案</h5><p>实现方案很简单，仍然采用了原版Transformer的seq2seq结构，创新点为</p><ol type="1"><li>multiHead self-attention变为了两路并行，分别为一半的通道数（多头）。如下图a所示。其中左半部分为正常的fully attention，它用来捕获全局信息。右半部分为CNN卷积，用来捕获布局信息。最终二者通过FFN层融合。这个架构称为LONG-SHORT RANGE ATTENTION (LSRA)，长短期Attention。</li><li>为了进一步降低计算量，作者将CNN转变为了一个depth wise卷积和一个线性全连接。dw卷积在mobileNet中有讲过，不清楚可自行谷歌。</li></ol><p><img src="E:\myBlog\source_posts\WQtJvpbP9rUczhs.png" alt="在这里插入图片描述"></p><h5 id="实验结果-4">4.3 实验结果</h5><h6 id="计算复杂度">计算复杂度</h6><p><img src="E:\myBlog\source_posts\w6b1LlqIpZgt8fX.png" alt="在这里插入图片描述"> 如上图，在文本摘要任务上，Lite Transformer计算量相比Base Transformer，减少了2.5倍。同时Rouge指标基本没变。</p><h6 id="模型体积">模型体积</h6><p><img src="E:\myBlog\source_posts\JYzeocGBUr6jy4W.png" alt="在这里插入图片描述"> Lite Transformer模型体积只有Transformer的2.5分之一，通过8bit量化和剪枝，最终模型体积下降了18.2倍。</p><h3 id="邱锡鹏教授nlp预训练模型综述">🚀邱锡鹏教授：NLP预训练模型综述</h3><h4 id="引言"><strong>1.引言</strong></h4><p>随深度学习的发展，多种神经网络都被应用在 NLP 任务中，比如 CNN、RNN、GNN 和 attention 机制等，但由于现有的数据集对于大部分有监督 NLP 任务来说都很小，因此，早期的模型对 NLP 任务来说都很“浅”，往往只包含 1-3 层。</p><p>而预训练模型（Pre-trained Models, PTMs）的出现将NLP带入一个新的时代，更“深”的模型和训练技巧的增强也使得 PTMs 由“浅”变“深”，在多项任务都达到了 SOTA 性能。</p><h4 id="背景-2"><strong>2.背景</strong></h4><h5 id="语言表示学习"><strong>2.1 语言表示学习</strong></h5><p>对于语言来说，一个好的表示应当描绘语言的内在规则比如词语含义、句法结构、语义角色甚至语用。</p><p><img src="E:\myBlog\source_posts\ZmF3yXaiHPME1LQ.png" alt="img"></p><p>而分布式表示的核心思想就是通过低维实值向量来描述一段文本的意义，而向量的每一个维度都没有对于意义，整体则代表一个具体的概念。图 1 是 NLP 的通用神经体系架构。</p><p>有两种 embedding（词嵌入）方式：上下文嵌入和非上下文嵌入，两者的区别在于词的 embedding 是否根据词出现的上下文动态地改变。</p><p><strong>非上下文嵌入：</strong>表示语言的第一步就是将分离的语言符号映射到分布式嵌入空间中。也就是对于词汇表中的每个单词（词根），通过 lookup table 映射到一个向量。</p><p>这种嵌入方式有两个局限：一是一个词通过这种方法获得的词嵌入总是静态且与上下文无关的，无法处理多义词；二是难以解决不在词汇表中的词（针对这个问题，很多 NLP 任务提出了字符级或词根级的词表示，如 CharCNN、FastText、Byte-Pair Encoding (BPE)）。</p><p><strong>上下文嵌入：</strong>为解决多义性和上下文相关的问题，将词在不同上下文的语义做区分。通过对词（词根）的 token 加一层 Neural Contextual Encoder（神经上下文编码器）得到词的上下文嵌入。</p><p>有了上下文嵌入，就可以代入到具体的NLP任务中</p><p>以下是NLP领域主要模型的发展</p><p>随着BERT的出现，模型的规模越来越大，参数越来越多，训练的无标签数据集越来越大。因为随着参数增大，模型的性能并没有出现饱和的状态。GPT3有1750亿个参数。</p><p><img src="E:\myBlog\source_posts\image-20201114154032790.png" alt="image-20201114154032790"></p><p>在问答任务AQuAD 2.0 中，预训练模型精度已经和人类差不多了</p><p><img src="E:\myBlog\source_posts\image-20201114164427442.png" alt="image-20201114164427442"></p><p>felf-attention 机制 ，得到“the”的上下文表示</p><p><img src="E:\myBlog\source_posts\image-20201114154601028.png" alt="image-20201114154601028"></p><h5 id="为什么要预训练"><strong>2.3 为什么要预训练？</strong></h5><p>模型参数的数量增长迅速，而为了训练这些参数，就需要更大的数据集来避免过拟合，但是标注的数据集太少，大规模的标注数据集成本非常高，很多时候需要专家来进行标注。 而相比之下，大规模未标注的语料却很容易构建。</p><p>为了利用大量的未标注文本数据，我们可以先从其中学习一个好的表示，再将这些表示用在别的任务中。这一通过 PTMs 从未标注大规模数据集中提取表示的预训练过程在很多 NLP 任务中都取得了很好的表现。</p><p>预训练的优点可以总结为以下三点：1 在大规模语料上通过预训练学习通用语言表示对下游任务很有帮助，更好地泛化到不同的任务；2) 预训练提供了更好的模型初始化参数，使得在目标任务上有更好的泛化性能和更快的收敛速度；3) 预训练是一种有效的正则化方法，能够避免复杂模型在小数据集上过拟合。</p><h5 id="预训练任务汇总">预训练任务汇总</h5><p>自监督学习：去⼈为地构造⼀些任务，这些 任务在实际中可能不存在 , 但是可以通过这些任务, 去学习到⼀些隐含 的知识</p><p><img src="E:\myBlog\source_posts\image-20201114160905548.png" alt="image-20201114160905548"></p><h4 id="elmo模型">ELMo模型</h4><p><img src="E:\myBlog\source_posts\image-20201114202903029.png" alt="image-20201114202903029"></p><p>ELMo的预训练过程</p><h5 id="如何压缩ptms"><strong>3.3 如何压缩PTMs</strong></h5><p>预训练模型往往包含至少几千万个参数，这也使得模型难以部署到生活中的线上服务以及资源有限的设备上，这就使得模型压缩成为一条可能能够压缩模型尺寸并提高计算效率的方法。</p><p>压缩 PTMs 一般有四个方法：</p><ul><li><p><strong>剪枝（pruning）：</strong>去除不那么重要的参数（e.g. 权重、层数、通道数、attention heads）</p></li><li><p><strong>量化（weight quantization）：</strong>使用占位更少（低精度）的参数</p></li><li><p><strong>参数共享（parameter sharing）：</strong>相似模型单元间共享参数</p></li><li><p><strong>知识蒸馏（knowledge diistillation）：</strong>用一些优化目标从大型 teacher 模型学习一个小的 student 模型，一些利用知识蒸馏的 PTMs 见表 3</p></li></ul><p><img src="E:\myBlog\source_posts\5tdJqBHve3XYuCo.png" alt="img"></p><h4 id="如何将ptms应用至下游任务"><strong>4.如何将PTMs应用至下游任务</strong></h4><p><img src="E:\myBlog\source_posts\image-20201114165642819.png" alt="image-20201114165642819"></p><p>预训练完模型数据集不要了，只保留模型以及参数，迁移到下游任务，在目标数据集上进行微调</p><h4 id="未来方向"><strong>7.未来方向</strong></h4><h5 id="ptms的上界"><strong>7.1 PTMs的上界</strong></h5><p>随 BERT 的出现，我们可以发现，很多模型都可以通过更长的训练步长不在和更大的语料来提升性能</p><p>PTMs 的共同目标都是学习语言的本质通用知识(或者说是世界的知识)，然而，随着模型的不断加深，语料的不断增大，训练模型的花销也越来越大。一种更可行的解决方案是设计更有效的模型架构、自监督预训练任务、优化器和软硬件方面的技巧等。</p><h5 id="面向任务的预训练与模型压缩"><strong>7.2 面向任务的预训练与模型压缩</strong></h5><p>任务定向</p><p>模型精简</p><p>在实践中，不同的下游任务要求 PTMs 拥有不同的功能。而 PTMs 与下游目标任务间的差异通常表现在两方面：模型架构与数据分布。较大的 PTMs 通常情况下会有更好的性能，但实际问题是如何在低容量设备和低时延应用上使用如此庞大的 PTM。</p><p>除此之外，我们可以通过模型压缩来将通用 PTMs 教给面向对象的 PTM。尽管 CV 中对 CNNs 的压缩已经非常成熟，但 Tansformer 的全连接结构使得模型压缩非常具有挑战性。</p><hr><p>BERT模型从模型创新角度看一般，创新不算大。但是效果太好了，基本刷新了很多NLP的任务的最好性能，有些任务还被刷爆了，这个才是关键。</p><p>另外一点是Bert具备广泛的通用性，就是说绝大部分NLP任务都可以采用类似的两阶段模式直接去提升效果，这个第二关键。客观的说，把Bert当做最近两年NLP重大进展的集大成者更符合事实。</p><p>预训练最初是应用在图像领域的</p><p>模型中大量参数通过大的训练集合比如 ImageNet预先训练好直接拿来初始化大部分网络结构参数，然后再用下游任务上 Fine-tuning过程去调整参数让它们更适合解决下游任务。</p><p>CNN结构来说，不同层级的神经元学习到了不同类型的图像特征，由底向上特征形成层级结构预训练好的网络参数，尤其是底层的网络参数抽取出特征跟具体任务越无关，越具备任务的通用性，所以这是为何一般用底层预训练好的参数初始化新任务 网络参数的原因</p><h3 id="nlp领域的预训练模型">NLP领域的预训练模型</h3><h4 id="word-embedding考古史">Word Embedding考古史</h4><p><img src="E:\myBlog\source_posts\4aFBLDCEQmgXp9Z.jpg" alt="img"></p><p>什么是语言模型？其实看上面这张PPT上扣下来的图就明白了，为了能够量化地衡量哪个句子更像一句人话，可以设计如上图所示函数，核心函数P的思想是根据句子里面前面的一系列前导单词预测后面跟哪个单词的概率大小（理论上除了上文之外，也可以引入单词的下文联合起来预测单词出现概率）。句子里面每个单词都有个根据上文预测自己的过程，把所有这些单词的产生概率乘起来，数值越大代表这越像一句人话。语言模型压下暂且不表，我隐约预感到我这么讲你可能还是不太会明白，但是大概这个意思，不懂的可以去网上找，资料多得一样地汗牛冲动。</p><p>假设现在让你设计一个神经网络结构，去做这个语言模型的任务，就是说给你很多语料做这个事情，训练好一个神经网络，训练好之后，以后输入一句话的前面几个单词，要求这个网络输出后面紧跟的单词应该是哪个，你会怎么做？</p><p><img src="E:\myBlog\source_posts\SUI2jF7xEsaz9Z4.jpg" alt="img"></p><p>你可以像上图这么设计这个网络结构，这其实就是大名鼎鼎的中文人称“神经网络语言模型”，英文小名NNLM的网络结构，用来做语言模型。这个工作有年头了，是个陈年老工作，是Bengio 在2003年发表在JMLR上的论文。它生于2003，火于2013，以后是否会不朽暂且不知，但是不幸的是出生后应该没有引起太大反响，沉寂十年终于时来运转沉冤得雪，在2013年又被NLP考古工作者从海底湿淋淋地捞出来了祭入神殿。为什么会发生这种技术奇遇记？你要想想2013年是什么年头，是深度学习开始渗透NLP领域的光辉时刻，万里长征第一步，而NNLM可以算是南昌起义第一枪。在深度学习火起来之前，极少有人用神经网络做NLP问题，如果你10年前坚持用神经网络做NLP，估计别人会认为你这人神经有问题。所谓红尘滚滚，谁也挡不住历史发展趋势的车轮，这就是个很好的例子。</p><p>上面是闲话，闲言碎语不要讲，我们回来讲一讲NNLM的思路。先说训练过程，现在看其实很简单，见过RNN、LSTM、CNN后的你们回头再看这个网络甚至显得有些简陋。学习任务是输入某个句中单词Wt=“Bert”前面句子的t-1个单词，要求网络正确预测单词Bert，即最大化：</p><p><img src="https://i.loli.net/2020/10/30/k8XpuUwT47WOo5Q.png" alt="image-20201030202319906" style="zoom:50%;"></p><p>前面任意单词Wi用Onehot编码（比如：0001000）作为原始单词输入，之后乘以矩阵Q后获得向量C（Wi），每个单词的 C（Wi）拼接，上接隐层，然后接softmax去预测后面应该后续接哪个单词。这个 C（Wi）是什么？这其实就是单词对应的Word Embedding值，那个矩阵Q包含V行，V代表词典大小，每一行内容代表对应单词的Word embedding值。只不过Q的内容也是网络参数，需要学习获得，训练刚开始用随机值初始化矩阵Q，当这个网络训练好之后，矩阵Q的内容被正确赋值，每一行代表一个单词对应的Word embedding值。所以你看，通过这个网络学习语言模型任务，这个网络不仅自己能够根据上文预测后接单词是什么，同时获得一个副产品，就是那个矩阵Q，这就是单词的Word Embedding是被如何学会的。</p><p>2013年最火的用语言模型做Word Embedding的工具是Word2Vec，后来又出了Glove，Word2Vec是怎么工作的呢？看下图。</p><p><img src="E:\myBlog\source_posts\bXq5RvhS3niaTxB.jpg" alt="img"></p><p>Word2Vec的网络结构其实和NNLM是基本类似的，只是这个图长得清晰度差了点，看上去不像，其实它们是亲兄弟。不过这里需要指出：尽管网络结构相近，而且也是做语言模型任务，但是其训练方法不太一样。Word2Vec有两种训练方法，一种叫CBOW，核心思想是从一个句子里面把一个词抠掉，用这个词的上文和下文去预测被抠掉的这个词；第二种叫做Skip-gram，和CBOW正好反过来，输入某个单词，要求网络预测它的上下文单词。而你回头看看，NNLM是怎么训练的？是输入一个单词的上文，去预测这个单词。这是有显著差异的。为什么Word2Vec这么处理？原因很简单，因为Word2Vec和NNLM不一样，NNLM的主要任务是要学习一个解决语言模型任务的网络结构，语言模型就是要看到上文预测下文，而word embedding只是无心插柳的一个副产品。但是Word2Vec目标不一样，它单纯就是要word embedding的，这是主产品，所以它完全可以随性地这么去训练网络。</p><p>为什么要讲Word2Vec呢？这里主要是要引出CBOW的训练方法，BERT其实跟它有关系，后面会讲它们之间是如何的关系，当然它们的关系BERT作者没说，是我猜的，至于我猜的对不对，后面你看后自己判断。</p><p><img src="E:\myBlog\source_posts\aM9lpsNOS7vrmnq.jpg" alt="img"></p><p>使用Word2Vec或者Glove，通过做语言模型任务，就可以获得每个单词的Word Embedding，那么这种方法的效果如何呢？上图给了网上找的几个例子，可以看出有些例子效果还是很不错的，一个单词表达成Word Embedding后，很容易找出语义相近的其它词汇。</p><p>我们的主题是预训练，那么问题是Word Embedding这种做法能算是预训练吗？这其实就是标准的预训练过程。要理解这一点要看看学会Word Embedding后下游任务是怎么用它的。</p><p><img src="E:\myBlog\source_posts\YEf95lnIW28OGRa.jpg" alt="img"></p><p>假设如上图所示，我们有个NLP的下游任务，比如QA，就是问答问题，所谓问答问题，指的是给定一个问题X，给定另外一个句子Y,要判断句子Y是否是问题X的正确答案。问答问题假设设计的网络结构如上图所示，这里不展开讲了，懂得自然懂，不懂的也没关系，因为这点对于本文主旨来说不关键，关键是网络如何使用训练好的Word Embedding的。它的使用方法其实和前面讲的NNLM是一样的，句子中每个单词以Onehot形式作为输入，然后乘以学好的Word Embedding矩阵Q，就直接取出单词对应的Word Embedding了。这乍看上去好像是个查表操作，不像是预训练的做法是吧？其实不然，那个Word Embedding矩阵Q其实就是网络Onehot层到embedding层映射的网络参数矩阵。所以你看到了，使用Word Embedding等价于什么？等价于把Onehot层到embedding层的网络用预训练好的参数矩阵Q初始化了。这跟前面讲的图像领域的低层预训练过程其实是一样的，区别无非Word Embedding只能初始化第一层网络参数，再高层的参数就无能为力了。下游NLP任务在使用Word Embedding的时候也类似图像有两种做法，一种是Frozen，就是Word Embedding那层网络参数固定不动；另外一种是Fine-Tuning，就是Word Embedding这层参数使用新的训练集合训练也需要跟着训练过程更新掉。</p><p>上面这种做法就是18年之前NLP领域里面采用预训练的典型做法，之前说过，Word Embedding其实对于很多下游NLP任务是有帮助的，只是帮助没有大到闪瞎忘记戴墨镜的围观群众的双眼而已。那么新问题来了，为什么这样训练及使用Word Embedding的效果没有期待中那么好呢？答案很简单，因为Word Embedding有问题呗。这貌似是个比较弱智的答案，关键是Word Embedding存在什么问题？这其实是个好问题。</p><p><img src="E:\myBlog\source_posts\U3YwNJ1RmydDukM.jpg" alt="img"></p><p>这片在Word Embedding头上笼罩了好几年的乌云是什么？是多义词问题。我们知道，多义词是自然语言中经常出现的现象，也是语言灵活性和高效性的一种体现。多义词对Word Embedding来说有什么负面影响？如上图所示，比如多义词Bank，有两个常用含义，但是Word Embedding在对bank这个单词进行编码的时候，是区分不开这两个含义的，因为它们尽管上下文环境中出现的单词不同，但是在用语言模型训练的时候，不论什么上下文的句子经过word2vec，都是预测相同的单词bank，而同一个单词占的是同一行的参数空间，这导致两种不同的上下文信息都会编码到相同的word embedding空间里去。所以word embedding无法区分多义词的不同语义，这就是它的一个比较严重的问题。</p><p>你可能觉得自己很聪明，说这可以解决啊，确实也有很多研究人员提出很多方法试图解决这个问题，但是从今天往回看，这些方法看上去都成本太高或者太繁琐了，有没有简单优美的解决方案呢？</p><p>ELMO提供了一种简洁优雅的解决方案。</p><h3 id="从word-embedding到elmo">从Word Embedding到ELMO</h3><p>ELMO是“Embedding from Language Models”的简称，其实这个名字并没有反应它的本质思想，提出ELMO的论文题目：“Deep contextualized word representation”更能体现其精髓，而精髓在哪里？在deep contextualized这个短语，一个是deep，一个是context，其中context更关键。在此之前的Word Embedding本质上是个静态的方式，所谓静态指的是训练好之后每个单词的表达就固定住了，以后使用的时候，不论新句子上下文单词是什么，这个单词的Word Embedding不会跟着上下文场景的变化而改变，所以对于比如Bank这个词，它事先学好的Word Embedding中混合了几种语义 ，在应用中来了个新句子，即使从上下文中（比如句子包含money等词）明显可以看出它代表的是“银行”的含义，但是对应的Word Embedding内容也不会变，它还是混合了多种语义。这是为何说它是静态的，这也是问题所在。ELMO的本质思想是：我事先用语言模型学好一个单词的Word Embedding，此时多义词无法区分，不过这没关系。在我实际使用Word Embedding的时候，单词已经具备了特定的上下文了，这个时候我可以根据上下文单词的语义去调整单词的Word Embedding表示，这样经过调整后的Word Embedding更能表达在这个上下文中的具体含义，自然也就解决了多义词的问题了。所以ELMO本身是个根据当前上下文对Word Embedding动态调整的思路。</p><p><img src="E:\myBlog\source_posts\m6NvFoRGhWbji83.jpg" alt="img"></p><p>ELMO采用了典型的两阶段过程，第一个阶段是利用语言模型进行预训练；第二个阶段是在做下游任务时，从预训练网络中提取对应单词的网络各层的Word Embedding作为新特征补充到下游任务中。上图展示的是其预训练过程，它的网络结构采用了双层双向LSTM，目前语言模型训练的任务目标是根据单词 Wi 的上下文去正确预测单词 Wi ， Wi 之前的单词序列Context-before称为上文，之后的单词序列Context-after称为下文。图中左端的前向双层LSTM代表正方向编码器，输入的是从左到右顺序的除了预测单词外 Wi 的上文Context-before；右端的逆向双层LSTM代表反方向编码器，输入的是从右到左的逆序的句子下文Context-after；每个编码器的深度都是两层LSTM叠加。这个网络结构其实在NLP中是很常用的。使用这个网络结构利用大量语料做语言模型任务就能预先训练好这个网络，如果训练好这个网络后，输入一个新句子 Snew ，句子中每个单词都能得到对应的三个Embedding:最底层是单词的Word Embedding，往上走是第一层双向LSTM中对应单词位置的Embedding，这层编码单词的句法信息更多一些；再往上走是第二层LSTM中对应单词位置的Embedding，这层编码单词的语义信息更多一些。也就是说，ELMO的预训练过程不仅仅学会单词的Word Embedding，还学会了一个双层双向的LSTM网络结构，而这两者后面都有用。</p><p><img src="E:\myBlog\source_posts\ymSXKwFh8WBcEd5.jpg" alt="img"></p><p>上面介绍的是ELMO的第一阶段：预训练阶段。那么预训练好网络结构后，如何给下游任务使用呢？上图展示了下游任务的使用过程，比如我们的下游任务仍然是QA问题，此时对于问句X，我们可以先将句子X作为预训练好的ELMO网络的输入，这样句子X中每个单词在ELMO网络中都能获得对应的三个Embedding，之后给予这三个Embedding中的每一个Embedding一个权重a，这个权重可以学习得来，根据各自权重累加求和，将三个Embedding整合成一个。然后将整合后的这个Embedding作为X句在自己任务的那个网络结构中对应单词的输入，以此作为补充的新特征给下游任务使用。对于上图所示下游任务QA中的回答句子Y来说也是如此处理。因为ELMO给下游提供的是每个单词的特征形式，所以这一类预训练的方法被称为“Feature-based Pre-Training”。至于为何这么做能够达到区分多义词的效果，你可以想一想，其实比较容易想明白原因。</p><p><img src="E:\myBlog\source_posts\b8ToQxv5BPgELI1.jpg" alt="img"></p><p>上面这个图是TagLM采用类似ELMO的思路做命名实体识别任务的过程，其步骤基本如上述ELMO的思路，所以此处不展开说了。TagLM的论文发表在2017年的ACL会议上，作者就是AllenAI里做ELMO的那些人，所以可以将TagLM看做ELMO的一个前导工作。前几天这个PPT发出去后有人质疑说FastAI的在18年4月提出的ULMFiT才是抛弃传统Word Embedding引入新模式的开山之作，我深不以为然。首先TagLM出现的更早而且模式基本就是ELMO的思路；另外ULMFiT使用的是三阶段模式，在通用语言模型训练之后，加入了一个领域语言模型预训练过程，而且论文重点工作在这块，方法还相对比较繁杂，这并不是一个特别好的主意，因为领域语言模型的限制是它的规模往往不可能特别大，精力放在这里不太合适，放在通用语言模型上感觉更合理；再者，尽管ULFMiT实验做了6个任务，但是都集中在分类问题相对比较窄，不如ELMO验证的问题领域广，我觉得这就是因为第二步那个领域语言模型带来的限制。所以综合看，尽管ULFMiT也是个不错的工作，但是重要性跟ELMO比至少还是要差一档，当然这是我个人看法。每个人的学术审美口味不同，我个人一直比较赞赏要么简洁有效体现问题本质要么思想特别游离现有框架脑洞开得异常大的工作，所以ULFMiT我看论文的时候就感觉看着有点难受，觉得这工作没抓住重点而且特别麻烦，但是看ELMO论文感觉就赏心悦目，觉得思路特别清晰顺畅，看完暗暗点赞，心里说这样的文章获得NAACL2018最佳论文当之无愧，比ACL很多最佳论文也好得不是一点半点，这就是好工作带给一个有经验人士的一种在读论文时候就能产生的本能的感觉，也就是所谓的这道菜对上了食客的审美口味。</p><p><img src="E:\myBlog\source_posts\6y7VvCDm9NRpHJx.jpg" alt="img"></p><p>前面我们提到静态Word Embedding无法解决多义词的问题，那么ELMO引入上下文动态调整单词的embedding后多义词问题解决了吗？解决了，而且比我们期待的解决得还要好。上图给了个例子，对于Glove训练出的Word Embedding来说，多义词比如play，根据它的embedding找出的最接近的其它单词大多数集中在体育领域，这很明显是因为训练数据中包含play的句子中体育领域的数量明显占优导致；而使用ELMO，根据上下文动态调整后的embedding不仅能够找出对应的“演出”的相同语义的句子，而且还可以保证找出的句子中的play对应的词性也是相同的，这是超出期待之处。之所以会这样，是因为我们上面提到过，第一层LSTM编码了很多句法信息，这在这里起到了重要作用。</p><p><img src="E:\myBlog\source_posts\YFAVkuIxemlaHPN.jpg" alt="img"></p><p>ELMO经过这般操作，效果如何呢？实验效果见上图，6个NLP任务中性能都有幅度不同的提升，最高的提升达到25%左右，而且这6个任务的覆盖范围比较广，包含句子语义关系判断，分类任务，阅读理解等多个领域，这说明其适用范围是非常广的，普适性强，这是一个非常好的优点。</p><p><img src="E:\myBlog\source_posts\LpMSFe5kX7Qxroh.jpg" alt="img"></p><p>那么站在现在这个时间节点看，ELMO有什么值得改进的缺点呢？首先，一个非常明显的缺点在特征抽取器选择方面，ELMO使用了LSTM而不是新贵Transformer，Transformer是谷歌在17年做机器翻译任务的“Attention is all you need”的论文中提出的，引起了相当大的反响，很多研究已经证明了Transformer提取特征的能力是要远强于LSTM的。如果ELMO采取Transformer作为特征提取器，那么估计Bert的反响远不如现在的这种火爆场面。另外一点，ELMO采取双向拼接这种融合特征的能力可能比Bert一体化的融合特征方式弱，但是，这只是一种从道理推断产生的怀疑，目前并没有具体实验说明这一点。</p><p>我们如果把ELMO这种预训练方法和图像领域的预训练方法对比，发现两者模式看上去还是有很大差异的。除了以ELMO为代表的这种基于特征融合的预训练方法外，NLP里还有一种典型做法，这种做法和图像领域的方式就是看上去一致的了，一般将这种方法称为“基于Fine-tuning的模式”，而GPT就是这一模式的典型开创者。</p><h3 id="从word-embedding到gpt">从Word Embedding到GPT</h3><p><img src="E:\myBlog\source_posts\IDl2hH8j3JVdx6F.jpg" alt="img"></p><p>GPT是“Generative Pre-Training”的简称，从名字看其含义是指的生成式的预训练。GPT也采用两阶段过程，第一个阶段是利用语言模型进行预训练，第二阶段通过Fine-tuning的模式解决下游任务。上图展示了GPT的预训练过程，其实和ELMO是类似的，主要不同在于两点：首先，特征抽取器不是用的RNN，而是用的Transformer，上面提到过它的特征抽取能力要强于RNN，这个选择很明显是很明智的；其次，GPT的预训练虽然仍然是以语言模型作为目标任务，但是采用的是单向的语言模型，所谓“单向”的含义是指：语言模型训练的任务目标是根据 Wi 单词的上下文去正确预测单词 Wi ， Wi 之前的单词序列Context-before称为上文，之后的单词序列Context-after称为下文。ELMO在做语言模型预训练的时候，预测单词 Wi 同时使用了上文和下文，而GPT则只采用Context-before这个单词的上文来进行预测，而抛开了下文。这个选择现在看不是个太好的选择，原因很简单，它没有把单词的下文融合进来，这限制了其在更多应用场景的效果，比如阅读理解这种任务，在做任务的时候是可以允许同时看到上文和下文一起做决策的。如果预训练时候不把单词的下文嵌入到Word Embedding中，是很吃亏的，白白丢掉了很多信息。</p><p>这里强行插入一段简单提下Transformer，尽管上面提到了，但是说的还不完整，补充两句。首先，Transformer是个叠加的“自注意力机制（Self Attention）”构成的深度网络，是目前NLP里最强的特征提取器，注意力这个机制在此被发扬光大，从任务的配角不断抢戏，直到Transformer一跃成为踢开RNN和CNN传统特征提取器，荣升头牌，大红大紫。注意力机制可以参考“<a href="https://zhuanlan.zhihu.com/p/37601161" target="_blank" rel="noopener">深度学习中的注意力模型</a>”，补充下相关基础知识，如果不了解注意力机制你肯定会落后时代的发展。而介绍Transformer比较好的文章可以参考以下两篇文章：一个是Jay Alammar可视化地介绍Transformer的博客文章<a href="https://link.zhihu.com/?target=https%3A//jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a> ，非常容易理解整个机制，建议先从这篇看起；然后可以参考哈佛大学NLP研究组写的“<a href="https://link.zhihu.com/?target=http%3A//nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer.</a> ”，代码原理双管齐下，讲得非常清楚。我相信上面两个文章足以让你了解Transformer了，所以这里不展开介绍。</p><p>其次，我的判断是Transformer在未来会逐渐替代掉RNN成为主流的NLP工具，RNN一直受困于其并行计算能力，这是因为它本身结构的序列性依赖导致的，尽管很多人在试图通过修正RNN结构来修正这一点，但是我不看好这种模式，因为给马车换轮胎不如把它升级到汽车，这个道理很好懂，更何况目前汽车的雏形已经出现了，干嘛还要执着在换轮胎这个事情呢？是吧？再说CNN，CNN在NLP里一直没有形成主流，CNN的最大优点是易于做并行计算，所以速度快，但是在捕获NLP的序列关系尤其是长距离特征方面天然有缺陷，不是做不到而是做不好，目前也有很多改进模型，但是特别成功的不多。综合各方面情况，很明显Transformer同时具备并行性好，又适合捕获长距离特征，没有理由不在赛跑比赛中跑不过RNN和CNN。</p><p>好了，题外话结束，我们再回到主题，接着说GPT。上面讲的是GPT如何进行第一阶段的预训练，那么假设预训练好了网络模型，后面下游任务怎么用？它有自己的个性，和ELMO的方式大有不同。</p><p><img src="E:\myBlog\source_posts\3Gr9vqoPHkSfcAg.jpg" alt="img"></p><p>上图展示了GPT在第二阶段如何使用。首先，对于不同的下游任务来说，本来你可以任意设计自己的网络结构，现在不行了，你要向GPT的网络结构看齐，把任务的网络结构改造成和GPT的网络结构是一样的。然后，在做下游任务的时候，利用第一步预训练好的参数初始化GPT的网络结构，这样通过预训练学到的语言学知识就被引入到你手头的任务里来了，这是个非常好的事情。再次，你可以用手头的任务去训练这个网络，对网络参数进行Fine-tuning，使得这个网络更适合解决手头的问题。就是这样。看到了么？这有没有让你想起最开始提到的图像领域如何做预训练的过程（请参考上图那句非常容易暴露年龄的歌词）？对，这跟那个模式是一模一样的。</p><p>这里引入了一个新问题：对于NLP各种花样的不同任务，怎么改造才能靠近GPT的网络结构呢？</p><p><img src="E:\myBlog\source_posts\iJqb8TYLwCvdSVk.jpg" alt="img"></p><p>GPT论文给了一个改造施工图如上，其实也很简单：对于分类问题，不用怎么动，加上一个起始和终结符号即可；对于句子关系判断问题，比如Entailment，两个句子中间再加个分隔符即可；对文本相似性判断问题，把两个句子顺序颠倒下做出两个输入即可，这是为了告诉模型句子顺序不重要；对于多项选择问题，则多路输入，每一路把文章和答案选项拼接作为输入即可。从上图可看出，这种改造还是很方便的，不同任务只需要在输入部分施工即可。</p><p><img src="E:\myBlog\source_posts\qnLcVGo5IK6riYh.jpg" alt="img"></p><p>GPT的效果是非常令人惊艳的，在12个任务里，9个达到了最好的效果，有些任务性能提升非常明显。</p><p><img src="E:\myBlog\source_posts\96zdAXvcOmJTuP2.jpg" alt="img"></p><p>那么站在现在的时间节点看，GPT有什么值得改进的地方呢？其实最主要的就是那个单向语言模型，如果改造成双向的语言模型任务估计也没有Bert太多事了。当然，即使如此GPT也是非常非常好的一个工作，跟Bert比，其作者炒作能力亟待提升。</p><h3 id="bert的诞生">Bert的诞生</h3><p><img src="E:\myBlog\source_posts\rSJAqOMB4sathDg.jpg" alt="img"></p><p>我们经过跋山涉水，终于到了目的地Bert模型了。</p><p>Bert采用和GPT完全相同的两阶段模型，首先是语言模型预训练；其次是使用Fine-Tuning模式解决下游任务。和GPT的最主要不同在于在预训练阶段采用了类似ELMO的双向语言模型，当然另外一点是语言模型的数据规模要比GPT大。所以这里Bert的预训练过程不必多讲了。</p><p><img src="E:\myBlog\source_posts\61JpWKSZ5fF3tNk.jpg" alt="img"></p><p>第二阶段，Fine-Tuning阶段，这个阶段的做法和GPT是一样的。当然，它也面临着下游任务网络结构改造的问题，在改造任务方面Bert和GPT有些不同，下面简单介绍一下。</p><p><img src="E:\myBlog\source_posts\UTQdhtVA7PzcIlF.jpg" alt="img"></p><p>在介绍Bert如何改造下游任务之前，先大致说下NLP的几类问题，说这个是为了强调Bert的普适性有多强。通常而言，绝大部分NLP问题可以归入上图所示的四类任务中：一类是序列标注，这是最典型的NLP任务，比如中文分词，词性标注，命名实体识别，语义角色标注等都可以归入这一类问题，它的特点是句子中每个单词要求模型根据上下文都要给出一个分类类别。第二类是分类任务，比如我们常见的文本分类，情感计算等都可以归入这一类。它的特点是不管文章有多长，总体给出一个分类类别即可。第三类任务是句子关系判断，比如Entailment，QA，语义改写，自然语言推理等任务都是这个模式，它的特点是给定两个句子，模型判断出两个句子是否具备某种语义关系；第四类是生成式任务，比如机器翻译，文本摘要，写诗造句，看图说话等都属于这一类。它的特点是输入文本内容后，需要自主生成另外一段文字。</p><p><img src="E:\myBlog\source_posts\mxJybVWl2OatfUc.jpg" alt="img"></p><p>对于种类如此繁多而且各具特点的下游NLP任务，Bert如何改造输入输出部分使得大部分NLP任务都可以使用Bert预训练好的模型参数呢？上图给出示例，对于句子关系类任务，很简单，和GPT类似，加上一个起始和终结符号，句子之间加个分隔符即可。对于输出来说，把第一个起始符号对应的Transformer最后一层位置上面串接一个softmax分类层即可。对于分类问题，与GPT一样，只需要增加起始和终结符号，输出部分和句子关系判断任务类似改造；对于序列标注问题，输入部分和单句分类是一样的，只需要输出部分Transformer最后一层每个单词对应位置都进行分类即可。从这里可以看出，上面列出的NLP四大任务里面，除了生成类任务外，Bert其它都覆盖到了，而且改造起来很简单直观。尽管Bert论文没有提，但是稍微动动脑子就可以想到，其实对于机器翻译或者文本摘要，聊天机器人这种生成式任务，同样可以稍作改造即可引入Bert的预训练成果。只需要附着在S2S结构上，encoder部分是个深度Transformer结构，decoder部分也是个深度Transformer结构。根据任务选择不同的预训练数据初始化encoder和decoder即可。这是相当直观的一种改造方法。当然，也可以更简单一点，比如直接在单个Transformer结构上加装隐层产生输出也是可以的。不论如何，从这里可以看出，NLP四大类任务都可以比较方便地改造成Bert能够接受的方式。这其实是Bert的非常大的优点，这意味着它几乎可以做任何NLP的下游任务，具备普适性，这是很强的。</p><p><img src="E:\myBlog\source_posts\e7tSMGZjDHmY1Ck.jpg" alt="img"></p><p>Bert采用这种两阶段方式解决各种NLP任务效果如何？在11个各种类型的NLP任务中达到目前最好的效果，某些任务性能有极大的提升。一个新模型好不好，效果才是王道。</p><p><img src="E:\myBlog\source_posts\RniS8uQhpDmcHN6.jpg" alt="img"></p><p>到这里我们可以再梳理下几个模型之间的演进关系。从上图可见，Bert其实和ELMO及GPT存在千丝万缕的关系，比如如果我们把GPT预训练阶段换成双向语言模型，那么就得到了Bert；而如果我们把ELMO的特征抽取器换成Transformer，那么我们也会得到Bert。所以你可以看出：Bert最关键两点，一点是特征抽取器采用Transformer；第二点是预训练的时候采用双向语言模型。</p><p>那么新问题来了：对于Transformer来说，怎么才能在这个结构上做双向语言模型任务呢？乍一看上去好像不太好搞。我觉得吧，其实有一种很直观的思路，怎么办？看看ELMO的网络结构图，只需要把两个LSTM替换成两个Transformer，一个负责正向，一个负责反向特征提取，其实应该就可以。当然这是我自己的改造，Bert没这么做。那么Bert是怎么做的呢？我们前面不是提过Word2Vec吗？我前面肯定不是漫无目的地提到它，提它是为了在这里引出那个CBOW训练方法，所谓写作时候埋伏笔的“草蛇灰线，伏脉千里”，大概就是这个意思吧？前面提到了CBOW方法，它的核心思想是：在做语言模型任务的时候，我把要预测的单词抠掉，然后根据它的上文Context-Before和下文Context-after去预测单词。其实Bert怎么做的？Bert就是这么做的。从这里可以看到方法间的继承关系。当然Bert作者没提Word2Vec及CBOW方法，这是我的判断，Bert作者说是受到完形填空任务的启发，这也很可能，但是我觉得他们要是没想到过CBOW估计是不太可能的。</p><p>从这里可以看出，在文章开始我说过Bert在模型方面其实没有太大创新，更像一个最近几年NLP重要技术的集大成者，原因在于此，当然我不确定你怎么看，是否认同这种看法，而且我也不关心你怎么看。其实Bert本身的效果好和普适性强才是最大的亮点。</p><p><img src="E:\myBlog\source_posts\LO9j7cIxEJCe2Ay.jpg" alt="img"></p><p>那么Bert本身在模型和方法角度有什么创新呢？就是论文中指出的Masked 语言模型和Next Sentence Prediction。而Masked语言模型上面讲了，本质思想其实是CBOW，但是细节方面有改进。</p><p><img src="E:\myBlog\source_posts\75DVNACdHgtRbS9.jpg" alt="img"></p><p>Masked双向语言模型向上图展示这么做：随机选择语料中15%的单词，把它抠掉，也就是用[Mask]掩码代替原始单词，然后要求模型去正确预测被抠掉的单词。但是这里有个问题：训练过程大量看到[mask]标记，但是真正后面用的时候是不会有这个标记的，这会引导模型认为输出是针对[mask]这个标记的，但是实际使用又见不到这个标记，这自然会有问题。为了避免这个问题，Bert改造了一下，15%的被上天选中要执行[mask]替身这项光荣任务的单词中，只有80%真正被替换成[mask]标记，10%被狸猫换太子随机替换成另外一个单词，10%情况这个单词还待在原地不做改动。这就是Masked双向语音模型的具体做法。</p><p><img src="E:\myBlog\source_posts\MTCajrZPKuF51Ne.jpg" alt="img"></p><p>至于说“Next Sentence Prediction”，指的是做语言模型预训练的时候，分两种情况选择两个句子，一种是选择语料中真正顺序相连的两个句子；另外一种是第二个句子从语料库中抛色子，随机选择一个拼到第一个句子后面。我们要求模型除了做上述的Masked语言模型任务外，附带再做个句子关系预测，判断第二个句子是不是真的是第一个句子的后续句子。之所以这么做，是考虑到很多NLP任务是句子关系判断任务，单词预测粒度的训练到不了句子关系这个层级，增加这个任务有助于下游句子关系判断任务。所以可以看到，它的预训练是个多任务过程。这也是Bert的一个创新。</p><p><img src="E:\myBlog\source_posts\XQ17TqJcYpPoA3i.jpg" alt="img"></p><p>上面这个图给出了一个我们此前利用微博数据和开源的Bert做预训练时随机抽出的一个中文训练实例，从中可以体会下上面讲的masked语言模型和下句预测任务。训练数据就长这种样子。</p><p><img src="E:\myBlog\source_posts\E1vcQhzTsgbLqkF.jpg" alt="img"></p><p>顺带讲解下Bert的输入部分，也算是有些特色。它的输入部分是个线性序列，两个句子通过分隔符分割，最前面和最后增加两个标识符号。每个单词有三个embedding:位置信息embedding，这是因为NLP中单词顺序是很重要的特征，需要在这里对位置信息进行编码；单词embedding,这个就是我们之前一直提到的单词embedding；第三个是句子embedding，因为前面提到训练数据都是由两个句子构成的，那么每个句子有个句子整体的embedding项对应给每个单词。把单词对应的三个embedding叠加，就形成了Bert的输入。</p><p><img src="E:\myBlog\source_posts\wHfCcyhaiXPMx1d.jpg" alt="img"></p><p>至于Bert在预训练的输出部分如何组织，可以参考上图的注释。</p><p><img src="E:\myBlog\source_posts\B5wItXbYpPr619Z.jpg" alt="img"></p><p>我们说过Bert效果特别好，那么到底是什么因素起作用呢？如上图所示，对比试验可以证明，跟GPT相比，双向语言模型起到了最主要的作用，对于那些需要看到下文的任务来说尤其如此。而预测下个句子来说对整体性能来说影响不算太大，跟具体任务关联度比较高。</p><p><img src="E:\myBlog\source_posts\u9mKAEGjp4fa7ys.jpg" alt="img"></p><p>最后，我讲讲我对Bert的评价和看法，我觉得Bert是NLP里里程碑式的工作，对于后面NLP的研究和工业应用会产生长久的影响，这点毫无疑问。但是从上文介绍也可以看出，从模型或者方法角度看，Bert借鉴了ELMO，GPT及CBOW，主要提出了Masked 语言模型及Next Sentence Prediction，但是这里Next Sentence Prediction基本不影响大局，而Masked LM明显借鉴了CBOW的思想。所以说Bert的模型没什么大的创新，更像最近几年NLP重要进展的集大成者，这点如果你看懂了上文估计也没有太大异议，如果你有大的异议，杠精这个大帽子我随时准备戴给你。如果归纳一下这些进展就是：首先是两阶段模型，第一阶段双向语言模型预训练，这里注意要用双向而不是单向，第二阶段采用具体任务Fine-tuning或者做特征集成；第二是特征抽取要用Transformer作为特征提取器而不是RNN或者CNN；第三，双向语言模型可以采取CBOW的方法去做（当然我觉得这个是个细节问题，不算太关键，前两个因素比较关键）。Bert最大的亮点在于效果好及普适性强，几乎所有NLP任务都可以套用Bert这种两阶段解决思路，而且效果应该会有明显提升。可以预见的是，未来一段时间在NLP应用领域，Transformer将占据主导地位，而且这种两阶段预训练方法也会主导各种应用。</p><p>另外，我们应该弄清楚预训练这个过程本质上是在做什么事情，本质上预训练是通过设计好一个网络结构来做语言模型任务，然后把大量甚至是无穷尽的无标注的自然语言文本利用起来，预训练任务把大量语言学知识抽取出来编码到网络结构中，当手头任务带有标注信息的数据有限时，这些先验的语言学特征当然会对手头任务有极大的特征补充作用，因为当数据有限的时候，很多语言学现象是覆盖不到的，泛化能力就弱，集成尽量通用的语言学知识自然会加强模型的泛化能力。如何引入先验的语言学知识其实一直是NLP尤其是深度学习场景下的NLP的主要目标之一，不过一直没有太好的解决办法，而ELMO/GPT/Bert的这种两阶段模式看起来无疑是解决这个问题自然又简洁的方法，这也是这些方法的主要价值所在。</p><p>对于当前NLP的发展方向，我个人觉得有两点非常重要，一个是需要更强的特征抽取器，目前看Transformer会逐渐担当大任，但是肯定还是不够强的，需要发展更强的特征抽取器；第二个就是如何优雅地引入大量无监督数据中包含的语言学知识，注意我这里强调地是优雅，而不是引入，此前相当多的工作试图做各种语言学知识的嫁接或者引入，但是很多方法看着让人牙疼，就是我说的不优雅。目前看预训练这种两阶段方法还是很有效的，也非常简洁，当然后面肯定还会有更好的模型出现。</p><p>完了，这就是自然语言模型预训练的发展史。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      对最近一段时间读的论文以及自己的对NLP领域transformer系列模型的了解做一个总结
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-11-11-论文分享</title>
    <link href="http://yoursite.com/2020/11/11/2020-11-11-%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"/>
    <id>http://yoursite.com/2020/11/11/2020-11-11-%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/</id>
    <published>2020-11-11T07:04:42.000Z</published>
    <updated>2020-11-23T02:13:58.638Z</updated>
    
    <content type="html"><![CDATA[<p><img src="E:\myBlog\source_posts\image-20201111153229465.png" alt="image-20201111153229465"></p><blockquote><p>ICLR 2020</p><p>code url (official tf) : <a href="https://github.com/google-research/ALBERT" target="_blank" rel="noopener" class="uri">https://github.com/google-research/ALBERT</a></p><p>code url (unofficial torch): <a href="https://github.com/huggingface/transformers" target="_blank" rel="noopener" class="uri">https://github.com/huggingface/transformers</a></p><p>cited ：646</p></blockquote><h3 id="问题">问题</h3><p>BERT参数量有点多，消耗算力较大，能否去压缩BERT模型，构建一个轻量级的BERT？</p><h3 id="解决">解决</h3><p>提出两个降低参数的策略 + 自监督损失（SOP）</p><h4 id="降低参数">降低参数</h4><p>降低参数1：factorized embedding parameterization ，将大型的token 嵌入矩阵分解为两个小矩阵。使得</p><p>降低参数2：cross-layer parameter sharing ， 以防随着层数加深参数增加 。 之前的参数共享策略只是关注于transformer这种的encoder-decoder模型中而不是BERT这种预训练-微调模型中。</p><p>这两个策略在减少大量参数且提高参数效率的同时，对BERT的性能影响不大</p><p>具体参考如下：</p><p><img src="E:\myBlog\source_posts\image-20201121140455423.png" alt="image-20201121140455423"></p><h4 id="自损失监督sop">自损失监督(SOP)</h4><p>代替BERT中的NSP策略，使得模型更加关注句子内的一致性</p><h4 id="结果">结果</h4><p>模型ALBERT：提高训练速度，减少内存消耗</p><p>在语言理解任务上达到SOTA水平，具体参考如下：</p><p><img src="E:\myBlog\source_posts\image-20201121142012520.png" alt="image-20201121142012520"></p><h3 id="模型">模型</h3><h4 id="config-设置">config 设置</h4><p>ALBERT模型的config设置和BERT基本一致</p><p><img src="E:\myBlog\source_posts\image-20201121143009555.png"></p><h4 id="factorized-embedding-parameterization">factorized embedding parameterization</h4><p>之前的BERT模型都是Embedding size = hiden size ， （是次优的）</p><p><img src="E:\myBlog\source_posts\image-20201122094115198.png" alt="image-20201122094115198"></p><p><img src="E:\myBlog\source_posts\image-20201122094556848.png" alt="image-20201122094556848"></p><h3 id="实验">实验</h3><h3 id="总结">总结</h3><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      记录值得分享的论文;
《ALBERT》: 一个轻量级的BERT

    
    </summary>
    
    
      <category term="论文分享" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-11-09-pytorch反向传播以及参数更新理解</title>
    <link href="http://yoursite.com/2020/11/09/2020-11-09-pytorch%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%BB%A5%E5%8F%8A%E5%8F%82%E6%95%B0%E6%9B%B4%E6%96%B0%E7%90%86%E8%A7%A3/"/>
    <id>http://yoursite.com/2020/11/09/2020-11-09-pytorch%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%BB%A5%E5%8F%8A%E5%8F%82%E6%95%B0%E6%9B%B4%E6%96%B0%E7%90%86%E8%A7%A3/</id>
    <published>2020-11-09T07:26:05.000Z</published>
    <updated>2020-11-10T10:43:40.718Z</updated>
    
    <content type="html"><![CDATA[<h3 id="反向传播以及更新">反向传播以及更新</h3><h4 id="方法一手动计算变量">方法一：手动计算变量</h4><p>这种方法不常用，因为一般的模型参数太多了</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="comment"># 定义参数</span></span><br><span class="line">w1 = Variable(torch.FloatTensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]),requires_grad = <span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 定义输出</span></span><br><span class="line">d = torch.mean(w1)</span><br><span class="line"><span class="comment"># 反向求导</span></span><br><span class="line">d.backward()</span><br><span class="line"><span class="comment"># 定义学习率等参数</span></span><br><span class="line">lr = <span class="number">0.001</span></span><br><span class="line"><span class="comment"># 手动更新参数</span></span><br><span class="line">w1.data.zero_() <span class="comment"># BP求导更新参数之前,需先对导数置0</span></span><br><span class="line">w1.data.sub_(lr*w1.grad.data)<span class="number">12345678910111213</span></span><br></pre></td></tr></tbody></table></figure><blockquote><p><em>一个网络中通常有很多变量,如果按照上述的方法手动求导,然后更新参数,是很麻烦的,这个时候可以调用torch.optim</em></p></blockquote><h4 id="方法二使用torch.optim">方法二:使用torch.optim</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="comment"># 这里假设我们定义了一个网络,为net</span></span><br><span class="line">steps = <span class="number">10000</span></span><br><span class="line"><span class="comment"># 定义一个optim对象</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr = <span class="number">0.01</span>)</span><br><span class="line"><span class="comment"># 在for循环中更新参数</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(steps):</span><br><span class="line">  optimizer.zero_grad() <span class="comment"># 对网络中参数当前的导数置0，清零梯度缓存</span></span><br><span class="line">  output = net(input) <span class="comment"># 网络前向计算</span></span><br><span class="line">  loss = criterion(output, target) <span class="comment"># 通过定义损失函数：criterion，计算误差，得到网络的损失值：loss；</span></span><br><span class="line">  loss.backward() <span class="comment">#　通过loss.backward()完成误差的反向传播，通过pytorch的内在机制完成自动求导得到每个参数的梯度。</span></span><br><span class="line">  optimizer.step() <span class="comment"># 更新参数123456789101112131415</span></span><br></pre></td></tr></tbody></table></figure><blockquote><p>torch.optim只用于参数更新和对参数的梯度置０，不能计算参数的梯度，在使用torch.optim进行参数更新之前，需要写前向与反向传播求导的代码</p></blockquote><h4 id="注">注</h4><p>loss是反向传播整个计算图/模型（有一条传播路径）的节点参数，其中一个模型可以认为是一个连通图，是由数据传播的，比如encoder和decoder之间会有隐藏向量Z进行连接，那么就是一个计算图，那么loss反向传播就会更新所有的参数。参数在定义时默认就是可动态更新的。</p><h3 id="variable-parameter的区别">Variable &amp; Parameter的区别</h3><p>之所以有Variable这个数据结构，是为了引入计算图（自动求导），方便构建神经网络。也就是一般模型网络（计算图）的输入是Variable类型的，是要外部给值的，返回的是tensor类型。</p><p>不同于Parameter，Parameter一般是随机初始化，然后根据loss反向传播被动更新值</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x = Variable(torch.Tensor(array), requires_grad = True) #可以自求导更新，若一个节点requires_grad被设置为True，那么计图中所有依赖它求得的节点的requires_grad都为True</span><br></pre></td></tr></tbody></table></figure><p>Pytorch主要通过引入<code>nn.Parameter</code>类型的变量和<code>optimizer机制</code>来解决自动更新多个参数的问题。</p><p>Parameter是Variable的子类，本质上和后者一样，只不过<strong>parameter默认是求梯度的</strong>，同时一个网络net中的parameter变量是可以通过 <code>net.parameters()</code> 来很方便地访问到的，只需将网络中所有需要训练更新的参数定义为Parameter类型，再佐以optimizer，就能够完成所有参数的更新了</p><p>Parameter是torch.autograd.Variable的一个字类，常被用于Module的参数。例如权重和偏置。自动加入参数列表，可以进行保存恢复。和Variable具有相同的运算。</p><p>Parameter的require_grad默认设置为true。Varaible默认设置为False.</p><p>Parameters类是<a href="https://pytorch.apachecn.org/docs/1.0/tensors.html#torch.Tensor" target="_blank" rel="noopener"><code>Tensor</code></a> 的子类, 不过相对于它的父类，Parameters类有一个很重要的特性就是当其在 <a href="https://pytorch.apachecn.org/docs/1.0/nn.html#torch.nn.Module" target="_blank" rel="noopener"><code>Module</code></a>类中被使用并被当做这个<a href="https://pytorch.apachecn.org/docs/1.0/nn.html#torch.nn.Module" target="_blank" rel="noopener"><code>Module</code></a>类的模块属性的时候，那么这个Parameters对象会被自动地添加到这个<a href="https://pytorch.apachecn.org/docs/1.0/nn.html#torch.nn.Module" target="_blank" rel="noopener"><code>Module</code></a>类的参数列表(list of parameters)之中，同时也就会被添加入此<a href="https://pytorch.apachecn.org/docs/1.0/nn.html#torch.nn.Module" target="_blank" rel="noopener"><code>Module</code></a>类的 <a href="https://pytorch.apachecn.org/docs/1.0/nn.html#torch.nn.Module.parameters" target="_blank" rel="noopener"><code>parameters()</code></a>方法所返回的参数迭代器中。而Parameters类的父类Tensor类也可以被用为构建模块的属性，但不会被加入参数列表。这样主要是因为，有时可能需要在模型中存储一些非模型参数的临时状态，比如RNN中的最后一个隐状态。而通过使用非<a href="https://pytorch.apachecn.org/docs/1.0/nn.html#torch.nn.Parameter" target="_blank" rel="noopener"><code>Parameter</code></a>的Tensor类，可以将这些临时变量注册(register)为模型的属性的同时使其不被加入参数列表。</p><p><strong>我们可以这样简单区分，在计算图中，数据（包括输入数据和计算过程中产生的feature map等）是 variable 类型，该类型不会被保存到模型中。 </strong></p><p><strong>网络的权重是 parameter 类型，在计算过程中会被更新，将会被保存到模型中。</strong></p><blockquote><p><a href="https://www.jianshu.com/p/cb739922ce88" target="_blank" rel="noopener" class="uri">https://www.jianshu.com/p/cb739922ce88</a></p><p><a href="https://zhoef.com/2019/08/12/16_Pytorch_Basic/" target="_blank" rel="noopener" class="uri">https://zhoef.com/2019/08/12/16_Pytorch_Basic/</a></p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      pytorch反向传播以及参数更新理解
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-11-09-安装jupyter远程访问服务器</title>
    <link href="http://yoursite.com/2020/11/09/2020-11-09-%E5%AE%89%E8%A3%85jupyter%E8%BF%9C%E7%A8%8B%E8%AE%BF%E9%97%AE%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
    <id>http://yoursite.com/2020/11/09/2020-11-09-%E5%AE%89%E8%A3%85jupyter%E8%BF%9C%E7%A8%8B%E8%AE%BF%E9%97%AE%E6%9C%8D%E5%8A%A1%E5%99%A8/</id>
    <published>2020-11-09T06:19:07.000Z</published>
    <updated>2020-11-19T08:42:52.995Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言">前言</h3><p>想要去用jupyter远程访问我们实验室的服务器，于是参考网上教程如下：</p><h3 id="具体操作">具体操作</h3><h4 id="一.-ubuntu下安装jupyter-notebook">一. Ubuntu下安装jupyter notebook</h4><h5 id="使用anaconda安装">1. 使用Anaconda安装</h5><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install jupyter notebook</span><br></pre></td></tr></tbody></table></figure><h5 id="使用pip安装">2. 使用pip安装</h5><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install jupyter notebook</span><br></pre></td></tr></tbody></table></figure><h4 id="二.-jupyter-notebook-配置">二. Jupyter notebook 配置</h4><h5 id="生成配置文件">1. 生成配置文件</h5><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter notebook --generate-config</span><br></pre></td></tr></tbody></table></figure><h5 id="创建密码">2. 创建密码</h5><p>使用python中的<code>passwd()</code>创建密码，终端输入<code>ipython</code>打开ipython并输入:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">from</span> notebook.auth <span class="keyword">import</span> passwd</span><br><span class="line">In [<span class="number">2</span>]: passwd()</span><br><span class="line">Enter password: ******</span><br><span class="line">Verify password: ******</span><br><span class="line">Out [<span class="number">2</span>]: <span class="string">'sha1:...'</span>  <span class="comment">#应该是密钥</span></span><br></pre></td></tr></tbody></table></figure><p>复制Out [2] 显示的密码（'sha1:...' 包括引号）。</p><h5 id="修改jupyter-notebook的配置文件">3. 修改jupyter notebook的配置文件</h5><ul><li>打开配置文件</li></ul><figure class="highlight vim"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">vim</span> ~/.jupyter/jupyter_notebook_config.<span class="keyword">py</span></span><br></pre></td></tr></tbody></table></figure><ul><li>在该文件中做如下修改或直接在文件尾端添加：</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">c.NotebookApp.allow_remote_access = <span class="literal">True</span> <span class="comment">#允许远程连接</span></span><br><span class="line">c.NotebookApp.ip=<span class="string">'*'</span> <span class="comment"># 设置所有ip皆可访问</span></span><br><span class="line">c.NotebookApp.password = <span class="string">u'sha:..'</span> <span class="comment">#之前复制的密码</span></span><br><span class="line">c.NotebookApp.open_browser = <span class="literal">False</span> <span class="comment"># 禁止自动打开浏览器</span></span><br><span class="line">c.NotebookApp.port =<span class="number">23333</span> <span class="comment">#任意指定一个端口 ，我们指定的是23333</span></span><br></pre></td></tr></tbody></table></figure><h4 id="启动jupyter-notebook">4. 启动jupyter notebook</h4><p>终端输入：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter notebook</span><br></pre></td></tr></tbody></table></figure><p>或使用<code>nohup</code>后台运行 jupyter notebook:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup jupyter notebook &gt;~/jupyter.log <span class="number">2</span>&gt;&amp;<span class="number">1</span> &amp;  <span class="comment">#后台挂起，并且将输出重定向到jupyter.log文件中</span></span><br></pre></td></tr></tbody></table></figure><h4 id="远程访问jupyter-notebook">5. 远程访问jupyter notebook</h4><p>本地浏览器输入<code>http://(服务器地址):(配置文件中设定的端口)</code>； 假设服务器地址为210.30.97.69，配置的端口为23333，这里的浏览器输入地址应为<code>http://210.30.97.69:23333</code>； 即可访问jupyter notebook。</p><h3 id="注">注</h3><p>只能通过和服务器所在的局域网来访问，也就是校园网可以正常访问服务器，外网不可。试过用电脑连接我手机热点，无法访问</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      安装jupyter远程访问服务器
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-11-07-论文分享</title>
    <link href="http://yoursite.com/2020/11/07/2020-11-07-%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"/>
    <id>http://yoursite.com/2020/11/07/2020-11-07-%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/</id>
    <published>2020-11-07T13:48:05.000Z</published>
    <updated>2020-11-09T06:32:04.995Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2020/11/09/7bSkvNlUTsB6VoC.png" alt="image-20201107220502247"></p><blockquote><p>ACL 2020</p><p>code url (official torch) : <a href="https://github.com/namisan/mt-dnn" target="_blank" rel="noopener" class="uri">https://github.com/namisan/mt-dnn</a></p><p>被引用次数：11</p></blockquote><h3 id="背景">背景</h3><p>泛化性和鲁棒性对于机器学习来说是很重要的，对抗训练可以增强鲁棒性，但是同时也会使泛化性受到损失；</p><p>BERT等大型自然语言模型已经在泛化性方面取得了巨大的进步，然而这种预训练模型容易受到对抗攻击</p><h3 id="问题">问题</h3><p>如何使得大型NLP模型兼得泛化性和鲁棒性？</p><h3 id="解决">解决</h3><p>提出了一种通用算法ALUM (Adversarial training for large neural Language Models), 把对抗训练用到了预训练和微调两个阶段，通过对抗训练来提高模型的泛化性和鲁棒性。</p><p>对抗训练的方法是针对embedding space，通过最大化对抗损失、最小化模型损失的方式进行对抗，在下游任务上取得了一致的效果提升。</p><p>这种对抗训练方法不仅能够在BERT上有提高，而且在RoBERTa这种已经预训练好的模型上也能有所提高，说明对抗训练的确可以帮助模型纠正易错点。</p><p>算法可以应用在任何基于transformer的语言模型中</p><h3 id="贡献">贡献</h3><p><img src="https://i.loli.net/2020/11/09/sTFuUcA7MvNkre5.png" alt="image-20201108095152820"></p><h3 id="模型">模型</h3><h4 id="准备">准备</h4><p>tokenization使用的是BPE（Byte-PairEncoding）</p><p>模型基于BERT和 RoBERTa模型，但是在训练策略上与前两者有所改动如下：</p><p>在一个epoch中，掩码率以每经过20%的epoch，增加5%掩码率的增速使得掩码率从5%增加到25%</p><p><img src="https://i.loli.net/2020/11/09/gd1sfkUao5wqhrc.png" alt="image-20201108100621048"></p><h4 id="标准训练目标函数">标准训练目标函数</h4><p>标准的预训练和微调函数都可以认为是在训练数据上进行最小化标准差</p><p><img src="https://i.loli.net/2020/11/09/2C9JdbBunrHYxTl.png" alt="image-20201108101520342"></p><h4 id="对抗训练">对抗训练</h4><p><img src="https://i.loli.net/2020/11/09/iLw28ZgrYTVWbnU.png" alt="image-20201108133249939"></p><h4 id="alum算法">ALUM算法</h4><p>基于几个关键想法：</p><ol type="1"><li><p>扰动embedding空间，优于直接对输入文本应用扰动。</p></li><li><p>通过虚拟对抗训练为标准目标添加正则化项。比传统的对抗训练有效果，尤其是在标签有噪声时。</p></li></ol><p><img src="https://i.loli.net/2020/11/09/O5fCEFvolWSPiDZ.png" alt="img"></p><p>其中超参α用于调节标准差和鲁棒差的平衡</p><p>（预训练α = 10，微调α = 1）</p><ul><li>因为有最大化操作，所以训练昂贵。</li><li>有利于embedding邻域的标签平滑。</li></ul><h4 id="算法流程">算法流程</h4><p>首先使用标准目标（1）训练模型；然后使用虚拟对抗训练（3）继续训练。</p><p><img src="https://i.loli.net/2020/11/09/mYZrkBKpTVelFsa.png" alt="image-20201108103125543"></p><p><img src="https://i.loli.net/2020/11/09/mdbNu7MXOc5WZy6.png" alt="image-20201108133930095"></p><h3 id="总结">总结</h3><p>本文提出了一种通用的对抗性训练算法ALUM：</p><ul><li><p>对抗预训练可以显著提高泛化能力和鲁棒性。</p></li><li><p>ALUM大大提高了BERT和RoBERTa在各种NLP任务中的准确性，并且可以与对抗微调相结合以获得进一步的收益。</p></li><li><p>未来的发展方向：</p><ul><li>进一步研究对抗性预训练在提高泛化和鲁棒性方面的作用；</li><li>对抗性训练加速；</li><li>将ALUM应用于其他领域。</li></ul></li></ul><p>论文提出了一个通用的模型无关的对抗训练算法架构，可以应用在任何基于transformer的语言模型中。可以尝试去结合模型</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      记录值得分享的论文;
《Adversarial Training for Large Neural Language Models》: 提出一个对抗训练算法ALUM，用于提高模型的鲁棒性。此算法可以应用在任何基于transformer的语言模型中

    
    </summary>
    
    
      <category term="论文分享" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-11-06-论文分享</title>
    <link href="http://yoursite.com/2020/11/06/2020-11-06-%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"/>
    <id>http://yoursite.com/2020/11/06/2020-11-06-%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/</id>
    <published>2020-11-06T01:56:14.000Z</published>
    <updated>2020-11-11T07:13:54.487Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2020/11/06/dEebUnIiFygwDWk.png" alt="image-20201106095828031"></p><blockquote><p>EMNLP 2020</p><p>code url (official torch) : <a href="https://github.com/ChunyuanLI/Optimus" target="_blank" rel="noopener" class="uri">https://github.com/ChunyuanLI/Optimus</a></p><p>被引用次数：9</p></blockquote><h3 id="背景">背景</h3><p>在NLP领域中，VAE是一个很有效的生成模型和表示学习框架</p><p>PLM（Pre-trained language models）一般可以分为两种。（1）基于transformer的encoder的BERT， 用于自然语言理解任务。它能够输出一个上下文的表示信息，用于下游任务；（2）基于transformer的decoder的GPT-2，用于自然语言生成任务。它能够以自回归的方式产生文本序列（机器翻译）</p><h3 id="问题">问题</h3><p>前人想着去结合语言理解任务和语言生成任务，如UniLM、T5模型，效果有提升，但是这些模型缺少一种在紧密（compact）空间（低维空间）中显式的建模，导致很难在一个abstract level去控制语言的生成和表示</p><p><img src="https://i.loli.net/2020/11/06/3hC7D1c2vMRGubZ.png" alt="image-20201106102922590"></p><p>VAE可以克服这种局限性，可以生成higher-level 的句子表示，从而控制low-level 的word-by-word generation。</p><p>但是目前VAE都是应用在浅层的模型中，例如two-layer LSTMs ，这限制了模型的表现</p><h3 id="解决">解决</h3><p>提出了OPTIMUS， the first large-scale pre-trained deep latent variable models for natural language. ，一个统一的潜在编码空间在大型文本库（large text corpus）训练完之后，在多个下游任务（自然语言理解、自然语言生成）中进行微调</p><p>以下是OPTIMUS的优点，它结合了BERT和GPT-2的优势，用于处理自然语言任务，同时相比于BERT和GPT-2，克服了它们的局限性</p><p><img src="https://i.loli.net/2020/11/06/1pkceDbh7qXGxzK.png" alt="image-20201106103607562"></p><p><img src="https://i.loli.net/2020/11/06/BbtwAayhgp8frzD.png" alt="image-20201106103657363"></p><h3 id="贡献">贡献</h3><ul><li><p>提出首个大规模预训练隐变量生成模型OPTIMUS；</p></li><li><p>高效地将隐变量和预训练的GPT-2相结合（ Latent vector injection），提出两种结合方法；</p></li><li><p>发现大规模预训练可以减缓KL Vanishing的问题；</p></li><li><p>在多个任务上取得显著的效果。</p></li></ul><h3 id="模型">模型</h3><h4 id="目标函数">目标函数</h4><p>一般的自然语言模型（如GPT-2）的生成目标，依靠前面的输出的token来预测后面的token，通常训练是通过maximum likelihood estimate (MLE). 但是这样也有局限性，前面已经提到了</p><p><img src="https://i.loli.net/2020/11/06/EN3LTgnCxiowcJ8.png" alt="image-20201106105452675"></p><p>在生成阶段（训练阶段），模型的生成基于隐变量z，对于给定的文本x，VAE的生成目标，相比于公式（1）多了一个条件z，即显式地依赖z。</p><p><strong>z是高层次的语义特征，来指导生成低层次的x，即句法和词汇</strong></p><p><img src="https://i.loli.net/2020/11/06/piNRVdgaPS3yxLI.png" alt="image-20201106105946819"></p><p>这里θ表示的是用于文本生成的<strong>解码器</strong>。而隐变量是通过一个<strong>编码器</strong>得到的，可以形式化为<img src="https://i.loli.net/2020/11/06/NMvZcI8k6R4KxUV.png" alt="image-20201106115812296"></p><p>此时的证据下界（ELBO）就是</p><p><img src="https://i.loli.net/2020/11/06/6rVTSBlHvmYnqLi.png" alt="image-20201106115847275"></p><p>在本文中，添加了一个超参β， 用于控制训练过程。</p><p>所以目标函数可以转化为如下形式，<code>Lβ</code></p><p><img src="https://i.loli.net/2020/11/06/fWcLjZQ1iEOty5F.png" alt="image-20201106120223123"></p><p><code>LE</code>是重构损失（ (or negative log-likelihood(NLL)）， <code>LR</code>是KL散度（正则项），用于让生成的z逼近先验<code>p(z)</code></p><h4 id="模型架构">模型架构</h4><p><img src="https://i.loli.net/2020/11/06/eQYDwAVuWnOs25d.png" alt="image-20201106120651825"></p><p>可以看出来，模型架构比较简单，但是一些细节也要考虑一下</p><p>基本流程如下：</p><p>1.使用预训练好的BERT和GPT-2参数，用于OPTIMUS模型encoder和decoder参数的初始化；</p><p>此时BERT(L=12,H=768,A=12,Total Parameters=110M) and GPT-2 (L=12,H=768, A=12，,Total Parameters=117M），其中L表示transformer block的层数；H表示中间隐藏层的维度；A表示自注意力头的个数</p><blockquote><p>可以发现，BERT和GPT-2的超参L、H、A都是一样的</p></blockquote><ol start="2" type="1"><li>对于初始化后的OPTIMUS，在大型文本库（large text corpus）的训练集下进行预训练</li><li>预训练完OPTIMUS，再在具体的下游任务上进行微调</li></ol><h4 id="connecting-bert-gpt-2">Connecting BERT &amp; GPT-2</h4><p>同时在连接BERT和GPT-2中，存在一些问题</p><h5 id="问题1-tokenization如何分词">问题1 Tokenization（如何分词）</h5><p>BERT和GPT-2采取了不同的分词方法，如何去表示句子？</p><h5 id="解决-1">解决</h5><p>在BERT中用的是Word Piece Embeddings (WPE)分词方法，在GPT-2中, 用的是 Byte Pair Encoding (BPE)，</p><p>本文中同时采取了两种方法</p><p><img src="https://i.loli.net/2020/11/06/rXBTJk6bOqMG5Iy.png" alt="image-20201106122651781"></p><p>等于没说。。。</p><p>是否可以统一分词方法 ？</p><h5 id="问题2-融合隐变量和gpt-2">问题2 融合隐变量和GPT-2</h5><p>如何高效地将Z融合进GPT-2中？</p><h5 id="解决-2">解决</h5><h5 id="section"></h5><p><img src="https://i.loli.net/2020/11/06/SPL4c7rhjRyznHx.png" alt="image-20201106104604195"></p><p>该如何把隐变量<code>z</code>提供给解码器呢？</p><p>本文提供两种方法，分别为记忆（Memory）和嵌入（Embedding）：</p><p><img src="https://i.loli.net/2020/11/06/2cu9DGXztrTRFLd.png" alt="image-20201106114441841"></p><p>经过实验验证，使用Memory比Embedding方法更有效，作者给出理由如下，就是Memory能够使得decoder在每一层都能直接获得潜在信息，而Embedding只能在输入输出才能获得信息。 Memory能从潜在信息中获得更多的信息用于生成任务</p><p><img src="https://i.loli.net/2020/11/06/qebzUvKSh1AOC2N.png" alt="image-20201106114209547"></p><blockquote><p><strong>在本文中，默认Memory和Embedding方法一起使用</strong></p></blockquote><h4 id="optimus的预训练">OPTIMUS的预训练</h4><p>存在一个问题，就是当VAE和auto regressive models在一起训练时，会出现“KL-vanishing problem”，或者说是“posterior collapse”</p><p><img src="https://i.loli.net/2020/11/06/pi7UOIJXy5dQqFg.png" alt="image-20201106130104016"></p><p>ELBO 包含reconstruction loss和KL loss两部分。我们的目标是最大化ELBO，等价于最小化KL项并最大化reconstruction项。存在如下问题：</p><p>问题1.对于reconstruction部分，当扮演p(x|z)角色的decoder足够强大，仅凭自己就可以model q(x)分布，那么也没有必要去依赖z。</p><p>问题2.对于KL项，如果简单的将data x和latent variable z无关并让q(z|x) = q(z) = p(z)，即posterior退化为和prior一样的高斯，<strong>KL就可以取得最小值0</strong>。</p><p>所以针对上述问题，本文在预训练的时候做了如下优化：</p><p><img src="https://i.loli.net/2020/11/06/fWcLjZQ1iEOty5F.png" alt="image-20201106120223123"></p><ul><li><p>对β使用循环调度（cyclical schedule），一共10个 periods 来加强β的作用；</p></li><li><p>每一个 period中，在训练的前一半，设置β=0 只训练encoder，避免了上述问题1；对后一半的前一半，将其增长到1，对最后四分之一，固定为1；</p></li><li><p>当β≠0的时候，加入KL thresholding scheme （KL 阈值），保证KL项始终大于一个常数 λ，这样可以避免了上述问题2，避免LR项取得最小值0。此时LR被替换为 hinge loss</p><p><img src="https://i.loli.net/2020/11/06/n3ZKDu7pEOSWaiN.png" alt="image-20201106131315827"></p></li></ul><p><img src="https://i.loli.net/2020/11/06/1VFDdL2fcsTexPj.png" alt="image-20201106130006529"></p><h3 id="实验">实验</h3><p>预训练OPTIMUS之后，剩下的就是对不同的任务进行微调了。本文在三类任务上实验：语言模型、受限文本生成和自然语言理解</p><p><img src="https://i.loli.net/2020/11/06/2YQ7lmnkhc618pV.png" alt="image-20201106110800369"></p><h3 id="总结">总结</h3><p>VAE和PLM在自然语言处理中都是很重要的部分，自然会想到将二者结合起来</p><p>总体来说，论文没太多创新点，而且在正文部分有些故弄玄虚，明明一个很简单的概念，说的让人无法理解，非要绕个弯子，可能这样会让人觉得更高深些吧，但是对于后来的研究者来说很痛苦</p><p>代码还没有看，准备再读读代码，加深模型的理解</p><h4 id="更改方向">更改方向</h4><p>1.用更好的方法去解决KL vanishing 问题</p><p>2.统一分词方法是否有更好的结果</p><p>3.使用最新的VAE方法/PLM方法去解决问题</p><p>4.结合T-CVAE框架</p><p>5.因为NLP模型普遍对于对抗攻击很敏感，所以增加对抗思想，提高模型的鲁棒性。结合论文《Adversarial Training for Large Neural Language Models》（ACL 2020）中的通用算法ALUM去解决对抗攻击的问题</p><h3 id="参考">参考</h3><blockquote><p>OPTIMUS <a href="https://zhuanlan.zhihu.com/p/143517152" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/143517152</a></p><p>KL vanishing <a href="https://zhuanlan.zhihu.com/p/64071467" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/64071467</a></p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      记录值得分享的论文;
《OPTIMUS》: 结合VAE和BERT、GPT-2,提出首个大规模预训练隐变量生成模型OPTIMUS，解决自然语言生成和理解任务

    
    </summary>
    
    
      <category term="论文分享" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-11-04-论文分享</title>
    <link href="http://yoursite.com/2020/11/04/2020-11-04-%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"/>
    <id>http://yoursite.com/2020/11/04/2020-11-04-%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/</id>
    <published>2020-11-04T13:59:43.000Z</published>
    <updated>2020-11-07T14:00:20.434Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2020/11/05/d4IuoeCYcaxO8v9.png" alt="image-20201104220110364"></p><blockquote><p>IJCAI 2019</p><p>code url (official tf ，无torch版本) : <a href="https://github.com/sodawater/T-CVAE" target="_blank" rel="noopener" class="uri">https://github.com/sodawater/T-CVAE</a></p><p>被引用次数：12</p></blockquote><h3 id="背景">背景</h3><p>故事补全是一个非常具有挑战性的任务，即为一个不完整的故事生成缺失的情节。</p><p>它涉及两个方面：<strong>理解和生成</strong>。故事理解包括识别人物角色等。生成是基于理解的下一步，即根据给定故事中的线索进行推理。一个好的故事情节应该是有意义的和连贯的上下文。此外，输入文本的不连续性使得理解和生成更加困难。</p><p>本模型的任务：给定一个故事的任何四个句子，目标是生成缺失的句子，即缺失的情节，来完成这个故事。 （基于ROCStories的常识故事语料库）</p><p><img src="https://i.loli.net/2020/11/05/iSonHTZ7I5g9Atz.png" alt="根据缺失的故事，不同的补全句子"></p><h3 id="问题">问题</h3><p>前人的研究都关注于为不完整的故事选择或产生一个合理的结局。这些任务是我们故事完成任务的特殊化，因此先前的方法不适合生成故事的开始或中间情节。并且倾向于生成泛型和非连贯性的情节。</p><p>1.如何去补全缺失中间部分的故事 ？</p><p>2.怎样使补全的句子是<strong>有意义的、连贯的、多样的</strong> ？</p><h3 id="解决">解决</h3><p>我们提出了一种新的<strong>基于transformer的条件变量自动编码模型</strong>（T-CVAE）</p><p>1.Transformer：作为模型基础，并采用了一个改进的<strong>具有共享自我注意层</strong>的Transformer，这种共享自我注意层能够使解码器同时关注到编码器和解码器的状态，以此能够使模型获取更多的上下文线索。</p><p>2.条件变分自编码模型：提高生成的多样性和一致性</p><h3 id="贡献">贡献</h3><p><img src="https://i.loli.net/2020/11/05/aH6sB72CvqQTPoj.png" alt="贡献"></p><p>可以看出，并没有太多的贡献。</p><h3 id="模型">模型</h3><h4 id="模型流程">模型流程</h4><p><img src="https://i.loli.net/2020/11/05/3HsRKVLjtPhfcx2.png" alt="image-20201105104914786"></p><p>数据在encoder和decoder的流动</p><p><img src="https://i.loli.net/2020/11/05/iJFKlqYbGe3CnjO.png" alt="image-20201105105933096"></p><h4 id="后验网络-pzxy">后验网络 P(z|x,y)</h4><p><img src="https://i.loli.net/2020/11/05/b7exp5tuoq4W6V3.png" alt="image-20201105105357954"></p><h4 id="先验网络pzx">先验网络P(z|x)</h4><p><img src="https://i.loli.net/2020/11/05/TxVdC3bBXFpjnUY.png" alt="image-20201105105419533"></p><h4 id="组合层combination-layer">组合层（combination layer）</h4><p>并不是利用z来直接初始化decoder的状态，而是利用了组合层。之后再经过Linear层和softmax层输出最终的预测</p><p><img src="https://i.loli.net/2020/11/05/owdjV8YQah6eqpB.png" alt="image-20201105105628307"></p><h4 id="目标函数">目标函数</h4><p>由于z上的积分是难以解决的，因此我们应用变分推理并优化相应的证据下限（ELBO）：</p><p><img src="https://i.loli.net/2020/11/05/I3iZ2EebhXOzyTk.png" alt="image-20201105104715312"></p><p><img src="https://i.loli.net/2020/11/05/ACP2tKvEyraOoJh.png" alt="image-20201105104837678"></p><p>训练目标：</p><p>1.最大化重构y的概率，这样可以使得后验网络和情节生成器 ( p(y|x,z) ) 做出的预测更加接近于标准值 ；</p><p>2.最小化z的先验分布和后验分布的KL散度，这样当标准值不存在的时候（推理），可以促使先验网络去产生合理的概率分布</p><h3 id="实验">实验</h3><p><img src="https://i.loli.net/2020/11/05/2KNSnML9rlE6dvT.png" alt="image-20201104222356336"></p><p><img src="https://i.loli.net/2020/11/05/9z3pS2onE6vlBgy.png" alt="消融实验"></p><p><img src="https://i.loli.net/2020/11/05/ahGK4kXudPfMU25.png" alt="image-20201104222411554"></p><h3 id="总结">总结</h3><p>总体来说，论文最大的贡献就是将CVAE和transformer结合了起来，处理故事补全的问题，transformer模型上并没有太大的创新。但是VAE和transformer系列模型结合的思路值得我去借鉴。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      记录值得分享的论文;
《T-CVAE》 :　结合VAE和transformer，提出基于transformer的条件变量自动编码模型（T-CVAE），用于解决故事补全的任务

    
    </summary>
    
    
      <category term="论文分享" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-10-31-注意力机制总结</title>
    <link href="http://yoursite.com/2020/10/31/2020-10-31-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2020/10/31/2020-10-31-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%80%BB%E7%BB%93/</id>
    <published>2020-10-31T02:19:17.000Z</published>
    <updated>2020-10-31T08:27:16.216Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言">前言</h3><p>注意力模型最近几年在深度学习各个领域被广泛使用，无论是图像处理、语音识别还是自然语言处理的各种不同类型的任务中，都很容易遇到注意力模型的身影。所以，了解注意力机制的工作原理对于关注深度学习技术发展的技术人员来说有很大的必要。</p><blockquote><p>RNN做机器翻译有它自身的弱点，Attention正是为了克服这个弱点而出现的。所以，要理解Attention，就要搞明白两件事： - RNN在做机器翻译时有什么弱点 - Attention是如何克服这个弱点的</p></blockquote><h3 id="rnn做机器翻译的经典思路-encoder-decoder">RNN做机器翻译的经典思路 encoder-decoder</h3><p>用RNN做机器翻译时，通常需要两个RNN网络，一个用来将接收待翻译语句，对其进行编码，最后输出一个vector，这个网络叫encoder。然后，该vector会作为输入，传给另一个RNN网络，该网络用来根据vector产生目标语言的翻译语句，这个网络叫做decoder。如下图所示：</p><p><img src="https://i.loli.net/2020/10/31/UOF2qxEoRB7ftsP.png" alt="image-20201031104117636"></p><p>上图中间的Context就是我们这里说的第一个RNN产生的vector</p><h3 id="encoder-decoder的缺点在哪里">encoder-decoder的缺点在哪里？</h3><p>encoder-decoder最大的缺点是，encoder接收了不管多长的语句，最后输出的只是最后一个vector，当语句很长时，这个vector能否有效地表示该语句是很值得怀疑的。 如何解决这个问题呢？我们很自然会想到，第一个RNN其实在中间会产生很多输出，这些输出都被我们抛弃了，我们只用了最后的一个。如果能利用上中间的输出，兴许可以解决问题。Attention正是利用上了这些中间的输出。</p><h3 id="attention是如何利用中间的输出的">Attention是如何利用中间的输出的</h3><p>先上图，再来解释：</p><p><img src="https://i.loli.net/2020/10/31/ixs6baohzDmjfPn.png" alt="image-20201031104152681"></p><p>上图中的A是我们的encoder， B是我们的decoder。 可以想象，A网络接收了一个四个字的句子，对每个字都产生了一个输出（这些输出都是一个vector），我们称其为s1，s2，s3，s4。</p><p>我们看上图的B网络，在第一个B产生的hidden state（称其为h1）除了传给下一个cell外，还传到了A网络，这里就是Attention发挥作用的地方，我们来看看发生了什么。</p><p><strong>第一步</strong>： h1 分别与s1，s2，s3，s4做点积，产生了四个数，称其为m1，m2，m3，m4（这些都是标量，不是向量了！）</p><p><strong>第二步</strong>： m1，m2，m3，m4 传到一个softmax层，产生一个概率分布a1，a2，a3， a4。</p><p><strong>第三步</strong>： 将a1，a2，a3， a4 与s1，s2，s3，s4分别相乘，再相加，得到得到一个vector，称其为Attention vector。</p><p><strong>第四步</strong>：</p><p>Attention vector 将作为输入传到B网络的第二个cell中，参与预测。</p><p>以上就是Attention机制的基本思想了。我们看到，Attention vector 实际上融合了s1，s2，s3，s4的信息，具体的融合是用一个概率分布来达到的，而这个概率分布又是通过B网络上一个cell的hidden state与s1，s2，s3，s4进行点乘得到的。 Attention vector实际上达到了让B网络聚焦于A网络输出的某一部分的作用。</p><h3 id="attention中产生概率分布的两种方法">Attention中产生概率分布的两种方法</h3><p>在第3部分中，我们的概率分布来自于h与s的点积再做softmax，这只是最基本的方式。在实际中，我们可以有不同的方法来产生这个概率分布，每一种方法都代表了一种具体的Attention机制。</p><ul><li><h4 id="加法attention">1 加法Attention</h4><p>在加法Attention中，我们不再让h与s做点积，而是做如下的运算：</p><p><img src="https://i.loli.net/2020/10/31/dOAr8BHlK25vQuU.png" alt="image-20201031104221351"></p></li></ul><p>va和Wa都是可以训练的参数。h与s之间的分号表示将二者接到一起产生一个更长的vector。这样产生的数再送往softmax层，进而产生一个概率分布。</p><p>当然，我们还可以这么做：</p><p><img src="https://i.loli.net/2020/10/31/Hu4eWOfmQLk9DrK.png" alt="image-20201031104229311"></p><p>这里只是不再把h与s接到一起而已，本质上没有什么区别的。</p><ul><li><h4 id="乘法attention">2 乘法Attention</h4><p>乘法Attention将h与s做如下的运算：</p><p><img src="https://i.loli.net/2020/10/31/V87BZWl3sMUeGvb.png" alt="image-20201031104239592"></p></li></ul><p>显然，乘法Attention的参数更少，效率自然也会更高一些。</p><h3 id="attention机制的扩展">Attention机制的扩展</h3><p>Attention机制的核心在于对一个序列数据进行聚焦，这个聚焦是通过一个概率分布来实现的。这种机制其实有很强的普适性，可以用在各个方面。</p><p>比如，根据图片产生描述该图片的文字， 首先，图片会经过CNN进行特征的提取，提取的数据会输入到产生描述文字的RNN中，这里，我们可以引入Attention机制，让我们在产生下一个文字时，聚焦于我们正在描述的图片部位。</p><p>其次，在句子表示中，self Attention机制是成功扩展的Attention的范例。其基本原理如下：</p><p>假如我们用一个RNN读入了一个句子，产生了h1， h2，h3，h4四个hidden state。 为了得到该句子的摘要，我们可以这样做： 对每一个h计算一个分数：</p><p><img src="https://i.loli.net/2020/10/31/7ltIKSuAZFiBzQn.png" alt="image-20201031104249976"></p><p>四个h共产生了4个分数，将这四个分数送入一个softmax层，产生一个概率分布，根据这个概率分布对四个h进行加和，得到句子摘要的第一个vector。如下图所示：</p><p><img src="https://i.loli.net/2020/10/31/x2DGOb9jBoNJweq.png" alt="image-20201031104257776"></p><p>为了得到更多的vector，我们可以把上面图中的小写va换成一个矩阵，然后，我们的a也就变成了多个概率分布组成的矩阵，每个概率分布都可以用来与h进行加和产生一个vector，这样我们就产生了摘要的多个vector，如下图所示：</p><p><img src="https://i.loli.net/2020/10/31/9esynl6cDKfmEiS.png" alt="image-20201031104320465"></p><h3 id="人类的视觉注意力">人类的视觉注意力</h3><p>从注意力模型的命名方式看，很明显其借鉴了人类的注意力机制，因此，我们首先简单介绍人类视觉的选择性注意力机制。</p><p><img src="https://i.loli.net/2020/10/31/516P2JBYrgOToyU.jpg" alt="人类的视觉注意力"></p><p>视觉注意力机制是人类视觉所特有的大脑信号处理机制。人类视觉通过快速扫描全局图像，获得需要重点关注的目标区域，也就是一般所说的注意力焦点，而后对这一区域投入更多注意力资源，以获取更多所需要关注目标的细节信息，而抑制其他无用信息。</p><p>这是人类利用有限的注意力资源从大量信息中快速筛选出高价值信息的手段，是人类在长期进化中形成的一种生存机制，人类视觉注意力机制极大地提高了视觉信息处理的效率与准确性。</p><p>图1形象化展示了人类在看到一副图像时是如何高效分配有限的注意力资源的，其中红色区域表明视觉系统更关注的目标，很明显对于图1所示的场景，人们会把注意力更多投入到人的脸部，文本的标题以及文章首句等位置。</p><p>深度学习中的注意力机制从本质上讲和人类的选择性视觉注意力机制类似，核心目标也是从众多信息中选择出对当前任务目标更关键的信息。</p><h3 id="encoder-decoder框架">Encoder-Decoder框架</h3><p>要了解深度学习中的注意力模型，就不得不先谈Encoder-Decoder框架，因为目前大多数注意力模型附着在Encoder-Decoder框架下，当然，其实注意力模型可以看作一种通用的思想，本身并不依赖于特定框架，这点需要注意。</p><p>Encoder-Decoder框架可以看作是一种深度学习领域的研究模式，应用场景异常广泛。图2是文本处理领域里常用的Encoder-Decoder框架最抽象的一种表示。</p><p><img src="https://i.loli.net/2020/10/31/ohBVdz1KDn4ZcOb.png" alt="抽象的文本处理领域的Encoder-Decoder框架"></p><p>文本处理领域的Encoder-Decoder框架可以这么直观地去理解：可以把它看作适合处理由一个句子（或篇章）生成另外一个句子（或篇章）的通用处理模型。对于句子对&lt;Source,Target&gt;，我们的目标是给定输入句子Source，期待通过Encoder-Decoder框架来生成目标句子Target。Source和Target可以是同一种语言，也可以是两种不同的语言。而Source和Target分别由各自的单词序列构成：</p><p><img src="https://i.loli.net/2020/10/31/qrCwtas6iRVKYbA.png" alt="img"></p><p>Encoder顾名思义就是对输入句子Source进行编码，将输入句子通过非线性变换转化为中间语义表示C：</p><p><img src="https://i.loli.net/2020/10/31/M7EXx2KPgeHFQhL.png" alt="img"></p><p>对于解码器Decoder来说，其任务是根据句子Source的中间语义表示C和之前已经生成的历史信息 y1,y2,y3...yi-1来生成i时刻要生成的单词yi：</p><p><img src="https://i.loli.net/2020/10/31/6uHkShXDNKOlBQ3.png" alt="img"></p><p>每个yi都依次这么产生，那么看起来就是整个系统根据输入句子Source生成了目标句子Target。如果Source是中文句子，Target是英文句子，那么这就是解决机器翻译问题的Encoder-Decoder框架；如果Source是一篇文章，Target是概括性的几句描述语句，那么这是文本摘要的Encoder-Decoder框架；如果Source是一句问句，Target是一句回答，那么这是问答系统或者对话机器人的Encoder-Decoder框架。由此可见，在文本处理领域，Encoder-Decoder的应用领域相当广泛。</p><p>Encoder-Decoder框架不仅仅在文本领域广泛使用，在语音识别、图像处理等领域也经常使用。比如对于语音识别来说，图2所示的框架完全适用，区别无非是Encoder部分的输入是语音流，输出是对应的文本信息；而对于“图像描述”任务来说，Encoder部分的输入是一副图片，Decoder的输出是能够描述图片语义内容的一句描述语。一般而言，文本处理和语音识别的Encoder部分通常采用RNN模型，图像处理的Encoder一般采用CNN模型。</p><h3 id="attention模型">Attention模型</h3><p>本节先以机器翻译作为例子讲解最常见的Soft Attention模型的基本原理，之后抛离Encoder-Decoder框架抽象出了注意力机制的本质思想，然后简单介绍最近广为使用的Self Attention的基本思路。</p><h4 id="soft-attention模型">Soft Attention模型</h4><p>图2中展示的Encoder-Decoder框架是没有体现出“注意力模型”的，所以可以把它看作是注意力不集中的分心模型。为什么说它注意力不集中呢？请观察下目标句子Target中每个单词的生成过程如下：</p><p><img src="https://i.loli.net/2020/10/31/KNWS6Ax1uI285cH.png" alt="img"></p><p>其中f是Decoder的非线性变换函数。从这里可以看出，在生成目标句子的单词时，不论生成哪个单词，它们使用的输入句子Source的语义编码C都是一样的，没有任何区别。</p><p>而语义编码C是由句子Source的每个单词经过Encoder</p><p>编码产生的，这意味着不论是生成哪个单词，y1,y2还是y3，其实句子Source中任意单词对生成某个目标单词yi来说影响力都是相同的，这是为何说这个模型没有体现出注意力的缘由。这类似于人类看到眼前的画面，但是眼中却没有注意焦点一样。</p><p>如果拿机器翻译来解释这个分心模型的Encoder-Decoder框架更好理解，比如输入的是英文句子：Tom chase Jerry，Encoder-Decoder框架逐步生成中文单词：“汤姆”，“追逐”，“杰瑞”。</p><p>在翻译“杰瑞”这个中文单词的时候，分心模型里面的每个英文单词对于翻译目标单词“杰瑞”贡献是相同的，很明显这里不太合理，显然“Jerry”对于翻译成“杰瑞”更重要，但是分心模型是无法体现这一点的，这就是为何说它没有引入注意力的原因。</p><p>没有引入注意力的模型在输入句子比较短的时候问题不大，但是如果输入句子比较长，此时所有语义完全通过一个中间语义向量来表示，单词自身的信息已经消失，可想而知会丢失很多细节信息，这也是为何要引入注意力模型的重要原因。</p><p>上面的例子中，如果引入Attention模型的话，应该在翻译“杰瑞”的时候，体现出英文单词对于翻译当前中文单词不同的影响程度，比如给出类似下面一个概率分布值：</p><p>（Tom,0.3）(Chase,0.2) (Jerry,0.5)</p><p>每个英文单词的概率代表了翻译当前单词“杰瑞”时，注意力分配模型分配给不同英文单词的注意力大小。这对于正确翻译目标语单词肯定是有帮助的，因为引入了新的信息。</p><p>同理，目标句子中的每个单词都应该学会其对应的源语句子中单词的注意力分配概率信息。这意味着在生成每个单词yi的时候，原先都是相同的中间语义表示C会被替换成根据当前生成单词而不断变化的Ci。理解Attention模型的关键就是这里，即由固定的中间语义表示C换成了根据当前输出单词来调整成加入注意力模型的变化的Ci。增加了注意力模型的Encoder-Decoder框架理解起来如图3所示。</p><p><img src="https://i.loli.net/2020/10/31/oe6E7C9sUHrVuNZ.png" alt="引入注意力模型的Encoder-Decoder框架"></p><p>即生成目标句子单词的过程成了下面的形式：</p><p><img src="https://i.loli.net/2020/10/31/q8ZFEu4BSI1lo9z.png" alt="img"></p><p>而每个Ci可能对应着不同的源语句子单词的注意力分配概率分布，比如对于上面的英汉翻译来说，其对应的信息可能如下：</p><p><img src="https://i.loli.net/2020/10/31/TBg8KZhoyiOlUst.png" alt="img"></p><p>其中，f2函数代表Encoder对输入英文单词的某种变换函数，比如如果Encoder是用的RNN模型的话，这个f2函数的结果往往是某个时刻输入xi后隐层节点的状态值；g代表Encoder根据单词的中间表示合成整个句子中间语义表示的变换函数，一般的做法中，g函数就是对构成元素加权求和，即下列公式：</p><p><img src="https://i.loli.net/2020/10/31/W9SnjkNw3BVasdc.png" alt="img"></p><p>其中，Lx代表输入句子Source的长度，aij代表在Target输出第i个单词时Source输入句子中第j个单词的注意力分配系数，而hj则是Source输入句子中第j个单词的语义编码。假设下标i就是上面例子所说的“ 汤姆” ，那么Lx就是3，h1=f(“Tom”)，h2=f(“Chase”),h3=f(“Jerry”)分别是输入句子每个单词的语义编码，对应的注意力模型权值则分别是0.6,0.2,0.2，所以g函数本质上就是个加权求和函数。如果形象表示的话，翻译中文单词“汤姆”的时候，数学公式对应的中间语义表示Ci的形成过程类似图4。</p><p><img src="https://i.loli.net/2020/10/31/LRJZMaUbN9VuxGr.png" alt="Attention的形成过程"></p><p>这里还有一个问题：生成目标句子某个单词，比如“汤姆”的时候，如何知道Attention模型所需要的输入句子单词注意力分配概率分布值呢？就是说“汤姆”对应的输入句子Source中各个单词的概率分布：(Tom,0.6)(Chase,0.2) (Jerry,0.2) 是如何得到的呢？</p><p>为了便于说明，我们假设对图2的非Attention模型的Encoder-Decoder框架进行细化，Encoder采用RNN模型，Decoder也采用RNN模型，这是比较常见的一种模型配置，则图2的框架转换为图5。</p><p><img src="https://i.loli.net/2020/10/31/ebTh6NzIdlVG1L2.png" alt="RNN作为具体模型的Encoder-Decoder框架"></p><p>那么用下图可以较为便捷地说明注意力分配概率分布值的通用计算过程。</p><p><img src="https://i.loli.net/2020/10/31/TEXoxdzgrHOuiMQ.jpg" alt="注意力分配概率计算"></p><p>对于采用RNN的Decoder来说，在时刻i，如果要生成yi单词，我们是可以知道Target在生成Yi之前的时刻i-1时，隐层节点i-1时刻的输出值Hi-1的，而我们的目的是要计算生成Yi时输入句子中的单词“Tom”、“Chase”、“Jerry”对Yi来说的注意力分配概率分布，那么可以用Target输出句子i-1时刻的隐层节点状态Hi-1去一一和输入句子Source中每个单词对应的RNN隐层节点状态hj进行对比，即通过函数F(hj,Hi-1)来获得目标单词yi和每个输入单词对应的对齐可能性，这个F函数在不同论文里可能会采取不同的方法，然后函数F的输出经过Softmax进行归一化就得到了符合概率分布取值区间的注意力分配概率分布数值。</p><p>绝大多数Attention模型都是采取上述的计算框架来计算注意力分配概率分布信息，区别只是在F的定义上可能有所不同。图7可视化地展示了在英语-德语翻译系统中加入Attention机制后，Source和Target两个句子每个单词对应的注意力分配概率分布。</p><p><img src="https://i.loli.net/2020/10/31/T1tNMbc6DE3HkGQ.png" alt="英语-德语翻译的注意力概率分布"></p><p>上述内容就是经典的Soft Attention模型的基本思想，那么怎么理解Attention模型的物理含义呢？一般在自然语言处理应用里会把Attention模型看作是输出Target句子中某个单词和输入Source句子每个单词的对齐模型，这是非常有道理的。</p><p>目标句子生成的每个单词对应输入句子单词的概率分布可以理解为输入句子单词和这个目标生成单词的对齐概率，这在机器翻译语境下是非常直观的：传统的统计机器翻译一般在做的过程中会专门有一个短语对齐的步骤，而注意力模型其实起的是相同的作用。</p><p><img src="https://i.loli.net/2020/10/31/zYmvXW8pHiO2VUo.jpg" alt="Google 神经网络机器翻译系统结构图"></p><p>图8所示即为Google于2016年部署到线上的基于神经网络的机器翻译系统，相对传统模型翻译效果有大幅提升，翻译错误率降低了60%，其架构就是上文所述的加上Attention机制的Encoder-Decoder框架，主要区别无非是其Encoder和Decoder使用了8层叠加的LSTM模型。</p><h4 id="attention机制的本质思想">Attention机制的本质思想</h4><p>如果把Attention机制从上文讲述例子中的Encoder-Decoder框架中剥离，并进一步做抽象，可以更容易看懂Attention机制的本质思想。</p><p><img src="https://i.loli.net/2020/10/31/y3BToK96rsgUZfF.png" alt="Attention机制的本质思想"></p><p>我们可以这样来看待Attention机制（参考图9）：将Source中的构成元素想象成是由一系列的&lt;Key,Value&gt;数据对构成，此时给定Target中的某个元素Query，通过计算Query和各个Key的相似性或者相关性，得到每个Key对应Value的权重系数，然后对Value进行加权求和，即得到了最终的Attention数值。所以本质上Attention机制是对Source中元素的Value值进行加权求和，而Query和Key用来计算对应Value的权重系数。即可以将其本质思想改写为如下公式：</p><p><img src="https://i.loli.net/2020/10/31/ipGlzuFcmS8n2VR.png" alt="img"></p><p>其中，Lx=||Source||代表Source的长度，公式含义即如上所述。上文所举的机器翻译的例子里，因为在计算Attention的过程中，Source中的Key和Value合二为一，指向的是同一个东西，也即输入句子中每个单词对应的语义编码，所以可能不容易看出这种能够体现本质思想的结构。</p><p>当然，从概念上理解，把Attention仍然理解为从大量信息中有选择地筛选出少量重要信息并聚焦到这些重要信息上，忽略大多不重要的信息，这种思路仍然成立。聚焦的过程体现在权重系数的计算上，权重越大越聚焦于其对应的Value值上，即权重代表了信息的重要性，而Value是其对应的信息。</p><p>从上图可以引出另外一种理解，也可以将Attention机制看作一种软寻址（Soft Addressing）:Source可以看作存储器内存储的内容，元素由地址Key和值Value组成，当前有个Key=Query的查询，目的是取出存储器中对应的Value值，即Attention数值。通过Query和存储器内元素Key的地址进行相似性比较来寻址，之所以说是软寻址，指的不像一般寻址只从存储内容里面找出一条内容，而是可能从每个Key地址都会取出内容，取出内容的重要性根据Query和Key的相似性来决定，之后对Value进行加权求和，这样就可以取出最终的Value值，也即Attention值。所以不少研究人员将Attention机制看作软寻址的一种特例，这也是非常有道理的。</p><p>至于Attention机制的具体计算过程，如果对目前大多数方法进行抽象的话，可以将其归纳为两个过程：第一个过程是根据Query和Key计算权重系数，第二个过程根据权重系数对Value进行加权求和。而第一个过程又可以细分为两个阶段：第一个阶段根据Query和Key计算两者的相似性或者相关性；第二个阶段对第一阶段的原始分值进行归一化处理；这样，可以将Attention的计算过程抽象为如图10展示的三个阶段。</p><p><img src="https://i.loli.net/2020/10/31/tzO9fhXUeBblmVG.png" alt="三阶段计算Attention过程"></p><p>在第一个阶段，可以引入不同的函数和计算机制，根据Query和某个Key_i，计算两者的相似性或者相关性，最常见的方法包括：求两者的向量点积、求两者的向量Cosine相似性或者通过再引入额外的神经网络来求值，即如下方式：</p><p><img src="https://i.loli.net/2020/10/31/9xpPOa7ohFf3u1d.png" alt="img"></p><p>第一阶段产生的分值根据具体产生的方法不同其数值取值范围也不一样，第二阶段引入类似SoftMax的计算方式对第一阶段的得分进行数值转换，一方面可以进行归一化，将原始计算分值整理成所有元素权重之和为1的概率分布；另一方面也可以通过SoftMax的内在机制更加突出重要元素的权重。即一般采用如下公式计算：</p><p><img src="https://i.loli.net/2020/10/31/XFW5tcSGjqBnIyN.png" alt="img"></p><p>第二阶段的计算结果a_i即为value_i对应的权重系数，然后进行加权求和即可得到Attention数值：</p><p><img src="https://i.loli.net/2020/10/31/soa1M9LIPGi3krC.png" alt="img"></p><p>通过如上三个阶段的计算，即可求出针对Query的Attention数值，目前绝大多数具体的注意力机制计算方法都符合上述的三阶段抽象计算过程。</p><h4 id="self-attention模型">Self Attention模型</h4><p>通过上述对Attention本质思想的梳理，我们可以更容易理解本节介绍的Self Attention模型。Self Attention也经常被称为intra Attention（内部Attention），最近一年也获得了比较广泛的使用，比如Google最新的机器翻译模型内部大量采用了Self Attention模型。</p><p>在一般任务的Encoder-Decoder框架中，输入Source和输出Target内容是不一样的，比如对于英-中机器翻译来说，Source是英文句子，Target是对应的翻译出的中文句子，Attention机制发生在Target的元素Query和Source中的所有元素之间。而Self</p><p>Attention顾名思义，指的不是Target和Source之间的Attention机制，而是Source内部元素之间或者Target内部元素之间发生的Attention机制，也可以理解为Target=Source这种特殊情况下的注意力计算机制。其具体计算过程是一样的，只是计算对象发生了变化而已，所以此处不再赘述其计算过程细节。</p><p>如果是常规的Target不等于Source情形下的注意力计算，其物理含义正如上文所讲，比如对于机器翻译来说，本质上是目标语单词和源语单词之间的一种单词对齐机制。那么如果是Self Attention机制，一个很自然的问题是：通过Self Attention到底学到了哪些规律或者抽取出了哪些特征呢？或者说引入Self Attention有什么增益或者好处呢？我们仍然以机器翻译中的Self Attention来说明，图11和图12是可视化地表示Self Attention在同一个英语句子内单词间产生的联系。</p><p><img src="https://i.loli.net/2020/10/31/BqbvNSUWyrnTjt4.jpg" alt="可视化Self Attention实例"></p><p><img src="https://i.loli.net/2020/10/31/q2iCXflnh5oH1WE.jpg" alt="可视化Self Attention实例"></p><p>从两张图（图11、图12）可以看出，Self Attention可以捕获同一个句子中单词之间的一些句法特征（比如图11展示的有一定距离的短语结构）或者语义特征（比如图12展示的its的指代对象Law）。</p><p>很明显，引入Self Attention后会更容易捕获句子中长距离的相互依赖的特征，因为如果是RNN或者LSTM，需要依次序序列计算，对于远距离的相互依赖的特征，要经过若干时间步步骤的信息累积才能将两者联系起来，而距离越远，有效捕获的可能性越小。</p><p>但是Self Attention在计算过程中会直接将句子中任意两个单词的联系通过一个计算步骤直接联系起来，所以远距离依赖特征之间的距离被极大缩短，有利于有效地利用这些特征。除此外，Self Attention对于增加计算的并行性也有直接帮助作用。这是为何Self Attention逐渐被广泛使用的主要原因。</p><h4 id="attention机制的应用">Attention机制的应用</h4><p>前文有述，Attention机制在深度学习的各种应用领域都有广泛的使用场景。上文在介绍过程中我们主要以自然语言处理中的机器翻译任务作为例子，下面分别再从图像处理领域和语音识别选择典型应用实例来对其应用做简单说明。</p><p><img src="https://i.loli.net/2020/10/31/Yog7WcVFCXbRv3U.jpg" alt="图片-描述任务的Encoder-Decoder框架"></p><p>图片描述（Image-Caption）是一种典型的图文结合的深度学习应用，输入一张图片，人工智能系统输出一句描述句子，语义等价地描述图片所示内容。很明显这种应用场景也可以使用Encoder-Decoder框架来解决任务目标，此时Encoder输入部分是一张图片，一般会用CNN来对图片进行特征抽取，Decoder部分使用RNN或者LSTM来输出自然语言句子（参考上图）。</p><p>此时如果加入Attention机制能够明显改善系统输出效果，Attention模型在这里起到了类似人类视觉选择性注意的机制，在输出某个实体单词的时候会将注意力焦点聚焦在图片中相应的区域上。图14给出了根据给定图片生成句子“A person is standing on a beach with a surfboard.”过程时每个单词对应图片中的注意力聚焦区域。</p><p><img src="https://i.loli.net/2020/10/31/RGVXYj8W2yQsqtz.jpg" alt="图片生成句子中每个单词时的注意力聚焦区域"></p><p>下图给出了另外四个例子形象地展示了这种过程，每个例子上方左侧是输入的原图，下方句子是人工智能系统自动产生的描述语句，上方右侧图展示了当AI系统产生语句中划横线单词的时候，对应图片中聚焦的位置区域。比如当输出单词dog的时候，AI系统会将注意力更多地分配给图片中小狗对应的位置。</p><p><img src="https://i.loli.net/2020/10/31/v7Eubye6A4dSLZp.jpg" alt="图像描述任务中Attention机制的聚焦作用"></p><p><img src="https://i.loli.net/2020/10/31/Yt5fPBJWRAaUHNv.jpg" alt="语音识别中音频序列和输出字符之间的Attention"></p><p>语音识别的任务目标是将语音流信号转换成文字，所以也是Encoder-Decoder的典型应用场景。Encoder部分的Source输入是语音流信号，Decoder部分输出语音对应的字符串流。</p><p>上图可视化地展示了在Encoder-Decoder框架中加入Attention机制后，当用户用语音说句子 how much would a woodchuck chuck 时，输入部分的声音特征信号和输出字符之间的注意力分配概率分布情况，颜色越深代表分配到的注意力概率越高。从图中可以看出，在这个场景下，Attention机制起到了将输出字符和输入语音信号进行对齐的功能。</p><p>上述内容仅仅选取了不同AI领域的几个典型Attention机制应用实例，Encoder-Decoder加Attention架构由于其卓越的实际效果，目前在深度学习领域里得到了广泛的使用，了解并熟练使用这一架构对于解决实际问题会有极大帮助。</p><h3 id="总结">总结</h3><p>通过以上的内容，我们了解到，Attention机制最初用来克服RNN做机器翻译时的缺点，然后，人们发现，Attention机制具有广泛的适用性，于是它又被扩展到了产生图片描述，做句子摘要等任务上。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      注意力机制模型的总结
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-10-30-从embedding到BERT预训练模型</title>
    <link href="http://yoursite.com/2020/10/30/2020-10-30-%E4%BB%8Eembedding%E5%88%B0BERT%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"/>
    <id>http://yoursite.com/2020/10/30/2020-10-30-%E4%BB%8Eembedding%E5%88%B0BERT%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/</id>
    <published>2020-10-30T11:53:49.000Z</published>
    <updated>2020-10-30T12:38:47.914Z</updated>
    
    <content type="html"><![CDATA[<p>Bert最近很火，应该是最近最火爆的AI进展，网上的评价很高，那么Bert值得这么高的评价吗？我个人判断是值得。那为什么会有这么高的评价呢？是因为它有重大的理论或者模型创新吗？其实并没有，从模型创新角度看一般，创新不算大。但是架不住效果太好了，基本刷新了很多NLP的任务的最好性能，有些任务还被刷爆了，这个才是关键。另外一点是Bert具备广泛的通用性，就是说绝大部分NLP任务都可以采用类似的两阶段模式直接去提升效果，这个第二关键。客观的说，把Bert当做最近两年NLP重大进展的集大成者更符合事实。</p><p>本文的主题是自然语言处理中的预训练过程，会大致说下NLP中的预训练技术是一步一步如何发展到Bert模型的，从中可以很自然地看到Bert的思路是如何逐渐形成的，Bert的历史沿革是什么，继承了什么，创新了什么，为什么效果那么好，主要原因是什么，以及为何说模型创新不算太大，为何说Bert是近年来NLP重大进展的集大成者。我们一步一步来讲，而串起来这个故事的脉络就是自然语言的预训练过程，但是落脚点还是在Bert身上。要讲自然语言的预训练，得先从图像领域的预训练说起。</p><h3 id="图像领域的预训练">图像领域的预训练</h3><p>自从深度学习火起来后，预训练过程就是做图像或者视频领域的一种比较常规的做法，有比较长的历史了，而且这种做法很有效，能明显促进应用的效果。</p><p><img src="https://i.loli.net/2020/10/30/HSZgfVhv5cAO2Qt.jpg" alt="img"></p><p>那么图像领域怎么做预训练呢，上图展示了这个过程，我们设计好网络结构以后，对于图像来说一般是CNN的多层叠加网络结构，可以先用某个训练集合比如训练集合A或者训练集合B对这个网络进行预先训练，在A任务上或者B任务上学会网络参数，然后存起来以备后用。假设我们面临第三个任务C，网络结构采取相同的网络结构，在比较浅的几层CNN结构，网络参数初始化的时候可以加载A任务或者B任务学习好的参数，其它CNN高层参数仍然随机初始化。之后我们用C任务的训练数据来训练网络，此时有两种做法，一种是浅层加载的参数在训练C任务过程中不动，这种方法被称为“Frozen”;另外一种是底层网络参数尽管被初始化了，在C任务训练过程中仍然随着训练的进程不断改变，这种一般叫“Fine-Tuning”，顾名思义，就是更好地把参数进行调整使得更适应当前的C任务。一般图像或者视频领域要做预训练一般都这么做。</p><p>这么做有几个好处，首先，如果手头任务C的训练集合数据量较少的话，现阶段的好用的CNN比如Resnet/Densenet/Inception等网络结构层数很深，几百万上千万参数量算起步价，上亿参数的也很常见，训练数据少很难很好地训练这么复杂的网络，但是如果其中大量参数通过大的训练集合比如ImageNet预先训练好直接拿来初始化大部分网络结构参数，然后再用C任务手头比较可怜的数据量上Fine-tuning过程去调整参数让它们更适合解决C任务，那事情就好办多了。这样原先训练不了的任务就能解决了，即使手头任务训练数据也不少，加个预训练过程也能极大加快任务训练的收敛速度，所以这种预训练方式是老少皆宜的解决方案，另外疗效又好，所以在做图像处理领域很快就流行开来。</p><p>那么新的问题来了，为什么这种预训练的思路是可行的？</p><p><img src="https://i.loli.net/2020/10/30/hnsrCeotdP4jLSq.jpg" alt="img"></p><p>目前我们已经知道，对于层级的CNN结构来说，不同层级的神经元学习到了不同类型的图像特征，由底向上特征形成层级结构，如上图所示，如果我们手头是个人脸识别任务，训练好网络后，把每层神经元学习到的特征可视化肉眼看一看每层学到了啥特征，你会看到最底层的神经元学到的是线段等特征，图示的第二个隐层学到的是人脸五官的轮廓，第三层学到的是人脸的轮廓，通过三步形成了特征的层级结构，越是底层的特征越是所有不论什么领域的图像都会具备的比如边角线弧线等底层基础特征，越往上抽取出的特征越与手头任务相关。正因为此，所以预训练好的网络参数，尤其是底层的网络参数抽取出特征跟具体任务越无关，越具备任务的通用性，所以这是为何一般用底层预训练好的参数初始化新任务网络参数的原因。而高层特征跟任务关联较大，实际可以不用使用，或者采用Fine-tuning用新数据集合清洗掉高层无关的特征抽取器。</p><p><img src="https://i.loli.net/2020/10/30/RrSMsuboYdOzvKl.jpg" alt="img"></p><p>一般我们喜欢用ImageNet来做网络的预训练，主要有两点，一方面ImageNet是图像领域里有超多事先标注好训练数据的数据集合，分量足是个很大的优势，量越大训练出的参数越靠谱；另外一方面因为ImageNet有1000类，类别多，算是通用的图像数据，跟领域没太大关系，所以通用性好，预训练完后哪哪都能用，是个万金油。分量足的万金油当然老少通吃，人人喜爱。</p><p>听完上述话，如果你是具备研究素质的人，也就是说具备好奇心，你一定会问下面这个问题：”既然图像领域预训练这么好用，那干嘛自然语言处理不做这个事情呢？是不是搞NLP的人比搞CV的傻啊？就算你傻，你看见人家这么做，有样学样不就行了吗？这不就是创新吗，也许能成，万一成了，你看，你的成功来得就是这么突然!”</p><p>嗯，好问题，其实搞NLP的人一点都不比你傻，早就有人尝试过了，不过总体而言不太成功而已。听说过word embedding吗？2003年出品，陈年技术，馥郁芳香。word embedding其实就是NLP里的早期预训练技术。当然也不能说word embedding不成功，一般加到下游任务里，都能有1到2个点的性能提升，只是没有那么耀眼的成功而已。</p><p>没听过？那下面就把这段陈年老账讲给你听听。</p><h3 id="word-embedding考古史">Word Embedding考古史</h3><p>这块大致讲讲Word Embedding的故事，很粗略，因为网上关于这个技术讲的文章太多了，汗牛冲动，我不属牛，此刻更没有流汗，所以其实丝毫没有想讲Word Embedding的冲动和激情，但是要说预训练又得从这开始，那就粗略地讲讲，主要是引出后面更精彩的部分。在说Word Embedding之前，先更粗略地说下语言模型，因为一般NLP里面做预训练一般的选择是用语言模型任务来做。</p><p><img src="https://i.loli.net/2020/10/30/4aFBLDCEQmgXp9Z.jpg" alt="img"></p><p>什么是语言模型？其实看上面这张PPT上扣下来的图就明白了，为了能够量化地衡量哪个句子更像一句人话，可以设计如上图所示函数，核心函数P的思想是根据句子里面前面的一系列前导单词预测后面跟哪个单词的概率大小（理论上除了上文之外，也可以引入单词的下文联合起来预测单词出现概率）。句子里面每个单词都有个根据上文预测自己的过程，把所有这些单词的产生概率乘起来，数值越大代表这越像一句人话。语言模型压下暂且不表，我隐约预感到我这么讲你可能还是不太会明白，但是大概这个意思，不懂的可以去网上找，资料多得一样地汗牛冲动。</p><p>假设现在让你设计一个神经网络结构，去做这个语言模型的任务，就是说给你很多语料做这个事情，训练好一个神经网络，训练好之后，以后输入一句话的前面几个单词，要求这个网络输出后面紧跟的单词应该是哪个，你会怎么做？</p><p><img src="https://i.loli.net/2020/10/30/SUI2jF7xEsaz9Z4.jpg" alt="img"></p><p>你可以像上图这么设计这个网络结构，这其实就是大名鼎鼎的中文人称“神经网络语言模型”，英文小名NNLM的网络结构，用来做语言模型。这个工作有年头了，是个陈年老工作，是Bengio 在2003年发表在JMLR上的论文。它生于2003，火于2013，以后是否会不朽暂且不知，但是不幸的是出生后应该没有引起太大反响，沉寂十年终于时来运转沉冤得雪，在2013年又被NLP考古工作者从海底湿淋淋地捞出来了祭入神殿。为什么会发生这种技术奇遇记？你要想想2013年是什么年头，是深度学习开始渗透NLP领域的光辉时刻，万里长征第一步，而NNLM可以算是南昌起义第一枪。在深度学习火起来之前，极少有人用神经网络做NLP问题，如果你10年前坚持用神经网络做NLP，估计别人会认为你这人神经有问题。所谓红尘滚滚，谁也挡不住历史发展趋势的车轮，这就是个很好的例子。</p><p>上面是闲话，闲言碎语不要讲，我们回来讲一讲NNLM的思路。先说训练过程，现在看其实很简单，见过RNN、LSTM、CNN后的你们回头再看这个网络甚至显得有些简陋。学习任务是输入某个句中单词Wt=“Bert”前面句子的t-1个单词，要求网络正确预测单词Bert，即最大化：</p><p><img src="https://i.loli.net/2020/10/30/k8XpuUwT47WOo5Q.png" alt="image-20201030202319906" style="zoom:50%;"></p><p>前面任意单词Wi用Onehot编码（比如：0001000）作为原始单词输入，之后乘以矩阵Q后获得向量C（Wi），每个单词的 C（Wi）拼接，上接隐层，然后接softmax去预测后面应该后续接哪个单词。这个 C（Wi）是什么？这其实就是单词对应的Word Embedding值，那个矩阵Q包含V行，V代表词典大小，每一行内容代表对应单词的Word embedding值。只不过Q的内容也是网络参数，需要学习获得，训练刚开始用随机值初始化矩阵Q，当这个网络训练好之后，矩阵Q的内容被正确赋值，每一行代表一个单词对应的Word embedding值。所以你看，通过这个网络学习语言模型任务，这个网络不仅自己能够根据上文预测后接单词是什么，同时获得一个副产品，就是那个矩阵Q，这就是单词的Word Embedding是被如何学会的。</p><p>2013年最火的用语言模型做Word Embedding的工具是Word2Vec，后来又出了Glove，Word2Vec是怎么工作的呢？看下图。</p><p><img src="https://i.loli.net/2020/10/30/bXq5RvhS3niaTxB.jpg" alt="img"></p><p>Word2Vec的网络结构其实和NNLM是基本类似的，只是这个图长得清晰度差了点，看上去不像，其实它们是亲兄弟。不过这里需要指出：尽管网络结构相近，而且也是做语言模型任务，但是其训练方法不太一样。Word2Vec有两种训练方法，一种叫CBOW，核心思想是从一个句子里面把一个词抠掉，用这个词的上文和下文去预测被抠掉的这个词；第二种叫做Skip-gram，和CBOW正好反过来，输入某个单词，要求网络预测它的上下文单词。而你回头看看，NNLM是怎么训练的？是输入一个单词的上文，去预测这个单词。这是有显著差异的。为什么Word2Vec这么处理？原因很简单，因为Word2Vec和NNLM不一样，NNLM的主要任务是要学习一个解决语言模型任务的网络结构，语言模型就是要看到上文预测下文，而word embedding只是无心插柳的一个副产品。但是Word2Vec目标不一样，它单纯就是要word embedding的，这是主产品，所以它完全可以随性地这么去训练网络。</p><p>为什么要讲Word2Vec呢？这里主要是要引出CBOW的训练方法，BERT其实跟它有关系，后面会讲它们之间是如何的关系，当然它们的关系BERT作者没说，是我猜的，至于我猜的对不对，后面你看后自己判断。</p><p><img src="https://i.loli.net/2020/10/30/aM9lpsNOS7vrmnq.jpg" alt="img"></p><p>使用Word2Vec或者Glove，通过做语言模型任务，就可以获得每个单词的Word Embedding，那么这种方法的效果如何呢？上图给了网上找的几个例子，可以看出有些例子效果还是很不错的，一个单词表达成Word Embedding后，很容易找出语义相近的其它词汇。</p><p>我们的主题是预训练，那么问题是Word Embedding这种做法能算是预训练吗？这其实就是标准的预训练过程。要理解这一点要看看学会Word Embedding后下游任务是怎么用它的。</p><p><img src="https://i.loli.net/2020/10/30/YEf95lnIW28OGRa.jpg" alt="img"></p><p>假设如上图所示，我们有个NLP的下游任务，比如QA，就是问答问题，所谓问答问题，指的是给定一个问题X，给定另外一个句子Y,要判断句子Y是否是问题X的正确答案。问答问题假设设计的网络结构如上图所示，这里不展开讲了，懂得自然懂，不懂的也没关系，因为这点对于本文主旨来说不关键，关键是网络如何使用训练好的Word Embedding的。它的使用方法其实和前面讲的NNLM是一样的，句子中每个单词以Onehot形式作为输入，然后乘以学好的Word Embedding矩阵Q，就直接取出单词对应的Word Embedding了。这乍看上去好像是个查表操作，不像是预训练的做法是吧？其实不然，那个Word Embedding矩阵Q其实就是网络Onehot层到embedding层映射的网络参数矩阵。所以你看到了，使用Word Embedding等价于什么？等价于把Onehot层到embedding层的网络用预训练好的参数矩阵Q初始化了。这跟前面讲的图像领域的低层预训练过程其实是一样的，区别无非Word Embedding只能初始化第一层网络参数，再高层的参数就无能为力了。下游NLP任务在使用Word Embedding的时候也类似图像有两种做法，一种是Frozen，就是Word Embedding那层网络参数固定不动；另外一种是Fine-Tuning，就是Word Embedding这层参数使用新的训练集合训练也需要跟着训练过程更新掉。</p><p>上面这种做法就是18年之前NLP领域里面采用预训练的典型做法，之前说过，Word Embedding其实对于很多下游NLP任务是有帮助的，只是帮助没有大到闪瞎忘记戴墨镜的围观群众的双眼而已。那么新问题来了，为什么这样训练及使用Word Embedding的效果没有期待中那么好呢？答案很简单，因为Word Embedding有问题呗。这貌似是个比较弱智的答案，关键是Word Embedding存在什么问题？这其实是个好问题。</p><p><img src="https://i.loli.net/2020/10/30/U3YwNJ1RmydDukM.jpg" alt="img"></p><p>这片在Word Embedding头上笼罩了好几年的乌云是什么？是多义词问题。我们知道，多义词是自然语言中经常出现的现象，也是语言灵活性和高效性的一种体现。多义词对Word Embedding来说有什么负面影响？如上图所示，比如多义词Bank，有两个常用含义，但是Word Embedding在对bank这个单词进行编码的时候，是区分不开这两个含义的，因为它们尽管上下文环境中出现的单词不同，但是在用语言模型训练的时候，不论什么上下文的句子经过word2vec，都是预测相同的单词bank，而同一个单词占的是同一行的参数空间，这导致两种不同的上下文信息都会编码到相同的word embedding空间里去。所以word embedding无法区分多义词的不同语义，这就是它的一个比较严重的问题。</p><p>你可能觉得自己很聪明，说这可以解决啊，确实也有很多研究人员提出很多方法试图解决这个问题，但是从今天往回看，这些方法看上去都成本太高或者太繁琐了，有没有简单优美的解决方案呢？</p><p>ELMO提供了一种简洁优雅的解决方案。</p><h3 id="从word-embedding到elmo">从Word Embedding到ELMO</h3><p>ELMO是“Embedding from Language Models”的简称，其实这个名字并没有反应它的本质思想，提出ELMO的论文题目：“Deep contextualized word representation”更能体现其精髓，而精髓在哪里？在deep contextualized这个短语，一个是deep，一个是context，其中context更关键。在此之前的Word Embedding本质上是个静态的方式，所谓静态指的是训练好之后每个单词的表达就固定住了，以后使用的时候，不论新句子上下文单词是什么，这个单词的Word Embedding不会跟着上下文场景的变化而改变，所以对于比如Bank这个词，它事先学好的Word Embedding中混合了几种语义 ，在应用中来了个新句子，即使从上下文中（比如句子包含money等词）明显可以看出它代表的是“银行”的含义，但是对应的Word Embedding内容也不会变，它还是混合了多种语义。这是为何说它是静态的，这也是问题所在。ELMO的本质思想是：我事先用语言模型学好一个单词的Word Embedding，此时多义词无法区分，不过这没关系。在我实际使用Word Embedding的时候，单词已经具备了特定的上下文了，这个时候我可以根据上下文单词的语义去调整单词的Word Embedding表示，这样经过调整后的Word Embedding更能表达在这个上下文中的具体含义，自然也就解决了多义词的问题了。所以ELMO本身是个根据当前上下文对Word Embedding动态调整的思路。</p><p><img src="https://i.loli.net/2020/10/30/m6NvFoRGhWbji83.jpg" alt="img"></p><p>ELMO采用了典型的两阶段过程，第一个阶段是利用语言模型进行预训练；第二个阶段是在做下游任务时，从预训练网络中提取对应单词的网络各层的Word Embedding作为新特征补充到下游任务中。上图展示的是其预训练过程，它的网络结构采用了双层双向LSTM，目前语言模型训练的任务目标是根据单词 Wi 的上下文去正确预测单词 Wi ， Wi 之前的单词序列Context-before称为上文，之后的单词序列Context-after称为下文。图中左端的前向双层LSTM代表正方向编码器，输入的是从左到右顺序的除了预测单词外 Wi 的上文Context-before；右端的逆向双层LSTM代表反方向编码器，输入的是从右到左的逆序的句子下文Context-after；每个编码器的深度都是两层LSTM叠加。这个网络结构其实在NLP中是很常用的。使用这个网络结构利用大量语料做语言模型任务就能预先训练好这个网络，如果训练好这个网络后，输入一个新句子 Snew ，句子中每个单词都能得到对应的三个Embedding:最底层是单词的Word Embedding，往上走是第一层双向LSTM中对应单词位置的Embedding，这层编码单词的句法信息更多一些；再往上走是第二层LSTM中对应单词位置的Embedding，这层编码单词的语义信息更多一些。也就是说，ELMO的预训练过程不仅仅学会单词的Word Embedding，还学会了一个双层双向的LSTM网络结构，而这两者后面都有用。</p><p><img src="https://i.loli.net/2020/10/30/ymSXKwFh8WBcEd5.jpg" alt="img"></p><p>上面介绍的是ELMO的第一阶段：预训练阶段。那么预训练好网络结构后，如何给下游任务使用呢？上图展示了下游任务的使用过程，比如我们的下游任务仍然是QA问题，此时对于问句X，我们可以先将句子X作为预训练好的ELMO网络的输入，这样句子X中每个单词在ELMO网络中都能获得对应的三个Embedding，之后给予这三个Embedding中的每一个Embedding一个权重a，这个权重可以学习得来，根据各自权重累加求和，将三个Embedding整合成一个。然后将整合后的这个Embedding作为X句在自己任务的那个网络结构中对应单词的输入，以此作为补充的新特征给下游任务使用。对于上图所示下游任务QA中的回答句子Y来说也是如此处理。因为ELMO给下游提供的是每个单词的特征形式，所以这一类预训练的方法被称为“Feature-based Pre-Training”。至于为何这么做能够达到区分多义词的效果，你可以想一想，其实比较容易想明白原因。</p><p><img src="https://i.loli.net/2020/10/30/b8ToQxv5BPgELI1.jpg" alt="img"></p><p>上面这个图是TagLM采用类似ELMO的思路做命名实体识别任务的过程，其步骤基本如上述ELMO的思路，所以此处不展开说了。TagLM的论文发表在2017年的ACL会议上，作者就是AllenAI里做ELMO的那些人，所以可以将TagLM看做ELMO的一个前导工作。前几天这个PPT发出去后有人质疑说FastAI的在18年4月提出的ULMFiT才是抛弃传统Word Embedding引入新模式的开山之作，我深不以为然。首先TagLM出现的更早而且模式基本就是ELMO的思路；另外ULMFiT使用的是三阶段模式，在通用语言模型训练之后，加入了一个领域语言模型预训练过程，而且论文重点工作在这块，方法还相对比较繁杂，这并不是一个特别好的主意，因为领域语言模型的限制是它的规模往往不可能特别大，精力放在这里不太合适，放在通用语言模型上感觉更合理；再者，尽管ULFMiT实验做了6个任务，但是都集中在分类问题相对比较窄，不如ELMO验证的问题领域广，我觉得这就是因为第二步那个领域语言模型带来的限制。所以综合看，尽管ULFMiT也是个不错的工作，但是重要性跟ELMO比至少还是要差一档，当然这是我个人看法。每个人的学术审美口味不同，我个人一直比较赞赏要么简洁有效体现问题本质要么思想特别游离现有框架脑洞开得异常大的工作，所以ULFMiT我看论文的时候就感觉看着有点难受，觉得这工作没抓住重点而且特别麻烦，但是看ELMO论文感觉就赏心悦目，觉得思路特别清晰顺畅，看完暗暗点赞，心里说这样的文章获得NAACL2018最佳论文当之无愧，比ACL很多最佳论文也好得不是一点半点，这就是好工作带给一个有经验人士的一种在读论文时候就能产生的本能的感觉，也就是所谓的这道菜对上了食客的审美口味。</p><p><img src="https://i.loli.net/2020/10/30/6y7VvCDm9NRpHJx.jpg" alt="img"></p><p>前面我们提到静态Word Embedding无法解决多义词的问题，那么ELMO引入上下文动态调整单词的embedding后多义词问题解决了吗？解决了，而且比我们期待的解决得还要好。上图给了个例子，对于Glove训练出的Word Embedding来说，多义词比如play，根据它的embedding找出的最接近的其它单词大多数集中在体育领域，这很明显是因为训练数据中包含play的句子中体育领域的数量明显占优导致；而使用ELMO，根据上下文动态调整后的embedding不仅能够找出对应的“演出”的相同语义的句子，而且还可以保证找出的句子中的play对应的词性也是相同的，这是超出期待之处。之所以会这样，是因为我们上面提到过，第一层LSTM编码了很多句法信息，这在这里起到了重要作用。</p><p><img src="https://i.loli.net/2020/10/30/YFAVkuIxemlaHPN.jpg" alt="img"></p><p>ELMO经过这般操作，效果如何呢？实验效果见上图，6个NLP任务中性能都有幅度不同的提升，最高的提升达到25%左右，而且这6个任务的覆盖范围比较广，包含句子语义关系判断，分类任务，阅读理解等多个领域，这说明其适用范围是非常广的，普适性强，这是一个非常好的优点。</p><p><img src="https://i.loli.net/2020/10/30/LpMSFe5kX7Qxroh.jpg" alt="img"></p><p>那么站在现在这个时间节点看，ELMO有什么值得改进的缺点呢？首先，一个非常明显的缺点在特征抽取器选择方面，ELMO使用了LSTM而不是新贵Transformer，Transformer是谷歌在17年做机器翻译任务的“Attention is all you need”的论文中提出的，引起了相当大的反响，很多研究已经证明了Transformer提取特征的能力是要远强于LSTM的。如果ELMO采取Transformer作为特征提取器，那么估计Bert的反响远不如现在的这种火爆场面。另外一点，ELMO采取双向拼接这种融合特征的能力可能比Bert一体化的融合特征方式弱，但是，这只是一种从道理推断产生的怀疑，目前并没有具体实验说明这一点。</p><p>我们如果把ELMO这种预训练方法和图像领域的预训练方法对比，发现两者模式看上去还是有很大差异的。除了以ELMO为代表的这种基于特征融合的预训练方法外，NLP里还有一种典型做法，这种做法和图像领域的方式就是看上去一致的了，一般将这种方法称为“基于Fine-tuning的模式”，而GPT就是这一模式的典型开创者。</p><h3 id="从word-embedding到gpt">从Word Embedding到GPT</h3><p><img src="https://i.loli.net/2020/10/30/IDl2hH8j3JVdx6F.jpg" alt="img"></p><p>GPT是“Generative Pre-Training”的简称，从名字看其含义是指的生成式的预训练。GPT也采用两阶段过程，第一个阶段是利用语言模型进行预训练，第二阶段通过Fine-tuning的模式解决下游任务。上图展示了GPT的预训练过程，其实和ELMO是类似的，主要不同在于两点：首先，特征抽取器不是用的RNN，而是用的Transformer，上面提到过它的特征抽取能力要强于RNN，这个选择很明显是很明智的；其次，GPT的预训练虽然仍然是以语言模型作为目标任务，但是采用的是单向的语言模型，所谓“单向”的含义是指：语言模型训练的任务目标是根据 Wi 单词的上下文去正确预测单词 Wi ， Wi 之前的单词序列Context-before称为上文，之后的单词序列Context-after称为下文。ELMO在做语言模型预训练的时候，预测单词 Wi 同时使用了上文和下文，而GPT则只采用Context-before这个单词的上文来进行预测，而抛开了下文。这个选择现在看不是个太好的选择，原因很简单，它没有把单词的下文融合进来，这限制了其在更多应用场景的效果，比如阅读理解这种任务，在做任务的时候是可以允许同时看到上文和下文一起做决策的。如果预训练时候不把单词的下文嵌入到Word Embedding中，是很吃亏的，白白丢掉了很多信息。</p><p>这里强行插入一段简单提下Transformer，尽管上面提到了，但是说的还不完整，补充两句。首先，Transformer是个叠加的“自注意力机制（Self Attention）”构成的深度网络，是目前NLP里最强的特征提取器，注意力这个机制在此被发扬光大，从任务的配角不断抢戏，直到Transformer一跃成为踢开RNN和CNN传统特征提取器，荣升头牌，大红大紫。注意力机制可以参考“<a href="https://zhuanlan.zhihu.com/p/37601161" target="_blank" rel="noopener">深度学习中的注意力模型</a>”，补充下相关基础知识，如果不了解注意力机制你肯定会落后时代的发展。而介绍Transformer比较好的文章可以参考以下两篇文章：一个是Jay Alammar可视化地介绍Transformer的博客文章<a href="https://link.zhihu.com/?target=https%3A//jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a> ，非常容易理解整个机制，建议先从这篇看起；然后可以参考哈佛大学NLP研究组写的“<a href="https://link.zhihu.com/?target=http%3A//nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer.</a> ”，代码原理双管齐下，讲得非常清楚。我相信上面两个文章足以让你了解Transformer了，所以这里不展开介绍。</p><p>其次，我的判断是Transformer在未来会逐渐替代掉RNN成为主流的NLP工具，RNN一直受困于其并行计算能力，这是因为它本身结构的序列性依赖导致的，尽管很多人在试图通过修正RNN结构来修正这一点，但是我不看好这种模式，因为给马车换轮胎不如把它升级到汽车，这个道理很好懂，更何况目前汽车的雏形已经出现了，干嘛还要执着在换轮胎这个事情呢？是吧？再说CNN，CNN在NLP里一直没有形成主流，CNN的最大优点是易于做并行计算，所以速度快，但是在捕获NLP的序列关系尤其是长距离特征方面天然有缺陷，不是做不到而是做不好，目前也有很多改进模型，但是特别成功的不多。综合各方面情况，很明显Transformer同时具备并行性好，又适合捕获长距离特征，没有理由不在赛跑比赛中跑不过RNN和CNN。</p><p>好了，题外话结束，我们再回到主题，接着说GPT。上面讲的是GPT如何进行第一阶段的预训练，那么假设预训练好了网络模型，后面下游任务怎么用？它有自己的个性，和ELMO的方式大有不同。</p><p><img src="https://i.loli.net/2020/10/30/3Gr9vqoPHkSfcAg.jpg" alt="img"></p><p>上图展示了GPT在第二阶段如何使用。首先，对于不同的下游任务来说，本来你可以任意设计自己的网络结构，现在不行了，你要向GPT的网络结构看齐，把任务的网络结构改造成和GPT的网络结构是一样的。然后，在做下游任务的时候，利用第一步预训练好的参数初始化GPT的网络结构，这样通过预训练学到的语言学知识就被引入到你手头的任务里来了，这是个非常好的事情。再次，你可以用手头的任务去训练这个网络，对网络参数进行Fine-tuning，使得这个网络更适合解决手头的问题。就是这样。看到了么？这有没有让你想起最开始提到的图像领域如何做预训练的过程（请参考上图那句非常容易暴露年龄的歌词）？对，这跟那个模式是一模一样的。</p><p>这里引入了一个新问题：对于NLP各种花样的不同任务，怎么改造才能靠近GPT的网络结构呢？</p><p><img src="https://i.loli.net/2020/10/30/iJqb8TYLwCvdSVk.jpg" alt="img"></p><p>GPT论文给了一个改造施工图如上，其实也很简单：对于分类问题，不用怎么动，加上一个起始和终结符号即可；对于句子关系判断问题，比如Entailment，两个句子中间再加个分隔符即可；对文本相似性判断问题，把两个句子顺序颠倒下做出两个输入即可，这是为了告诉模型句子顺序不重要；对于多项选择问题，则多路输入，每一路把文章和答案选项拼接作为输入即可。从上图可看出，这种改造还是很方便的，不同任务只需要在输入部分施工即可。</p><p><img src="https://i.loli.net/2020/10/30/qnLcVGo5IK6riYh.jpg" alt="img"></p><p>GPT的效果是非常令人惊艳的，在12个任务里，9个达到了最好的效果，有些任务性能提升非常明显。</p><p><img src="https://i.loli.net/2020/10/30/96zdAXvcOmJTuP2.jpg" alt="img"></p><p>那么站在现在的时间节点看，GPT有什么值得改进的地方呢？其实最主要的就是那个单向语言模型，如果改造成双向的语言模型任务估计也没有Bert太多事了。当然，即使如此GPT也是非常非常好的一个工作，跟Bert比，其作者炒作能力亟待提升。</p><h3 id="bert的诞生">Bert的诞生</h3><p><img src="https://i.loli.net/2020/10/30/rSJAqOMB4sathDg.jpg" alt="img"></p><p>我们经过跋山涉水，终于到了目的地Bert模型了。</p><p>Bert采用和GPT完全相同的两阶段模型，首先是语言模型预训练；其次是使用Fine-Tuning模式解决下游任务。和GPT的最主要不同在于在预训练阶段采用了类似ELMO的双向语言模型，当然另外一点是语言模型的数据规模要比GPT大。所以这里Bert的预训练过程不必多讲了。</p><p><img src="https://i.loli.net/2020/10/30/61JpWKSZ5fF3tNk.jpg" alt="img"></p><p>第二阶段，Fine-Tuning阶段，这个阶段的做法和GPT是一样的。当然，它也面临着下游任务网络结构改造的问题，在改造任务方面Bert和GPT有些不同，下面简单介绍一下。</p><p><img src="https://i.loli.net/2020/10/30/UTQdhtVA7PzcIlF.jpg" alt="img"></p><p>在介绍Bert如何改造下游任务之前，先大致说下NLP的几类问题，说这个是为了强调Bert的普适性有多强。通常而言，绝大部分NLP问题可以归入上图所示的四类任务中：一类是序列标注，这是最典型的NLP任务，比如中文分词，词性标注，命名实体识别，语义角色标注等都可以归入这一类问题，它的特点是句子中每个单词要求模型根据上下文都要给出一个分类类别。第二类是分类任务，比如我们常见的文本分类，情感计算等都可以归入这一类。它的特点是不管文章有多长，总体给出一个分类类别即可。第三类任务是句子关系判断，比如Entailment，QA，语义改写，自然语言推理等任务都是这个模式，它的特点是给定两个句子，模型判断出两个句子是否具备某种语义关系；第四类是生成式任务，比如机器翻译，文本摘要，写诗造句，看图说话等都属于这一类。它的特点是输入文本内容后，需要自主生成另外一段文字。</p><p><img src="https://i.loli.net/2020/10/30/mxJybVWl2OatfUc.jpg" alt="img"></p><p>对于种类如此繁多而且各具特点的下游NLP任务，Bert如何改造输入输出部分使得大部分NLP任务都可以使用Bert预训练好的模型参数呢？上图给出示例，对于句子关系类任务，很简单，和GPT类似，加上一个起始和终结符号，句子之间加个分隔符即可。对于输出来说，把第一个起始符号对应的Transformer最后一层位置上面串接一个softmax分类层即可。对于分类问题，与GPT一样，只需要增加起始和终结符号，输出部分和句子关系判断任务类似改造；对于序列标注问题，输入部分和单句分类是一样的，只需要输出部分Transformer最后一层每个单词对应位置都进行分类即可。从这里可以看出，上面列出的NLP四大任务里面，除了生成类任务外，Bert其它都覆盖到了，而且改造起来很简单直观。尽管Bert论文没有提，但是稍微动动脑子就可以想到，其实对于机器翻译或者文本摘要，聊天机器人这种生成式任务，同样可以稍作改造即可引入Bert的预训练成果。只需要附着在S2S结构上，encoder部分是个深度Transformer结构，decoder部分也是个深度Transformer结构。根据任务选择不同的预训练数据初始化encoder和decoder即可。这是相当直观的一种改造方法。当然，也可以更简单一点，比如直接在单个Transformer结构上加装隐层产生输出也是可以的。不论如何，从这里可以看出，NLP四大类任务都可以比较方便地改造成Bert能够接受的方式。这其实是Bert的非常大的优点，这意味着它几乎可以做任何NLP的下游任务，具备普适性，这是很强的。</p><p><img src="https://i.loli.net/2020/10/30/e7tSMGZjDHmY1Ck.jpg" alt="img"></p><p>Bert采用这种两阶段方式解决各种NLP任务效果如何？在11个各种类型的NLP任务中达到目前最好的效果，某些任务性能有极大的提升。一个新模型好不好，效果才是王道。</p><p><img src="https://i.loli.net/2020/10/30/RniS8uQhpDmcHN6.jpg" alt="img"></p><p>到这里我们可以再梳理下几个模型之间的演进关系。从上图可见，Bert其实和ELMO及GPT存在千丝万缕的关系，比如如果我们把GPT预训练阶段换成双向语言模型，那么就得到了Bert；而如果我们把ELMO的特征抽取器换成Transformer，那么我们也会得到Bert。所以你可以看出：Bert最关键两点，一点是特征抽取器采用Transformer；第二点是预训练的时候采用双向语言模型。</p><p>那么新问题来了：对于Transformer来说，怎么才能在这个结构上做双向语言模型任务呢？乍一看上去好像不太好搞。我觉得吧，其实有一种很直观的思路，怎么办？看看ELMO的网络结构图，只需要把两个LSTM替换成两个Transformer，一个负责正向，一个负责反向特征提取，其实应该就可以。当然这是我自己的改造，Bert没这么做。那么Bert是怎么做的呢？我们前面不是提过Word2Vec吗？我前面肯定不是漫无目的地提到它，提它是为了在这里引出那个CBOW训练方法，所谓写作时候埋伏笔的“草蛇灰线，伏脉千里”，大概就是这个意思吧？前面提到了CBOW方法，它的核心思想是：在做语言模型任务的时候，我把要预测的单词抠掉，然后根据它的上文Context-Before和下文Context-after去预测单词。其实Bert怎么做的？Bert就是这么做的。从这里可以看到方法间的继承关系。当然Bert作者没提Word2Vec及CBOW方法，这是我的判断，Bert作者说是受到完形填空任务的启发，这也很可能，但是我觉得他们要是没想到过CBOW估计是不太可能的。</p><p>从这里可以看出，在文章开始我说过Bert在模型方面其实没有太大创新，更像一个最近几年NLP重要技术的集大成者，原因在于此，当然我不确定你怎么看，是否认同这种看法，而且我也不关心你怎么看。其实Bert本身的效果好和普适性强才是最大的亮点。</p><p><img src="https://i.loli.net/2020/10/30/LO9j7cIxEJCe2Ay.jpg" alt="img"></p><p>那么Bert本身在模型和方法角度有什么创新呢？就是论文中指出的Masked 语言模型和Next Sentence Prediction。而Masked语言模型上面讲了，本质思想其实是CBOW，但是细节方面有改进。</p><p><img src="https://i.loli.net/2020/10/30/75DVNACdHgtRbS9.jpg" alt="img"></p><p>Masked双向语言模型向上图展示这么做：随机选择语料中15%的单词，把它抠掉，也就是用[Mask]掩码代替原始单词，然后要求模型去正确预测被抠掉的单词。但是这里有个问题：训练过程大量看到[mask]标记，但是真正后面用的时候是不会有这个标记的，这会引导模型认为输出是针对[mask]这个标记的，但是实际使用又见不到这个标记，这自然会有问题。为了避免这个问题，Bert改造了一下，15%的被上天选中要执行[mask]替身这项光荣任务的单词中，只有80%真正被替换成[mask]标记，10%被狸猫换太子随机替换成另外一个单词，10%情况这个单词还待在原地不做改动。这就是Masked双向语音模型的具体做法。</p><p><img src="https://i.loli.net/2020/10/30/MTCajrZPKuF51Ne.jpg" alt="img"></p><p>至于说“Next Sentence Prediction”，指的是做语言模型预训练的时候，分两种情况选择两个句子，一种是选择语料中真正顺序相连的两个句子；另外一种是第二个句子从语料库中抛色子，随机选择一个拼到第一个句子后面。我们要求模型除了做上述的Masked语言模型任务外，附带再做个句子关系预测，判断第二个句子是不是真的是第一个句子的后续句子。之所以这么做，是考虑到很多NLP任务是句子关系判断任务，单词预测粒度的训练到不了句子关系这个层级，增加这个任务有助于下游句子关系判断任务。所以可以看到，它的预训练是个多任务过程。这也是Bert的一个创新。</p><p><img src="https://i.loli.net/2020/10/30/XQ17TqJcYpPoA3i.jpg" alt="img"></p><p>上面这个图给出了一个我们此前利用微博数据和开源的Bert做预训练时随机抽出的一个中文训练实例，从中可以体会下上面讲的masked语言模型和下句预测任务。训练数据就长这种样子。</p><p><img src="https://i.loli.net/2020/10/30/E1vcQhzTsgbLqkF.jpg" alt="img"></p><p>顺带讲解下Bert的输入部分，也算是有些特色。它的输入部分是个线性序列，两个句子通过分隔符分割，最前面和最后增加两个标识符号。每个单词有三个embedding:位置信息embedding，这是因为NLP中单词顺序是很重要的特征，需要在这里对位置信息进行编码；单词embedding,这个就是我们之前一直提到的单词embedding；第三个是句子embedding，因为前面提到训练数据都是由两个句子构成的，那么每个句子有个句子整体的embedding项对应给每个单词。把单词对应的三个embedding叠加，就形成了Bert的输入。</p><p><img src="https://i.loli.net/2020/10/30/wHfCcyhaiXPMx1d.jpg" alt="img"></p><p>至于Bert在预训练的输出部分如何组织，可以参考上图的注释。</p><p><img src="https://i.loli.net/2020/10/30/B5wItXbYpPr619Z.jpg" alt="img"></p><p>我们说过Bert效果特别好，那么到底是什么因素起作用呢？如上图所示，对比试验可以证明，跟GPT相比，双向语言模型起到了最主要的作用，对于那些需要看到下文的任务来说尤其如此。而预测下个句子来说对整体性能来说影响不算太大，跟具体任务关联度比较高。</p><p><img src="https://i.loli.net/2020/10/30/u9mKAEGjp4fa7ys.jpg" alt="img"></p><p>最后，我讲讲我对Bert的评价和看法，我觉得Bert是NLP里里程碑式的工作，对于后面NLP的研究和工业应用会产生长久的影响，这点毫无疑问。但是从上文介绍也可以看出，从模型或者方法角度看，Bert借鉴了ELMO，GPT及CBOW，主要提出了Masked 语言模型及Next Sentence Prediction，但是这里Next Sentence Prediction基本不影响大局，而Masked LM明显借鉴了CBOW的思想。所以说Bert的模型没什么大的创新，更像最近几年NLP重要进展的集大成者，这点如果你看懂了上文估计也没有太大异议，如果你有大的异议，杠精这个大帽子我随时准备戴给你。如果归纳一下这些进展就是：首先是两阶段模型，第一阶段双向语言模型预训练，这里注意要用双向而不是单向，第二阶段采用具体任务Fine-tuning或者做特征集成；第二是特征抽取要用Transformer作为特征提取器而不是RNN或者CNN；第三，双向语言模型可以采取CBOW的方法去做（当然我觉得这个是个细节问题，不算太关键，前两个因素比较关键）。Bert最大的亮点在于效果好及普适性强，几乎所有NLP任务都可以套用Bert这种两阶段解决思路，而且效果应该会有明显提升。可以预见的是，未来一段时间在NLP应用领域，Transformer将占据主导地位，而且这种两阶段预训练方法也会主导各种应用。</p><p>另外，我们应该弄清楚预训练这个过程本质上是在做什么事情，本质上预训练是通过设计好一个网络结构来做语言模型任务，然后把大量甚至是无穷尽的无标注的自然语言文本利用起来，预训练任务把大量语言学知识抽取出来编码到网络结构中，当手头任务带有标注信息的数据有限时，这些先验的语言学特征当然会对手头任务有极大的特征补充作用，因为当数据有限的时候，很多语言学现象是覆盖不到的，泛化能力就弱，集成尽量通用的语言学知识自然会加强模型的泛化能力。如何引入先验的语言学知识其实一直是NLP尤其是深度学习场景下的NLP的主要目标之一，不过一直没有太好的解决办法，而ELMO/GPT/Bert的这种两阶段模式看起来无疑是解决这个问题自然又简洁的方法，这也是这些方法的主要价值所在。</p><p>对于当前NLP的发展方向，我个人觉得有两点非常重要，一个是需要更强的特征抽取器，目前看Transformer会逐渐担当大任，但是肯定还是不够强的，需要发展更强的特征抽取器；第二个就是如何优雅地引入大量无监督数据中包含的语言学知识，注意我这里强调地是优雅，而不是引入，此前相当多的工作试图做各种语言学知识的嫁接或者引入，但是很多方法看着让人牙疼，就是我说的不优雅。目前看预训练这种两阶段方法还是很有效的，也非常简洁，当然后面肯定还会有更好的模型出现。</p><p>完了，这就是自然语言模型预训练的发展史。</p><h3 id="参考">参考</h3><blockquote><p><a href="https://zhuanlan.zhihu.com/p/49271699" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/49271699</a></p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      从embedding到BERT预训练模型
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-10-30-论文分享</title>
    <link href="http://yoursite.com/2020/10/30/2020-10-30-%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"/>
    <id>http://yoursite.com/2020/10/30/2020-10-30-%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/</id>
    <published>2020-10-30T11:27:41.000Z</published>
    <updated>2020-11-07T14:00:28.089Z</updated>
    
    <content type="html"><![CDATA[<h3 id="xlnet-generalized-autoregressive-pretraining-for-language-understanding">XLNet: Generalized Autoregressive Pretraining for Language Understanding</h3><p><del>BERT和GPT都是只使用了transformer的encoder和decoder部分，原本transformer层也是可以attend to 双向的，但是GPT为了要基于前面的序列预测下一个word，所以只有上文信息，所以像decoder一样mask掩码掉了，只能利用上文的信息；而BERT没有进行掩码，为了更加利用好双向的关系，BERT在transformer的基础上使用了MLM的策略，主要处理的是自然语言理解的任务。</del></p><blockquote><p>NIPS 2019</p><p>authors ： ZhilinYang , ZihangDai （Carnegie Mellon University, Google AI BrainTeam ）</p><p>code url (official tf) : <a href="https://github.com/zihangdai/xlnet" target="_blank" rel="noopener" class="uri">https://github.com/zihangdai/xlnet</a></p><p>code url (unofficial torch): <a href="https://github.com/huggingface/transformers" target="_blank" rel="noopener" class="uri">https://github.com/huggingface/transformers</a></p></blockquote><p><del>论文名字的含义：一般的AR模型更适合处理自然语言生成的任务，比如transformer、transformer-XL； 而AE更适合处理自然语言理解的任务。本文通过XLNet模型，能够是AR预训练（结合了transformer-XL的思想）能够泛化到处理多个自然语言理解的问题上（与BERT功能类似）</del></p><h4 id="背景">背景</h4><p><img src="https://i.loli.net/2020/11/01/MmsCokbayGLrXdW.png" alt="image-20201031210916531"></p><p>AR语言模型（transformer-XL）只是训练编码一个单向的上下文，然而这限制了其在更多应用场景的效果，比如阅读理解这种任务，在做任务的时候是可以允许同时看到上文和下文一起做决策的。 所以AR语言模型适合自然语言生成的任务 （机器翻译等）</p><p><img src="https://i.loli.net/2020/11/01/8WRguEihUAc1P6G.png" alt="image-20201031211538083"></p><p>因为没有将密度估计作为目标函数的一部分，所以AE语言模型（BERT）就可以获取双向信息，利用上下文信息进行重建masked token。适合自然语言理解任务（阅读理解，问答等）</p><p>借助对双向上下文进行建模的功能，像BERT这种的基于denoising autoencoding （AE）比基于autoregressive language modeling（AR）的方法具有更好的性能。</p><h4 id="问题">问题</h4><p><img src="https://i.loli.net/2020/11/01/LzoWnBEO7GcY5IZ.png" alt="image-20201101103324693"></p><p>BERT的MLM策略的缺点</p><ol type="1"><li><p>mask掉的词之间的联系忽略了，即BERT假设被mask掉的词之间是独立无依赖的</p></li><li><p>pretrain （有mask）和fine-tune（无mask）直接有区别 （pretrain-ﬁnetune discrepancy）</p></li></ol><h4 id="解决">解决</h4><p><img src="https://i.loli.net/2020/11/01/XfrVJ79nZQezpKw.png" alt="image-20201031212429943"></p><p>本文结合AR LM和AE LM，在Transformer-XL的基础上提出generalized autoregressive method，也就是XLNet。</p><p>（1）没有使用BERT的MLM，而是用的是PLM策略。即通过最大化因式分解的所有排列的预期似然性来实现双向上下文学习，克服MLM的缺点。</p><p>所有的分解序列作为一个集合，对所有采样序列，XLNet按照AR LM的计算方式求对数似然期望的极大值。</p><p>（2）XLNet将来自最先进的自回归模型Transformer-XL的思想（segment recurrence mechanism和relative encoding scheme）整合到预训练中，能够提升涉及长文本序列时的效果</p><p>（3）引入Masked Two-Stream Self-Attention 策略来解决PLM出现的目标预测歧义（the ambiguity in target prediction）问题</p><p>每一步在随机排列之后的token，进行的都是一个AR语言模型的过程（在排列顺序中，根据前面的token来预测当前的token ），这样进行T次（具体次数是超参），原则上就可以克服AR中只能看到原始序列顺序之前token的缺点，可以关注到双向信息。</p><h4 id="模型">模型</h4><h5 id="背景知识">背景知识</h5><p>给定文本序列x=[x1,…,xT]，语言模型(AR)的目标是调整参数使得训练数据上的似然函数最大：</p><p><img src="https://i.loli.net/2020/11/01/Zcnh7PNyGksS2JV.png" alt="image-20201031221410608"></p><p>记号x&lt;t表示t时刻之前的所有x，也就是x1:xt−1。hθ(x1:t−1)是RNN或者Transformer。e(x)是词x的embedding。</p><p>BERT是去噪(denoising)自编码的方法。对于序列x，BERT会随机挑选15%的Token变成[MASK]得到带噪声版本的x^。假设被Mask部分的原始值为x¯，那么BERT希望尽量根据上下文恢复(猜测)出原始值了：</p><p><img src="https://i.loli.net/2020/11/01/OXGfrqR6PZKTiEC.png" alt="image-20201031221357201"></p><p>上面的公式中，mt=1表示xt被Mask掉，Hθ是一个Transformer，它把长度为T的序列x映射为隐状态的向量序列。</p><p>不同点：</p><ol type="1"><li>BERT是“≈” ，因为BERT假设被mask掉的词之间是独立无依赖的，没考虑之间的关系，而AR是“=”</li><li>BERT的输入是BERT会随机挑选15%的Token变成[MASK]得到带噪声版本的x^，而AR是原始的x序列</li><li>BERT可以获取上下文的双向信息，而AR只能获得上文token信息</li></ol><h5 id="permutation-language-modeling-plm">Permutation Language Modeling （ PLM ）</h5><h6 id="思想">思想</h6><p>提出了一种序列语言建模目标，它不仅可以保留 AR 模型的优点，同时也允许模型捕获双向语境。</p><p>具体来说，一个长度为 T 的序列 x 拥有 T! 种不同的排序方式，可以执行有效的自回归因式分解。在本文中选择了T种</p><p>如果模型参数在所有因式分解顺序中共享，那么预计模型将学习从两边的所有位置上收集信息。</p><p>以下图为例，对于序列[1,2,3,4]有24种排列方式，那么下图中四种排列方式的，该序列的期望函数分别是：</p><p><img src="https://i.loli.net/2020/11/01/zNDVbehHL9SRtYf.png" alt="image-20201101004521295"></p><p><img src="https://i.loli.net/2020/11/01/Hnt4SWwmQPdbloB.png" alt="image-20201031223904029"></p><p>在给定相同输入序列 x（但因式分解顺序不同）时预测 token x3的示例</p><p>相比于普通的语言模型只能学习一种方向的依赖关系，排列语言模型会学习各种顺序的预测方法</p><p>问题：遍历T!种路径，然后学习语言模型的参数。计算量非常大</p><p>解决：随机采样T！中的部分排列</p><p>PLM的目标是调整模型参数使得下面的似然概率最大：</p><p><img src="https://i.loli.net/2020/11/01/cBRaNlDnrYuoF37.png" alt="image-20201031224727513"></p><p>其中ZT表示长度为T的序列的所有排列组成的集合，则z∈ZT是一种排列方法。</p><p>用Xzt表示排列的第t个元素，zt表示第t个位置，而z&lt;t表示z的第1到第t-1个元素。</p><p>这样pretrain和finetune阶段就一样了，输入都是原始序列，通过attention mask实现随机产生的排列。例如排列是2-4-3-1，那么在预测X3的时候就只有2、4作为先验，并且2、4的位置信息是通过Zt来体现的，这样也保留了排列的时序信息。</p><p>注意：上面的模型只会遍历概率的分解顺序，并不会改变原始词的顺序。</p><p>实现：</p><p>通过Attention Mask来对应不同的分解方法。比如p(x1|x3)p(x2|x1x3)p(x3)，我们可以在用Transformer编码x1时候让它可以Attend to x3，而把x2Mask掉；编码x3的时候把x1,x2都Mask掉。</p><p><img src="https://i.loli.net/2020/11/01/qzQkLmKdMcPrUy7.png" alt="image-20201031235156078"></p><p>将上述的策略结合AR语言模型，那么就可以避免BERT的问题</p><h5 id="基于目标感知的双流注意力模型">基于目标感知的双流注意力模型</h5><p>问题：</p><p><img src="https://i.loli.net/2020/11/01/SUGq9x5MBKeTpsZ.png" alt="image-20201101005004082"></p><p><img src="https://i.loli.net/2020/11/01/UothZ8Lk6S5AWqG.png" alt="image-20201101005315581"></p><p>这两个概率不应该相等的，但是对比这两个公式会发现，这两个公式的概率是相等的。为什么会出现这样的情况呢？上面问题的关键是<strong>模型无法知道当前mask掉的文本在原始序列中的位置。在Transformer中输入的embedding会加入position embedding，输入已经带入了位置信息，但是我们重新排列之后模型无法预测当前位置在原始序列中的位置，因此我们需要让模型来预测当前文本的位置。</strong> 那么在模型中当前位置的文本的概率计算方式则如下所示，其中g（θ）不仅需要输入当前位置之前的文本，还需要输入他们在原始文本中的位置。</p><p><img src="https://i.loli.net/2020/11/01/RWPnZ5dwYAIbU2c.png" alt="image-20201101005459040"></p><p>XLNet 打乱了句子的顺序，这时在预测的时候 token 的位置信息会非常重要，同时在预测的时候也必须将 token 的内容信息遮掩起来 (否则输入包含了要预测的内容信息，模型就无法学到知识)。<strong>也就是说 XLNet 需要看到 token 的位置信息，但是又不能看到 token 的内容信息</strong></p><h6 id="双流self-attention">双流self-attention</h6><p><strong>1.Query Stream</strong>，对于每一个 token，其对应的 Query Stream 只包含了该 token 的位置信息，注意是 token 在原始句子的位置信息，不是重新排列的位置信息。</p><p><strong>2.Content Stream</strong>，对于每一个 token，其对应的 Content Stream 包含了该 token 的内容信息。</p><p>查询表征单元(Query Representation)：查询表征单元和我们上述需要注意的点相同，<strong>可以看到上下文的信息和当前位置，不可以看到当前的Token</strong>，例如[1,2,3,4]在第4个位置只能看到[1,2,3]。查询表征单元中矩阵Q由于计算了各个位置的信息，保留了当前位置，但是KV矩阵分别表示各个context的重要性，没有计算当前位置。</p><p><img src="https://i.loli.net/2020/11/01/CU1PnWlaArwDN8E.png" alt="img"></p><p>内容表征单元(Context Representation):内容表征单元和我们上文中说的Transformer一致，<strong>可以看到上下文的信息和当前的Token</strong>，例如文本序列[1,2,3,4]，在第4个位置，内容表征单元可以看到[1,2,3,4]，在第3个位置内容表征单元可以看到[1,2,3]。如下图所示QKV矩阵的计算都包含了当前位置。</p><p><img src="https://i.loli.net/2020/11/01/xD53wJVQsdIoC7i.png" alt="img"></p><p><strong>Query Stream 和 Content Stream 组合</strong></p><p>XLNet 将 Query Stream 和 Content Stream 组合在一起，整体架构如下图所示。</p><p><img src="https://i.loli.net/2020/11/01/wyni1NAtfYsGrV4.png" alt="image-20201101011712221"></p><p>图中最下面的一层是输入层，其中 e(x) 是单词的词向量，表示输入的 Content Stream，而 w 表示输入的位置信息，即 Query Stream。</p><p>图中的掩码矩阵，红色表示不遮掩，白色表示遮掩。第 1 行表示 token 1 的掩码，可以看到，1 是句子的最后一个 token，因此可以看到之前的所有 token (3,2,4)。3 是句子的第一个 token，看不到句子的任何信息，因此第 3 行都是白色的 (表示遮掩)。</p><h6 id="partial-prediction">Partial Prediction</h6><p>XLNet 将句子重新排列，然后根据排列后的顺序使用 AR 方式预测，但是由于句子是随机排列的，会导致优化比较困难且收敛速度慢。因此 XLNet 采用了 Partial Prediction (部分预测) 的方式进行训练，对于排列后的句子，只预测句子末尾的 1/K 个 token。</p><p>例如 K=4，就是只预测最后 1/4 的 token。给定句子 [1,2,3,4,5,6,7,8] 和一种随机排列 [2,8,3,4,5,1,7,6]，则只预测 7 和 6。论文中训练 XLNet-Large 时使用的 K 为 6，大约是预测末尾 <strong>14.3%</strong>的 token。</p><h4 id="实验">实验</h4><p><img src="https://i.loli.net/2020/11/01/i7qlvJjTtxAgw4a.png" alt="image-20201101012324865"></p><p><img src="https://i.loli.net/2020/11/01/pIOMyiU9V2Y6wSz.png" alt="image-20201101012421507"></p><p>消融实验</p><p>排列语言模型和transfomer-xl对效果的影响很大。而且NSP任务对效果的影响倒是几乎没有，这也是上文中我们没有用NSP任务的原因。</p><h4 id="总结">总结</h4><p>XLNet 的核心思想是 PLM，排列原来的句子，然后预测末尾的单词。这样可以学习到单词之间的依赖关系，而且可以利用 token 前后向的信息。</p><blockquote><p><a href="http://aiblog.top/2019/07/12/XLNET%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E5%8F%8A%E7%90%86%E8%A7%A3/" target="_blank" rel="noopener">http://aiblog.top/2019/07/12/XLNET%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E5%8F%8A%E7%90%86%E8%A7%A3/</a></p><p><a href="https://my.oschina.net/u/4373067/blog/4476706" target="_blank" rel="noopener" class="uri">https://my.oschina.net/u/4373067/blog/4476706</a></p><p><a href="https://baijiahao.baidu.com/s?id=1654814515140351919&amp;wfr=spider&amp;for=pc" target="_blank" rel="noopener" class="uri">https://baijiahao.baidu.com/s?id=1654814515140351919&amp;wfr=spider&amp;for=pc</a></p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      记录值得分享的论文;
《XLNet》:

    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-10-30-预训练模型总结</title>
    <link href="http://yoursite.com/2020/10/30/2020-10-30-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2020/10/30/2020-10-30-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93/</id>
    <published>2020-10-30T11:24:42.000Z</published>
    <updated>2020-10-30T12:33:02.872Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言">前言</h3><p>Bert模型自18年10月推出，到目前为止快两年了。它卜一问世即引起轰动，之后，各种改进版本的预训练模型（Pre-Training Model, PTM）与应用如过江之鲫，层出不穷。Bert及它的继任者们，确实也不负众望，在NLP各个领域攻城略地，所向披靡，多种NLP数据集竞赛榜单，连续多年被各种新出现的预训练模型霸榜，有些榜单，个别模型已经把指标刷到超过人类。</p><p>那么，在近两年的时间里，诸多改进模型中，有哪些令人印象深刻的新模型？在那些表现突出的新模型中，是哪些因素导致它们的良好表现？预训练模型技术本身有重大的改动或创新么？或者，关于预训练模型，目前有哪些相对明确的结论？根据目前的技术发展水准，如何根据现有结论，来打造最强的预训练模型？本文通过梳理现有技术文献，试图来回答上述一系列问题。本文的数据都客观有出处，但是对数据的解读，带有严重的个人色彩，偏颇难免，还请谨慎参考。</p><p>我们知道，在预训练模型框架下，解决NLP问题，会划分为序列进行的两阶段：第一阶段是预训练阶段，然后是Fine-tuning阶段，本文集中在预训练阶段。</p><p><img src="https://i.loli.net/2020/10/30/DL9wbrUlSxFBGNm.jpg" alt="img"></p><p>如果我们一句话宏观地归纳预训练模型要做的事情（参考上图），其实很好理解，就是下面这句话：</p><p>在Transformer作为特征抽取器基础上，选定合适的模型结构，通过某种自监督学习任务，逼迫Transformer从大量无标注的自由文本中学习语言知识。这些语言知识以模型参数的方式，存储在Transformer结构中，以供下游任务使用。</p><p>我们见到的形形色色的预训练模型，无非就是，实现上述思路的具体做法而已。你可以换个模型结构，可以换个学习任务，也可以换个其它的部件，无非就是各种试，当然，有些做法相对有效，有些做法效果差些。一般而言，通常所说的预训练模型，都是从自由文本中学习语言知识，很明显，我们可以引入新型的知识或数据，比如人类已经挖掘好的结构化知识、多模态数据、多语言数据等，引入这些知识来促进模型理解语言，或者解决特殊类型的任务。</p><p>后文会先介绍预训练模型中常见的几种模型结构，并给出目前能得出的结论。然后，我们会找出目前表现比较好的那些预训练模型，并分析它们起作用的主要因素是什么。接下来，会简要介绍几种非自由文本类知识学习的预训练基本方法。</p><p>在谈这些之前，我们先从RoBERTa讲起。如果时光倒退半年多，你会发现，这是一个价值被严重低估的模型，其实，它很重要。</p><h3 id="预训练模型中的强基准roberta">预训练模型中的强基准：RoBERTa</h3><p>严格来说，原始的Bert模型是个未完成的半成品，而RoBERTa才是遵循Bert思路的完成品，或者说，Bert是进行时中的RoBERTa，也就是说下列等式成立Bert=RoBERTing。为什么这么说呢？因为，我们可以把RoBERTa看作是得到充分训练的Bert模型，而原始版本的Bert模型训练不够充分，这种模型是否得到充分训练的微小差异，能够极大提升原始版本Bert模型的效果。</p><p><img src="https://i.loli.net/2020/10/30/wvSEJsGYH1cpFbx.jpg" alt="img"></p><p>在原始Bert模型的基础上，RoBERTa通过实验，证明了如下几点：</p><ol type="1"><li>进一步增加预训练数据数量，能够改善模型效果；</li><li>延长预训练时间或增加预训练步数，能够改善模型效果；</li><li>急剧放大预训练的每个Batch的Batch Size，能够明显改善模型效果；</li><li>拿掉预训练任务中的Next Sentence Prediction子任务，它不必要存在；</li><li>输入文本的动态Masking策略有帮助；</li></ol><p>上面列出的五项改进中，第四项和第五项改动，对最终的模型效果影响不大，暂时可忽略。第一点改进增加预训练数据对模型效果有帮助，这个符合直觉。第二项和第三项则涉及到模型是否得到充分训练，本质上这两项相结合，代表了更充分训练的Bert模型。如上面的性能对比图所示，如果以Bert Large作为对比基准，可以发现：仅仅将Batch Size放大，三个数据集上的效果就获得了明显提升，如果再加入新的数据，以及不断增加训练步数，还能持续获得效果的进一步提升。可以看出，RoBERTa效果明显比Bert large好，在相同数据情况下，甚至超过了知名度很高的XLNet。这主要归功于数据规模的增加，以及更充分的训练过程，其中更充分的训练过程发挥的作用更大些。这是为何说RoBERTa 在某种意义上，其实是一个完成版本或者加强版本的Bert模型。</p><p>纵观目前公开的预训练模型，我们可以发现，RoBERTa是其中一个效果非常好的超强基准模型。这句话有几个意思：</p><p>首先，尽管看上去RoBERTa也没做啥技术或者模型改进，只是把Bert模型训练得更充分了一些，但是，它的效果是非常好的。目前为止，效果能够明显超过它的模型很少，屈指可数，这个“屈指可数”，不是虚指，是它的字面含义。这个事实，其实隐含了很大的信息量，它说明了一个什么问题呢？您可以想一想，我的看法在后面小节内容会提到。</p><p>其次，对于一个改进模型来说，理论上都应该引入RoBERTa作为对比Baseline，而改进模型的效果，如果不能具备说服力地超过RoBERTa的话，那么这种改进的有效性，多少是成问题的，除非你强调改进模型的优势不在效果好，而在其它方面，比如更小更快等。</p><p>再次，后续的改进预训练模型，从策略角度讲，应该在设计之初，就站在RoBERTa的巨人肩膀上，就是说在增加一定数据量的前提下，增大Batch Size，加长预训练时间，让模型得到充分训练。因为，如果你不这么做，大概率你的效果是很难比过RoBERTa的，而目前我们能够见到的效果很突出的模型，你如果细究，会发现其实都已经引入了RoBERTa的关键要素了，关于这一点，在后面小节我们会做分析。</p><p>还有，对于追求落地效果的人来说，比如公司里做业务的同学，建议以RoBERTa为基础模型来做应用。</p><h3 id="预训练的发动机模型结构">预训练的发动机：模型结构</h3><p>对于预训练模型来说，目前的主流模型大都采用Transformer作为特征抽取器，现阶段看，Transformer的潜力仍然没有被充分挖掘，还有很大潜力可挖，意思是，Transformer效果足够好，而且还可以更好，貌似改进Transformer并非当务之急的事情。预训练模型的知识，是通过Transformer在训练迭代中从数据中不断学习，并以模型参数的形式编码到模型中的。虽然，大家都是用的Transformer，但是怎么用它搭建模型结构学习效率更高？这是一个问题。所谓学习效率高，就是给定相同大小规模的训练数据，它能编码更多的知识到模型里，这就意味着它的学习效率更高。不同的Transformer用法，会产生不同的模型结构，就会导致不同结构的差异化的学习效率。本节我们归纳下目前能得到的，关于模型结构的现有研究结论，会介绍常见的五种模型结构。当然，这里用模型结构来表达不足够确切，因为除了模型结构外，一般还包含自监督的学习方法，常见的学习方法包括AutoEncoding(简称AE)和AutoRegressive(简称AR)。AE即我们常说的双向语言模型，而AR则代表从左到右的单向语言模型。</p><ul><li><strong>Encoder-AE结构</strong></li></ul><p><img src="https://i.loli.net/2020/10/30/1klNabKIiu73QJq.jpg" alt="img"></p><p>Encoder-AE结构如上图所示。这其实是包括原始版本Bert在内的，大多数后续改进模型采取的结构。整个结构就是一个标准的Transformer，在语言模型预训练的时候，采用AE方法。也就是说，输入句中的未被Mask的任意单词两两可见，但是被Mask掉的单词之间都相互独立，互不可见。在预测某个被Mask掉的单词的时候，所有其它被Mask的单词都不起作用，但是句内未被Mask掉的所有单词，都可以参与当前单词的预测。可以看出，Encoder-AE是个采用双向语言模型的单Transformer结构。</p><p><img src="https://i.loli.net/2020/10/30/isTaJS6GdQLPBZU.jpg" alt="img"></p><p>模型结构比较（From BART）</p><p><img src="https://i.loli.net/2020/10/30/Opc45h7Z3PF1yq6.jpg" alt="img"></p><p>模型结构比较（From Google T5）</p><p>从目前对比实验看（上面两图），除了下文要讲述的Encoder-Decoder结构外，貌似对于语言理解类的NLP任务，这种结构都是效果最好的，但是对于语言生成类的任务，这种结构效果相对很差。也就是说，这种结构比较适合做语言理解类的任务。</p><ul><li><strong>Decoder-AR结构</strong></li></ul><p><img src="https://i.loli.net/2020/10/30/LWPdHeDOo5XRQK2.jpg" alt="img"></p><p>Decoder-AR结构如上图所示。它和Encoder-AE结构相同，都是采用单个的标准Transformer，主要区别在于：语言模型预训练的时候，采用AR方法，就是从左到右逐个生成单词，第i个单词 Wi 只能看到它之前的第1到第（i-1）个单词W1,...,Wi-1 ，不能看到后面的单词。采用这种结构的典型模型就是GPT1、GPT2、GPT3系列了。GPT3在文本生成任务方面的表现，确实是出乎意料地好。当然，这不能仅仅归功于这个结构本身，更复杂的模型和更大量的数据可能是主因。可以看出，Decoder-AR结构是个单向语言模型的单Transformer结构。</p><p>从目前对比实验看（参考Encoder-AE小节的两张效果对比图），除了Encoder-Decoder结构外，貌似对于语言生成类的任务，这种结构是效果最好的结构之一。但是相应的，语言理解类的任务，采用这种结构，效果比Encoder-AE结构差距非常明显，这也好理解，因为只看到上文看不到下文，对于很多语言理解类任务而言，信息损失很大，所以效果不好也在情理之中。也就是说，这种结构比较适合做语言生成类的任务。</p><ul><li><strong>Encoder-Decoder结构</strong></li></ul><p>既然Encoder-AE比较适合做语言理解类的任务，Encoder-AR比较适合做语言生成类的任务。那么，我们能否结合两者的优势，使得预训练模型既能做好生成类NLP任务，又能做好理解类任务呢？这是个很自然的想法，而Encoder-Decoder结构就是如此将两者结合的。最早明确提出使用Encoder-Decoder结构做通用领域预训练的，应该是微软提出的MASS模型，不过和这里介绍的做法有差异。</p><p><img src="https://i.loli.net/2020/10/30/7NG2hnjXitKTOcZ.jpg" alt="img"></p><p>Encoder-Decoder结构如上图所示。这种结构在Encoder侧，单独使用一个Transformer，采用了Encoder-AE的结构。也就是说，编码阶段采用双向语言模型，任意两个单词两两可见，以更充分地编码输入信息；而在Decoder侧，使用另外一个Transformer，采用了Decoder-AR结构，从左到右逐个生成单词。</p><p>当然，Decoder侧和标准的Decoder-AR不同的地方还是有的：Decoder侧生成的单词 Wi，除了像Decoder-AR结构一样能看到在它之前生成的单词序列 W1,...,Wi-1 外，还能看到Encoder侧的所有输入单词 。而这一般是通过Decoder侧对Encoder侧单词，进行Attention操作方式来实现的，这种Attention一般放在Encoder顶层Transformer Block的输出上。</p><p>在进行预训练的时候，Encoder和Decoder会同时对不同Mask部分进行预测：Encoder侧双向语言模型生成被随机Mask掉的部分单词；Decoder侧单向语言模型从左到右生成被Mask掉的一部分连续片断。两个任务联合训练，这样Encoder和Decoder两侧都可以得到比较充分地训练。</p><p><img src="https://i.loli.net/2020/10/30/SOAQbIj9czkhVK6.jpg" alt="img"></p><p>模型结构比较（From UniLM v2）</p><p>从目前对比实验看，无论是语言理解类的任务（参考Encoder-AE部分Google T5论文中展示的效果对比图），还是语言生成类的任务（参考上面来自于UniLM v2的效果对比），貌似Encoder-Decoder结构相对其它几种结构来说，效果都是最好的之一。而且，它有另外一个优点，就是用这个结构，可以同时做生成类和理解类的NLP任务，基本做到了不同任务在模型结构上的统一，这点还是很好的，一个结构可以到处使用，比较方便。但是，它也有个问题，因为两侧各用了一个Transformer，所以相对其它结构参数量翻倍，计算量也增加了，就是说比其它模型笨重。而且，Encoder-Decoder结构比其它结构效果好，很可能主要原因来自于参数量增加导致的模型容量增大，当然这是个人猜测。目前，采用这个结构的效果很好的模型包括Google T5以及BART等模型。</p><ul><li><strong>Prefix LM</strong></li></ul><p><img src="https://i.loli.net/2020/10/30/2b1usgtZPwJa58D.jpg" alt="img"></p><p>Prefix LM结构是Google T5论文中给出的叫法，这种结构最早由UniLM模型提出，我们沿用Google T5的这种称谓。如果深入分析的话，Prefix LM其实是Encoder-Decoder模型的变体：标准的Encoder-Decoder模型，Encoder和Decoder各自使用一个独立的Transformer；而Prefix LM，相当于Encoder和Decoder通过分割的方式，分享了同一个Transformer结构，Encoder部分占用左部，Decoder部分占用右部，这种分割占用是通过在Transformer内部使用Attention Mask来实现的。与标准Encoder-Decoder类似，Prefix LM在Encoder部分采用AE模式，就是任意两个单词都相互可见，Decoder部分采用AR模式，即待生成的单词可以见到Encoder侧所有单词和Decoder侧已经生成的单词，但是不能看未来尚未产生的单词，就是说是从左到右生成。</p><p>目前的一些对比实验证明，在其它条件相同的情况下，关于语言理解类的任务（参考Encoder-AE部分Google T5论文中的相关实验），Prefix LM结构的效果要弱于标准Encoder-Decoder结构。这里是值得深入思考下的，因为看上去Prefix LM和标准的Encoder-Decoder结构是等价的。那么，为什么它的效果比不过Encoder-Decoder结构呢？我想，一方面的原因估计是两者的参数规模差异导致的；另外一方面，可能与它这种模式的Decoder侧对Encoder侧的Attention机制有关。在Decoder侧，Transformer的每层 Block对Encoder做Attention的时候，标准的Encoder-Decoder模式，Attention是建立在Encoder侧的最后输出上，这样可以获得更全面完整的全局整合信息；而Prefix LM这种结构，Decoder侧的每层Transformer对Encoder侧的Attention，是建立在Encoder的对应层上的，因为这种模式的Encoder和Decoder分割了同一个Transformer结构，Attention只能在对应层内的单词之间进行，很难低层跨高层。这可能是影响这种结构效果的原因之一。当然这只是个人猜测，无证据证明，还请谨慎参考。</p><p>关于语言生成类的任务，Prefix LM效果虽然要弱于Encoder-Decoder结构（参考Encoder-Decoder小节UniLM v2论文效果对比图），但是总体而言，两者相差不大，相对其它模型，Prefix LM结构在生成类任务表现也比较突出。</p><p>Prefix LM因为是Encoder-Decoder的变体，所以可以看出，它的优势也在于可以同时进行语言理解和语言生成类任务，而且相对Encoder-Decoder来说，因为只用了一个Transformer，所以模型比较轻，这是Prefix LM的优势。缺点则是在效果方面，貌似要弱于Encoder-Decoder模型的效果，语言理解类任务相对有明显差距，生成类任务的效果相差不大。</p><ul><li><strong>Permuted Language Model(PLM)</strong></li></ul><p>PLM最早是在XLNet的论文中提出的，目前有些后续模型也在PLM上进行改进，所以我们把PLM也放在这里一起说一下。</p><p>PLM一样采用单个Transformer模型作为主干结构，但是从训练方法上来说，是个很另类也很有创意的做法，是种“形为AR，实为AE”的做法。在语言模型预训练过程中，它看上去遵循AR从左到右的输入过程，这符合一般生成任务的外在表现形式，但是在内部通过Attention Mask，实际做法其实是AE的做法，无非是把AE的做法隐藏在Transformer内部。它和AE从细节来说，主要有两个区别：首先，预训练过程中，输入句子去掉了Mask标记，改为内部Attention Mask，以保持预训练过程和下游任务Fine-tuning的一致性。关于这一点，目前有实验证明这个虽然有积极影响，但是影响不大（ELECTRA针对预训练过程是否带Mask 标记做了效果对比，带Mask标记的Bert模型GLUE得分82.2，去掉Mask标记利用其它单词代替的对比模型GLUE得分82.4）；其次，也是它和AE的最主要区别，PLM认为被Mask掉的单词之间是相互有影响的，先产生的被Mask掉的单词，应该对后生成的被Mask掉的单词，在预测的时候发生作用，而标准的AE则认为被Mask掉的单词是相互独立的，相互之间不产生作用。</p><p><img src="https://i.loli.net/2020/10/30/eqPo6kEBMm1Unaz.jpg" alt="img"></p><p>其实，如果你仔细分析下PLM的预训练过程，会发现本质上PLM是Prefix LM的一种变体。上图给出了个例子来说明这种情况，对于某个输入句子，PLM首先会进行单词顺序随机变换，然后选定变换后句子的末尾一部分单词进行Mask，被Mask的单词预测顺序是有序的，按照变换后在句中先后顺序来预测，上面例子中会先预测 X1 ，然后再预测 X5 。在预测 X1 的时候，未被Mask的上下文 [X2,X3,X4] 会对预测 X1 有帮助；假设已经预测并输出了 X1 ，在预测 X5 的时候，未被Mask掉的上下文 [X2,X3,X4] ，以及刚预测出的 X1 ，会对预测 X5 有帮助。其实你想，这等价于什么？等价于以 X4 作为边界切割开的Prefix LM模型，Encoder端包含 [X2,X3,X4] ，Decoder侧包含 [X1,X5] ，在预测 X5 的时候，不仅能看到Encoder侧的所有输入，也能看到Decoder侧之前的输出 X1 。当然，因为每个输入句子的长度各异，被Mask掉的单词个数也不固定，所以看上去Encoder和Decoder的边界根据输入句子，边界是在动态变化的。所以，PLM其实是一种边界变化的Prefix LM变体结构。当然，上面纯属个人推理过程，不保证正确性，谨慎参考。</p><p>如果不考虑XLNet里的其它因素，单纯看PLM结构的话，目前有些对比实验，貌似PLM在语言理解类任务中，效果不及Encoder-AE（参考UniLM v2论文中的对比实验，未在本文列出，可参考论文）；在语言生成类任务中，效果略微优于Encoder-AE，但是距离Decoder-AR差距较大（参考Encoder-AE描述部分BART的对比实验）。在两类任务中，都有点上不着村，下不着店的感觉，就是都还可以，但都不够好的感觉。XLNet效果确实是很好的，但是，这说明XLNet效果好，真正起作用的貌似不是PLM，而是其它因素。</p><p>上面内容简述了常见的五种预训练模型结构，如果总结一下的话：</p><p>首先，从模型效果来看，Encoder-Decoder结构无论在语言理解类还是语言生成类任务中，都是效果最好的。当然，效果好的原因很可能在于模型参数多，模型容量大，而不一定是自身结构带来的优势。它的优点一个是效果好，一个是能够将理解和生成任务统一在一个框架下；缺点是参数多计算多，所以模型比较重。采用这个结构的代表模型包括Google T5和BART。</p><p>其次，因为Encoder-Decoder模型比较重，所以，如果从相对轻量结构里进行选择的话，对于语言理解类任务，Encoder-AE结构相对而言效果较好，代表模型很多，典型的比如ALBert、RoBERTa；对于语言生成类任务，Decoder-AR结构和Prefix LM结构相对而言效果较好，都可考虑，Decoder-AR的代表模型是GPT系列，Prefix LM的代表模型是UniLM。语言理解类任务应该用AE任务，语言生成类任务应该用AR任务，这点也很明确了。</p><p>谈完了模型结构，下面我们来盘点下表现比较好的预训练模型，并分析下效果好背后的原因。</p><h3 id="强者的狂欢为什么有些模型表现这么好">强者的狂欢：为什么有些模型表现这么好</h3><p>目前Bert的改进模型有很多，有的表现非常突出，有的表现一般。我的主要目的是想找出那些表现好的模型，并分析下，到底是哪些因素导致这些模型效果超群的。</p><p>首先，我们需要先找出那些表现特别好的模型出来，我这里说的表现好，主要是从模型效果角度来说的，就是那些在公开数据集上指标比较高的模型。一种比较简单的方法就是：找GLUE、SuperGLUE、SQuAD 2.0这几个大规模NLP数据上，那些打榜模型中排名前列的。你可以看一下，自从Bert出现后，这几个榜单，都长年被预训练模型霸榜，指标在被各种新的预训练模型快速刷高，直到超过人类的水准。一般而言，能够打榜把指标刷到前列的，都是好模型，说明这些模型真的能打（插句闲话，这点其实特别值得推荐领域借鉴，就是有个大规模高难度数据集，供各种模型长年刷榜，这其实是促进领域技术进步很好的手段）。当然，也有一些新模型，可能未必会去打榜，所以作为补充措施，我又从比较新的文献中，找出一些模型，前提是它在文献中报道的效果要比RoBERTa好。这样，我筛出了一批表现优秀的模型，包括：RoBERTa，Google T5，ALBERT，ELECTRA，XLNet，GPT3，BART，UNILM v2, StructBert，MacBert。这些模型要么在某个榜单前几名，要么论文实验结果显示效果非常好，二者占其一。这里面，GPT3是个纯生成模型，ELECTRA相对而言方法比较特殊，在后面我会单独说下它。需要说明的是，ERNIE和NEZHA模型，效果也是非常好的，能够排在某些榜单前列。但是因为它们对应的论文比较早，我猜测现在打榜的模型，估计和原始论文中的做法，已经做了变动，但是具体怎么变的不清楚，所以没有在上面列表中列出。上述表单，应该基本囊括了目前时间（2020年9月）绝大多数效果最好的预训练模型了。</p><p>上述模型，都能找到对应的文章，可供仔细分析模型的有效因素。如果你仔细分析上述各个模型的共性，会发现，那些真正有效的因素会慢慢浮出水面。我在这里归纳一下：促进模型性能快速提高的因素，主要包含下列几方面。而且，这几方面的因素是可叠加的，就是说，如果一个模型采纳其中越多的因素，那么这个模型的效果表现可能会更好。</p><p>首先，更高质量、更多数量的预训练数据。</p><p><img src="https://i.loli.net/2020/10/30/E8VF4gCRMDjSlbm.jpg" alt="img"></p><p>关于预训练数据对模型效果的影响，Google T5做了大量对比实验，目前的结论，如果归纳一下的话，应该是这样的：在保证预训练数据质量的前提下，数据规模越大模型效果越好。这里需要注意的是，数据规模越大越好，这点其实从Bert一出来，就是一个容易想到的重要因素。因为数据量越多，数据里蕴含的知识也越多，那么模型能学到的东西越多，所以模型效果会更好，这是一个靠简单推理就能得出的结论。但是，它是有前提的，前提是数据质量要高，光数据量大不行，很多乱七八糟的数据，反而会对模型效果带来负面影响。</p><p>第二，增加模型容量及复杂度。</p><p><img src="https://i.loli.net/2020/10/30/YuIyA7FQh3JCxpW.jpg" alt="img"></p><p>所谓增加模型容量及复杂度，指的是增加Transformer模型的参数量，一般而言，模型容量越大，模型的表达能力越强。最直接的增加模型容量的方式就是增加Transformer Block层深，比如可以从Bert base的12层，增加到Bert Large的24层，还可以继续增加到比如36层，这是纵向增加复杂度，Google T5走的这条路（从上图可以看出，模型容量增加到4倍后，有些数据集效果相对Baseline有大幅度的提升）。除此外，还可以横向增加模型复杂度，比如在固定Transformer层深的情况下，可以通过放大Transformer中构件的大小，比如Hidden Size的增大，FFN层对隐层的放大，Multi-Head Self Attention的Attention头的增加，等多种方式来做到这一点。ALBERT走的这条路，它的xxLarge模型效果最好，只用了12层Transformer Block，但是Hidden Size达到了4096。</p><p>这两种模式还可以相互结合，就是同时纵向和横向增加模型复杂度，GPT 3即是如此，将模型复杂度这点推到了极致。单词特征的Embedding不会放的太大，一般采用64或者128大小，ALBERT证明了如果单词特征Embedding跟着Transformer内部的Hidden Size同步放大，效果反而会降低。也就是说，增加模型容量指的是放大Transformer模型本身的参数量，但不包括输入层Embedding的参数。</p><p>第三，更充分地训练模型；</p><p>这里所谓的“更充分”，一般指的是放大Batch Size、增加预训练步数，就是RoBERTa做的那两个事情。这块上文有述，这里不再赘述。</p><p>第四，有难度的预训练任务；</p><p><img src="https://i.loli.net/2020/10/30/sILxebBoQuO7wt4.jpg" alt="img"></p><p>原始的Bert预训练，有两个训练任务：一个是单词级的Mask语言模型MLM，一个是句子级的下一句预测任务NSP。RoBERTa证明了NSP对于模型效果没什么影响，所以拿掉了这个任务。有很多研究集中在这一块，采取了五花八门的预训练任务（如上图所示）。那么哪些预训练任务相对而言更有效呢？目前已经能够得出些比较明确的结论。</p><p>如果归纳一下的话，应该是这样的：对于单词级的Mask语言模型来说，Span类的预训练任务效果最好。所谓Span类的任务，就是Mask掉的不是一个独立的单词，而是一个连续的单词片断，要求模型正确预测片断内的所有单词。Span类任务，只是一个统称，它会有一些衍生的变体，比如N-Gram，就是Span模型的一个变体，再比如Mask掉的不是单词而是短语，本质上也是Span类任务的变体，这里我们统称为Span类任务。</p><p>目前有相当多的研究证明Span类任务是效果最好的，最近有些工作（微软的ProphetNet和百度的ERNIE-GEN）进一步说明，Span内多个单词独立被生成效果会更好。所谓独立生成，举个例子，假设被Mask掉的片断是：X1,X2,X3 ，之前一般Span类的预训练是顺序生成片断内的单词，就是先生成 X1 ，然后根据上下文及 X1 ，生成 X2 ，这么个顺序，就是说序列生成片断内单词。而独立生成，就是根据上下文，同时生成 X1,X2,X3 , 被生成的单词之间无影响。所以目前单词级的Mask语言模型，独立生成的Span类任务，应该是目前效果最好的。</p><p>对于句子级的任务，NSP任务学习两个句子是否连续句：正例由两个连续句子构成，负例则随机选择一句跟在前一句之后，要求模型预测两者是否连续句子。本质上，NSP在预测两个句子是否表达相近主题，而这个任务，相对MLM来说，过于简单了，导致模型学不到什么知识。ALBERT采用了句子顺序预测SOP（Sentence Order Prediction）：跟NSP一样，两个连续出现的句子作为正例，但是在构造负例的时候，则交换句子正确顺序，要求模型预测两个句子出现顺序是否正确，这样增加任务难度，StructBERT也采取了类似的做法。实验证明SOP是有效的句子级预测任务。</p><p>总而言之，目前证明Span类任务是有效的单词级任务，SOP是有效的句子级任务。目前看，预训练任务越有难度，则预训练模型越能高效率地学习知识，所以寻找更新的更有难度的预训练任务是有较大探索空间以及成功可能的。</p><p>上面列了四个主要因素，那么，还有其它因素么？我的猜测是基本没有了，尽管可能还有一些差异化的改进点是有效的，但它对最终效果的贡献，应该不是特别大，起码不像上述四个因素那么大。上面四个因素，如果进一步要划分重要性的话，估计前三个都很重要，第四个相对而言影响稍小一些。当然，同样地，这是我个人的猜测，谨慎参考。</p><p><img src="https://i.loli.net/2020/10/30/jeYAPHi61MaJFNf.jpg" alt="img"></p><p>如果我们根据上述可叠加的有效因素，来分析现有模型，可得出如上图所示列表（具备某因素的模型，对应的格子做了标记）。从上表中，我们可以得出一些结论：</p><p>首先，所有这些效果表现突出的模型，都增加了更多的高质量预训练数据。另外，通过增大Batch Size以及增加预训练步数方式，都使得模型得到更充分地训练。也就是说，所有这些表现突出的模型，都是站在RoBERTa模型的肩膀上的。其实，只要你站在RoBERTa肩膀上，效果都不会太差，剩下的问题是能比它好多少的问题。</p><p>其次，如果我来冒昧地做个判断的话，貌似对于语言理解类任务来说，估计Google T5和ALBERT是效果最好的预训练模型；而对于语言生成类的任务来说，估计GPT3是效果最好的模型。对于Google T5和ALBERT模型来说，两者都采纳了绝大部分有效因素，主要不同在于预训练任务，Google T5采用了Span类单词级任务，而ALBERT采用了SOP类句子级任务。这三个表现最突出的模型，和其它模型最大的区别，大概率在于它们在增加更多高质量数据的同时，走了大规模提升模型容量的路子。也就是说，在增加数据规模基础上大规模增加模型容量，这应该是拉开不同模型效果最主要的因素。</p><p>再次，我们可以据此预测，如果一个模型，采纳了上述所有有效因素，那么可以获得当前技术水准下的最好模型效果，就如上表中最后一行展示的，目前仍未知的Model X那样。就是说，这个模型应该是这样的：在RoBERTa模型基础上，增加更多高质量数据的同时，充分放大模型容量，而预训练任务则是单词类Span任务和句子类SOP任务的结合。当然，估计这里面起到主要作用的还是大量数据+大模型的因素。</p><p><img src="https://i.loli.net/2020/10/30/BPVtOXFLJye1ZEC.jpg" alt="img"></p><p>这里单独说下ELECTRA，这是一个比较独特的预训练方法(参考上图)。 它形式上采取了类似GAN的模式，但是本质上并非GAN，因为缺乏GAN最关键的生成器和判别器的对抗训练过程。ELECTRA联合训练了小的生成器以及大的判别器，它强迫判别器对生成器产生的所有单词，做个是否经过改写的判断，这无疑增加了模型的学习效率，因为原先的MLM只学习15%的被Mask单词，而ELECTRA对所有单词都要进行判断，并从中学习。ELECTRA论文做了分析，模型的绝大多数收益来自于全部单词参与训练这一步。这意味着，ELECTRA这种所有单词全员参与训练过程的模式，能够在其它条件相同的情况下（模型复杂度，数据量等），使得模型获得更高的学习效率，这个结论和做法还是很有价值的。本质上，ELECTRA这种提升模型效率的方法，和上面所述其它模型的各种做法，是相互互补的。就是说，在ELECTRA的训练模式下，增加训练数据、增加模型规模、模型充分训练，有可能获得更好的模型效果。</p><h2 id="暴力美学简单粗暴但有效"><strong>暴力美学：简单粗暴但有效</strong></h2><p>前文有述，RoBERTa是个非常强的Baseline，相对目前表现最强的Google T5和ALBERT模型，其实RoBERTa与这两个天花板模型之间，它们之间的性能Gap并不是特别大。其它表现突出的模型，要我猜，性能应该介于RoBERTa这个Baseline和两个天花板模型之间。而所有这些模型之间的主要差异，极有可能是模型容量的大小差异带来的。</p><p>从某种角度上看，我们可以认为：RoBERTa可以被看作是经过更充分训练的Bert模型，而ALBERT/Google T5可以理解为进一步增加了模型复杂度的RoBERTa增强版本。从Bert到RoBERTa，再到ALBERT/Google T5，这三类模型，很可能代表了自Bert出现来的最主要技术进展。所以，从模型改进的角度看，自从Bert诞生后近两年，并没有出现特别有效的模型改进方法。尽管从解决NLP任务效果的角度看，新的预训练模型相比Bert有了巨大的提升，但是这些提升，大致可以理解为是因为引入更多高质量数据、采用更多模型参数、模型训练更充分以及增加训练任务难度这几点综合导致的。而其中，在RoBERTa这种充分训练的模型基础上，增加数据，并加上更大的模型，可能在其中起到了主导作用。</p><p>由此进一步推理，我们可以得出如下结论：目前预训练模型都采用的Transformer结构，从模型容量或模型复杂度来说是足够复杂的。就是说，Transformer结构本身，目前并非制约预训练模型效果的瓶颈，我们可以仅仅通过增加高质量数据、增加模型复杂度配以更充分地模型训练，就仍然能够极大幅度地提升Bert的性能。</p><p>这说明了什么呢？这说明了大数据+大模型的暴力美学，这条粗暴简洁但有效的路子，还远远没有走到尽头，还有很大的潜力可挖。尽管这带来的副作用是：好的预训练模型，训练成本会非常高，这不是每个研究者都能够承受的。但是，我的意见，这其实是个好事情。如果仅仅通过加数据、扩模型就能获得更好的效果，这么简单的方式就能推动模型效果不断上升，推动更多应用获得更好效果，这不是天大的好事么？ 至于由此带来的大模型落地难的问题，我相信可以通过搭配知识蒸馏等把模型做小的方案来获得解决。就是说，很可能预训练模型发展会走出一个哑铃模式：两头大，中间小。两个大头中，一头是越来越大的预训练模型，一头是追求各种技术来实用化地把模型做小，这两端会越来越重要。</p><p>如果上述假设成立，即预训练领域的暴力美学依然暴力且美丽，那么从今往后的模型改进，我们应该怎么走呢？我的感觉，应该优先探索大数据+大模型的路，先走到暴力美学的尽头，然后再集中精力探索模型本身的改进。就是说，我们应该先把数据红利吃完，而不是优先发展新型模型，当然两者可以并行做，但是原则上，新型模型优先级不如先把数据红利吃完。为什么这么说呢？因为，目前很多研究表明：大多数改进新模型带来的提升，根本比不过提升数据质量数量的同时扩充模型容量带来的收益。而一些新模型的有效性，在数据量小的时候可能是有效的，但很可能发生的一幕是，当数据增大模型容量加大后，很多改进不再有效。也就是说，目前很多新模型的作用，很可能是增加了特殊类型的语言知识的编码和泛化能力，但是，这是完全可以通过增加数据数量和质量，并加大模型来达成的，这种方式又比较简单直观。所以，这是为何我觉得应该先把精力放到“大数据+大模型” 上，然后再集中精力进行模型改进的主要原因。</p><h2 id="知识补习班其它知识的引入"><strong>知识补习班：其它知识的引入</strong></h2><p>本文开头讲过，大多数预训练模型是从自由文本中学习语言知识。但是，很明显，我们能让模型学的，肯定不止自由文本这一种类型。理论上，任何包含知识的数据，都有些先验知识可供预训练模型学习。我的感觉，预训练模型的发展，会越来越像人脑，日益变成一个黑盒子。就是说，我们可以通过一定手段，喂给它数据，它就会学会其中包含的知识。但是，它是怎么学会的，学到了什么，这很可能对我们来说，会越来越难以理解，就是说，随着预训练模型学习领域的拓展，这个黑盒子，可能会越来越黑。下面我们介绍两个典型的其它领域，看看预训练模型是怎么学的。当然，我相信这种预训练方式，会拓展到越来越多的其它类型的数据或领域，这也是预训练模型领域，一个比较明晰的发展趋势。</p><ul><li><strong>显示知识的引入</strong></li></ul><p>原始Bert的语言学知识，是从大量自由文本中自主学习的，那么很自然的一个问题就是：我们过去已经通过一些技术手段，归纳出大量的结构化知识，比如知识图谱；或者已经建立了很多知识分析工具，比如命名实体识别系统等。那么能否利用这些知识识别工具，抑或已有的结构化知识，让预训练模型能够直接学到这些知识？</p><p>目前也有很多工作在做这个事情，就是让预训练模型能够编码更多的结构化知识或者语言知识。至于如何做，有两种典型的思路：一种以百度ERNIE为代表；一种以清华ERNIE为代表。这两个工作是最早做这个事情的，差不多同时出来，但思路不同，正好是两种具备代表性的方案。</p><p><img src="https://i.loli.net/2020/10/30/OjoakV1tr5MJYEN.jpg" alt="img"></p><p>百度ERNIE的思路是：在预训练阶段被Mask掉的对象上做文章，我们可以使用比如命名实体识别工具／短语识别工具，将输入中的命名实体或者部分短语Mask掉（参考上图），这些被Mask掉的片断，代表了某种类型的语言学知识，通过这种方式，强迫预训练模型去强化地学习相关知识。</p><p><img src="https://i.loli.net/2020/10/30/mwGZYnuSbeJj6xV.jpg" alt="img"></p><p>清华ERNIE则是另外一种思路：我们已经有些结构化知识或者实体关系知识等现成的外部知识库，可以在预训练的过程中，通过工具找出句中的命名实体，句中的命名实体可以触发知识库中其它相关实体，然后预训练模型通过特殊的结构，来融合文本和结构化知识，以进一步促进语言的理解（参考上图）。这是另外一种思路。</p><p>关于知识的融入，后续还有很多工作，但是大体走的是上面两条路线之一。关于将显示知识或者结构化知识引入预训练模型，我是这么看的，纯属个人意见：</p><p>我觉得，假设说我们用来预训练的数据量特别特别大，而且特征抽取器的能力特别强。理论上，结构化知识是蕴含在这些文本内的，因为我们的外部知识库也是通过技术手段从自由文本里挖掘出来的。假设上面两个条件同时能够被满足，理论上，不太需要单独再把结构化知识独立补充给Bert这类预训练模型，预训练模型应该能够直接从自由文本中就学会这些知识。但是，以我们目前的技术条件，上面两个条件完全被满足，还是有一定难度的。于是，在这种约束下，感觉独立强化知识，让Bert在编码的时候更重视这些结构化知识，看上去是有一定补充作用的。我猜测，比较高频出现的知识，已经能够通过常规的语言模型预训练能够捕获了，很可能对于那些偏冷门的知识，引入结构化知识，会对预训练模型做下游任务有直接促进作用。而可以预见的是：随着机器资源能力越来越强大，如果在第一个预训练阶段，不断加大数据数量和质量，不断增加Transformer模型容量，那么，单独补充结构化知识给预训练模型，收益可能会越来越小。当然，以目前的技术发展阶段，感觉这个事情还有空间和潜力可挖掘。当然，上面说的是通用知识，如果手上的外部知识库，领域性很强，通用训练数据中包含的相关领域数据很少，那么，直接把知识引入，对于解决问题还是很有必要的。</p><ul><li><strong>多模态预训练</strong></li></ul><p>随着存储容量越来越大、网络传输速度越来越快、计算速度越来越强，除了传统的文字内容外，图片、视频、音频等各种多模态信息在互联网的内容占比中越来越多。如何融合多种模态信息进行内容理解，就变得越来越重要。那么，能否将多模态信息纳入预训练的框架之内呢？这是个非常有现实价值的问题。</p><p>前文有述，自由文本的预训练，本质上是让模型从海量自由文本中，通过语言模型等任务，来学习其中蕴含的的语言学知识。由此自然引发的问题就是：多模态预训练也是要将某种新型的知识塞到模型参数里，那么，这是一种什么样的知识呢？本质上，多模态预训练要学习的知识是两种模态之间，或者多种模态之间，的知识单元映射关系。比如对于文字-图片这两种多模态信息来说，我们可以把图片想像成一种特殊类型的语言，多模态预训练希望让模型学会这两种不同模态之间的语义映射关系，比如能够将单词“苹果”和图片中出现的苹果区域建立起联系。或者说，希望通过将不同模态的信息映射到相同的语义空间，来学会两者之间的语义映射关系。</p><p><img src="https://i.loli.net/2020/10/30/mrwA5gk478FcNHT.jpg" alt="img"></p><p>如果我们能够成功地学会这种不同媒介间的语义映射，那么就可以做很多有意思的事情，比如说句话，搜出与这句话语义相近的图片（参考上图）；或者反过来，输入一个图片，能够找到或者生成对应的文字描述。再比如VQA（参考上图），就是给定一张图片，你可以针对图片提出一些问题，AI系统能够回答你的问题，给出正确答案。这涉及到图片-文字的跨媒体问答以及一些跨媒体的知识推理。而要想实现这种能力，如何通过预训练模型，让模型学会两种模态之间的语义映射关系就是至关重要的。</p><p>我们面临的第一个问题是：从什么样的数据里来学习不同模态之间的语义映射关系呢？自由文本的预训练模型，可以采纳海量无标注数据来做，然而，多模态预训练要学习不同模态信息间的语义映射关系，所以需要有标注好的“模态1-模态2”的对齐数据，比如：标注好的“文本-图片”或者“文本-视频”平行数据。只有具备跨模态对齐数据，模型才有可能从中学习不同媒介类型之间的语义映射关系。从这个角度讲，相对自由文本预训练来说，多模态预训练因为需要模态对齐训练数据，而这种数据往往是需要人工标注的，所以可获得的数据难度及成本就高了很多，明显不如文本预训练那么自由。</p><p>总体而言，目前的多模态预训练任务中，通常都是“双模态”预训练，常见的包括“文本-图片”、“文本-视频”、“视频-音频”等模态类型组合。其中， 相对而言，“文本-图片”类型的任务技术发展比较快，其它类型的多模态类型发展相对缓慢，我猜测这里的主要原因在于可用标注数据的差异。“文本-图片”目前有一些规模达到几十万到上百万规模的标注数据集合，典型的比如MS-COCO、Visual Gnome等，而其它类型的模态组合数据貌似缺乏大规模数据集合，这严重影响了领域技术进展。下面我们从“文本-图片”这种模态组合来宏观介绍下多模态预训练的常规做法，其它模态组合的技术方案差不太多，所缺的可能主要是标注好的模态对齐数据。</p><p>我们从模型结构和训练目标这两个角度来阐述。目前的大多数技术方案大同小异，主要差异在于采用了不同的模型结构及与不同训练目标的差异组合。</p><p>假设我们有“文本-图片”两种模态数据，需要联合学习三种预训练模型：文本模态自身的预训练模型，图片模态自身的预训练模型，以及两个模态之间的语义对齐预训练模型。从模型结构来说，目前主流的结构有两种：双流交互模型以及单流交互模型。</p><p><img src="https://i.loli.net/2020/10/30/tkIUsf5xFBVwuq4.jpg" alt="img"></p><p>典型双流交互模型结构如上图LXMERT模型所示。文本编码器代表一个流，一般采用Transformer模型捕捉文本单词之间的关系；图片编码器代表另外一个流，一般也是采用Transformer模型，对于图片来说，一般用Faster-RCNN模型识别出图片中包含的多个物体及其对应的矩形位置信息，将高置信度的物体及其对应的位置信息作为图片侧Transformer的输入，用来学习图片中物品的相互关系；在两个流之上，再加入额外的Transformer模型，用于融合两个模态的语义映射关系。在这种双流结构上，模型同时学习文本预训练目标、图片预训练目标，以及图片-文本对齐预训练目标。一般文本预训练目标和标准的Bert做法类似，通过随机Mask一部分文本单词的语言模型来做；图片预训练目标类似，可以Mask掉图片中包含的部分物品，要求模型正确预测物品类别或者预测物品Embedding编码；为了能够让两个模态语义对齐，一般还要学习一个跨模态目标，常规做法是将对齐语料中的“文本-图片”作为正例，随机选择部分图片或者文本作为负例，来要求模型正确做二分类问题，通过这种方式逼迫模型学习两种模态间的对齐关系。典型的双流模型包括LXMERT、ViLBERT等。</p><p><img src="https://i.loli.net/2020/10/30/hyXxRlIvs63Tabm.jpg" alt="img"></p><p>典型的单流交互模型结构如上图Unicoder-VL模型所示。单流和双流的区别在于：单流模型只用一个Transformer，而双流模型，如上所述，需要三个Transformer各自分工协作。输入的图片，经过上述的Faster-RCNN物体识别和位置编码后，和文本单词拼接，整体作为Transformer模型的输入。也就是说，单流模型靠单个Transformer，同时学习文本内部单词交互、图片中包含物体之间大的交互，以及文本-图片之间的细粒度语义单元之间的交互信息。单流模型的预训练目标，与双流交互模型是类似的，往往也需要联合学习文本预训练、图片预训练以及对齐预训练三个目标。典型的单流模型包括Unicoder-VL、VisualBERT、VL-VERT、UNITER等。</p><p><img src="https://i.loli.net/2020/10/30/5aqhOLVsnGuNIjf.jpg" alt="img"></p><p>经过多模态预训练之后，是否模型能够建立起不同模态信息之间的语义映射关系呢？答案可以参考上图：经过预训练后，输入一句话以及对应的图片进入模型，对于文本中的某个单词，我们可以观察这个单词与图片中哪块区域联系密切（根据Attention强度信息可以看出）。从上图示例可以看出，预训练模型确实学会了不同模态单词语义之间的映射关系。</p><p>多模态模型经过预训练之后，针对具体的应用任务，可以采取第二阶段Fine-tuning的模式增强应用效果。从上述描述可见，单流模型结构相对简单，模型参数也相对少些，而且能够在模型底层及早对不同模态之间的语义直接建立联系，所以看起来比双流模式更有发展前景，但是从目前的各种研究对比实验结果看，貌似两种方法的效果在伯仲之间。不过，可以得出的结论是，采用预训练模型的多模态方法，比不用预训练的传统方法，在应用效果上是有明显提升的。</p><p>目前来看，如果希望多模态预训练有更快速的技术发展，以下几个方面是需要重点关注的：</p><p>首先，也是最重要的，可能是急需构建不同模态间的大规模对齐数据。目前，“图片-文本”类型的对齐数据规模尚可，但是继续扩大数据规模无疑是有益的；对其它类型的模态组合而言，大规模的标准对齐数据比较缺乏，这会严重制约多模态预训练的发展。所以明显需要数据先行，这是发展技术的前提条件；</p><p>其次，感觉在自由文本预训练研究领域中，目前得到的一些得到验证的经验，推理起来，应该是能够直接迁移到多模态预训练领域的。典型的经验，比如：在扩大数据规模的同时，增加模型复杂度。增加模型复杂度包括图片特征抽取器模型复杂度（已经有实验验证加深ResNet模型对效果提升明显），以及增加对应的Transformer层深，放大Transformer的Hidden Size等，相信这是能够大幅提升多模态预训练的首选手段；再比如文本预训练任务中的Mask对象，采用Span方式而非单词方式（已有工作这么做了），加大Batch Size延长训练时间等训练方法优化手段，想来都应该是有益的；从训练目标来说，目前的模态间对齐任务还是有点类似NSP这种句子分类任务，明显偏简单了一些，这块可以考虑引入更有难度的对齐任务，以及实体级别细粒度的对齐任务，来增强模态对齐模型的效果。</p><p>再次，可以考虑由目前的两模态向真正的多模态扩展，比如三模态动态联合训练，目前常见的是“文本-图片”，或者“文本-视频”，通常是两模态结构，后面可以考虑“文本-图片-音频”，或者“文本-视频-音频”等三模态甚至更多模态的联合预训练。当然，这么做的前提，仍然是得先有多模态的对齐数据。</p><h2 id="多多益善从两阶段模型到四阶段模型"><strong>多多益善：从两阶段模型到四阶段模型</strong></h2><p>经典的预训练模型框架下，一般我们解决NLP问题有两个阶段：第一阶段是模型预训练阶段，预训练模型从文本等信息中学习语言知识；第二阶段是Fine-tuning阶段，根据手上的有监督数据，对模型参数进行微调，以获得更好的任务效果。</p><p>前文有述，预训练阶段的最明显发展趋势是大数据+大模型，在数据质量有保障的前提下，数据量越大，模型容量越大，预训练阶段学到的语言知识效果越好。其实，关于预训练数据，目前还有很多研究，能够得出另外一个结论：从领域、题材、类型等不同角度看，如果预训练数据和手上任务数据越接近，则预训练模型带来的收益就越大。</p><p>很多时候，我们手头上的任务数据有很强的领域性，比如可能是计算机领域的，因为预训练数据一般具备通用性，即使大量预训练文本里包含部分计算机类的文本，整体占比也很小。于是，这种情况下，由于领域差异比较大，预训练模型带给手头任务的收益，就没期望中那么大。一种直观的，也是不少人在用的解决方案是：把领域性文本，也加入到预训练数据中，一同参与预训练过程，这样能够增加预训练文本和手上任务的相似性，就能提升任务效果。事实上，这样做也确实能解决这个问题。但是，有一个问题：预训练阶段往往会兼顾模型的通用性，尽可能兼顾各种下游任务，希望模型能在不同领域都有效。而且，从趋势看，数据规模和模型规模会越来越大，也就是训练成本会越来越高。所以，这种把领域数据添加到预训练数据一起训练的做法，一则影响模型通用性，二则实现成本高，看上去就不是特别好的方法。</p><p>目前看，要解决这个问题，比较好的方法是把两个阶段分离：第一阶段仍然采取大数据、大模型，走通用普适、各种任务都能受益的路子，不特意考虑领域特点，因为兼顾不过来；第二阶段，在第一阶段训练好的通用预训练模型基础上，利用领域数据，再做一次预训练，等于把通用的预训练模型往领域方向拉动一下。这样两个阶段各司其职，有独立的优化目标，也能兼顾通用性和领域适配性。</p><p><img src="https://i.loli.net/2020/10/30/ELKz135biWBOFyr.jpg" alt="img"></p><p>上面这个方法，我猜应该不少人都已经在这么做了，论文“Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks”也通过大量实验验证了领域数据预训练（DAPT）的有效性，再结合它得出的另外一个重要结论：用手上的任务数据，无论大小，如果做一次任务级数据预训练（TAPT），也就是拿着手上任务数据，在通用预训练模型基础上，再做一次预训练，也能够有效提升任务效果。综合这个文章和其它有关文章的结论，我们不难看出，要想更好地提升任务效果，我们应该从传统的两阶段模型，拓展到如下四阶段模型（参考上图）：</p><p>第一个阶段：通用预训练</p><p>这就是传统两阶段模式中的第一阶段。这个阶段不仅仅追求效果好，也追求领域通用性。它的优化目标是：在尽可能多的下游任务场景中，效果都尽可能好，但不单独考虑某个特殊领域的效果如何。这个阶段，目前看总的发展趋势是：在数据质量有保证的前提下，增加数据数量，以及数据的多样性，同时提升模型复杂度，这样可以提供普遍有效的模型增强能力。很明显，这个阶段，一般只有土豪公司才能做得起，而且从趋势看，会越来越如此。将来的发展模式可能是，超级土豪公司不断优化这个模型，然后放出来供大家用，有能力做这个事情的人，应该会越来越少。</p><p>第二个阶段：领域预训练</p><p>在第一阶段训练好的通用预训练模型基础上，利用不同领域的自由文本，构建多个、不同领域的领域预训练模型。比如我们可以分别收集计算机领域、生物领域、电商领域…等等，多个不同领域的无标注自由文本数据。在第一阶段通用模型基础上，分别用各个领域数据，再分别做一次预训练，这样我们就得到了适合解决各个不同领域的预训练模型：计算机领域、生物领域、电商领域…..等等多个不同的预训练模型。下游任务可以根据自己任务的领域，选择适配性好的领域预训练模型来使用。</p><p>这个阶段的预训练模型，在训练的时候，有个独特的问题需要解决：灾难遗忘问题。所谓“灾难遗忘”，就是说，当你用领域数据进行预训练的时候，因为会调整第一阶段预训练模型的参数，这种偏向领域性的参数调整，可能会导致第一阶段模型学好的参数被改写，这意味着：经过第二阶段预训练，第一阶段预训练模型里学会的很多通用语言知识，可能会被冲掉。灾难遗忘就是这个意思。灾难遗忘问题，对于预训练模型，尤其是领域预训练模型来说，是个很关键也很重要的问题，目前也有一些解决方案，限于篇幅，这里就不展开了。</p><p>这个阶段的预训练，因为数据量相比第一阶段会小很多，所以其实中农公司甚至贫农公司也能做得起，不存在土豪门槛，大家应该都能做。当然，一般我们只做跟自己手头任务相关的领域的预训练模型。如果你想做很多领域的预训练模型，那估计也要备足银行卡。估计后续也会有土豪公司做好很多不同领域的预训练模型，供大家个性化适配使用，虽说目前还没有，但是推断起来，这是个大概率会发生的事件。</p><p>第三个阶段：任务预训练</p><p>在前两个预训练模型基础上，比如从第二个阶段里面的多个不同的领域预训练模型中，选择和手头任务适配的那个领域预训练模型，在这个模型基础上，用手头数据，抛掉数据标签，再做一次预训练，无论手上任务数据有多少。比如手上任务是计算机领域的，那么从第二阶段的多个领域模型里面，选择计算机领域适配过的预训练模型，在这个模型基础上进行一次任务级别的预训练。这样应该能明显提升任务效果。</p><p>第四阶段：任务Fine-tuning</p><p>这是传统两阶段的第二阶段，做法一样，没什么好讲的。</p><p>当然，如果你手上的任务没有那么强的领域性，可以跳过第二阶段，也就是那个领域预训练模型阶段，走剩余的三阶段模式即可，无论如何，任务预训练都是值得做的一个事情。</p><h2 id="聚沙成塔如何建造强大的预训练模型"><strong>聚沙成塔：如何建造强大的预训练模型</strong></h2><p>上文从不同角度或维度，总结了预训练模型某个方面的一些结论，我们综合起来看一下。不论出于什么目的，打榜也好，把手头应用做得更出色也好，如果我们综合各个维度的现有信息，那么，在当前技术水准下，如何构造强大的预训练模型，貌似是可以得出相对明晰结论的。因为NLP里面既有语言理解类任务，也有语言生成类任务，两者差异较大，所以我们分头来看。</p><p><img src="https://i.loli.net/2020/10/30/VdANbfwutJ3MyWn.jpg" alt="img"></p><p>对于语言理解类任务，我假设你的任务不是领域性特别强那种类型的，建议采取如下技术方案：</p><p>使用三阶段模型：通用预训练+任务预训练+任务Fine-tuning。在做完第一阶段预训练后，用手头任务数据，抛掉标签，再做一次任务预训练，然后任务Fine-tuning。</p><p>模型结构建议采取Encoder+Decoder结构，或者Encoder-AE结构；预训练任务配置两个：独立生成Span类语言模型及SOP句子任务；在质量优先的前提下，增加预训练数据的数量；比较关键的一点是，一定要增加模型容量：可以纵向增加Transformer Block层深，或者横向调大Transformer相应位置可配置参数大小。当然，如果你不差钱，两个可以一起上。另外，要使得模型得到充分训练，就是说增大训练过程中的Batch Size和训练步长。</p><p>对于语言生成类任务，建议采取如下技术方案：</p><p>使用两阶段模型：通用预训练+任务Fine-tuning。模型结构建议采取Encoder+Decoder结构，或者Decoder-AR结构；预训练任务采用独立生成Span类语言模型；在质量优先的前提下，增加预训练数据的数量；同样，比较关键的一点是，一定要增加模型容量：可以纵向增加Transformer Block层深，或者横向调大Transformer相应位置可配置参数大小。当然，如果你不差钱，两个可以一起上。另外，也要使得模型得到充分训练，就是说增大训练过程中的Batch Size和训练步长。</p><p>相信采取上述技术方案，你能在打榜过程中获得很好的名次，或者在实际工作中能比较快地完成自己的KPI或OKR。当然，如果是走落地应用的路子，关于知识蒸馏等一系列如何将模型做小这方面，记得要多花点功夫。</p><h3 id="参考">参考</h3><blockquote><p><a href="https://zhuanlan.zhihu.com/p/254821426" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/254821426</a></p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      对于BERT以及之后的预训练模型的总结概括
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-10-29-深度学习调参技巧汇总</title>
    <link href="http://yoursite.com/2020/10/29/2020-10-29-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%B0%83%E5%8F%82%E6%8A%80%E5%B7%A7%E6%B1%87%E6%80%BB/"/>
    <id>http://yoursite.com/2020/10/29/2020-10-29-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%B0%83%E5%8F%82%E6%8A%80%E5%B7%A7%E6%B1%87%E6%80%BB/</id>
    <published>2020-10-29T11:11:29.000Z</published>
    <updated>2020-10-29T13:16:10.809Z</updated>
    
    <content type="html"><![CDATA[<script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script&gt;
        document.querySelectorAll(&#39;.github-emoji&#39;)
          .forEach(el =&gt; {
            if (!el.dataset.src) { return; }
        
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-10-28-transformer综述</title>
    <link href="http://yoursite.com/2020/10/28/2020-10-28-transformer%E7%BB%BC%E8%BF%B0/"/>
    <id>http://yoursite.com/2020/10/28/2020-10-28-transformer%E7%BB%BC%E8%BF%B0/</id>
    <published>2020-10-28T12:39:37.000Z</published>
    <updated>2020-11-07T13:57:10.149Z</updated>
    
    <content type="html"><![CDATA[<h3 id="transformer家族1----transformer详解和源码分析">🚀Transformer家族1 -- Transformer详解和源码分析</h3><h4 id="transformer总体结构">1 Transformer总体结构</h4><p>近几年NLP领域有了突飞猛进的发展，预训练模型功不可没。当前利用预训练模型（pretrain models）在下游任务中进行fine-tune，已经成为了大部分NLP任务的固定范式。Transformer摒弃了RNN的序列结构，完全采用attention和全连接，严格来说不属于预训练模型。但它却是当前几乎所有pretrain models的基本结构，为pretrain models打下了坚实的基础，并逐步发展出了transformer-XL，reformer等优化架构。本文结合论文和源码，对transformer基本结构，进行详细分析。</p><p>Transformer是谷歌在2017年6月提出，发表在NIPS2017上。论文地址 <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need</a>。 分析的代码为Harvardnlp的代码，基于PyTorch， 地址 <a href="https://github.com/harvardnlp/annotated-transformer" target="_blank" rel="noopener">annotated-transformer</a></p><p>Transformer主体框架是一个<strong>encoder-decoder</strong>结构，去掉了RNN序列结构，完全基于attention和全连接。在WMT2014英语翻译德语任务上，bleu值达到了28.4，达到当时的SOTA。其总体结构如下所示</p><p><img src="https://i.loli.net/2020/10/28/6z5wSWXlaQnAF2E.png" alt="在这里插入图片描述"></p><p>总体为一个典型的encoder-decoder结构。代码如下</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 整个模型入口</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_model</span><span class="params">(src_vocab, tgt_vocab, N=<span class="number">6</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">               d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, h=<span class="number">8</span>, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">    <span class="string">"Helper: Construct a model from hyperparameters."</span></span><br><span class="line">    c = copy.deepcopy</span><br><span class="line"></span><br><span class="line">    <span class="comment"># multiHead attention</span></span><br><span class="line">    attn = MultiHeadedAttention(h, d_model)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># feed-forward</span></span><br><span class="line">    ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># position-encoding</span></span><br><span class="line">    position = PositionalEncoding(d_model, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 整体为一个encoder-decoder</span></span><br><span class="line">    model = EncoderDecoder(</span><br><span class="line">        <span class="comment"># encoder编码层</span></span><br><span class="line">        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),</span><br><span class="line"></span><br><span class="line">        <span class="comment"># decoder解码层</span></span><br><span class="line">        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 编码层输入，输入语句进行token embedding和position embedding</span></span><br><span class="line">        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 解码层输入，同样需要做token embedding和position embedding</span></span><br><span class="line">        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),</span><br><span class="line"></span><br><span class="line">        <span class="comment"># linear + softmax，查找vocab中概率最大的字</span></span><br><span class="line">        Generator(d_model, tgt_vocab))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># This was important from their code. </span></span><br><span class="line">    <span class="comment"># Initialize parameters with Glorot / fan_avg.</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">            nn.init.xavier_uniform(p)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"><span class="number">1234567891011121314151617181920212223242526272829303132333435363738</span></span><br></pre></td></tr></tbody></table></figure><p>make_model为Transformer模型定义的入口，它先定义了multi-head attention、feed-forward、position-encoding等一系列子模块，然后定义了一个encoder-decoder结构并返回。下面来看encoder-decoder定义。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderDecoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    一个标准的encoder和decoder框架，可以自定义embedding、encoder、decoder等</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, encoder, decoder, src_embed, tgt_embed, generator)</span>:</span></span><br><span class="line">        super(EncoderDecoder, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># encoder和decoder通过构造函数传入，可灵活更改</span></span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line"></span><br><span class="line">        <span class="comment"># src和target的embedding，也是通过构造函数传入，方便灵活更改</span></span><br><span class="line">        self.src_embed = src_embed</span><br><span class="line">        self.tgt_embed = tgt_embed</span><br><span class="line"></span><br><span class="line">        <span class="comment"># linear + softmax</span></span><br><span class="line">        self.generator = generator</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, src, tgt, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="string">"Take in and process masked src and target sequences."</span></span><br><span class="line">        <span class="comment"># 先对输入进行encode，然后再通过decode输出</span></span><br><span class="line">        <span class="keyword">return</span> self.decode(self.encode(src, src_mask), src_mask,</span><br><span class="line">                            tgt, tgt_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, src, src_mask)</span>:</span></span><br><span class="line">        <span class="comment"># 先对输入进行embedding，然后再经过encoder</span></span><br><span class="line">        <span class="keyword">return</span> self.encoder(self.src_embed(src), src_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span><span class="params">(self, memory, src_mask, tgt, tgt_mask)</span>:</span></span><br><span class="line">        <span class="comment"># 先对目标进行embedding，然后经过decoder</span></span><br><span class="line">        <span class="keyword">return</span> self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)</span><br><span class="line"><span class="number">12345678910111213141516171819202122232425262728293031</span></span><br></pre></td></tr></tbody></table></figure><p>encoder-decoder定义了一个标准的编码解码框架，其中编码器、解码器均可以自定义，有很强的泛化能力。模块运行时会调用forward函数，它先对输入进行encode，然后再通过decode输出。我们就不详细展开了。</p><h4 id="encoder">2 encoder</h4><h5 id="encoder定义">2.1 encoder定义</h5><p>encoder分为两部分</p><ol type="1"><li><strong>输入层embedding</strong>。输入层对inputs文本做token embedding，并对每个字做position encoding，然后叠加在一起，作为最终的输入。</li><li><strong>编码层encoding</strong>。编码层是多层结构相同的layer堆叠而成。每个layer又包括两部分，multi-head self-attention和feed-forward全连接，并在每部分加入了残差连接和归一化。</li></ol><p>代码实现上也验证了这一点。我们看EncoderDecoder类中的encode函数，它先利用输入embedding层对原始输入进行embedding，然后再通过编码层进行encoding。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderDecoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, src, src_mask)</span>:</span></span><br><span class="line">        <span class="comment"># 先对输入进行embedding，然后再经过encoder</span></span><br><span class="line">        <span class="keyword">return</span> self.encoder(self.src_embed(src), src_mask)</span><br><span class="line"><span class="number">1234</span></span><br></pre></td></tr></tbody></table></figure><h5 id="输入层embedding">2.2 输入层embedding</h5><p>原始文本经过embedding层进行向量化，它包括token embedding和position embedding两层。</p><h6 id="token-embedding">2.2.1 token embedding</h6><p>token embedding对文本进行向量化，一般来说有两种方式</p><ol type="1"><li>采用<strong>固定词向量</strong>，比如利用Word2vec预先训练好的。这种方式是LSTM时代常用的方式，比较简单省事，无需训练。但由于词向量是固定的，不能解决一词多义的问题，词语本身也不是contextual的，没有结合上下文语境信息，另外对于不在词向量中的词语，比如特定领域词语或者新词，容易出现OOV问题。</li><li>随机初始化，然后<strong>训练</strong>。这种方式比较麻烦，需要大规模训练语料，但能解决固定词向量的一系列问题。Transformer采用了这种方式。</li></ol><p>另外，基于Transformer的BERT模型在中文处理时，直接基于字做embedding，优点有</p><ol type="1"><li>无需分词，故不会引入分词误差。事实上，只要训练语料充分，模型自然就可以学到分词信息了。</li><li>中文字个数固定，不会导致OOV问题</li><li>中文字相对词，数量少很多，embedding层参数大大缩小，减小了模型体积，并加快了训练速度。</li></ol><p>事实上，就算在LSTM时代，很多case中，我们也碰到过基于字的embedding的效果比基于词的要好一些。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Embeddings</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="comment"># token embedding，随机初始化训练，然后查表找到每个字的embedding</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, vocab)</span>:</span></span><br><span class="line">        super(Embeddings, self).__init__()</span><br><span class="line">        <span class="comment"># 构建一个随机初始化的词向量表，[vocab_size, d_model]。 bert中的设置为[21128, 768]</span></span><br><span class="line">        self.lut = nn.Embedding(vocab, d_model)</span><br><span class="line">        self.d_model = d_model</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># 从词向量表中查找字对应的embedding向量</span></span><br><span class="line">        <span class="keyword">return</span> self.lut(x) * math.sqrt(self.d_model)</span><br><span class="line"></span><br><span class="line"><span class="number">123456789101112</span></span><br></pre></td></tr></tbody></table></figure><p>由代码可见，Transformer采用的是随机初始化，然后训练的方式。词向量维度为[vocab_size, d_model]。例如BERT中为[21128, 768]，参数量还是很大的。ALBert针对embedding层进行矩阵分解，大大减小了embedding层体积。</p><h6 id="position-encoding">2.2.2 position encoding</h6><p>首先一个问题，为啥要进行位置编码呢。原因在于self-attention，将任意两个字之间距离缩小为1，丢失了字的位置信息，故我们需要加上这一信息。我们也可以想到两种方法</p><ol type="1"><li><strong>固定编码</strong>。Transformer采用了这一方式，通过奇数列cos函数，偶数列sin函数方式，利用三角函数对位置进行固定编码。</li><li><strong>动态训练</strong>。BERT采用了这种方式。先随机初始化一个embedding table，然后训练得到table 参数值。predict时通过embedding_lookup找到每个位置的embedding。这种方式和token embedding类似。</li></ol><p>哪一种方法好呢？个人以为各有利弊</p><ol type="1"><li>固定编码方式简洁，不需要训练。且不受embedding table维度影响，理论上可以支持任意长度文本。（但要尽量避免预测文本很长，但训练集文本较短的case）</li><li>动态训练方式，在语料比较大时，准确度比较好。但需要训练，且最致命的是，限制了输入文本长度。当文本长度大于position embedding table维度时，超出的position无法查表得到embedding（可以理解为OOV了）。这也是为什么BERT模型文本长度最大512的原因。</li></ol><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="comment"># 位置编码。transformer利用编码方式实现，无需训练。bert则采用训练embedding_lookup方式</span></span><br><span class="line">    <span class="comment"># 编码方式文本语句长度不受限，但准确度不高</span></span><br><span class="line">    <span class="comment"># 训练方式文本长度会受position维度限制（这也是为什么bert只能处理最大512个字原因），但训练数据多时，准确率高</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, dropout, max_len=<span class="number">5000</span>)</span>:</span></span><br><span class="line">        super(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 采用sin和cos进行position encoding</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) *</span><br><span class="line">                             -(math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)        <span class="comment"># 偶数列</span></span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)        <span class="comment"># 奇数列</span></span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        self.register_buffer(<span class="string">'pe'</span>, pe)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># token embedding和position encoding加在一起</span></span><br><span class="line">        x = x + Variable(self.pe[:, :x.size(<span class="number">1</span>)], </span><br><span class="line">                         requires_grad=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br><span class="line"></span><br><span class="line"><span class="number">123456789101112131415161718192021222324</span></span><br></pre></td></tr></tbody></table></figure><p>由代码可见，position encoding直接采用了三角函数。对偶数列采用sin，奇数列采用cos。 <img src="https://i.loli.net/2020/10/28/zPU8Z9scbDNkS5A.png" alt="在这里插入图片描述"></p><h5 id="编码层">2.3 编码层</h5><p>Encoder层是Transformer的核心，它由<strong>N层相同结构的layer</strong>（默认6层）堆叠而成。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Core encoder is a stack of N layers"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layer, N)</span>:</span></span><br><span class="line">        super(Encoder, self).__init__()</span><br><span class="line">        <span class="comment"># N层堆叠而成，每一层结构都是相同的，训练参数不同</span></span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># layer normalization</span></span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        <span class="number">45</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask)</span>:</span></span><br><span class="line">        <span class="comment"># 1 经过N层堆叠的multi-head attention + feed-forward</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2 对encoder最终输出结果进行layer-norm归一化。层间和层内子模块都做过 add + dropout + layer-norm</span></span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br><span class="line"><span class="number">1234567891011121314151617</span></span><br></pre></td></tr></tbody></table></figure><p>encoder的定义很简洁。先经过N层相同结构的layer，然后再进行归一化输出。重点我们来看layer的定义。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Encoder is made up of self-attn and feed forward (defined below)"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, self_attn, feed_forward, dropout)</span>:</span></span><br><span class="line">        super(EncoderLayer, self).__init__()</span><br><span class="line">        <span class="comment"># 1 self_attention</span></span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2 feed_forward</span></span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3 残差连接。encoder和decoder，每层结构，每个子结构，都有残差连接。</span></span><br><span class="line">        <span class="comment"># add + drop-out + layer-norm</span></span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">2</span>)</span><br><span class="line">        self.size = size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask)</span>:</span></span><br><span class="line">        <span class="comment"># 经过self_attention, 然后和输入进行add + layer-norm</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, mask))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 经过feed_forward， 此模块也有add + layer-norm</span></span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">1</span>](x, self.feed_forward)</span><br><span class="line"></span><br><span class="line"><span class="number">12345678910111213141516171819202122</span></span><br></pre></td></tr></tbody></table></figure><p>encoder layer分为两个子模块</p><ol type="1"><li><strong>self attention</strong>, 并对输入attention前的和经过attention输出的，做残差连接。残差连接先经过layer-norm归一化，然后进行dropout，最后再做add。后面我们详细分析</li><li><strong>feed-forward</strong>全连接，也有残差连接的存在，方式和self attention相同。</li></ol><h6 id="multiheadedattention">2.3.1 MultiHeadedAttention</h6><p>MultiHeaded Attention采用多头self-attention。它先将隐向量切分为h个头，然后每个头内部进行self-attention计算，最后再concat再一起。</p><p><img src="https://i.loli.net/2020/10/28/K4lYeqtN7jUR5fx.png" alt="在这里插入图片描述"> 代码如下</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadedAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, h, d_model, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        super(MultiHeadedAttention, self).__init__()</span><br><span class="line">        <span class="keyword">assert</span> d_model % h == <span class="number">0</span></span><br><span class="line">        <span class="comment"># d_model为隐层维度，也是embedding的维度，h为多头个数。</span></span><br><span class="line">        <span class="comment"># d_k为每个头的隐层维度，要除以多头个数。也就是加入了多头，总隐层维度不变。</span></span><br><span class="line">        self.d_k = d_model // h</span><br><span class="line">        self.h = h</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 线性连接</span></span><br><span class="line">        self.linears = clones(nn.Linear(d_model, d_model), <span class="number">4</span>)</span><br><span class="line">        self.attn = <span class="literal">None</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, mask=None)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 输入mask，在decoder的时候有用到。decode时不能看到要生成字之后的字，所以需要mask</span></span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        nbatches = query.size(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 1) q, k, v形状变化，加入多头， [batch, L, d_model] =&gt; [batch, h, L, d_model/h]</span></span><br><span class="line">        query, key, value = [l(x).view(nbatches, <span class="number">-1</span>, self.h, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">             <span class="keyword">for</span> l, x <span class="keyword">in</span> zip(self.linears, (query, key, value))]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2) attention计算</span></span><br><span class="line">        x, self.attn = attention(query, key, value, mask=mask, </span><br><span class="line">                                 dropout=self.dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3) 多头结果concat在一起，还原为初始形状</span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(nbatches, <span class="number">-1</span>, self.h * self.d_k)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 4）最后经过一个线性层</span></span><br><span class="line">        <span class="keyword">return</span> self.linears[<span class="number">-1</span>](x)</span><br><span class="line"><span class="number">123456789101112131415161718192021222324252627282930313233</span></span><br></pre></td></tr></tbody></table></figure><p>下面重点来看单个头的self-attention。也就是论文中的“Scaled Dot-Product Attention”。attention本质上是一个向量的加权求和。它探讨的是每个位置对当前位置的贡献。步骤如下</p><ol type="1"><li>q向量和每个位置的k向量计算点积，然后除以向量长度的根号。计算点积可以认为是进行权重计算。除以向量长度原因是向量越长，q*k值理论上会越大，故需要在向量长度上做归一化。</li><li><strong>attention-mask</strong>。mask和输入矩阵shape相同，mask矩阵中值为0位置对应的输入矩阵的值更改为-1e9，一个非常非常小的数，经过softmax后趋近于0。decoder中使用了mask，后面我们详细分析。</li><li>softmax归一化，使得q向量和每个位置的k向量的score分布到（0, 1）之间</li><li>加权系数乘以每个位置v向量，然后加起来。</li></ol><p>公式如下：<img src="https://i.loli.net/2020/10/28/C8lDMeBadc4yftg.png" alt="在这里插入图片描述"> 代码如下</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention</span><span class="params">(query, key, value, mask=None, dropout=None)</span>:</span></span><br><span class="line">    <span class="comment"># attention计算，self_attention和soft-attention都是使用这个函数</span></span><br><span class="line">    <span class="comment"># self-attention, q k v 均来自同一文本。要么是encoder，要么是decoder</span></span><br><span class="line">    <span class="comment"># soft-attention, q来自decoder，k和v来自encoder，从而按照decoder和encoder相关性，将encoder信息融合进来</span></span><br><span class="line">    d_k = query.size(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 利用q * k计算两向量间相关度，相关度高则权重大。</span></span><br><span class="line">    <span class="comment"># 除以根号dk的原因是，对向量长度进行归一化。q和k的向量长度越长，q*k的值越大</span></span><br><span class="line">    scores = torch.matmul(query, key.transpose(<span class="number">-2</span>, <span class="number">-1</span>)) / math.sqrt(d_k)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># attention-mask，将 mask中为1的 元素所在的索引，在a中相同的的索引处替换为 value</span></span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        scores = scores.masked_fill(mask == <span class="number">0</span>, <span class="number">-1e9</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># softmax归一化</span></span><br><span class="line">    p_attn = F.softmax(scores, dim = <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># dropout</span></span><br><span class="line">    <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        p_attn = dropout(p_attn)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 最后利用归一化后的加权系数，乘以每一个v向量，再加和在一起，作为attention后的向量。每个字对应一个向量</span></span><br><span class="line">    <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br><span class="line"><span class="number">1234567891011121314151617181920212223</span></span><br></pre></td></tr></tbody></table></figure><p>self-attention和soft-attention共用了这个函数，他们之间的唯一区别是<strong>q k v向量的来源不同</strong>。self-attention中q k v 均来自同一文本。而decoder的soft-attention，q来自于decoder，k和v来自于encoder。它体现的是encoder对decoder的加权贡献。</p><h6 id="positionwisefeedforward">2.3.2 PositionwiseFeedForward</h6><p>feed-forward本质是一个两层的全连接，全连接之间加入了relu非线性和dropout。比较简单，代码如下</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionwiseFeedForward</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="comment"># 全连接层</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, d_ff, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        super(PositionwiseFeedForward, self).__init__()</span><br><span class="line">        <span class="comment"># 第一层全连接  [d_model, d_ff]</span></span><br><span class="line">        self.w_1 = nn.Linear(d_model, d_ff)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第二层全连接 [d_ff, d_model]</span></span><br><span class="line">        self.w_2 = nn.Linear(d_ff, d_model)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># dropout</span></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># 全连接1 -&gt; relu -&gt; dropout -&gt; 全连接2</span></span><br><span class="line">        <span class="keyword">return</span> self.w_2(self.dropout(F.relu(self.w_1(x))))</span><br><span class="line"><span class="number">12345678910111213141516</span></span><br></pre></td></tr></tbody></table></figure><p>总体过程是：<strong>全连接1 -&gt; relu -&gt; dropout -&gt; 全连接2</strong>。两层全连接内部没有shortcut，这儿不要搞混了。</p><h6 id="sublayerconnection">2.3.3 SublayerConnection</h6><p>在每层的self-attention和feed-forward模块中，均应用了残差连接。残差连接先对输入进行layerNorm归一化，然后送入attention或feed-forward模块，然后经过dropout，最后再和原始输入相加。这样做的好处是，让每一层attention和feed-forward模块的输入值，均是经过归一化的，保持在一个量级上，从而可以加快收敛速度。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SublayerConnection</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A residual connection followed by a layer norm.</span></span><br><span class="line"><span class="string">    Note for code simplicity the norm is first as opposed to last.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, dropout)</span>:</span></span><br><span class="line">        super(SublayerConnection, self).__init__()</span><br><span class="line">        <span class="comment"># layer-norm 归一化</span></span><br><span class="line">        self.norm = LayerNorm(size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># dropout</span></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, sublayer)</span>:</span></span><br><span class="line">        <span class="comment"># 先对输入进行layer-norm, 然后经过attention等相关模块，再经过dropout，最后再和输入相加</span></span><br><span class="line">        <span class="keyword">return</span> x + self.dropout(sublayer(self.norm(x)))</span><br><span class="line"><span class="number">12345678910111213141516</span></span><br></pre></td></tr></tbody></table></figure><p>从forward函数可见，先对输入进行layer-norm, 然后经过attention等相关模块，再经过dropout，最后再和输入相加。残差连接的作用就不说了，参考ResNet。</p><h4 id="decoder">3 decoder</h4><p>decoder结构和encoder大体相同，也是堆叠了N层相同结构的layer（默认6层）。不同的是，decoder的每个子层包括三层。</p><ol type="1"><li><strong>masked multi-head self-attention</strong>。这一部分和encoder基本相同，区别在于decoder为了保证模型不能看见要预测字的后面位置的字，加入了mask，从而避免未来信息的穿越问题。mask为一个上三角矩阵，上三角全为1，下三角和对角线全为0</li><li><strong>multi-head soft-attention</strong>。soft-attention和self-attention结构基本相同，甚至实现函数都是同一个。唯一的区别在于，self-attention的q k v矩阵来自同一个，所以叫self-attention。而soft-attention的q来自decoder，k和v来自encoder。表征的是encoder的整体输出对于decoder的贡献。</li><li><strong>feed-forward</strong>。这一块基本相同。</li></ol><p>另外三个模块均使用了残差连接，步骤仍然为 layerNorm -&gt; attention等模块 -&gt; dropout -&gt; 和输入进行add</p><p>decoder每个layer代码如下</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Decoder is made of self-attn, src-attn, and feed forward (defined below)"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, self_attn, src_attn, feed_forward, dropout)</span>:</span></span><br><span class="line">        super(DecoderLayer, self).__init__()</span><br><span class="line">        self.size = size</span><br><span class="line"></span><br><span class="line">        <span class="comment"># self-attention 自注意力</span></span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line"></span><br><span class="line">        <span class="comment"># soft-attenton, encoder的输出对decoder的作用</span></span><br><span class="line">        self.src_attn = src_attn</span><br><span class="line"></span><br><span class="line">        <span class="comment"># feed-forward 全连接</span></span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 残差连接</span></span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, memory, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="comment"># memory为encoder最终输出</span></span><br><span class="line">        m = memory</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1 对decoder输入做self-attention, 再和输入做残差连接</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, tgt_mask))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2 对encoder输出和decoder当前进行soft-attention，此处也有残差连接</span></span><br><span class="line">        x = self.sublayer[<span class="number">1</span>](x, <span class="keyword">lambda</span> x: self.src_attn(x, m, m, src_mask))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3 feed-forward全连接，也有残差连接</span></span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">2</span>](x, self.feed_forward)</span><br><span class="line"><span class="number">123456789101112131415161718192021222324252627282930</span></span><br></pre></td></tr></tbody></table></figure><h4 id="输出层">4 输出层</h4><p>decoder的输出作为最终输出层的输入，经过两步</p><ol type="1"><li>linear线性连接，也即是w * x + b</li><li>softmax归一化，向量长度等于vocabulary的长度，得到vocabulary中每个字的概率。利用beam-search等方法，即可得到生成结果。</li></ol><p>这一层比较简单，代码如下</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Define standard linear + softmax generation step."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, vocab)</span>:</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        self.proj = nn.Linear(d_model, vocab)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># 先经过linear线性层，然后经过softmax得到归一化概率分布</span></span><br><span class="line">        <span class="comment"># 输出向量长度等于vocabulary的维度</span></span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(self.proj(x), dim=<span class="number">-1</span>)</span><br><span class="line"><span class="number">12345678910</span></span><br></pre></td></tr></tbody></table></figure><h4 id="总结">5 总结</h4><p>Transformer相比LSTM的优点</p><ol type="1"><li><strong>完全的并行计算</strong>，Transformer的attention和feed-forward，均可以并行计算。而LSTM则依赖上一时刻，必须串行</li><li><strong>减少长程依赖</strong>，利用self-attention将每个字之间距离缩短为1，大大缓解了长距离依赖问题</li><li><strong>提高网络深度</strong>。由于大大缓解了长程依赖梯度衰减问题，Transformer网络可以很深，基于Transformer的BERT甚至可以做到24层。而LSTM一般只有2层或者4层。网络越深，高阶特征捕获能力越好，模型performance也可以越高。</li><li><strong>真正的双向网络</strong>。Transformer可以同时融合前后位置的信息，而双向LSTM只是简单的将两个方向的结果相加，严格来说仍然是单向的。</li><li><strong>可解释性强</strong>。完全基于attention的Transformer，可以表达字与字之间的相关关系，可解释性更强。</li></ol><p>Transformer也不是一定就比LSTM好，它的缺点如下</p><ol type="1"><li>文本长度很长时，比如篇章级别，<strong>计算量爆炸</strong>。self-attention的计算量为O(n^2), n为文本长度。Transformer-xl利用层级方式，将计算速度提升了1800倍</li><li>Transformer位置信息只靠<strong>position encoding</strong>，效果比较一般。当语句较短时，比如小于10个字，Transformer效果不一定比LSTM好</li><li>Transformer参数量较大，在大规模数据集上，效果远好于LSTM。但在<strong>小规模数据集</strong>上，如果不是利用pretrain models，效果不一定有LSTM好。</li></ol><h3 id="transformer家族2----编码长度优化transformer-xllongformer">🚀Transformer家族2 -- 编码长度优化（Transformer-XL、Longformer）</h3><h4 id="背景">1 背景</h4><p>NLP中经常出现长程依赖问题，比如一个词语可能和它距离上千位置的另一个词语有关系。长程关系的建立十分困难。常见序列结构模型都有一些难点，如下。</p><ol type="1"><li>在RNN中，由于反向传播梯度衰减和梯度爆炸问题，使得模型只能捕获较短距离。</li><li>LSTM利用门限机制，将连乘转变了为连加，提升了模型长程捕获能力，但梯度弥散问题没有从根本上得到解决，故其最大程度只能在400左右。</li><li>Transformer利用self-attention机制进行建模，使得任何两个位置token距离都为1。如果没有内存和算力的限制，Transformer理论上可以编码无限长的文本。但由于attention计算量十分大，而且计算复杂度和序列长度为O(n^2)关系，导致序列长度增加，内存和计算量消耗飞快增加。实际中由于内存和算力有限，一般只能编码一定长度，例如512。</li></ol><p>为了提升模型的长程编码能力，从而提升模型在长文本，特别是document-level语料上的效果，我们必须对Transformer编码长度进行优化。本文带来了Transformer-XL、Longformer，详细分析他们如何实现编码长度优化。</p><p>LongFormer通过降低attention计算所需内存和算力，来实现长文本编码。我们也可以把它归入到算力优化中。但鉴于其名字就重点体现了它的长距离能力，故还是放在了编码长度优化中，和Transformer-XL一起来分析</p><h4 id="transformer-xl">2 Transformer-XL</h4><p><img src="https://i.loli.net/2020/10/28/ctFMhR8IEdPVSZ1.png" alt="在这里插入图片描述"> 论文信息：2019年01月，谷歌 &amp; CMU，ACL 2019 论文地址 https://arxiv.org/abs/1901.02860 代码和模型地址 https://github.com/kimiyoung/transformer-xl</p><h5 id="为什么需要transformer-xl">2.1 为什么需要Transformer-XL</h5><p>为了解决长文本编码问题，原版Transformer采用了固定编码长度的方案，例如512个token。将长文本按照固定长度，切分为多个segment。每个segment内部单独编码，segment之间不产生交互信息。这种方式的问题如下</p><ol type="1"><li>模型无法建模超过固定编码长度的文本</li><li>segment之间没有交互信息，导致了文本碎片化。长语句的编码效果有待提升。</li><li>predict阶段，decoder每生成一个字，就往后挪一个，没有重复利用之前信息，导致计算量爆炸</li></ol><p>train和evaluate过程如下<img src="https://i.loli.net/2020/10/28/8YW7R4Q5J6zbmD1.png" alt="在这里插入图片描述"></p><h5 id="实现方法">2.2 实现方法</h5><h6 id="segment-level-recurrence-with-state-reuse-片段级递归和信息复用">2.2.1 Segment-Level Recurrence with State Reuse 片段级递归和信息复用</h6><p>Transformer-XL在编码后一个segment时，将前一个segment的隐层缓存下来。后一个segment的self-attention计算，会使用到前一个segment的隐层。后一个segment的第n+1层，对前一个segment的第n层隐层进行融合。故最大编码长度理论上为O(N × L)。在预测阶段，由于对segment隐层使用了缓存，故每预测一个词，不需要重新对之前的文本进行计算。大大提升了预测速度，最大可达到原始Transformer的1800倍。如下图所示 <img src="https://i.loli.net/2020/10/28/KmazXviordxyZuw.png" alt="在这里插入图片描述"></p><h6 id="relative-positional-encodings-相对位置编码">2.2.2 Relative Positional Encodings 相对位置编码</h6><p>segment递归中有个比较大的问题，就是如何区分不同segment中的相同位置。如果采用原版Transformer中的绝对编码方案，两者是无法区分的。如下 <img src="https://i.loli.net/2020/10/28/38l7XhVjZnqyuBe.png" alt="gh">不同segment中的相同位置，其position encoding会相同。这显然是有问题的。Transformer-XL将绝对位置编码改为了q和k之间的相对位置编码，代表了两个token之间的相对位置。从语义上讲，是make sense的。我们来看看具体实现方式。</p><p>绝对位置编码的attention计算如下 <img src="https://i.loli.net/2020/10/28/IKCVE7riwaQ9Zl1.png" alt="在这里插入图片描述">分为四部分</p><ol type="1"><li>query的token encoding和 key的token encoding，之间的关联信息</li><li>query的token encoding和 key的position encoding，之间的关联信息。Uj为绝对位置j的编码向量</li><li>query的position encoding和 key的token encoding，之间的关联信息。Ui为绝对位置i的编码向量</li><li>query的position encoding和 key的position encoding，之间的关联信息</li></ol><p>而采用相对位置编码后，attention计算如下 <img src="https://i.loli.net/2020/10/28/Id9fV3bxSnmHyu2.png" alt="在这里插入图片描述">同样包含四部分，仍然为二者token encoding和position encoding之间的关联关系。区别在于</p><ol type="1"><li>Ri-j为i和j之间相对位置编码，其中R为相对位置编码矩阵</li><li>u和v为query的位置编码，采用一个固定向量。因为采用相对位置编码后，无法对单个绝对位置进行编码了。文中称为global content bias，和global positional bias</li></ol><p>也有其他文章，采用了不同的相对位置编码方案。比如"Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.2018. Self-attention with relative position representations. arXiv preprint arXiv:1803.02155." 中只有a和b两部分，丢掉了c和d。Transformer-XL对两种方案进行了对比实验，证明前一种好。</p><h5 id="实验结果">2.3 实验结果</h5><h6 id="长文本编码效果">长文本编码效果</h6><p><img src="https://i.loli.net/2020/10/28/IyoR924J3BQWrOu.png" alt="在这里插入图片描述"> 在WikiText-103上的实验结果。WikiText-103包含词级别的超长文本，平均每篇文章长度为3.6K token。利用它可以验证模型的长文本编码能力。实验结果表明Transformer-XL large的PPL最低，效果最好。同时作者在One Billion Word、enwik8、text8上均进行了实验，都表明Transformer-XL效果最好。</p><h6 id="有效编码长度">有效编码长度</h6><p><img src="https://i.loli.net/2020/10/28/oKQv3akDcXWOZ7B.png" alt="在这里插入图片描述"> 模型可编码的有效长度如上，r为top-r难度样本上的表现。Transformer-XL比RNN长80%，比Transformer长450%。证明Transformer-XL可编码长度最长，长程捕获能力最强。</p><h6 id="预测速度">预测速度</h6><p><img src="https://i.loli.net/2020/10/28/g2TyZhWtRSEA7pB.png" alt="在这里插入图片描述"> 在不同segment长度下，模型预测速度的对比。和原始Transformer相比，预测速度最大可以提升1874倍。</p><h6 id="消融分析">消融分析</h6><p>文章对片段级递归和相对位置编码两个Method进行了消融分析，如下 <img src="https://i.loli.net/2020/10/28/4EeY78JcQojSuhD.png" alt="在这里插入图片描述"> 两个改进点均有作用，其中片段级递归作用更大。</p><h4 id="longformer">3 Longformer</h4><p><img src="https://i.loli.net/2020/10/28/KhcS2VIyvNEAwrg.png" alt="在这里插入图片描述"> 论文信息：2020年04月，allenai 论文地址 https://arxiv.org/abs/2004.05150 代码和模型地址 https://github.com/allenai/longformer</p><h5 id="改进方法">3.1 改进方法</h5><h6 id="attention稀疏化">3.1.1 attention稀疏化</h6><p>Transformer不能捕获长距离信息，本质原因还是因为计算量过大导致的。那我们通过降低attention计算量，是不是就可以提升长距离编码能力呢。答案是肯定的，LongFormer提出了三种attention稀疏化的方法，来降低计算量。 <img src="https://i.loli.net/2020/10/28/hDmpknyqTSIUEW7.png" alt="在这里插入图片描述"> a是原始的全连接方式的attention。后面三种为文章使用的稀疏attention。</p><ol type="1"><li>Sliding Window attention。滑窗方式的attention。假设序列长度为n，滑窗大小w，则每个位置的attention只和滑窗范围内的token进行计算，复杂度从O(n^2)变为了O(n * w)。当w &lt;&lt; n时，可以大大降低计算量。</li><li>Dilated Sliding Window attention。受到空洞卷积的启发，提出了空洞滑窗attention。看下面这张图就明白了。</li></ol><p><img src="https://i.loli.net/2020/10/28/ZwAy1H3kB7SjIP5.png" alt="在这里插入图片描述"></p><ol type="1"><li>Global Attention + sliding window。某些关键位置采用全局attention，这些位置的attention和所有token进行计算。而其他相对不关键的位置则采用滑窗attention。那什么叫关键位置呢？作者举例，分类问题中[CLS]为关键位置，需要计算全局attention。QA中question每个位置均为关键位置，同样计算全局attention。</li></ol><h6 id="tensor-virtual-machine-tvm">3.1.2 Tensor Virtual Machine (TVM)</h6><p>作者使用了TVM构建CUDA kernel，加快了longformer的速度，并降低了显存需求。这个又是另一个模型加速方面的话题，我们就先不展开了。</p><h5 id="实验结果-1">3.2 实验结果</h5><h6 id="大小模型效果">大小模型效果</h6><p><img src="https://i.loli.net/2020/10/28/ZThJDNXByQp2Lod.png" alt="在这里插入图片描述">作者在大小模型上均实验了LongFormer的效果。小模型为12 layers，512 hidden。大模型为30 layers，512 hidden。在text8和enwik8数据集上。小模型达到了SOTA。大模型比18层的Transformer-XL好，虽然不如Adaptive-span-Transformer和Compressive，但胜在可以pretrain-finetune</p><h6 id="消融分析-1">消融分析</h6><p><img src="https://i.loli.net/2020/10/28/pMCoW2ZLv6gq3j4.png" alt="在这里插入图片描述">消融分析中，可以发现</p><ol type="1"><li>Dilation空洞，有一定的收益</li><li>top layer滑窗大小比bottom layer大时，效果好一些。这个也是make sense的。因为top layer捕获高维语义，关联信息距离相对较远，窗口应该尽量大一些。</li></ol><h6 id="语料长度">语料长度</h6><p><img src="https://i.loli.net/2020/10/28/IjGfH7Mi1LK2hsC.png" alt="在这里插入图片描述"> 从上表中我们发现，语料都是特别长的长文本。LongFormer真的是document级别的Transformer。</p><h6 id="下游任务finetune效果">下游任务finetune效果</h6><p><img src="https://i.loli.net/2020/10/28/JdZUrh6L3QFBHIn.png" alt="在这里插入图片描述"> <img src="https://i.loli.net/2020/10/28/WlOZR3iaJSA2nIU.png" alt="在这里插入图片描述"> 第一个table为RoBERTa和LongFormer在问答、指代消解、分类任务中的对比。第二个table为这几个任务数据集平均文本长度。每项任务都是超出RoBERTa，当文本长度大于512时，performance提升特别明显。更加说明了长程捕获能力在NLP中的重要性。</p><h3 id="transformer家族3----计算效率优化adaptive-spanreformerlite-transformer">🚀Transformer家族3 -- 计算效率优化（Adaptive-Span、Reformer、Lite-Transformer）</h3><h4 id="背景-1">1 背景</h4><p>上文我们从编码长度优化的角度，分析了如何对Transformer进行优化。Transformer-XL、LongFormer等模型，通过片段递归和attention稀疏化等方法，将长文本编码能力提升到了很高的高度。基本已经克服了Transformer长文本捕获能力偏弱的问题，使得下游任务模型performance得到了较大提升，特别是文本较长（大于512）的任务上。</p><p>但Transformer计算量和内存消耗过大的问题，还亟待解决。事实上，Transformer-XL、LongFormer已经大大降低了内存和算力消耗。毕竟Transformer之所以长距离编码能力偏弱，就是因为其计算量是序列长度的平方关系，对算力需求过大，导致当前GPU/TPU不能满足需求。编码长度优化和计算量优化，二者是相辅相成的。但着眼于论文的出发点，我们还是分为两个不同的章节进行分析。毕竟总不能所有模型都放在一个章节吧（_）。</p><p>本文我们带来Adaptive-Span Transformer、Reformer、Lite-Transformer等几篇文章</p><h4 id="adaptive-span-transformer">2 Adaptive-Span Transformer</h4><p><img src="https://i.loli.net/2020/10/28/SZIDO7KcoGHqvhF.png" alt="在这里插入图片描述"> 论文信息：2019年5月，FaceBook，ACL2019 论文地址 https://arxiv.org/pdf/1905.07799.pdf 代码和模型地址 https://github.com/facebookresearch/adaptive-span</p><h5 id="为什么需要adaptive-span">2.1 为什么需要Adaptive-Span</h5><p>之前Transformer-XL将长文本编码能力提升到了较高高度，但是否每个layer的每个head，都需要这么长的attention呢？尽管使用了多种优化手段，长距离attention毕竟还是需要较大的内存和算力。研究发现，大部分head只需要50左右的attention长度，只有少部分head需要较长的attention。这个是make sense的，大部分token只和它附近的token有关联。如下图 <img src="https://i.loli.net/2020/10/28/nOstVdScKG68CDz.png" alt="在这里插入图片描述">我们是否可以实现attention span长度的自适应呢？让不同的layer的不同的head，自己去学习自己的attention span长度呢？Adaptive-Span Transformer给出了肯定答案。</p><h5 id="实现方案">2.2 实现方案</h5><p>文章设定每个attention head内的token计算，都使用同一个span长度。我们就可以利用attention mask来实现自适应span。对每个head都添加一个attention mask，mask为0的位置不进行attention计算。文章设计的mask函数如下 <img src="https://i.loli.net/2020/10/28/yRVqrjfJPgBKlIk.png" alt="在这里插入图片描述"> R为超参，控制曲线平滑度。其为单调递减函数，如下图。 <img src="https://i.loli.net/2020/10/28/hvoFmDIp1BjgV5f.png" alt="在这里插入图片描述"></p><h5 id="实验结果-2">2.3 实验结果</h5><p><img src="https://i.loli.net/2020/10/28/EDsBxfipL4HcOwF.png" alt="在这里插入图片描述"> 和Transformer家族其他很多模型一样，Adaptive-span也在字符级别的语言模型上进行了验证，数据集为text8。如上，Transformer注意力长度固定为512，结论如下</p><ol type="1"><li>Transformer-XL长程编码能力确实很强，平均span可达3800。</li><li>注意力长度确实不需要总那么长，Adaptive-Span大模型上，平均长度只有245</li><li>Adaptive-Span在算力需求很小（只有XL的1/3）的情况下，效果可以达到SOTA。</li></ol><p><img src="https://i.loli.net/2020/10/28/IDjAR2ucfELsegw.png" alt="在这里插入图片描述">上面是在enwik8上的结果。Adaptive-Span又一次在算力很小的情况下，达到了最优效果。值得注意的是，64层的Transformer居然需要120G的计算量，又一次证明了原版Transformer是多么的吃计算资源。另外Transformer-XL在节省计算资源上，其实也算可圈可点。</p><h4 id="reformer">3 Reformer</h4><p><img src="https://i.loli.net/2020/10/28/G8AVxR7fuCZdPHn.png" alt="在这里插入图片描述"> 论文信息：2020年1月，谷歌，ICLR2020 论文地址 https://arxiv.org/abs/2001.04451 代码和模型地址 https://github.com/google/trax/tree/master/trax/models/reformer</p><h5 id="为什么需要reformer">3.1 为什么需要Reformer</h5><p>Transformer内存和计算量消耗大的问题，一直以来广为诟病，并导致其一直不能在长文本上进行应用。（BERT、RoBERTa均设置最大长度为512）。Reformer认为Transformer有三大问题</p><ol type="1"><li>attention层计算量和序列长度为平方关系，导致无法进行长距离编码</li><li>内存占用和模型层数呈N倍关系，导致加深Transformer层数，消耗的内存特别大</li><li>feed-forward的dff比隐层dmodel一般大很多，导致FF层占用的内存特别大</li></ol><p>针对这几个问题，Reformer创新性的提出了三点改进方案</p><ol type="1"><li>LOCALITY-SENSITIVE HASHING 局部敏感hash，使得计算量从 O(L^2)降低为O(L log L) ,L为序列长度</li><li>Reversible Transformer 可逆Transformer，使得N层layers内存消耗变为只需要一层，从而使得模型加深不会受内存限制。</li><li>Feed-forward Chunking 分块全连接，大大降低了feed-forward层的内存消耗。</li></ol><p>Reformer是Transformer家族中最为关键的几个模型之一（去掉之一貌似都可以，顶多Transformer-XL不答应），其创新新也特别新颖，很多思想值得我们深入思考和借鉴。其效果也是特别明显，大大提高了内存和计算资源效率，编码长度可达64k。下面针对它的三点改进方案进行分析，有点难懂哦。</p><h5 id="实现方案-1">3.2 实现方案</h5><h6 id="locality-sensitive-hashing-局部敏感hash">3.2.1 LOCALITY-SENSITIVE HASHING 局部敏感hash</h6><p>局部敏感hash有点难懂，Reformer针对Transformer结构进行了深度灵魂拷问</p><h6 id="query和key必须用两套吗">Query和Key必须用两套吗</h6><p>Transformer主体结构为attention，原版attention计算方法如下 <img src="https://i.loli.net/2020/10/28/Z1fYDpO6BnLyHj4.png" alt="在这里插入图片描述"> 每个token，利用其query向量，和其他token的key向量进行点乘，从而代表两个token之间的相关性。归一化后，利用得到的相关性权重，对每个token的value向量进行加权求和。首先一个问题就是，query和key向量可以是同一套吗？我们可否利用key向量去和其他token的key计算相关性呢？</p><p>为此文章进行实验分析，证明是可行的。个人认为这一点也是make sense的。 <img src="https://i.loli.net/2020/10/28/YyTZDG1Bohkswit.png" alt="在这里插入图片描述">在文本和图像上，Q=K的attention，和普通attention，效果差别不大。</p><h6 id="必须和每个token计算相关性吗">必须和每个token计算相关性吗</h6><p>原版attention中，一个token必须和序列中其他所有token计算相关性，导致计算量随序列长度呈平方关系增长，大大制约了可编码最大长度。那必须和每个token计算相关性吗？其实之前Adaptive-Span Transformer也深度拷问过这个话题。它得出的结论是，对于大部分layer的multi-head，长度50范围内进行attention就已经足够了。不过Adaptive-Span采取的方法还是简单粗暴了一点，它约定每个head的attention span长度是固定的，并且attention span为当前token附近的其他token。</p><p>Adaptive-Span Transformer的这种方法显然还是没有抓住Attention计算冗余的痛点。Attention本质是加权求和，权重为两个token间的相关性。最终结果取决于较大的topk权重，其他权值较小的基本就是炮灰。并且softmax归一化更是加剧了这一点。小者更小，大者更大。为了减少计算冗余，我们可以只对相关性大的其他token的key向量计算Attention。</p><h6 id="怎么找到相关性大的向量呢">怎么找到相关性大的向量呢</h6><p>我们现在要从序列中找到与本token相关性最大的token，也就是当前key向量与哪些key向量相关性大。极端例子，如果两个向量完全相同，他们的相关性是最高的。确定两个高维向量的相关性确实比较困难，好在我们可以利用向量Hash来计算。</p><p>Reformer采用了局部敏感hash。我们让两个key向量在随机向量上投影，将它们划分到投影区间内。 <img src="https://i.loli.net/2020/10/28/gSHlhWRaoqfk9pN.png" alt="在这里插入图片描述"> 如图所示，划分了四个区间（4个桶bucket），进行了三次Hash。第一次Hash，使得上面两个向量分别归入bucket0和bucket3中，下面两个向量都归入bucket0。第二次Hash，上面两个向量和下面两个，均归入到bucket2中了。我们可以发现</p><ol type="1"><li>相似的向量，也就是相关性大的，容易归入到一个bucket中</li><li>局部敏感Hash还是有一定的错误率的，我们可以利用多轮Hash来缓解。这也是Reformer的做法，它采取了4轮和8轮的Hash。</li></ol><h6 id="整个流程">整个流程</h6><p>经过局部敏感Hash后，我们可以将相关性大的key归入同一个bucket中。这样只用在bucket内进行普通Attention即可，大大降低了计算冗余度。为了实现并行计算，考虑到每个bucket包含的向量数目可能不同，实际处理中需要多看一个bucket。整个流程如下 <img src="https://i.loli.net/2020/10/28/ZS5dCT8nVcaiob4.png" alt="在这里插入图片描述"></p><ol type="1"><li>让query等于key</li><li>局部敏感Hash（LSH）分桶。上图同一颜色的为同一个桶，共4个桶</li><li>桶排序，将相同的桶放在一起</li><li>为了实现并行计算，将所有桶分块（chunk），每个chunk大小相同</li><li>桶内计算Attention，由于之前做了分块操作，所以需要多看一个块。</li></ol><h6 id="多轮lsh">多轮LSH</h6><p>为了减少分桶错误率，文章采用了多次分桶，计算LSH Attention，Multi-round LSH attention。可以提升整体准确率。如下表。<img src="https://i.loli.net/2020/10/28/L4BDoiwTfaRjInv.png" alt="在这里插入图片描述"></p><h6 id="reversible-transformer-可逆transformer">3.2.2 REVERSIBLE TRANSFORMER 可逆Transformer</h6><p>LSH局部敏感Hash确实比较难理解，可逆Transformer相对好懂一些。这个方案是为了解决Transformer内存占用量，随layers层数线性增长的问题。为什么会线性增长呢？原因是反向传播中，梯度会从top layer向bottom layer传播，所以必须保存住每一层的Q K V向量，也就导致N层就需要N套Q K V。</p><p>那有没有办法不保存每一层的Q K V呢？可逆Transformer正是这个思路。它利用时间换空间的思想，只保留一层的向量，反向传播时，实时计算出之前层的向量。所以叫做Reversible。Reformer每一层分为两部分，x1和x2。输出也两部分，y1和y2。计算如下</p><p><img src="https://i.loli.net/2020/10/28/gKZT1mRFhEj48nU.png" alt="image-20201028214552153"></p><p>采用可逆残差连接后，模型效果基本没有下降。这也是make sense的，毕竟可逆是从计算角度来解决问题的，对模型本身没有改变。 <img src="https://i.loli.net/2020/10/28/hElWr4uHtUomVYG.png" alt="在这里插入图片描述"></p><h6 id="feed-forward-chunking-ff层分块">3.2.3 Feed-Forward chunking FF层分块</h6><p>针对fead-forward层内存消耗过大的问题，Reformer也给出了解决方案，就是FF层分块。如下 <img src="https://i.loli.net/2020/10/28/rpzwVXOSUavEs18.png" alt="在这里插入图片描述"></p><h5 id="实验结果-3">3.3 实验结果</h5><h6 id="内存和时间复杂度">内存和时间复杂度</h6><p>Reformer三个创新点，大大降低了内存和时间复杂度，消融分析如下 <img src="https://i.loli.net/2020/10/28/5ixhnpvKeQHrBw1.png" alt="在这里插入图片描述"></p><h6 id="模型效果">模型效果</h6><p>如下为在机器翻译上的效果。Reformer减少了算力消耗，同时也大大增加了长文本编码能力，故模型效果也得到了提升。如下。 <img src="https://i.loli.net/2020/10/28/8EtHVoBsfYxnmhO.png" alt="在这里插入图片描述"></p><h4 id="lite-transformer">4 Lite Transformer</h4><p><img src="https://i.loli.net/2020/10/28/9UfkunTVEpL5XBG.png" alt="在这里插入图片描述"> 论文信息：2020年4月，MIT &amp; 上海交大，ICLR2020 论文地址 https://arxiv.org/abs/2004.11886 代码和模型地址 https://github.com/mit-han-lab/lite-transformer</p><h5 id="为什么要做lite-transformer">4.1 为什么要做Lite Transformer</h5><p>主要出发点仍然是Transformer计算量太大，计算冗余过多的问题。跟Adaptive-Span Transformer和Reformer想法一样，Lite Transformer也觉得没必要做Full Attention，很多Attention连接是冗余的。不一样的是，它通过压缩Attention通道的方式实现，将多头减少了一半。与Base Transformer相比，计算量减少了2.5倍。并且文章使用了量化和剪枝技术，使得模型体积减小了18.2倍。</p><h5 id="实现方案-2">4.2 实现方案</h5><p>实现方案很简单，仍然采用了原版Transformer的seq2seq结构，创新点为</p><ol type="1"><li>multiHead self-attention变为了两路并行，分别为一半的通道数（多头）。如下图a所示。其中左半部分为正常的fully attention，它用来捕获全局信息。右半部分为CNN卷积，用来捕获布局信息。最终二者通过FFN层融合。这个架构称为LONG-SHORT RANGE ATTENTION (LSRA)，长短期Attention。</li><li>为了进一步降低计算量，作者将CNN转变为了一个depth wise卷积和一个线性全连接。dw卷积在mobileNet中有讲过，不清楚可自行谷歌。</li></ol><p><img src="https://i.loli.net/2020/10/28/WQtJvpbP9rUczhs.png" alt="在这里插入图片描述"></p><h5 id="实验结果-4">4.3 实验结果</h5><h6 id="计算复杂度">计算复杂度</h6><p><img src="https://i.loli.net/2020/10/28/w6b1LlqIpZgt8fX.png" alt="在这里插入图片描述"> 如上图，在文本摘要任务上，Lite Transformer计算量相比Base Transformer，减少了2.5倍。同时Rouge指标基本没变。</p><h6 id="模型体积">模型体积</h6><p><img src="https://i.loli.net/2020/10/28/JYzeocGBUr6jy4W.png" alt="在这里插入图片描述"> Lite Transformer模型体积只有Transformer的2.5分之一，通过8bit量化和剪枝，最终模型体积下降了18.2倍。</p><h4 id="其他">5 其他</h4><p>其他几篇文章，也建议拜读下</p><ol type="1"><li><a href="https://arxiv.org/abs/1904.10509" target="_blank" rel="noopener">Generating Long Sequences with Sparse Transformers</a> (OpenAI, 2019.04)</li><li><a href="https://arxiv.org/abs/1909.00015" target="_blank" rel="noopener">Adaptively Sparse Transformers</a> (EMNLP2019, 2019.09)</li><li><a href="https://arxiv.org/abs/1911.05507" target="_blank" rel="noopener">Compressive Transformers for Long-Range Sequence Modelling</a> (2019.11)</li><li><a href="https://arxiv.org/abs/2002.06170" target="_blank" rel="noopener">Transformer on a Diet</a> (2020.02)</li></ol><h3 id="transformer家族4----通用性优化universal-transformer">🚀Transformer家族4 -- 通用性优化（Universal-Transformer）</h3><h4 id="背景-2">1 背景</h4><p>之前讲Transformer的时候，也提到过它的通用性的缺点。相比于RNN，Transformer不是图灵完备的，虽然大多数任务都是吊打RNN，但在某些看起来极为简单的任务上，却表现很差，比如字符串拷贝等。这个问题其实也不算大，但谷歌还是给出了他的解决方案，也就是Universal Transformer。这篇看看就好了，个人感觉实际应用中作用有限。</p><h4 id="universal-transformer">2 Universal-Transformer</h4><p><img src="https://i.loli.net/2020/10/28/Lk3ymRQKZHpwN8e.png" alt="在这里插入图片描述">论文信息：2018年7月，谷歌，ICLR2019 论文地址 https://arxiv.org/abs/1807.03819 代码和模型地址 https://github.com/tensorflow/tensor2tensor</p><h5 id="为什么需要universal-transformer">2.1 为什么需要Universal-Transformer</h5><p>主要的出发点是原版Transformer不是图灵完备的，有些很简单的任务表现很差，比如字符串拷贝。序列任务还是比较偏好于迭代和递归变换，RNN正好满足了这一点，而Transformer不满足。这一点文章称作归纳偏置（Inductive Bias）。<a href="https://www.zhihu.com/question/41404496/answer/627673667" target="_blank" rel="noopener">深度学习的归纳偏置是什么？</a></p><h5 id="实现方案-3">2.2 实现方案</h5><h6 id="模型结构">模型结构</h6><p><img src="https://i.loli.net/2020/10/28/6aOdB5QqIYKSlEb.png" alt="在这里插入图片描述"> 如上所示为Universal-Transformer的结构，仍然为一个基于multiHead self-attention的seq2seq，几点不同</p><ol type="1"><li>引入了时间步step，从而实现了循环递归。除了第一次是原始信息作为输入，之后都是由前一个step的输出作为后一个的输入。</li><li>Feed-forward换成了Transition函数。根据task不同，可选择separable convolution分解卷积和fully-connected neural network全连接神经网络。</li><li>时间和位置编码，TimeStep embedding和Position embedding，新引入了TimeStep embedding，二者的编码公式和Transformer中的位置编码很像，如下</li></ol><p><img src="https://i.loli.net/2020/10/28/tdJkSqcoZ4FGHNQ.png" alt="在这里插入图片描述"></p><h6 id="adaptive-computation-timeact-自适应计算时间">Adaptive Computation Time（ACT） 自适应计算时间</h6><p>前人已经提到过ACT了，作者在模型中引用了。序列问题中，有些词语比其他的更模糊。他们需要进行更多次的计算。Universal-Transformer利用了ACT机制，可以对每个token设置自适应计算时间。模型会动态调整每个位置所需的计算steps。当某个位置停止计算后，直接copy它的隐状态到下一step。当所有位置都停止计算后，整个过程才停止。<img src="https://i.loli.net/2020/10/28/pXaRS5Q7fVDuxyW.png" alt="在这里插入图片描述"> 如上，不同位置token所需的计算steps是不同的。</p><h5 id="实验结果-5">2.3 实验结果</h5><h6 id="字符串任务">字符串任务</h6><p><img src="https://i.loli.net/2020/10/28/iT5HIsel3WjDCxA.png" alt="在这里插入图片描述">字符串复制、翻转、添加操作的效果。可以发现</p><ol type="1"><li>Transformer效果确实比较差，比LSTM差很多。这也验证了Transformer通用性确实有些问题，也就是本文的出发点</li><li>Universal-Transformer效果很好，超过LSTM很多，成功解决了原版Transformer的问题</li></ol><h6 id="机器翻译">机器翻译</h6><p><img src="https://i.loli.net/2020/10/28/LZTwqn9hjpBiAyr.png" alt="在这里插入图片描述">机器翻译上的结果，Universal-Transformer的BLEU比原版Transformer提高了0.9%</p><h3 id="transformer家族5----推理加速faster-transformer-turbotransformers">🚀Transformer家族5 -- 推理加速（Faster-Transformer 、TurboTransformers）</h3><h4 id="背景-3">1 背景</h4><p>之前介绍了从编码长度、计算效率、通用性等角度对Transformer进行优化，并介绍了几个重要模型。本文介绍如何进行Transformer推理加速。相比于离线训练，在线推理加速更加关键。一方面由于在线流量大，加速可带来硬件成本的节省。另一方面在线推理加速，可大大提升AI应用的用户体验。</p><p>事实上，之前的多种方法，特别是计算效率优化，对推理加速很有帮助。这些模型从算法的角度，进行了推理速度优化。本文主要从框架层的角度，讲解如何对推理进行加速。主要带来NVIDIA的Faster-Transformer框架和腾讯的Turbo-Transformer框架。</p><h4 id="faster-transformer">2 Faster-Transformer</h4><p>PPT资料：https://on-demand.gputechconf.com/gtc-cn/2019/pdf/CN9468/presentation.pdf 代码地址：https://github.com/NVIDIA/DeepLearningExamples/tree/master/FasterTransformer</p><h5 id="实现方案-4">实现方案</h5><p>Faster-Transformer算法结构和原版Transformer基本一致，主要是从框架层角度来实现计算加速。主要方法有</p><ol type="1"><li>算子融合。对除矩阵乘法外的所有算子，进行了合并。比如Add、Sub。从而减少了GPU kernel调度和显存读写。</li><li>半精度F16优化。</li><li>GELU激活函数、层正则化、softmax等调用频次很高的操作的优化</li></ol><h5 id="效果">效果</h5><p><img src="https://i.loli.net/2020/10/28/AGKsp9WPLQ2julY.png" alt="在这里插入图片描述"> Encoder效果对比如上。Faster-Transformer基本吊打TF XLA，提升速度一倍多。<img src="https://i.loli.net/2020/10/28/yYDIensgFQmv3HG.png" alt="在这里插入图片描述"> Decoder效果对比如上。对比了32bit和16bit的结果。Decoding FP32和Decoding FP16为Faster-Transformer 的结果，也是吊打原始TensorFlow。</p><h4 id="turbotransformers">3 <strong>TurboTransformers</strong></h4><p><img src="https://i.loli.net/2020/10/28/IKjLfbDRNgMd5rE.png" alt="在这里插入图片描述">代码地址 https://github.com/Tencent/TurboTransformers</p><h5 id="实现方案-5">实现方案</h5><ol type="1"><li>和Faster-Transformer一样，进行了算子融合。从而减少GPU kernel调用和显存占用</li><li>对于LayerNorm和softmax，由于不适合并行计算，重新开发并实现了并行计算版本。</li><li>内存缓存，避免频繁释放和分配内存。</li></ol><h5 id="和其他方案的对比">和其他方案的对比</h5><p><img src="https://i.loli.net/2020/10/28/BzXjLlFERMe34x2.png" alt="在这里插入图片描述"></p><h5 id="效果-1">效果</h5><p><img src="https://i.loli.net/2020/10/28/FnUxJrzqgL6eEks.png" alt="在这里插入图片描述">V100上的QPS，越高代表框架性能越好。对比了PyTorch、TensorFlow、Faster-Transformer、turboTransformers的效果，其中turboTransformers效果最好</p><h3 id="邱锡鹏教授nlp预训练模型综述">🚀邱锡鹏教授：NLP预训练模型综述</h3><h4 id="引言"><strong>1.引言</strong></h4><p>随深度学习的发展，多种神经网络都被应用在 NLP 任务中，比如 CNN、RNN、GNN 和 attention 机制等，但由于现有的数据集对于大部分有监督 NLP 任务来说都很小，因此，早期的模型对 NLP 任务来说都很“浅”，往往只包含 1-3 层。</p><p>而预训练模型（Pre-trained Models, PTMs）的出现将NLP带入一个新的时代，更“深”的模型和训练技巧的增强也使得 PTMs 由“浅”变“深”，在多项任务都达到了 SOTA 性能。</p><p>近日，复旦大学的邱锡鹏老师等人发布了预训练模型综述 *<strong>Pre-trained Models for Natural Language Processing: A Survey*</strong>，从背景、分类到应用与前景对 PTMs 做了详细而全面的调研。</p><p><img src="https://i.loli.net/2020/10/28/lLPuxzv8IVRTAQO.png" alt="img"></p><p><strong>论文标题：</strong>Pre-trained Models for Natural Language Processing: A Survey</p><p><strong>论文链接：</strong> https://arxiv.org/abs/2003.08271</p><h4 id="背景-4"><strong>2.背景</strong></h4><h5 id="语言表示学习"><strong>2.1 语言表示学习</strong></h5><p>对于语言来说，一个好的表示应当描绘语言的内在规则比如词语含义、句法结构、语义角色甚至语用。</p><p><img src="https://i.loli.net/2020/10/28/ZmF3yXaiHPME1LQ.png" alt="img"></p><p>而分布式表示的核心思想就是通过低维实值向量来描述一段文本的意义，而向量的每一个维度都没有对于意义，整体则代表一个具体的概念。图 1 是 NLP 的通用神经体系架构。</p><p>有两种 embedding（词嵌入）方式：上下文嵌入和非上下文嵌入，两者的区别在于词的 embedding 是否根据词出现的上下文动态地改变。</p><p><strong>非上下文嵌入：</strong>表示语言的第一步就是将分离的语言符号映射到分布式嵌入空间中。也就是对于词汇表中的每个单词（词根），通过 lookup table 映射到一个向量。</p><p>这种嵌入方式有两个局限：一是一个词通过这种方法获得的词嵌入总是静态且与上下文无关的，无法处理多义词；二是难以解决不在词汇表中的词（针对这个问题，很多 NLP 任务提出了字符级或词根级的词表示，如 CharCNN、FastText、Byte-Pair Encoding (BPE)）。</p><p><strong>上下文嵌入：</strong>为解决多义性和上下文相关的问题，将词在不同上下文的语义做区分。通过对词（词根）的 token 加一层 Neural Contextual Encoder（神经上下文编码器）得到词的上下文嵌入。</p><h3 id="section"></h3><h5 id="神经上下文编码器"><strong>2.2 神经上下文编码器</strong></h5><p><img src="https://i.loli.net/2020/10/28/DeQYroznLq4cCkt.png" alt="img"></p><p>如图 2 中所示，大部分的神经上下文编码器都可以被分为三类：卷积模型、序列模型、基于图的模型。</p><p><strong>卷积模型 ：</strong>卷积模型通过卷积操作将输入句子中的 embeddings 与其相邻的局部信息集成。</p><p><strong>序列模型 ：</strong>序列模型通常使用 RNN（如 LSTM 和 GRU）来描述词的上下文表示。实践中，双向 RNN 常用于收集词的两边信息，但表现往往会受到长程依赖问题的影响。</p><p><strong>基于图的模型 ：</strong>基于图的模型将词视做节点，通过预先定义的语言结构（如句法结构和语义联系）来学习上下文表示。但如何构造一个好的图结构往往严重依赖于专家知识和外部 NLP 工具，如依存分析器。</p><p>实际操作中往往直接通过一个全连接图来建模并让模型自己学习结构（一般通过自注意力机制）。一个典型的成功运用就是 Transformer。</p><p><strong>分析：</strong>卷积模型和序列模型都很难解决词之间的长程依赖问题，而 Transformer 虽然能更好地描述词之间的深层联系，却往往需要非常大的语料来训练，且容易在中等规模的数据集上过拟合。</p><h5 id="为什么要预训练"><strong>2.3 为什么要预训练？</strong></h5><p>正如上文提到的，模型参数的数量增长迅速，而为了训练这些参数，就需要更大的数据集来避免过拟合，而大规模的标注数据集成本又非常高。而相比之下，大规模未标注的语料却很容易构建。</p><p>为了利用大量的未标注文本数据，我们可以先从其中学习一个好的表示，再将这些表示用在别的任务中。这一通过 PTMs 从未标注大规模数据集中提取表示的预训练过程在很多 NLP 任务中都取得了很好的表现。</p><p>预训练的优点可以总结为以下三点：1 在大规模语料上通过预训练学习通用语言表示对下游任务很有帮助；2) 预训练提供了更好的模型初始化参数，使得在目标任务上有更好的泛化性能和更快的收敛速度；3) 预训练是一种有效的正则化方法，能够避免在小数据集上过拟合。</p><h4 id="ptms概述"><strong>3.PTMs概述</strong></h4><p>PTMs 的主要区别在于上下文编码器的使用、预训练任务和目标。上下文编码器已在 2.2 中做了叙述，接下来对预训练任务进行分析，并提出一种 PTMs 分类方法。</p><p><img src="https://i.loli.net/2020/10/28/LhoxNVG5rF3McDK.png" alt="img"></p><p>如图 3，这一部分内容作者在文中有一张非常详细的分类图可供参考。</p><p>表 1 从多个角度区分了文中提到的一些 PTMs。</p><p><img src="https://i.loli.net/2020/10/28/ZI176zJekFciyGr.png" alt="img"></p><h5 id="预训练任务"><strong>3.1 预训练任务</strong></h5><p>PTMs 按照预训练任务类型可以被分为两类：有监督学习、无监督学习/自监督学习。</p><p>有监督学习的预训练任务主要有机器翻译 (MT)，典型的模型是 CoVe。而下文进一步根据实现思路将自监督/无监督任务分为两类，一是基于上下文的 (LM, DAE, PLM)，二是基于对比的 (CTL)。</p><h4 id="section-1"></h4><h6 id="语言模型-lm"><strong>3.1.1 语言模型 (LM)</strong></h6><p>作为 NLP 中最常见的无监督任务，LM 一般指自回归 LM (auto-regressive LM) 或者单向 LM (unidirectional LM)。具体训练过程是基于一个大的语料，通过最大似然估计 (MLE) 训练计算一个句子出现的概率。</p><p>然而单向 LM 的缺点则是只能编码一个词左侧的文本和其自身，而更好的上下文应该编码左右两侧的文本。针对这一缺点，解决方案是双向 LM (BiLM)，即一个从左到右和一个从右到左的模型的组合。</p><h6 id="去噪声自编码器-denoising-autoencoder-dae"><strong>3.1.2 去噪声自编码器 (Denoising Autoencoder, DAE)</strong></h6><blockquote><p>这里将原文中 Masked Language Modeling (MLM) 与 DAE 合并为一个部分，因为一般将 BERT 中提出的 MLM 看作是基于 DAE 的思路实现的。</p></blockquote><p>DAE 的目的是通过向输入文本中添加噪声，利用含噪声的样本去重构不含噪声的输入。主要有五个实现方式：挡住 (MASK) token、删除 token、填充 token、句子排列、文本轮换。</p><p>MLM 随机选出一些词用 [MASK] 标记，然后去预测被 MASK 的词。但由于被 MASK 的词并不出现在 fine-tuning 的过程中，会导致预训练和微调的过程出现不一致性。针对这种情况，BERT 通过 80% [MASK]，10% 随机 token,10% 原 token 的方式来进行 mask。</p><p>而 MLM 的一种变体，<strong>Seq2SeqMLM</strong>，则是通过将 encoder-decoder (Seq2Seq) 应用到 MLM 上，这种变体有利于 Seq2Seq 类型的下游任务，比如 QA，总结和机器翻译。这一结构主要用在 MASS 和 T5 中。</p><p>而在 BERT 之后的很多论文都对 MLM 做了一些改进以增强性能，作者将其总结为 E-MLM (Enhanced Masked Language Modeling)。</p><p>其中 RoBERTa 使用动态 masking，UniLM 将对 mask 的预测扩展到三种任务：单向、双向和 Seq2Seq。XLM 通过一种串联并行双语句对叫做 TLM (translation language modeling) 的模型实现 MLM。</p><p>而 SpanBERT 和 StructBERT 则是引入了结构化信息。而 ERINE (Baidu) 则是选择 MASK 实体和短语，E-BERT 和 ERINE (THU) 则是利用了实体 embedding 方法，这三者都是借助了外部知识来丰富 MLM。</p><h6 id="排列语言模型plm"><strong>3.1.3 排列语言模型（PLM）</strong></h6><p>针对 MLM 中使用 MASK 导致的预训练与微调过程的不一致，Permuted Language Modeling (PLM) 对于一个给定序列，生成其所有可能排列进行采样作为训练的目标。值得注意的是，PLM 并不改变原始文本的位置，而是重新定义 token 预测的顺序。</p><h6 id="对比学习ctl"><strong>3.1.4 对比学习（CTL）</strong></h6><p>CTL (Contrastive Learning) 基于一种“learning by comparison”的思路，假设某些观测文本对比随机采样文本在语义上更相似，通过构建正样本和负样本并度量距离来实现学习。CTL 通常比 LM 具有更少的计算复杂度，也因此成为一个值得选择的 PTMs 训练标准。</p><h6 id="deep-infomax-dim"><strong>3.1.5 Deep InfoMax (DIM)</strong></h6><p>DIM 最初是在 CV 领域提出的用于最大化图像全局特征与局部特征之间的互信息（Mutual Information）的方法。</p><p>InfoWord 将 DIM 引入到语义表达学习中，提出用 DIM objective 以最大化句子的全局表示和一个 N-gram 的具备表示之间的互信息。</p><p>噪声对比估计（Noise-Contrastive Estimation，NCE）通过训练一个二元分类器来区分真实样本和假样本，训练词嵌入。NCE 的思想也被用在 word2vec 中。</p><h6 id="replaced-token-detection-rtd"><strong>3.1.6 Replaced Token Detection (RTD)</strong></h6><p>RTD 和 NCE 大体相同，根据上下文来预测 token 是否替换。</p><p>CBOW 的 negetive sampling 就可以看作是一个 RTD 的简单版本，其中采样是根据词汇表中的分布进行采样。</p><p>ELECTRA 基于 RTD 提出了一种新的 generator-discriminator 框架。首先用 MLM 任务训练 generator，再用 generator 的权重初始化 discriminator，再用判别任务（判别哪些 token 被 generator 替换过）训练 discriminator。</p><p>最终在下游任务只需要对 discriminator 进行 fine-tuning。TRD 也是一种很好的解决 MLM 导致的不一致问题的方法。</p><p>WKLM 则是通过在实体层面（entity-level）进行词替换，替换为同一个实体类型的实体名。</p><h5 id="section-2"></h5><h6 id="next-sentence-prediction-nsp"><strong>3.1.7 Next Sentence Prediction (NSP)</strong></h6><p>NSP 训练模型区分两个输入语句是否为训练语料中连续的片段，在选择预训练句对时，第二个句子 50% 是第一个句子实际的连续片段，50% 是语料中的随机段落。NSP 能够教会模型理解两个输入句子之间的联系，从而使得如 QA 和 NLI 这种对此类信息敏感的下游任务受益。</p><p>然而，近来 NSP 的必要性也遭到了质疑，XLNet 的作者发现不用 NSP loss 的单句训练优于使用 NSP 的句对训练。RoBERTa 的作者进一步分析表明：在对单个文本中的文本块训练时，去除 NSP 会在下游任务稍微提高性能。</p><h6 id="sentence-order-prediction-sop"><strong>3.1.8 Sentence Order Prediction (SOP)</strong></h6><p>NSP 结合了主题预测相关性预测，而因为主题预测更容易，模型将更依赖于主题预测。为更好建模句子之间的相关性，ALBERT 提出使用 SOP loss 替换 NSP loss，SOP 使用一个文档中的两个连续片段作为正样本，将这两个片段交换顺序作为负样本。</p><p>采用了 SOP 的 ALBERT 在多项下游任务中结果都优于 BERT。StructBERT 和 BERTje 也使用 SOP 作为自监督学习任务。</p><h3 id="section-3"></h3><h5 id="ptms的拓展"><strong>3.2 PTMs的拓展</strong></h5><h6 id="引入知识的ptms"><strong>3.2.1 引入知识的PTMs</strong></h6><p>通常 PTMs 都是用大量语料训练通用的语言表示，而将外部的领域知识引入到 PTMs 被证明式有效的。自 BERT 以来，就有很多预训练任务用以将外部知识纳入 PTMs，如：</p><p><strong>LIBERT：</strong>linguistically-informed BERT ，通过附加语言约束任务纳入了语言知识。</p><p><strong>SentiLR：</strong>通过对每个单词添加情感极性，将 MLM 拓展至 Label-Aware MLM (LA-MLM)，在多个情感分类任务达到 SOTA。</p><p><strong>SenseBERT：</strong>不仅能预测被 mask 的 token，还能预测 WordNet 中的 supersense。</p><p><strong>ERINE (THU)：</strong>将知识图谱中预训练的实体嵌入与文本中相应的实体提及相结合，以增强文本表示。</p><p><strong>KnowBERT：</strong>端到端将带实体连接模型与实体表示集成。</p><p><strong>KEPLER：</strong>将知识嵌入和语言模型对象联合。</p><p><strong>K-BERT：</strong>不同于以上几个模型通过实体嵌入引入知识图谱中的结构化信息，K-BERT 通过直接将知识图谱中相关三元组引入句子，获得一个 BERT 的拓展的树形输入。</p><p><strong>K-Adapter：</strong>针对不同预训练任务独立训练不同的适配器以引入多种知识，以解决上述模型在注入多种知识出现的遗忘问题。</p><h6 id="多模态ptms"><strong>3.2.2 多模态PTMs</strong></h6><p>随 PTMs 在 NLP 领域的广泛应用，一些多模态 PTMs 也被设计出来，在一些语音、视频、图像数据集上进行了预训练，比如：</p><ul><li><strong>视频-语言：</strong>VideoBERT、CBT</li><li><strong>图像-语言：</strong>用于 visual question answering (VQA) and visual commonsense reasoning (VCR)，如 ViLBERT、LXMERT、VisualBERT、B2T2、VLBERT、 Unicoder-VL、UNITER</li><li><strong>音频-文本：</strong>用于端到端 Speech Question Answering (SQA) 任务，如 SpeechBERT</li></ul><h4 id="section-4"></h4><h6 id="领域预训练ptms"><strong>3.2.3 领域预训练PTMs </strong></h6><p>大多数 PTMs 都是在 Wikipedia 这样的通用领域语料库上训练的，这就限制了他们在特定领域内的表现。</p><p>近期有一些用专业领域语料训练的 PTMs，比如：生物医学领域的 BioBERT，科学领域的 SciBERT，临床医学领域的 ClinicalBERT。还有一些工作尝试将预训练模型更好地使用目标应用，比如生物医学实体归一化、专利分类等。</p><h6 id="多语言与特定语言ptms"><strong>3.2.4 多语言与特定语言PTMs </strong></h6><p>学习多语言文本表示对于跨语言 NLP 任务是很重要的。早期工作着力于学习来自同一语义环境下的多语言词嵌入，这一方法往往缺乏语言间的校准。近期有如下几个多语言 PTMs：</p><p><strong>Multilingual-BERT：</strong>M-BERT，在 Wikipedia 上 104 种种语言的文本上进行 MLM 训练，每个训练样本都是单语言的，也没有专门设计跨语言目标，但即便如此，M-BERT 在跨语言任务上表现还是非常好。</p><p><strong>XLM：</strong>通过结合跨语言任务 TLM (translation language modeling)，提升了 M-BERT 的性能。</p><p><strong>Unicoder：</strong>提出三个跨语言预训练任务：1) cross-lingual word recovery; 2) cross-lingual paraphrase classification; 3) cross-lingual masked language model。</p><p>除此之外还有一些单语言的 PTMs：BERT-wwm，ZEN，NEZHA，ERNIE (Baidu)，BERTje，CamemBERT， FlauBERT ，RobBERT 。</p><h5 id="如何压缩ptms"><strong>3.3 如何压缩PTMs</strong></h5><p>预训练模型往往包含至少几千万个参数，这也使得模型难以部署到生活中的线上服务以及资源有限的设备上，这就使得模型压缩成为一条可能能够压缩模型尺寸并提高计算效率的方法。表 2 展示了一些压缩的 PTMs 的对比。</p><p><img src="https://i.loli.net/2020/10/28/6cBzpUQ3J7e1Xd2.png" alt="img"></p><p>压缩 PTMs 一般有四个方法：</p><ul><li><strong>剪枝（pruning）：</strong>去除不那么重要的参数（e.g. 权重、层数、通道数、attention heads）</li><li><strong>量化（weight quantization）：</strong>使用占位更少（低精度）的参数</li><li><strong>参数共享（parameter sharing）：</strong>相似模型单元间共享参数</li><li><strong>知识蒸馏（knowledge diistillation）：</strong>用一些优化目标从大型 teacher 模型学习一个小的 student 模型，一些利用知识蒸馏的 PTMs 见表 3。</li></ul><p><img src="https://i.loli.net/2020/10/28/5tdJqBHve3XYuCo.png" alt="img"></p><h4 id="如何将ptms应用至下游任务"><strong>4.如何将PTMs应用至下游任务</strong></h4><h5 id="迁移学习"><strong>4.1 迁移学习</strong></h5><p>迁移学习就是将源任务中的知识适应到目标任务，将 PTMs 适应到下游任务是一种顺序迁移学习任务。那么，如何迁移呢？我们需要考虑以下几个问题：</p><ul><li><strong>选择合适的预训练任务</strong>：近期，LM 是最流行的预训练任务，也有效解决了很多 NLP 问题。但不同的预训练任务在不同的下游任务上有不同的效果，比如 NSP 任务能帮助 PTM 理解句子之间的关系，因此 PTM 对于 QA 和 NLI 这样的下游任务很有帮助。</li><li><strong>选择合适的模型架构</strong>：比如 BERT 使用的 MLM 和 Transformer 结构使其擅长 NLU 任务，却很难生成语言。</li><li><strong>选择合适的语料</strong>：下游任务的数据应该接近 PTMs 的预训练任务。</li><li><strong>选择合适的layers</strong>：在“深”的预训练模型中，不同的 layer 往往描绘不同种类的信息。有三种选择 layers 的方式：1) 只用 Embedding，如 word2vec 和 Glove；2) Top Layer，如 BERT；3) All Layers，如 ELMo。</li><li><strong>是否进行fine-tune</strong>：模型迁移一般有两种方法：特征提取和 fine-tuning。特征提取的参数是冻结的，且往往需要特定任务的体系结构。fine-tunig 的参数是非冻结的，比特征提取方法更为通用且方便。</li></ul><h5 id="fine-tuning的策略"><strong>4.2 fine-tuning的策略</strong></h5><p>自 ULMFit 和 BERT 起，fine-tuning 已经成为 PTMs 主要的适配方法。这里有一些实用的 fine-tunig 策略：</p><ul><li>两阶段 fine-tuning：两阶段迁移的方法在预训练和 fine-tuning 阶段引入了一个中间阶段。在第一阶段，通过中间任务或语料来微调模型。在第二阶段，通过目标任务微调模型。</li><li>多任务 fine-tuning：liu等人在多任务学习框架下对 BERT 进行了微调，结果显示多任务学习和预训练是互补的方法。</li><li>采用额外的适配器 fine-tuning：fine-tuning 的主要缺点是参数效率低，在每一个下游任务上都有各自的 dine-tuning 参数。对此的解决方案是在固定原始参数时引入一些可以 fine-tuning 的适配器。</li><li>其他：逐层解冻而非连续 fine-tune 所有层；self-ensemble 和 self-distillation</li></ul><h4 id="一些ptms的资源"><strong>5.一些PTMs的资源</strong></h4><h5 id="一些开源的应用"><strong>一些开源的应用：</strong></h5><p><img src="https://i.loli.net/2020/10/28/jTUZBNqcrlm9hR2.png" alt="img"></p><p><strong>word2vec:</strong></p><p>https://github.com/tmikolov/word2vec</p><p><strong>GloVe:</strong></p><p>https://nlp.stanford.edu/projects/glove</p><p><strong>FastText:</strong></p><p>https://github.com/facebookresearch/fastText</p><p><strong>Transformers:</strong></p><p>https://github.com/huggingface/transformers</p><p><strong>Fairseq:</strong></p><p>https://github.com/pytorch/fairseq</p><p><strong>Flair:</strong></p><p>https://github.com/flairNLP/flair</p><p><strong>AllenNLP:</strong></p><p>https://github.com/allenai/allennlp</p><p><strong>FastNLP:</strong></p><p>https://github.com/fastnlp/fastNLP</p><p><strong>Chinese-BERT:</strong></p><p>https://github.com/ymcui/Chinese-BERT-wwm</p><p><strong>BERT:</strong></p><p>https://github.com/google-research/bert</p><p><strong>RoBERTa:</strong></p><p>https://github.com/pytorch/fairseq/tree/master/examples/roberta</p><p><strong>XLNet:</strong></p><p>https://github.com/zihangdai/xlnet/</p><p><strong>ALBERT:</strong></p><p>https://github.com/google-research/ALBERT</p><p><strong>T5:</strong></p><p>https://github.com/google-research/text-to-text-transfer-transformer</p><p><strong>ERNIE (Baidu):</strong></p><p>https://github.com/PaddlePaddle/ERNIE</p><p><strong>相关资源：</strong></p><p><strong>论文列表：</strong></p><p>https://github.com/thunlp/PLMpapers</p><p>https://github.com/tomohideshibata/BERT-related-papers</p><p>https://github.com/cedrickchee/awesome-bert-nlp</p><p><strong>BERT Lang Street（收集 BERT 在不同数据集和任务上的表现）：</strong></p><p>https://bertlang.unibocconi.it/</p><p><strong>BERTViz（应用 transformer 的模型的注意力可视化）：</strong></p><p>https://github.com/jessevig/bertviz</p><h4 id="应用"><strong>6.应用</strong></h4><h5 id="通用评估标准"><strong>6.1 通用评估标准</strong></h5><p>GLUE (The General Language Understanding Evaluation) 标准是一个集合了 9 个自然语言理解任务的标准。</p><p>其中包括：单个句子分类任务（CoLA和SST-2）、文本对分类任务（MNLI, RTE, WNLI, QQP, MRPC）、文本相似度任务（STSB）、相关性排行任务（QNLI）。GLUE 标准能够能够很好地评估模型的鲁棒性和通用性。</p><p>而近期 NLP 的快速发展促使了新的标准 SuperGLUE 的提出，相比 GLUE，SuperGLUE 有更多富有挑战性且多种多样的任务，如指代消解和 QA。</p><h3 id="section-5"></h3><h5 id="机器翻译-1"><strong>6.2 机器翻译</strong></h5><p>机器翻译（Machine Translation, MT）也是 NLP 的一项重要任务。几乎所有 MT 模型都使用了 encoder-decoder 框架。而近期随预训练模型的发展，也有不少尝试将 BERT 之类的预训练模型用于初始化 encoder，取得了一定成效。</p><h3 id="section-6"></h3><h5 id="问答系统"><strong>6.3 问答系统</strong></h5><p>问答系统（Question answering, QA）或是狭义概念的机器阅读理解（machine reading comprehension, MRC）也是 NLP 的重要任务。</p><p>从易到难，有三种类型的 QA 任务：单回合提取 QA (single-round extractive QA, SQuAD)、多回合生成QA (multi-round generative QA, CoQA)、多跳问答 (multi-hop QA, HotpotQA)。</p><p>针对提取 QA，有通过 PTM 初始化 encoder 的回溯阅读架构（retrospective reader architecture）；针对多回合生成 QA，有“PTM+Adversarial Training+Rationale Tagging+Knowledge Distillation”架构；针对多跳 QA，有“Select, Answer, and Explain” (SAE) 系统。</p><h5 id="情感分析"><strong>6.4 情感分析</strong></h5><p>BERT 通过在广泛使用的情感分析数据集 SST-2 上进行微调后，表现超过了先前的 SOTA 模型。而后又有很多将 BERT 进行调整以应用在 aspect 级的情感分析（ABSA）任务上。</p><h5 id="总结-1"><strong>6.5 总结</strong></h5><p>从长文本中总结出短文本也是近期 NLP 的热点。也有很多尝试将 PTM 应用在总结文本任务上，如将 BERT 通过插入 [CLS] token 来学习句子表示的模型 BERTSUM。</p><h5 id="命名实体识别"><strong>6.6 命名实体识别</strong></h5><p>命名实体识别（Named Entity Recognition, NER）也是知识提取的一个基础任务，在很多 NLP 任务上都有重要作用。TagLM 和 ELMo 利用预训练语言模型的最后一层的输入和各层的加权总和作为词嵌入的一部分。</p><h4 id="未来方向"><strong>7.未来方向</strong></h4><h5 id="ptms的上界"><strong>7.1 PTMs的上界</strong></h5><p>随 BERT 的出现，我们可以发现，很多模型都可以通过更长的训练步长不在和更大的语料来提升性能，比如去年的 T5 使用的 C4 数据集。而我们也可以通过加深模型来提升性能，比如 Turing-NLG 使用了 72 个 transformer 层。</p><p>PTMs 的共同目标都是学习语言的本质通用知识(或者说是世界的知识)，然而，随着模型的不断加深，语料的不断增大，训练模型的花销也越来越大。一种更可行的解决方案是设计更有效的模型架构、自监督预训练任务、优化器和软硬件方面的技巧等。ELECTRA 就是这个方向上一个很好的尝试。</p><h5 id="面向任务的预训练与模型压缩"><strong>7.2 面向任务的预训练与模型压缩</strong></h5><p>在实践中，不同的下游任务要求 PTMs 拥有不同的功能。而 PTMs 与下游目标任务间的差异通常表现在两方面：模型架构与数据分布。较大的 PTMs 通常情况下会有更好的性能，但实际问题是如何在低容量设备和低时延应用上使用如此庞大的 PTM。</p><p>除此之外，我们可以通过模型压缩来将通用 PTMs 教给面向对象的 PTM。尽管 CV 中对 CNNs 的压缩已经非常成熟，但 Tansformer 的全连接结构使得模型压缩非常具有挑战性。</p><h5 id="ptms架构"><strong>7.3 PTMs架构</strong></h5><p>Transformer 是 PTMs 的一个高效的框架，但 Transformer 的局限在于计算复杂度。由于 GPU 显存大小的限制，目前大多数 PTM 无法处理序列长度超过 512 个 token 的序列。搭配这一限制需要改进 Transformer 的结构，如 Transformer-XL。因此，寻求更有效的模型架构对于解决长程文本信息也是很重要的。</p><h5 id="fine-tunig中的知识迁移"><strong>7.4 Fine-tunig中的知识迁移 </strong></h5><p>Fine-tuning 是目前将 PTM 的知识迁移至下游任务的主要方法，但参数效率却很低，每个下游任务都有特定的 fine-tuned 参数。</p><p>一个可以改进的解决方案是固定 PTMs 的原始参数，并为特定任务添加小型的可微调的适配器，这样就可以在不同的下游任务使用共享的 PTMs。从 PTM‘s 中挖掘知识也可以更灵活，比如：知识提取、知识蒸馏、数据增加、将 PTMs 作为外部知识等等。</p><h5 id="ptms的可解释性与可靠性"><strong>7.5 PTMs的可解释性与可靠性 </strong></h5><p>PTMs 的深且非线性的架构使得决策制定的过程非常不透明。近期，可解释人工智能（explainable artificial intelligence, XAI）成为热点。通过对模型词嵌入的研究我们可以分析 PTMs 中的语言和世界知识，但更多有关注意力机制的可解释性的问题还值得探讨。</p><p>PTMs 这种深模型很容易受到对抗样本的扰动而产生错误的预测。在 CV 领域，对抗攻击与防御已经被广泛学习，而由于语言的特性，文本的对抗还非常具有挑战性。PTMs 的对抗防御也对于提升 PTMs 的鲁棒性很重要。</p><h4 id="总结-2"><strong>8.总结</strong></h4><p>邱锡鹏老师的这篇综述很全面地概括了预训练模型，也非常适合初学者当作一个 roadmap 来阅读。我们可以看到 NLP 的发展过程是非常令人感动的，从最开始的“要表示语言”的目标，使用词袋模型和 N-gram。</p><p>再想到“词语具有多义性”，所以需要有上下文，使用 LSTM。LSTM 只有单向，那就使用双向 LSTM。“想要更大范围的上下文”，就产生了 transformer。</p><p>“再大一些”，有了 transformer-XL。还是不够好，怎么办？“更多知识”，于是不断加大语料库，不断堆 GPU，直到 T5 探索了“Limits of Transfer Learning with a Uniﬁed Text-to-Text Transformer”。</p><p>模型太大，成本太高，那就压缩模型，改进框架，于是有了 ELECTRA。预训练模型缺乏尝试推理能力，那就知识提取，于是有了 COMET。每一步尝试都是在靠近语言的本质与世界的知识。</p><p><em>“The whole of science is nothing more than a refinement of everyday thinking.”</em></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      transformer综述
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-10-26-BERT论文阅读及详解</title>
    <link href="http://yoursite.com/2020/10/26/2020-10-26-BERT%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%8F%8A%E8%AF%A6%E8%A7%A3/"/>
    <id>http://yoursite.com/2020/10/26/2020-10-26-BERT%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%8F%8A%E8%AF%A6%E8%A7%A3/</id>
    <published>2020-10-26T12:05:46.000Z</published>
    <updated>2020-11-10T14:02:28.653Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言">前言</h3><p>BERT是谷歌发布的基于双向 Transformer的大规模预训练语言模型，该预训练模型能高效抽取文本信息并应用于各种NLP任务，并刷新了 11 项 NLP 任务的当前最优性能记录。</p><p>BERT的全称是基于Transformer的双向编码器表征，<strong>其中“双向”表示模型在处理某一个词时，它能同时利用前面的词和后面的词两部分信息</strong>。</p><p>BERT的本质上是通过在海量的语料的基础上运行自监督学习方法<strong>为单词学习一个好的特征表示</strong>，所谓自监督学习是指在没有人工标注的数据上运行的监督学习。</p><p>在以后特定的NLP任务中，<strong>我们可以直接使用BERT的特征表示作为该任务的词嵌入特征。</strong>所以BERT提供的是一个供其它任务迁移学习的模型，该模型可以根据任务微调或者固定之后作为特征提取器。</p><p>BERT仍然使用的是Transformer模型，它pretraining的不是普通的语言模型，而是Mask语言模型。在介绍Mask语言模型之前我们先介绍BERT的输入表示。</p><h3 id="section"></h3><h3 id="bert的总体结构">BERT的总体结构</h3><p>如图2-1，是Devlin等人在论文中给出的BERT结构示意图。BERT的输入是token序列对应的嵌入向量序列。在生命周期的不同阶段，输出是不同的：</p><p>在<strong>预训练阶段</strong>，BERT采用<strong>多任务策略</strong>，输出包括“下一个词语”和“是否为下一句”。</p><p>在<strong>微调和推断阶段</strong>，BERT(针对<strong>具体的任务</strong>)输出NER标签、答案位置等等。</p><p>这个示意图非常概括，BERT内部细节比较模糊。后面进行更详细的介绍。</p><p><img src="https://i.loli.net/2020/11/10/wo4WusfDvTS7ekZ.jpg" alt="img"></p><p>图 2-1 《BERT》中提供的BERT结构原图</p><h3 id="输入表示">输入表示</h3><p>BERT的输入的编码向量（长度是512）是3个嵌入特征的单位和，如图4，这三个词嵌入特征是：</p><ol type="1"><li>WordPiece 嵌入[6]：WordPiece是指将单词划分成一组有限的公共子词单元，能在单词的有效性和字符的灵活性之间取得一个折中的平衡。例如图4的示例中‘playing’被拆分成了‘play’和‘ing’；</li><li>位置嵌入（Position Embedding）：位置嵌入是指将单词的位置信息编码成特征向量，位置嵌入是向模型中引入单词位置关系的至关重要的一环。位置嵌入的具体内容参考我之前的<a href="https://link.zhihu.com/?target=https%3A//senliuy.gitbooks.io/advanced-deep-learning/content/di-er-zhang-ff1a-xu-lie-mo-xing/attention-is-all-you-need.html">分析</a>；</li><li>分割嵌入（Segment Embedding）：用于区分两个句子，例如B是否是A的下文（对话场景，问答场景等）。对于句子对，第一个句子的特征值是0，第二个句子的特征值是1。</li></ol><p>最后，说明一下图4中的两个特殊符号<code>[CLS]</code>和<code>[SEP]</code>，其中<code>[CLS]</code>表示该特征用于分类模型，对非分类模型，该符合可以省去。<code>[SEP]</code>表示分句符号，用于断开输入语料中的两个句子。</p><p><img src="https://i.loli.net/2020/11/10/9LqQ7Vk58lNs2dM.jpg" alt="img"></p><blockquote><p>对于情感分类这样的任务，只有一个句子，因此Segment id总是0；而对于Entailment任务，输入是两个句子，因此Segment是0或者1。</p></blockquote><p>BERT模型要求有一个固定的Sequence的长度，比如128。如果不够就在后面padding，否则就截取掉多余的Token，从而保证输入是一个固定长度的Token序列，后面的代码会详细的介绍。第一个Token总是特殊的[CLS]，它本身没有任何语义，因此它会(必须)编码整个句子(其它词)的语义。</p><h3 id="mask-lm">Mask LM</h3><p>为了解决只能利用单向信息的问题，<strong>BERT使用的是Mask语言模型而不是普通的语言模型</strong>。Mask语言模型有点类似与完形填空——给定一个句子，<strong>把其中某个词遮挡起来，让人猜测可能的词</strong>。</p><p>这里会随机的Mask掉15%的词，然后让BERT来预测这些Mask的词，通过调整模型的参数使得模型预测正确的概率尽可能大，这等价于交叉熵的损失函数。这样的Transformer在编码一个词的时候会(必须)参考上下文的信息。</p><p>但是这有一个问题：<strong>在Pretraining Mask LM时会出现特殊的Token [MASK]，但是在后面的fine-tuning时却不会出现，这会出现Mismatch的问题。</strong></p><p>因此BERT中，如果某个Token在被选中的15%个Token里，则按照下面的方式随机的执行：</p><ul><li><p>80%的概率替换成[MASK]，比如my dog is hairy → my dog is [MASK]</p></li><li><p>10%的概率替换成随机的一个词，比如my dog is hairy → my dog is apple</p></li><li><p>10%的概率替换成它本身，比如my dog is hairy → my dog is hairy</p><p><img src="https://i.loli.net/2020/11/10/FOB4nNjGZQ6eWKr.jpg" alt="img"></p></li></ul><blockquote><p>这样做的好处是，BERT并不知道[MASK]替换的是哪一个词，而且任何一个词都有可能是被替换掉的，比如它看到的apple可能是被替换的词。这样<strong>强迫模型在编码当前时刻的时候不能太依赖于当前的词，而要考虑它的上下文，甚至更加上下文进行”纠错”。</strong>比如上面的例子模型在编码apple是根据上下文my dog is应该把apple(部分)编码成hairy的语义而不是apple的语义。</p></blockquote><h3 id="预测句子关系">预测句子关系</h3><p>在有些任务中，比如问答，<strong>前后两个句子有一定的关联关系，我们希望BERT Pretraining的模型能够学习到这种关系。因此BERT还增加了一个新的任务——预测两个句子是否有关联关系</strong>。这是一种Multi-Task Learing。BERT要求的Pretraining的数据是一个一个的”文章”，比如它使用了BookCorpus和维基百科的数据，BookCorpus是很多本书，每本书的前后句子是有关联关系的；而维基百科的文章的前后句子也是有关系的。对于这个任务，<strong>BERT会以50%的概率抽取有关联的句子(注意这里的句子实际只是联系的Token序列，不是语言学意义上的句子)，另外以50%的概率随机抽取两个无关的句子，然后让BERT模型来判断这两个句子是否相关</strong>。比如下面的两个相关的句子：</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP]</span><br></pre></td></tr></tbody></table></figure><p>下面是两个不相关的句子：</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[CLS] the man [MASK] to the store [SEP] penguin [MASK] are flight ##less birds [SEP]</span><br></pre></td></tr></tbody></table></figure><h3 id="fine-tuning">Fine-Tuning</h3><p>BERT的Fine-Tuning如下图所示，共分为4类任务。</p><p><img src="https://i.loli.net/2020/11/10/tyI65WJibLkm8hB.png" alt="img"> <em>图：BERT的Fine-Tuning</em></p><p>对于普通的分类任务，输入是一个序列，如图中右上所示，<strong>所有的Token都是属于同一个Segment(Id=0)，我们用第一个特殊Token [CLS]的最后一层输出接上softmax进行分类，用分类的数据来进行Fine-Tuning</strong>。</p><p>对于相似度计算等输入为两个序列的任务，过程如图左上所示。两个序列的Token对应不同的Segment(Id=0/1)。我们也是用第一个特殊Token [CLS]的最后一层输出接上softmax进行分类，然后用分类数据进行Fine-Tuning。</p><p>第三类任务是<strong>序列标注，比如命名实体识别</strong>，输入是一个句子(Token序列)，<strong>除了[CLS]和[SEP]的每个时刻都会有输出的Tag</strong>，比如B-PER表示人名的开始，本章的序列标注部分已经介绍过怎么把NER变成序列标注的问题了，这里不再赘述。然后用输出的Tag来进行Fine-Tuning，过程如图右下所示。</p><p>第四类是问答类问题，比如SQuAD v1.1数据集，<strong>输入是一个问题和一段很长的包含答案的文字(Paragraph)，输出在这段文字里找到问题的答案。</strong></p><p>比如输入的问题是：</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Where do water droplets collide with ice crystals to form precipitation?</span><br></pre></td></tr></tbody></table></figure><p>包含答案的文字是：</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">... Precipitation forms as smaller droplets coalesce via collision with other rain drops or ice crystals within a cloud. ...</span><br></pre></td></tr></tbody></table></figure><p>正确答案是”within a cloud”。</p><p>论文中作者提到了另外的两个模型，分别是OpenAI GPT和ELMo。</p><p>图3展示了这3个模型架构的对比：</p><p><img src="https://i.loli.net/2020/11/10/boz1UqSNnXdhHvJ.png" alt="image-20201026200944962"></p><ul><li>BERT使用了双向的Transformer架构，预训练阶段使用了MLM和NSP。</li><li>OpenAI GPT使用了left-to-right的Transformer。</li><li>ELMo分别使用了left-to-right和right-to-left进行独立训练，然后将输出拼接起来，为下游任务提供序列特征。</li></ul><p>上面的三个模型架构中，只有BERT模型的表征在每一层都联合考虑到了左边和右边的上下文信息。另外，除了架构不同，还要说明的一点是：BERT和OpenAI GPT是基于fine-tuning的方法，而ELMo是基于feature-based的方法。</p><hr><h3 id="其它">其它</h3><p>[CLS]就是classification的意思，可以理解为用于下游的分类任务。</p><p>主要用于以下两种任务：</p><ul><li>单文本分类任务：对于文本分类任务，BERT模型在文本前插入一个[CLS]符号，并将该符号对应的输出向量作为整篇文本的语义表示，用于文本分类，如下图所示。可以理解为：<strong>与文本中已有的其它字/词相比，这个无明显语义信息的符号会更“公平”地融合文本中各个字/词的语义信息。</strong></li></ul><p><img src="https://i.loli.net/2020/11/10/89gX7PbO35AdpwB.png" alt="img"></p><ul><li>语句对分类任务：该任务的实际应用场景包括：问答（判断一个问题与一个答案是否匹配）、语句匹配（两句话是否表达同一个意思）等。<strong>对于该任务，BERT模型除了添加[CLS]符号并将对应的输出作为文本的语义表示，还对输入的两句话用一个[SEP]符号作分割，并分别对两句话附加两个不同的文本向量以作区分</strong>，如下图所示。</li></ul><p><img src="https://i.loli.net/2020/11/10/ZdvuYK6DyTrBFbA.png" alt="img"></p><hr><h4 id="模型架构">模型架构</h4><h4 id="输入表示-1">输入表示</h4><ul><li><p>我们的输入表示能够在一个标记序列中明确地表示单个文本句子或一对文本句子（例如，[问题，答案]）。一个词的输入=词的embeding+段embeding+位置embeding</p><p><img src="https://i.loli.net/2020/11/10/jkpW6lnm1QcIhut.png" alt="img"></p></li><li><p>对于词embeding论文使用<a href="https://arxiv.org/pdf/1609.08144.pdf" target="_blank" rel="noopener">WordPiece embeddings</a></p></li><li><p>每个序列的第一个字符始终是特殊分类embedding([CLS])。对应于该字符的最终隐藏状态（即，Transformer的输出）被视为<strong>整个序列表示</strong>常用于聚合用作分类任务。<strong>对于非分类任务，将忽略此向量。</strong></p></li></ul><h4 id="预训练">预训练</h4><ul><li>任务一：Masked LM<ul><li>标准条件语言模型只能从左到右或从右到左进行训练，因为双向条件语言模型将允许每个单词在多层self-attention中间接看到自己。<strong>为了避免当前要预测的词在self-attention中看到要预测的答案我们采样的方法是：随机屏蔽掉(mask)输入序列中一定比例的输入词，然后仅预测那些被屏蔽的词,称这个方法叫masked LM(MLM)，最后我们将这个被mask的词的最后隐藏层输出，输入到softmax层中预测这个被mask的词</strong></li><li>在论文的实验中我们<strong>每次mask掉一个序列的15%词</strong></li><li>该任务的两个缺点：<ul><li>第一个：这种操作使得预训练和微调之间不匹配，因为在微调期间可能没有[MASK]字符。为了缓解这种情况我们不总是用[MASK]词来替换被mask掉的词，而是80%的用[MASK]词来替换被mask掉的词，10%用一个随机词来替换被mask掉的词，再，10%保存源词不变。例子：<ul><li>原句：my dog is hairy 我们要mask掉hairy</li><li>80%：my dog is [MASK]</li><li>10%：my dog is apple</li><li>10%：my dog is hairy</li></ul></li><li>第二个：<strong>每个batch中只预测了15％的词，这表明模型可能需要更多的预训练步骤才能收敛。实验证明该任务的训练略微慢一点比起预测每一个词的语言模型。</strong></li></ul></li></ul></li><li>任务二：Next Sentence Prediction<ul><li>为了训练理解句子关系的模型，我们预先训练下一句话预测任务，该任务可以从任何单语言语料库中生成。具体地，在构建每个预训练样本时，选择句子A和B，50％B是A的实际下一句子， 50％B是来自语料库的一个随机句子，例子如下：<ul><li><strong>Input</strong> = [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP]</li><li><strong>Label</strong> = IsNext</li><li><strong>Input</strong> = [CLS] the man [MASK] to the store [SEP] penguin [MASK] are flight ##less birds [SEP]</li><li><strong>Label</strong> = NotNext</li></ul></li></ul></li><li>预训练过程设置：<ul><li>输入序列长度为512，batch_size=256,训练1000000步近似在33亿词的预料库上40 epochs</li><li>使用Adam优化器，learning_rate=1e-4, β_1= 0.9, β_2= 0.999,权重的L2正则项系数为0.01，学习率是预热步数：10000，学习率线性衰退，在每一层使用概率为0.1的dropout，激活函数使用<a href="https://arxiv.org/abs/1606.08415" target="_blank" rel="noopener">gelu</a></li></ul></li></ul><h4 id="微调">微调</h4><ul><li><p>对于序列水平的分类任务，我们<strong>获取第一个词[CLS]的最后隐藏层状态,再将C经过一个全连接层得到最后的预测分布，其中K是类别数。</strong>W也是这种特殊任务唯一添加的模型参数。</p></li><li><p>在微调的过程中BERT和W被同时微调。</p></li><li><p><strong>在微调中，大多数模型超参数与预训练相同，一般修改的超参数是：batch_size, learning_rate, epochs。 Dropout的概率始终保持在0.1</strong>。理论上说最佳超参数值随特定于任务不同而不同，但我们发现以下范围的可能值可以在所有任务中很好地工作：</p><ul><li>Batch size: 16, 32</li><li>Learning rate (Adam): 5e-5, 3e-5, 2e-5</li><li>Number of epochs: 3, 4</li></ul></li><li><p>我们还观察到，大数据集对超参数选择的敏感性远小于小数据集</p></li><li><p>微调总结图：</p><p><img src="https://i.loli.net/2020/11/10/w2IJaCbm6tqKVyo.png" alt="img"></p><p>图一：预测两个句子的关系，图二：是对当个句子分类。</p></li></ul><h4 id="模型对比">模型对比</h4><p><img src="https://i.loli.net/2020/11/10/CN4jbLg5SQIyYZP.png" alt="img"></p><h4 id="总结">总结</h4><h5 id="词嵌入语言模型的方法">词嵌入语言模型的方法</h5><ul><li><p>NLP词嵌入语言模型的方法：</p><ul><li><p>Feature-based方法</p><ul><li><p>Feature-based指利用预先训练好的语言模型的结果,作为当前特定任务模型（task-specific）的一个额外的特征引入到当前特定任务模型中，例如下图的语言模型</p><p><img src="https://i.loli.net/2020/11/10/t8e7J3joTYKqHzw.png" alt="img"></p><p>上图中，左边部分为序列标注模型，也就是task-specific model，每个任务可能不同，右边是两个预训练好的前向LM(Left-to-right)和后向LM(Right-To-Left), 将两个LM的结果进行了合并，并将LM embedding与词向量、第一层RNN输出、第二层RNN输出进行了concat操作</p></li><li><p>通常feature-based方法包括两步：</p><ul><li>首先在大的语料A上无监督地训练语言模型，训练完毕得到语言模型。</li><li>然后构造task-specific model例如序列标注模型，采用有label的语料B来有监地训练task-sepcific model，将语言模型的参数固定，语料B的训练数据经过语言模型得到LM embedding，作为task-specific model的额外特征</li></ul></li><li><p>ELMo是这方面的典型代表</p></li></ul></li></ul></li><li><p>Fine-tuning方法</p><ul><li><p>Fine-tuning方式是指在已经训练好的语言模型的基础上，加入少量的task-specific parameters, 例如对于分类问题在语言模型基础上加一层softmax网络，然后在新的语料上重新训练来进行fine-tune。</p></li><li><p>OpenAI GPT 是这一方法的典型代表，其模型如下所示:</p><p><img src="https://i.loli.net/2020/11/10/Ho3JzRbLeAkjiFM.png" alt="img"></p><p>GPT首先语言模型采用了Transformer Decoder的方法来进行训练，采用文本预测作为语言模型训练任务，训练完毕之后，加一层Linear Project来完成分类/相似度计算等NLP任务。</p></li><li><p>Fine-Tuning的方法工作包括两步：</p><ul><li>构造语言模型，采用大的语料A来训练语言模型</li><li>在语言模型基础上增加少量神经网络层来完成specific task model例如序列标注、分类等，然后采用有label的语料B来有监督地训练模型，这个过程中语言模型的参数并不固定.</li></ul></li><li><p>而BERT采用了fine-tuning的方法，并且在许多task-specific model中取得了最好的效果</p></li></ul></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      BERT论文阅读
    
    </summary>
    
    
    
  </entry>
  
</feed>
