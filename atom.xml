<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>思建的NLP之旅</title>
  
  <subtitle>沉淀自己</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2021-03-22T08:10:36.916Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>李思建</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>2021-03-22-动态规划</title>
    <link href="http://yoursite.com/2021/03/22/2021-03-22-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
    <id>http://yoursite.com/2021/03/22/2021-03-22-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/</id>
    <published>2021-03-22T07:43:46.000Z</published>
    <updated>2021-03-22T08:10:36.916Z</updated>
    
    <content type="html"><![CDATA[<p>参考</p><blockquote><p><a href="https://www.zhihu.com/question/39948290/answer/883302989" target="_blank" rel="noopener">如何理解动态规划？ - zhen tan的回答</a></p><p><a href="https://www.zhihu.com/question/39948290/answer/1309260344" target="_blank" rel="noopener">如何理解动态规划？ - 力扣（LeetCode）的回答</a></p></blockquote><h5 id="int-转string">int 转string</h5><ol type="1"><li><p>直接在int后面加一个空的字符串，java中默认<code>int类型和字符串类型相加，为字符串类型</code>。速度较慢</p><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">String s = i+<span class="string">""</span>;</span><br></pre></td></tr></tbody></table></figure></li><li><p>使用String的valueOf方法。 速度中间 .char也可以</p><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">String s = String.valueOf(i);</span><br></pre></td></tr></tbody></table></figure></li><li><p>使用int的封装类Integer，在Integer里面用他的toString方法。char也可以。速度较快 <span class="github-emoji" style="display:inline;vertical-align:middle;color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8">⭐</span></p><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">String s = Integer.toString(i);</span><br></pre></td></tr></tbody></table></figure></li></ol><p>注：</p><p>string.substring(int start, int end) :</p><p>从指定位置开始到指定位置结束截取字符串，不包括下标为end的字符。获取子字符串</p><h5 id="string-转-int">string 转 int</h5><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> i = Integer.parseInt(s);</span><br></pre></td></tr></tbody></table></figure><h5 id="char-转-string">char 转 string</h5><ol type="1"><li>高效方法，c可以是字符或者字符数组</li></ol><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">String s1 = String.valueOf(c);</span><br></pre></td></tr></tbody></table></figure><ol start="2" type="1"><li>使用Character里的toString()方法。Character.toString(char)方法实际上直接返回String.valueOf(char)</li></ol><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">String s = Character.toString(c);</span><br></pre></td></tr></tbody></table></figure><h5 id="string-转-char">string 转 char</h5><ol type="1"><li>使用String里的toCharArray()方法（返回值为char[]）可以得到将包含整个String的char数组。这样就能够使用数组下标来访问string中的任意位置的元素。</li></ol><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">char</span> [] chars = s.toCharArray();</span><br></pre></td></tr></tbody></table></figure><ol start="2" type="1"><li>使用方法 charAt (int index) 获取指定索引位置的字符,返回值为char类型。</li></ol><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">char</span> c = s.charAt(<span class="number">0</span>);</span><br></pre></td></tr></tbody></table></figure><h5 id="int-转char">int 转char</h5><p>int转string，再转char</p><h5 id="char-转int">char 转int</h5><ol type="1"><li>简单高效</li></ol><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> intNum = numChar - <span class="string">'0'</span>;</span><br></pre></td></tr></tbody></table></figure><ol start="2" type="1"><li>先转为String再转为int</li></ol><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">String string = String.valueOf(<span class="keyword">char</span>);</span><br><span class="line"><span class="keyword">int</span> num = Integer.ParseInt(string);</span><br></pre></td></tr></tbody></table></figure><blockquote><p>注： Integer.ParseInt(string) ，参数只能是string</p></blockquote><p>char之间的比较</p><p>直接用”==“</p><p>实际上java的基础类型比较都是用”==“，基础类型如下：</p><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">byte</span>：<span class="number">8</span>位，最大存储数据量是<span class="number">255</span>，存放的数据范围是-<span class="number">128</span>~<span class="number">127</span>之间。</span><br><span class="line"><span class="keyword">short</span>：<span class="number">16</span>位，最大数据存储量是<span class="number">65536</span>，数据范围是-<span class="number">32768</span>~<span class="number">32767</span>之间。</span><br><span class="line"><span class="keyword">int</span>：<span class="number">32</span>位，最大数据存储容量是<span class="number">2</span>的<span class="number">32</span>次方减<span class="number">1</span>，数据范围是负的<span class="number">2</span>的<span class="number">31</span>次方到正的<span class="number">2</span>的<span class="number">31</span>次方减<span class="number">1</span>。</span><br><span class="line"><span class="keyword">long</span>：<span class="number">64</span>位，最大数据存储容量是<span class="number">2</span>的<span class="number">64</span>次方减<span class="number">1</span>，数据范围为负的<span class="number">2</span>的<span class="number">63</span>次方到正的<span class="number">2</span>的<span class="number">63</span>次方减<span class="number">1</span>。</span><br><span class="line"><span class="keyword">float</span>：<span class="number">32</span>位，数据范围在<span class="number">3.4e-45</span>~<span class="number">1.4e38</span>，直接赋值时必须在数字后加上f或F。</span><br><span class="line"><span class="keyword">double</span>：<span class="number">64</span>位，数据范围在<span class="number">4.9e-324</span>~<span class="number">1.8e308</span>，赋值时可以加d或D也可以不加。</span><br><span class="line"><span class="keyword">boolean</span>：只有<span class="keyword">true</span>和<span class="keyword">false</span>两个取值。</span><br><span class="line"><span class="keyword">char</span>：<span class="number">16</span>位，存储Unicode码，用单引号赋值。</span><br></pre></td></tr></tbody></table></figure><p>string之间的比较</p><p>由于字符串是<code>对象类型</code>，所以不能 简单的用“==” 判断两个字符串是否相等，而使用 equals()方法比较两个对象的内容。 <code>a.equals(b）</code>。 相等则为 true；否则为 false。</p><p>注意： equals()方法比较的是<code>对象的内容</code>（区分字母的大小写格式） ，但是如果使用“==”操作符比较两个对象时， 比较的是<code>两个对象的内存地址</code>， 所以它们不相等 （即使内容相同， 不同对象的内存地址也是不相同的）</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;参考&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://www.zhihu.com/question/39948290/answer/883302989&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;如何理解动态规划？ - zhen
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2021-03-22-动态规划理解</title>
    <link href="http://yoursite.com/2021/03/22/2021-03-22-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E7%90%86%E8%A7%A3/"/>
    <id>http://yoursite.com/2021/03/22/2021-03-22-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E7%90%86%E8%A7%A3/</id>
    <published>2021-03-22T02:08:30.794Z</published>
    <updated>2021-03-22T07:41:29.528Z</updated>
    
    <content type="html"><![CDATA[<script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script&gt;
        document.querySelectorAll(&#39;.github-emoji&#39;)
          .forEach(el =&gt; {
            if (!el.dataset.src) { return; }
        
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2021-03-19-回溯法总结</title>
    <link href="http://yoursite.com/2021/03/19/2021-03-19-%E5%9B%9E%E6%BA%AF%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2021/03/19/2021-03-19-%E5%9B%9E%E6%BA%AF%E6%B3%95%E6%80%BB%E7%BB%93/</id>
    <published>2021-03-19T07:04:08.000Z</published>
    <updated>2021-03-19T07:49:47.233Z</updated>
    
    <content type="html"><![CDATA[<p>剑指offer38 字符串的排列</p><p>题目要求如下：</p><p><img src="E:\myBlog\source_posts\image-20210319150748118.png" alt="image-20210319150748118"></p><p>回溯法的特点就是深度优先遍历。也就是该问题的遍历顺序是1-&gt;2-&gt;3，然后从子节点3返回，从子节点2返回，再到1-&gt;3-&gt;2，以此类推。 状态的返回只有当前的节点不再满足问题的条件或者我们已经找到了问题的一个解时，才会返回，否则会以深度优先一直在解空间树内遍历下去。</p><p>一般的回溯法套路：</p><p>1.<strong>回溯出口</strong>，当找到了一个问题的解时，存储该解。 （） 2.<strong>回溯主体</strong>，就是遍历当前的状态的所有子节点，并判断<strong>下一个状态</strong>是否是满足问题条件的，如果满足问题条件，那么进入下一个状态。 （for循环） 3.<strong>状态返回</strong>，如果当前状态不满足条件，那么返回到前一个状态。</p><p>注意：<strong>程序的回溯，是靠程序运行时候的递归栈完成的。我该调头到哪，我要不要记录怎么调头，不是我们写代码时候关心的内容。回溯是程序自动完成的</strong></p><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//  def backtrack(路径，选择列表):</span></span><br><span class="line"><span class="comment">//    if 满足结束条件：     (回溯出口)</span></span><br><span class="line"><span class="comment">//     result.add(路径)</span></span><br><span class="line"><span class="comment">//    return</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//    for 选择 in 选择列表:     (回溯主体)</span></span><br><span class="line"><span class="comment">//      做选择</span></span><br><span class="line"><span class="comment">//      backtrack(路径，选择列表)</span></span><br><span class="line"><span class="comment">//      撤销选择                (状态返回)</span></span><br></pre></td></tr></tbody></table></figure><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">List&lt;String&gt; res = <span class="keyword">new</span> LinkedList&lt;&gt;();</span><br><span class="line">   <span class="keyword">char</span>[] c;</span><br><span class="line">   <span class="keyword">public</span> String[] permutation(String s) {</span><br><span class="line">       c = s.toCharArray();</span><br><span class="line">       dfs(<span class="number">0</span>);</span><br><span class="line">       <span class="keyword">return</span> res.toArray(<span class="keyword">new</span> String[res.size()]); <span class="comment">// 定义一个新的string数组，将res赋值到数组</span></span><br><span class="line">   }</span><br><span class="line">   <span class="function"><span class="keyword">void</span> <span class="title">dfs</span><span class="params">(<span class="keyword">int</span> x)</span> </span>{</span><br><span class="line">       <span class="keyword">if</span>(x == c.length - <span class="number">1</span>) {</span><br><span class="line">           res.add(String.valueOf(c));      <span class="comment">// 添加排列方案 . 将char数组变量c转换成字符串 </span></span><br><span class="line">           <span class="keyword">return</span>;</span><br><span class="line">       }</span><br><span class="line">       HashSet&lt;Character&gt; set = <span class="keyword">new</span> HashSet&lt;&gt;();</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i = x; i &lt; c.length; i++) {</span><br><span class="line">           <span class="keyword">if</span>(set.contains(c[i])) <span class="keyword">continue</span>; <span class="comment">// 在固定第x位的时候重复，因此剪枝</span></span><br><span class="line">           set.add(c[i]);</span><br><span class="line">           swap(i, x);                      <span class="comment">// 做出选择，将c[i]固定在第x位。剩余的c[i]并列的在x位</span></span><br><span class="line">           dfs(x + <span class="number">1</span>);                      <span class="comment">// 开启固定第 x + 1 位字符</span></span><br><span class="line">           swap(i, x);                      </span><br><span class="line">           <span class="comment">// 撤销选择。每次向下递归会交换两字符，回溯上来时，要把字符列表恢复至“交换前”的排序，这样才能保                 证下次交换是按照“全排列”的顺序进行的。 例如 abc ，a 交换的顺序应该是 a和b交换、a和c交换；                   那么在a和b交换完后就需要恢复，这样才能进行 a和c交换</span></span><br><span class="line">       }</span><br><span class="line">   }</span><br><span class="line">   <span class="function"><span class="keyword">void</span> <span class="title">swap</span><span class="params">(<span class="keyword">int</span> a, <span class="keyword">int</span> b)</span> </span>{</span><br><span class="line">       <span class="keyword">char</span> tmp = c[a];</span><br><span class="line">       c[a] = c[b];</span><br><span class="line">       c[b] = tmp;</span><br><span class="line">       }</span><br></pre></td></tr></tbody></table></figure><p>图示如下：</p><p><img src="E:\myBlog\source\_posts\1599403497-GATdFr-Picture2.png" alt="Picture2.png" style="zoom:50%;"></p><p><img src="https://i.loli.net/2021/03/19/H9Ct6Ejvq1SkOBr.png" alt="Snipaste_2021-02-01_15-31-42.png" style="zoom:50%;"></p><p>参考</p><blockquote><p><a href="https://blog.csdn.net/weixin_43208423/article/details/101081544" target="_blank" rel="noopener">理解回溯算法</a></p><p><a href="https://zhuanlan.zhihu.com/p/51882471" target="_blank" rel="noopener">LeetCode--回溯法心得</a></p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      刷leetcode时对回溯法的理解
    
    </summary>
    
    
    
      <category term="leetcode刷题" scheme="http://yoursite.com/tags/leetcode%E5%88%B7%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>2021-03-15-递归理解</title>
    <link href="http://yoursite.com/2021/03/15/2021-03-15-%E9%80%92%E5%BD%92%E7%90%86%E8%A7%A3/"/>
    <id>http://yoursite.com/2021/03/15/2021-03-15-%E9%80%92%E5%BD%92%E7%90%86%E8%A7%A3/</id>
    <published>2021-03-15T01:35:40.000Z</published>
    <updated>2021-03-15T03:09:05.445Z</updated>
    
    <content type="html"><![CDATA[<p>基本上，<strong>所有的递归问题都可以用递推公式来表示。有了这个递推公式，我们就可以很轻松地将它改为递归代码。</strong>。所以，遇到递归，先想<strong>递推公式</strong>。</p><h4 id="例1-比较明显的能递推公式的问题">例1: (比较明显的能递推公式的问题)</h4><ul><li>问题：斐波那契数列的第n项</li><li>递推公式：</li></ul><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">f(n)=f(n-1)+f(n-2) 其中，f(0)=0,f(1)=1</span><br></pre></td></tr></tbody></table></figure><ul><li>终止条件：</li></ul><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">if (n &lt;= 2) return 1;</span><br></pre></td></tr></tbody></table></figure><ul><li>递归代码：</li></ul><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">int f(int n) {</span><br><span class="line">  if (n &lt;= 2) return 1;</span><br><span class="line">  return f(n-1) + f(n-2);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h4 id="例2不那么明显的有递推公式的问题">例2:(不那么明显的有递推公式的问题)</h4><ul><li>问题：逆序打印一个数组</li><li>递推公式：</li></ul><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">假设令F(n)=逆序遍历长度为n的数组</span><br><span class="line">那么F(n)= 打印数组中下标为n的元素 + F(n-1)</span><br></pre></td></tr></tbody></table></figure><ul><li>终止条件：</li></ul><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">if (n &lt;0) return ;</span><br></pre></td></tr></tbody></table></figure><ul><li>递归代码：</li></ul><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">public void Print(int[] nums,int n){</span><br><span class="line">    if(n&lt;0) return;</span><br><span class="line">    System.out.println(nums[n]);</span><br><span class="line">    Print(nums,n-1);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>到这里，不知道大家对写递归有没有一些理解了。其实写递归不能总想着去把递归平铺展开，这样脑子里就会循环，一层一层往下调，然后再一层一层返回，试图想搞清楚计算机每一步都是怎么执行的，这样就很容易被绕进去。对于递归代码，这种试图想清楚整个递和归过程的做法，实际上是进入了一个思维误区。只要找到<strong>递推公式</strong>，我们就能很轻松地写出递归代码。</p><p>到这里，我想进一步跟大家说明我这个思路是比较能够容易出代码的，那么就树的遍历问题来和大家讲。递归总是和树分不开，其中，最典型的便是树的遍历问题。刚开始学的时候，不知道大家是怎么理解先／中／后序遍历的递归写法的，这里我提供我的思路供参考，以前序遍历为例：</p><ul><li>问题：二叉树的先序遍历</li><li>递推公式：</li></ul><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">令F(Root)为问题:遍历以Root为根节点的二叉树，</span><br><span class="line">令F(Root.left)为问题:遍历以F(Root.left)为根节点的二叉树</span><br><span class="line">令F(Root.right)为问题:遍历以F(Root.right)为根节点的二叉树</span><br><span class="line">那么其递推公式为：</span><br><span class="line">F(Root)=遍历Root节点+F(Root.left)+F(Root.right)</span><br></pre></td></tr></tbody></table></figure><ul><li>递归代码：</li></ul><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">public void preOrder(TreeNode node){</span><br><span class="line">    if(node==null) return;</span><br><span class="line">    System.out.println(node.val);</span><br><span class="line">    preOrder(node.left);</span><br><span class="line">    preOrder(node.righr);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p><strong>Rules Number Two</strong>, <strong>递归是一种关于某个重复动作(完成重复性的功能)的形式化描述</strong>。具体点讲，如果一个问题 A 可以分解为若干子问题 B、C、D，<strong>你可以假设子问题 B、C、D 已经解决，在此基础上思考如何解决问题 A</strong>。而且，你只需要思考问题 A 与子问题 B、C、D 两层之间的关系即可，不需要一层一层往下思考子问题与子子问题，子子问题与子子子问题之间的关系(也就是说，递归只能考虑当前层和下一层的关系，不能继续往下深入)。我们需要屏蔽掉递归细节，理解为完成了某种功能的形式化描述即可。</p><ul><li>问题：单向链表的反转 ？？？</li><li>递推公式：</li></ul><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">令F(node)为问题:反转以node为头节点的单向链表；</span><br><span class="line">一般，我们需要考虑F(n)和F(n-1)的关系，那么这里，如果n代表以node为头节点的单向链表，那么n-1就代表以node.next为头节点的单向链表.</span><br><span class="line">所以，我们令F(node.next)为问题：反转以node.next为头节点的单向链表；</span><br><span class="line">那么，F(node)和F(node.next)之间的关系是？这里我们来简单画个图，假设我们反转3个节点的链表：</span><br><span class="line">1 -&gt; 2 -&gt; 3</span><br><span class="line">那么，F(node=1)=F(node=2)+?</span><br><span class="line">这里假设子问题F(node=2)已经解决，那么我们如何解决F(node=1)：</span><br><span class="line">很明显，我们需要反转node=2和node=1， 即 node.next.next=node; 同时 node.next=null;</span><br><span class="line">所以，这个问题就可以是：F(node=1)=F(node=2)+ 反转node=2和node=1</span><br></pre></td></tr></tbody></table></figure><ul><li>递归代码：</li></ul><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">public ListNode reverseList(ListNode head) {</span><br><span class="line">        if(head == null || head.next == null) {  //终止条件并不难想</span><br><span class="line">            return head;</span><br><span class="line">        }</span><br><span class="line">        ListNode node = reverseList(head.next);</span><br><span class="line">        head.next.next = head;</span><br><span class="line">        head.next = null;</span><br><span class="line">        return node;  //按上面的例子，F(node=1)和F(node=2)它俩反转后的头节点是同一个</span><br><span class="line">    }</span><br></pre></td></tr></tbody></table></figure><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      通过leetcode刷题，记录对递归的理解
    
    </summary>
    
    
      <category term="算法" scheme="http://yoursite.com/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="算法" scheme="http://yoursite.com/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>2020-12-07-hexo上传文件并提供下载链接</title>
    <link href="http://yoursite.com/2020/12/07/2020-12-07-hexo%E4%B8%8A%E4%BC%A0%E6%96%87%E4%BB%B6%E5%B9%B6%E6%8F%90%E4%BE%9B%E4%B8%8B%E8%BD%BD%E9%93%BE%E6%8E%A5/"/>
    <id>http://yoursite.com/2020/12/07/2020-12-07-hexo%E4%B8%8A%E4%BC%A0%E6%96%87%E4%BB%B6%E5%B9%B6%E6%8F%90%E4%BE%9B%E4%B8%8B%E8%BD%BD%E9%93%BE%E6%8E%A5/</id>
    <published>2020-12-07T14:15:44.000Z</published>
    <updated>2020-12-08T11:19:38.218Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言">前言</h3><p>想要在博客中分享论文，但是一般都是直接在论文上做了笔记，所以就想着上传做好笔记的论文，并且提供下载会更方便一些</p><p>下载了hexo-pdf插件，可以预览，但是好像并不好用。于是就用一下办法</p><h3 id="步骤">步骤</h3><h4 id="提供下载链接">提供下载链接</h4><ol type="1"><li><p>首先修改程序配置文件 <code>_config.yml</code> 中的 <strong>post_asset_folder:</strong>，这个选项设置为 <code>True</code>。</p></li><li><p>在 source 文件夹下创建 <code>\papers</code> 文件夹，将我们需要上传的本地文件移动到这个文件夹。（如 <code>test.pdf</code>）</p></li><li><p>最后在文章中，按照下面的格式引入：</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ 点击下载文件 ](/papers/test.pdf) #以sourse为相对路径</span><br></pre></td></tr></tbody></table></figure></li><li><p>最后重新 <code>hexo d -g</code> 更新就可以了。</p></li></ol><p>效果如下：</p><p><img src="https://i.loli.net/2020/12/08/BDOlcKohYgvP136.png" alt="image-20201207221916631"></p><p>点击<code>点击下载文件</code>，可以跳转到浏览器自带的pdf预览，并提供下载（chrome浏览器情况下）</p><h4 id="提供预览功能-star">提供预览功能 <span class="github-emoji" style="display:inline;vertical-align:middle;color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8">⭐</span></h4><p>使用的是hexo-pdf插件</p><ol type="1"><li>在blog/目录下bash执行安装hexo-pdf插件</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-pdf --save</span><br></pre></td></tr></tbody></table></figure><ol start="2" type="1"><li><p>在主题配置文件中更改<code>pdf关键字</code>配置如下：</p><figure class="highlight yml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">pdf:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="comment"># Default height</span></span><br><span class="line">  <span class="attr">height:</span> <span class="string">500px</span></span><br><span class="line">  <span class="attr">pdfobject:</span></span><br><span class="line">    <span class="comment"># Use 2.1.1 as default, jsdelivr as default CDN, works everywhere even in China</span></span><br><span class="line">    <span class="attr">cdn:</span> <span class="string">//cdn.jsdelivr.net/npm/pdfobject@2.1.1/pdfobject.min.js</span></span><br><span class="line">    <span class="comment"># CDNJS, provided by cloudflare, maybe the best CDN, but not works in China</span></span><br><span class="line">    <span class="comment">#cdn: //cdnjs.cloudflare.com/ajax/libs/pdfobject/2.1.1/pdfobject.min.js</span></span><br></pre></td></tr></tbody></table></figure></li><li><p>可以对pdf显示的格式进行个性化设置 ，在自定义<code>custom.styl</code>中添加如下代码</p><figure class="highlight css"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.pdfobject-container</span> {</span><br><span class="line">  <span class="attribute">position</span>: relative;</span><br><span class="line">  <span class="attribute">overflow</span>: auto;</span><br><span class="line">  <span class="attribute">width</span>: <span class="number">100%</span>;</span><br><span class="line">  // height: unquote(hexo-config('pdf.height'));</span><br><span class="line">  <span class="selector-tag">height</span>: 100%;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure></li></ol><p>效果如下：</p><p><img src="https://i.loli.net/2020/12/08/PozJ3T4fjK6iXAp.png" alt="image-20201208182306107"></p><h3 id="注">注</h3><ol type="1"><li><p>不光可以上传pdf ，一般的文件（压缩包等）都是可以上传的，并提供下载链接。</p></li><li><p>我试过将pdf文件保存在_posts文件夹中，但是并不能显示下载和预览功能。只能是将pdf放在sourse文件夹下才可</p></li><li><p>papers内的pdf上传到了github的<a href="https://github.com/OopsAaron/OopsAaron.github.io" target="_blank" rel="noopener" class="uri">https://github.com/OopsAaron/OopsAaron.github.io</a>中和</p><p><a href="https://github.com/OopsAaron/myBlog" target="_blank" rel="noopener" class="uri">https://github.com/OopsAaron/myBlog</a> 中 （需要提交到github上）</p></li></ol><h3 id="具体示例">具体示例</h3><p>具体效果已经我的应用，可以参考我的另一篇博客<a href="https://lisijian.cn/2020/11/19/2020-11-19-%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/" target="_blank" rel="noopener">论文分享-LambdaNetworks</a></p><h3 id="参考">参考</h3><blockquote><p><a href="https://github.com/superalsrk/hexo-pdf" target="_blank" rel="noopener">hexo-pdf官方文档</a></p><p><a href="https://lingr7.github.io/2019/10/02/theme-hexo-pdf%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7-%E8%A7%A3%E5%86%B3hexo-pdf%E6%96%87%E4%BB%B6%E6%98%BE%E7%A4%BA%E4%B8%8D%E5%85%A8%E7%9A%84%E9%97%AE%E9%A2%98" target="_blank" rel="noopener">解决hexo-pdf文件显示不全的问题</a></p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      hexo上传文件并提供下载链接
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-11-26-ipad做网页笔记方法</title>
    <link href="http://yoursite.com/2020/11/26/2020-11-26-ipad%E5%81%9A%E7%BD%91%E9%A1%B5%E7%AC%94%E8%AE%B0%E6%96%B9%E6%B3%95/"/>
    <id>http://yoursite.com/2020/11/26/2020-11-26-ipad%E5%81%9A%E7%BD%91%E9%A1%B5%E7%AC%94%E8%AE%B0%E6%96%B9%E6%B3%95/</id>
    <published>2020-11-26T07:02:37.000Z</published>
    <updated>2020-12-13T13:16:38.866Z</updated>
    
    <content type="html"><![CDATA[<h3 id="问题">问题</h3><p>想将网页内容导出pdf，用ipad做笔记。但是在用chrome或者dge进行打印成pdf时因为网页本身的顶部的tab总会掩盖掉一部分网页内容</p><h3 id="解决">解决</h3><p>最终找到一个好方法</p><p>用Safari打开网页，然后在网页右上角点击分享按钮，会出现标记样式。点击就可以在网页端做笔记。</p><p><img src="https://i.loli.net/2020/11/26/HpRy5toOr1fcUPe.png"></p><p>不过毕竟在goodnotes做笔记习惯了，所以可以用goodnotes打开，然后就自动导入pdf到goodnotes中，就可以愉快做笔记了</p><p>虽然导入的pdf字体有点小，不过好在不会遮挡住任何网页内容。这是我想到的最合适我的方法了</p><h3 id="缺点">缺点</h3><p>如果网页过长，利用safari自带的产生pdf功能会对网页截不全 <span class="github-emoji" style="display:inline;vertical-align:middle;color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f622.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f622.png?v8">😢</span></p><p>为什么会截不全呢？</p><h3 id="其它">其它</h3><p>chrome里有很多的截藏插件可以方便获取网页内容。</p><p>印象笔记截藏插件，很好用，是我见过的获取网页内容最完美的工具，还可以只是截藏网页中自己想要的内容，但是我最主要还是想要获取pdf，然后导入ipad方便做笔记。但是在印象笔记网页端无法提供网页端功能，客户端又很卡，不知道什么原因。总之使用起来很不方便，于是电脑端放弃！ 在ipad下载了印象笔记，绑定账号后，可以同步阅读收藏的笔记，但是在导出pdf一项时，同样丢失些内容，就很神奇！放弃</p><p>有道笔记截藏插件似乎并不是对所有网站适用。有的网站无法启用此插件，放弃</p><p>其它的截藏插件也试过，无法达到想要的结果</p><h4 id="fireshot">fireshot</h4><p>最接近我想要的结果的插件就是fireshot了，插件挺不错在截图的时候不会覆盖掉网页端内容，可以保存为图片或者导出为pdf！ 这很接近我的目标。</p><p><img src="https://i.loli.net/2020/12/11/8JKrY9qFigIwdUu.png" alt="image-20201211162253078"></p><h5 id="缺点-1">缺点</h5><p>在导出的pdf中，会丢失一些像素，如果网页中有一些小图的话，可能就看不清其内容，不过总体是在可以接受的范围。</p><h4 id="pdfcrowd">Pdfcrowd</h4><p>这个网站不错，可以将要保存的网页链接复制到转换框内，就可以下载pdf了。同时还支持生成图片、gif！</p><p>生成效果不错，不会有覆盖的情况</p><p><img src="https://i.loli.net/2020/12/13/zdyVlgkWJBbmLuo.png" alt="image-20201213211110932"></p><p>缺点：</p><p>在生成的pdf中，每一页都会有一个底边栏，不过无伤大雅。比较满意了</p><p>网站链接如下：<a href="https://pdfcrowd.com/" target="_blank" rel="noopener" class="uri">https://pdfcrowd.com/</a></p><h3 id="总结">总结</h3><ol type="1"><li><p>一般都会使用Pdfcrowd，生成质量高，推荐！</p></li><li><p>对于网页中没有顶部或者底部状态栏的情况下（比如微信的公众号内容），那么就可以用chrome自带的虚拟打印功能可直接导出pdf （ctrl+P），这是最理想的情况</p></li><li><p>如果有状态栏情况，并且无需要仔细看的小图，那么就可以用fireshot插件导出pdf</p></li></ol><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      自己摸索的ipad做网页笔记的方法
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Practices阅读笔记</title>
    <link href="http://yoursite.com/2020/11/26/2020-11-26-Deep%20Learning%20for%20NLP%20Best%20Practices%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2020/11/26/2020-11-26-Deep%20Learning%20for%20NLP%20Best%20Practices%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</id>
    <published>2020-11-26T06:20:14.000Z</published>
    <updated>2020-11-26T07:18:35.534Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言">前言</h3><p>博客地址：<a href="http://ruder.io/deep-learning-nlp-best-practices/index.html" target="_blank" rel="noopener" class="uri">http://ruder.io/deep-learning-nlp-best-practices/index.html</a></p><h3 id="正文">正文</h3><p>本文尝试将阅读上述博客之后的得到的一些tips记录起来</p><h3 id="参考">参考</h3><blockquote><p><a href="https://ruder.io/deep-learning-nlp-best-practices/index.html" target="_blank" rel="noopener" class="uri">https://ruder.io/deep-learning-nlp-best-practices/index.html</a></p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      本篇博客写的不错，会在实践中给出很多指导
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-11-24-transformer优化-temp</title>
    <link href="http://yoursite.com/2020/11/24/2020-11-24-transformer%E4%BC%98%E5%8C%96-temp/"/>
    <id>http://yoursite.com/2020/11/24/2020-11-24-transformer%E4%BC%98%E5%8C%96-temp/</id>
    <published>2020-11-24T11:48:40.000Z</published>
    <updated>2020-12-01T11:50:50.484Z</updated>
    
    <content type="html"><![CDATA[<p>想法：</p><p>总体来说，基于原始的transformer模型，在这个基础上综合增加其它论文的创新点，以及最好能够有自己的创新点</p><p>思路有两个：</p><ol type="1"><li>通过现有的模型，去解决没有解决过的任务。比如NLP的模型去解决CV的任务</li><li>在现有的模型中去添加自己的想法，使得有更好的性能 ，如transformer-XL对transformer的改进</li></ol><p>一些想法：</p><ol type="1"><li><p>一些在bert上进行优化的模型是否可以运用到修改transformer的attention结构中。例如ConvBERT,AlBERT</p><p>ALBERT相比原始BERT其实更适合浅层Transformer，也可以作为之后的尝试方向。</p></li><li><p>降低transformer模型的计算量，已经存在的模型，模型剪枝、量化、蒸馏等精简技术，又或者修改 Attention 结构</p></li><li><p>解决自回归的问题，之前读过teacher-forcing 模型来进行改进</p></li><li><p>之前改进的模型如果用在图像领域，可以用在文本领域，如果这样做的话，是否有创新性呢？？</p></li><li><p>感觉最近的论文对于transformer的稀疏性研究挺多 ，是否可以借鉴几篇 。 对transformer进行优化</p></li><li><p>将transformer中的部分结构换成LSTM是否会效果好一些呢 ？ 结合attention和LSTM</p></li><li><p>重点基于transformer-XL和Reformer这些关注点高的论文</p></li><li><p>从经典论文引用的论文中去找到新方法去结合自己的模型</p></li><li><p>BERT以及其它的模型，可以不用这么多层数，不用这么多的数据集，去设计不同的自监督任务去改进模型。</p><p>可以像用word2vec一样使用这些pretrain模型，然后去专注task相关的研究（在你能接受的数据和计算资源范围内。这样是否可以呢 ？？</p><p>比如只有6层的BERT-base中，MLM策略是采用的自己设计的，或者融合其它模型的MLM策略，与BERT-base进行比较，发现效果有所提升，是否就可以发表论文？？</p><p>因为设计的自监督任务必须要与下游任务尽量相关的，这样才能在预训练阶段学到更多东西</p></li><li><p>对于一些优化器或者激活函数或者norm函数进行优化，（使用最新模型使用的优化器）可能会对模型效果有所提升</p></li><li><p>有空读读word2vec的内容，从之前的模型中去结合形成创新点</p></li><li><p>网上是否有综合了transformer-base模型的github项目。类似于hugging face对于BERT-base模型</p></li><li><p>将decoder的自回归变成非自回归形式的。参考论文&lt;Deep Encoder, Shallow Decoder: Reevaluating the Speed-Quality Tradeoff in Machine Translation&gt; 2020年论文，无代码</p></li><li><p>exposure bias问题的解决，参考<scheduled sampling="" for="" transformers="">，8.1分享。有torch代码（非官方）</scheduled></p></li><li><p>将一些部分设置为adaptive 动态的。</p></li></ol><p>总体就是以一个模型（如transformer-XL）为基础，在上面增加其它的思想，增加创新点</p><p>计划：</p><p>transformer-XL可以增强transformer处理长文本的能力，但是同时它的复杂度依然是n2，所以以transformer-XL为基础，来优化结构，降低复杂度，是可以解决的一个思路，同时如果能够解决exposure bias问题更好</p><p>以transformer-XL为基础进行改进，有pytorch代码。解决机器翻译问题，或者看邱锡鹏教授实验室的研究方向</p><p><a href="https://github.com/kimiyoung/transformer-xl" target="_blank" rel="noopener" class="uri">https://github.com/kimiyoung/transformer-xl</a></p><p>对transformer-XL进行压缩，减少memory。 或者参考XLNET， 也是结合transformer-xl</p><p>结合BERT的一些思想， 或者其它的压缩模型思想</p><p>线性核transformer进行结合 ， paperweekly 苏剑林的提议， 结合到transformer-xl中</p><p>将层数降低比较是否可以？</p><p>今年ICLR2020已经有一些工作对天transformer-XL进行改进 。借鉴查看</p><hr><p>疑问：</p><p>如果用bert去做图像的任务，会有以下问题</p><ol type="1"><li><p>BERT 训练需要的资源多，可能会训练的周期长。解决方法：将参数量减少，层数减少</p></li><li><p>bert 主要是进行自监督，也就是处理的是没有标签的数据。这样的数据在nlp很多，但是在图像上很多是有标签的</p></li></ol><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      transformer模型改进优化总结
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-11-20-论文分享</title>
    <link href="http://yoursite.com/2020/11/20/2020-11-20-%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"/>
    <id>http://yoursite.com/2020/11/20/2020-11-20-%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/</id>
    <published>2020-11-20T02:46:05.000Z</published>
    <updated>2020-11-26T06:16:10.478Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2020/11/20/ict1zGoAVsCSgbJ.png"></p><blockquote><p>NeurIPS 2020</p><p>code url (official tf) : <a href="https://github.com/yitu-opensource/ConvBert" target="_blank" rel="noopener" class="uri">https://github.com/yitu-opensource/ConvBert</a></p></blockquote><h3 id="背景">背景</h3><p>本文是国内的依图科技发表在NeurlPS 2020 上的一篇论文，今年的NeurlPS 2020 将在12月份温哥华举办，全球仅接受了1900篇论文，所以接受的论文很值得去阅读了。</p><p>本文是从模型基本的attention结构入手去改进BERT模型</p><p>本文的Abstract如下：</p><p><img src="https://i.loli.net/2020/11/20/Rb6es4qz9fjDcCd.png"></p><h3 id="问题">问题</h3><p>BERT-family模型现在不断刷新NLP领域，但是BERT模型的基础attention块也严重依赖全局的自我注意力块，并且消耗大量的memory以及计算资源。</p><p>降低BERT对于计算资源的利用是目前研究的问题</p><h3 id="解决">解决</h3><p>有很多工作从改进预训练任务或者利用知识蒸馏的方法优化模型的训练，但是少有改进attention模型结构（backbone architecture）的工作。</p><p>依图发现一些注意力头虽然是全局视角，但是只能关注到局部依赖，这样就存在一些计算冗余。</p><p>依图研发团队从这种模型结构本身的冗余出发，提出了一种基于跨度的动态卷积（span-based dynamic convolution）去代替<strong>一部分</strong>原有的attention去直接建模局部依赖。 （注意是一部分，而不是全部）</p><p>通过这种span-based dynamic convolution 和剩余的 self-attention heads 一起使得全局和局部学习更高效。</p><p>也就是将卷积整合到self-attention中去形成一个mixed attention mechanism ，以此来结合这二者的优势</p><h3 id="贡献">贡献</h3><p>提出了ConvBERT，通过全新的注意力模块，仅用 1/10 的训练时间和 1/6 的参数就获得了跟 BERT模型一样的精度。</p><p>具体如下：</p><p><img src="https://i.loli.net/2020/11/20/5GKrsQzLR7w43Bg.png"></p><h3 id="模型">模型</h3><h4 id="self-attention-dynamic-convolution-span-based-dynamic-convolution-的不同">Self-attention &amp; Dynamic convolution &amp; Span-based dynamic convolution 的不同</h4><p>其中可以认为attention weight = convolution kernel （都是代表的关联度的强弱）</p><p><img src="https://i.loli.net/2020/11/20/NDqxszOCaG4T5Ub.png"></p><p>在前人的Dynamic convolution基础上，基于span改进得到了span-based dynamic convolution。不只是接受单一的token，而是接受一段token的span来产生更多的自适应的卷积核，这样可以解决Dynamic convolution存在的问题，<strong>可以达到使得不同上下文的同一个token能够产生不同的卷积核。</strong>具体如下：</p><p><img src="https://i.loli.net/2020/11/20/ipIFCS5nBwW71xm.png"></p><h4 id="bottleneck-structure-grouped-linear">bottleneck structure &amp; grouped linear</h4><p>除了对于最基本的attention结构进行改动之外，本文还在另外两处对BERT进行了修改，即bottleneck structure &amp; grouped linear</p><p>bottleneck structure ： 通过将输入token嵌入到低维空间中来减少不必要的head的数量 （降维来减少head），以此来减少冗余并且提高效率</p><p>grouped linear ： 因为一般的前馈网络层（FFN）的维度会是输入输出维度的4倍，维度较高，这样就消耗了很多计算量，于是采用了groupedlinear operator 操作，在降低参数的同时也没有降低表示能力。</p><p>具体如下：</p><p><img src="https://i.loli.net/2020/11/20/glMfEBvmbS7arkX.png"></p><blockquote><p>注：</p><p>感觉grouped linear 和reformer模型中的FFN层的分块类似</p></blockquote><p>BERT和ConvBERT模型的attention map（attention map 可以理解成词与词之间的关系）的不同如下图所示。大多注意力主要集中在对角线，即主要学习到的是局部的注意力。这就意味着其中存在着冗余，也就是说很多 attention map 中远距离关系值是没有必要计算的。</p><p>ConvBERT的attention map不再过多关注局部的关系，这正是卷积模块减少冗余的作用体现</p><p><img src="https://i.loli.net/2020/11/20/iZTbkydfSw3zNJc.png"></p><p>很多head是不必要存在的，于是采用卷积操作来捕获局部依赖。原文如下：</p><p><img src="https://i.loli.net/2020/11/20/z1XMwN9E4quKpUJ.png"></p><h4 id="light-weight-dynamic-convolution">Light-weight &amp; dynamic convolution</h4><p>这两个模型都是Facebook AI Research发表在ICLR 2019上的论文中提出的，<a href="https://openreview.net/pdf?id=SkVhlh09tX" target="_blank" rel="noopener">原文地址</a></p><p><img src="https://i.loli.net/2020/11/20/MprAVtU3zRQFyfe.png"></p><p>模型基本如下：</p><p>先是引出了Light-weight convolution的运算操作：</p><p><img src="https://i.loli.net/2020/11/20/hP6dy915jSEMoLi.png"></p><p>其中X∈R^n×d 为输入的特征，而W∈R^k 则是卷积核(相当于是加权)，k 为卷积核的大小。轻量卷积的作用是将输入的每个词对应的特征附近的 k 个特征加权平均生成输出。</p><p>在此基础上，动态卷积（dynamic convolution ）可以写作</p><p><img src="https://i.loli.net/2020/11/20/ayimwIMseuzNbjA.png"></p><p>此处卷积核是由对应的词的特征经过线性变换和 softmax 之后产生的。</p><h4 id="span-based-dynamic-convolution">Span-based dynamic convolution</h4><p>相比于动态卷积，Span-based dynamic convolution 依赖的不是单一的token，而是local context</p><p><img src="https://i.loli.net/2020/11/20/JUOTimGSQa9o73E.png"></p><p>输入 X 先经过线性变换生成Q和V，同时经过卷积生成基于跨度的K_s，由Q⊙K_s经过线性变换以及softmax来产生卷积核与V进一步做轻量卷积，从而得到终的输出。</p><p>具体如下：</p><p><img src="https://i.loli.net/2020/11/20/FcnEjJLkDgT27Z9.png"></p><p>三者的不不同如下图</p><p><img src="https://i.loli.net/2020/11/20/Ev7wzpkdQsy3KRj.png"></p><h4 id="convbert模型的总体架构">ConvBERT模型的总体架构</h4><p>粉色背景的是span-based dynamic convolution。卷积和attention共享query但是有不同k，去分别生成attention map 和 convolution kernel （都是在softmax之后得到的），以此来关注局部和全局的依赖。</p><p><img src="https://i.loli.net/2020/11/20/vGYPtVUA1uq67Ek.png"></p><p>总体表示如下：</p><p><img src="https://i.loli.net/2020/11/20/aq3tfTYKN2xd9wg.png"></p><h3 id="实验">实验</h3><p>本文做了挺多的实验，并且达到了SOTA水平</p><p>预训练数据集：OpenWebText</p><p>测试数据集： GLUE &amp; SQuAD</p><h4 id="消融实验">消融实验</h4><ol type="1"><li>本文对于attention的三个创新点进行消融，具体实验结果如下：</li></ol><p><img src="https://i.loli.net/2020/11/20/BNCuJeG3OcKygZq.png"></p><ol start="2" type="1"><li>除此之外，本文还研究了核的大小对GLUE score的影响，当核大小=9时效果最好，本文的模型也采用了这种设计</li></ol><p><img src="https://i.loli.net/2020/11/20/iCw8nOuU2pjFHb9.png"></p><ol start="3" type="1"><li><p>本文将提出的Span-based Dynamic 与 经典的卷积进行对比实验，结果如下：</p><p><img src="https://i.loli.net/2020/11/20/OetQPKzr1bGpxuD.png"></p></li></ol><h4 id="综合实验">综合实验</h4><p>将ConvBERT和BERT还有ELECTRA在相似大小的情况下进行比较</p><p>在GLUE测试集上比较</p><p><img src="https://i.loli.net/2020/11/20/oQUFRbn97z4dLZa.png"></p><p>在SQuAD测试集上比较</p><p><img src="https://i.loli.net/2020/11/20/MtqUbFjgaROc5lf.png"></p><h3 id="总结">总结</h3><p>之前看过韩松实验室的Lite transformer模型，主要是对transformer模型的压缩。Lite transformer也是观察到transformer模型中的attention存在着冗余，于是也是提出了CNN结构结合self-attenion来一起学习全局和局部依赖。只是与本文不同的是，Lite transformer是通过将输入进行分流，一部分流入CNN块，来建模输入序列中的局部关系依赖，一部分流入self-attention块来迫使self-attention建模全局关系，以此减少了self-attention中的O(n2)复杂度，从而达到压缩的目的。</p><p>而本文是span-based dynamic convolution和self-attention的融合来改进attention机制，感觉是从根本上解决attention的固有缺点，并且效果也达到了SOTA水平，本文更优质一些</p><p>不过现在paper with code中除了官方给出的code，没有其他人贡献code，随着这篇论文被更多科研人员知道并使用改进，也会贡献更多的code吧，更希望hugging face 能够将本模型融入到github中，为NLP做出更多贡献</p><h3 id="参考">参考</h3><blockquote><p><a href="https://zhuanlan.zhihu.com/p/60482693" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/60482693</a></p><p><a href="https://www.jiqizhixin.com/articles/2020-11-12-5" target="_blank" rel="noopener" class="uri">https://www.jiqizhixin.com/articles/2020-11-12-5</a></p></blockquote><hr><p>自面世以来，Transformer 模型已经在多个领域取得了 SOTA 结果，包括自然语言处理、图像处理甚至是音乐处理。众所周知，Transformer 架构的核心是注意力模块，它计算输入序列中所有位置对的相似度得分。然而，随着输入序列长度的增加，注意力机制本身的问题也越来越突出，因为它需要二次方的计算时间来产生所有的相似度得分，用来存储这些得分的内存大小也是如此。</p><p>针对那些需要长距离注意力的应用，部分研究者已经提出了一些速度快、空间利用率高的方法，其中比较普遍的方法是稀疏注意力。</p><p><img src="E:\myBlog\source_posts\v2-6bc976426f0f0c68f43f26abe7500836_720w.jpg" alt="img">标准的稀疏化技术。</p><p>然而，稀疏注意力方法也有一些局限。首先，它们需要高效的稀疏矩阵乘法运算，但这并不是所有加速器都能做到的；其次，它们通常不能为自己的表示能力提供严格的理论保证；再者，它们主要针对 Transformer 模型和生成预训练进行优化；最后，它们通常会堆更多的注意力层来补偿稀疏表示，这使其很难与其他预训练好的模型一起使用，需要重新训练，消耗大量能源。</p><p>此外，稀疏注意力机制通常不足以解决常规注意力方法应用时所面临的所有问题，如指针网络。还有一些运算是无法稀疏化的，比如常用的 softmax 运算。</p><p>为了解决这些问题，来自谷歌、剑桥大学、DeepMind、阿兰 · 图灵研究所的研究者提出了一种新的 Transformer 架构——Performer。它的<strong>注意力机制能够线性扩展，因此能够在处理长序列的同时缩短训练时间</strong>。这点在 ImageNet64 等图像数据集和 PG-19 文本数据集等序列的处理过程中都非常有用。</p><p><img src="E:\myBlog\source_posts\v2-733efa2ccadc511ca629f3e72c7d84aa_720w.jpg" alt="img"></p><p>论文链接：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2009.14794.pdf">https://arxiv.org/pdf/2009.14794.pdf</a></p><p>Performer 使用一个高效的（线性）广义注意力框架（generalized attention framework），允许基于不同相似性度量（核）的一类广泛的注意力机制。该框架通过谷歌的新算法 FAVOR+（ Fast Attention Via Positive Orthogonal Random Features）来实现，后者能够提供注意力机制的可扩展低方差、无偏估计，这可以通过随机特征图分解（常规 softmax-attention）来表达。该方法在保持线性空间和时间复杂度的同时准确率也很有保证，也可以应用到独立的 softmax 运算。此外，该方法还可以和可逆层等其他技术进行互操作。</p><p>研究者表示，他们相信该研究为注意力、Transformer 架构和核方法提供了一种新的思维方式。</p><p>代码地址：<a href="https://link.zhihu.com/?target=https%3A//github.com/google-research/google-research/tree/master/performer">https://github.com/google-research/google-research/tree/master/performer</a></p><p>论文公布之后，Youtube 知名深度学习频道 Yannic Kilcher 对该文章进行了解读。</p><p><strong>广义的注意力机制</strong></p><p>在以往的注意力机制中，分别对应矩阵行与列的 query 和 key 输入相乘，通过 softmax 计算形成一个注意力矩阵，以存储相似度系数。值得注意的是，这种方法不能将 query-key 生成结果传递给非线性 softmax 计算之后再将其分解为原始的 query 和 key。然而，将注意力矩阵分解为原始 query 和 key 的随机非线性函数的乘积是可以的，即所谓的随机特征（random feature），这样就可以更加高效地对相似度信息进行编码。</p><p><img src="E:\myBlog\source_posts\v2-36bfb8693fd72033970bd83762575b3e_720w.jpg" alt="img"></p><p><em>标准注意力矩阵包括每一对 entry 的相似度系数，由 query 和 key 上的 softmax 计算组成，表示为 q 和 k。</em></p><p>常规的 softmax 注意力可以看作是由指数函数和高斯投影定义的非线性函数的一个特例。在这里我们也可以反向推理，首先实现一些更广义的非线性函数，隐式定义 query-key 结果中其他类型的相似性度量或核函数。研究者基于早期的核方法（kernel method），将其定义为广义注意力（generalized attention）。尽管对于大多核函数来说，闭式解并不存在，但这一机制仍然可以应用，因为它并不依赖于闭式解。</p><p>该研究首次证明了，任意注意力矩阵都可以通过随机特征在下游 Transformer 应用中实现有效地近似。实现这一点的的新机制是使用正随机特征，即原始 query 和 key 的正直非线性函数，这对于避免训练过程中的不稳定性至关重要，并实现了对常规 softmax 注意力的更准确近似。</p><p><strong>新算法 FAVOR+：通过矩阵相关性实现快速注意力</strong></p><p>上文描述的分解允许我们以线性而非二次内存复杂度的方式存储隐式注意力矩阵。我们还可以通过分解获得一个线性时间注意力机制。虽然在分解注意力矩阵之后，原始注意力机制与具有值输入的存储注意力矩阵相乘以获得最终结果，我们可以重新排列矩阵乘法以近似常规注意力机制的结果，并且不需要显式地构建二次方大小的注意力矩阵。最终生成了新算法 FAVOR+。</p><p><img src="E:\myBlog\source_posts\v2-131935e1ce0b0a5fb4fd1a0c6e2bc6f6_720w.jpg" alt="img"></p><p><em>左：标准注意力模块计算，其中通过执行带有矩阵 A 和值张量 V 的矩阵乘法来计算最终的预期结果；右：通过解耦低秩分解 A 中使用的矩阵 Q′和 K′以及按照虚线框中指示的顺序执行矩阵乘法，研究者获得了一个线性注意力矩阵，同时不用显式地构建 A 或其近似。</em></p><p>上述分析与双向注意力（即非因果注意力）相关，其中没有 past 和 future 的概念。对于输入序列中没有注意前后 token 的单向（即因果）注意力而言，研究者稍微修改方法以使用前缀和计算（prefix-sum computation），它们只存储矩阵计算的运行总数，而不存储显式的下三角常规注意力矩阵。</p><p><embed src="E:\myBlog\source_posts\v2-7698eb01869e11a5042e8f1742497f44_b.webp"></p><p><em>左：标准单向注意力需要 mask 注意力矩阵以获得其下三角部分；右：LHS 上的无偏近似可以通过前缀和获得，其中用于 key 和值向量的随机特征图的外积（outer-product）前缀和实现动态构建，并通过 query 随机特征向量进行左乘计算，以在最终矩阵中获得新行（new row）。</em></p><p><strong>性能</strong></p><p>研究者首先对 Performer 的空间和时间复杂度进行基准测试，结果表明，注意力的加速比和内存减少在实证的角度上近乎最优，也就是说，这非常接近在模型中根本不使用注意力机制的情况。</p><p><img src="E:\myBlog\source_posts\v2-e01a97ee8d354814ddf45830642aa026_720w.jpg" alt="img"></p><p><em>在以时间（T）和长度（L）为度量的双对数坐标轴中，常规 Transformer 模型的双向 timing。</em></p><p>研究者进一步证明，使用无偏 softmax 近似，该 Performer 模型在稍微进行微调之后可以向后兼容预训练 Transformer 模型，从而在提升推理速度的同时降低能耗，并且不需要从头训练预先存在的模型。</p><p><img src="E:\myBlog\source_posts\v2-d2a50f3dd7c0edebc63b98ed17329fde_720w.jpg" alt="img"></p><p><em>在 One Billion Word Benchmark (LM1B) 数据集上，研究者将原始预训练 Transformer 的权重迁移至 Performer 模型，使得初始非零准确度为 0.07（橙色虚线）。但在微调之后，Performer 的准确度在很少的梯度步数之后迅速恢复。</em></p><p><strong>应用示例：蛋白质建模</strong></p><p>蛋白质具有复杂的 3D 结构，是生命必不可少的拥有特定功能的大分子。和单词一样，蛋白质可以被看做线性序列，每个字符代表一种氨基酸。将 Transformers 应用于大型未标记的蛋白质序列语料库，生成的模型可用于精确预测折叠功能大分子。正如该研究理论结果所预测的那样，Performer-ReLU 在蛋白质序列数据建模方面表现良好，而 Performer-Softmax 与 Transformer 性能相媲美。</p><p><img src="E:\myBlog\source_posts\v2-4503e75224c3c3d47e6af8c4452b07e2_720w.jpg" alt="img"></p><p><em>Performer 在蛋白质序列建模时的性能。</em></p><p>下面可视化一个蛋白质 Performer 模型，该模型使用基于 ReLU 的近似注意力机制进行训练。研究者发现，Performer 的密集注意力近似有可能捕捉到跨多个蛋白质序列的全局相互作用。作为概念的证明，研究者在串联蛋白长序列上训练模型，这使得常规的 Transformer 模型内存过载。但由于具有良好的空间利用效率，Performer 不会出现这一问题。</p><p><img src="E:\myBlog\source_posts\v2-88300c475bac1e929f60c17cfb66b99b_720w.jpg" alt="img"></p><p><em>左：从注意力权重估计氨基酸相似性矩阵。该模型可以识别高度相似的氨基酸对，例如 (D,E) 和 (F,Y)。</em></p><p><img src="E:\myBlog\source_posts\v2-8c549a83a85d004d34937b39755de8db_720w.jpg" alt="img"></p><p><em>Performer 和 Transformer 在长度为 8192 的蛋白质序列上的性能。</em></p><p>随着 Transformer 的频繁跨界，越来越多的研究者开始关注其内存占用和计算效率的问题，比如机器之心前段时间介绍的《<a href="https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3MzI4MjgzMw%3D%3D%26mid%3D2650800087%26idx%3D2%26sn%3Dc0c347ae188499fffa0db62c75a0ba2a%26scene%3D21%23wechat_redirect">抛弃注意力，比 EfficientNet 快 3.5 倍，类 Transformer 新模型跨界视觉任务实现新 SOTA</a>》。在那篇文章中，研究者提出了一种名为「lambda」的层，这些层提供了一种捕获输入和一组结构化上下文元素之间长程交互的通用框架。类似的改进还在不断涌现，我们也将持续关注。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      记录值得分享的论文;
《ConvBERT》: 对BERT中attention结构的改进

    
    </summary>
    
    
      <category term="论文分享" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-11-19-论文分享</title>
    <link href="http://yoursite.com/2020/11/19/2020-11-19-%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"/>
    <id>http://yoursite.com/2020/11/19/2020-11-19-%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/</id>
    <published>2020-11-19T08:43:49.000Z</published>
    <updated>2020-12-08T11:27:38.228Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2020/11/19/9cQSCTG7V4IPsej.png"></p><blockquote><p>ICLR 2021 (under review)</p><p>code url (official torch) : <a href="https://github.com/lucidrains/lambda-networks" target="_blank" rel="noopener" class="uri">https://github.com/lucidrains/lambda-networks</a></p><p>code url (unofficial torch):<a href="https://github.com/leaderj1001/LambdaNetworks" target="_blank" rel="noopener" class="uri">https://github.com/leaderj1001/LambdaNetworks</a></p></blockquote><h3 id="背景">背景</h3><p>transformer的注意力机制在建模长依赖关系时有优势，但是同时也存在一定的缺点，就是需要去建立expensive attention maps (Q*K ) , 复杂度是O(n2)，而且很消耗内存，这样就限制了transformer来处理更长的序列或者是多维的输入，比如图像。</p><h3 id="问题">问题</h3><p>怎样去解决attention带来的资源的消耗以及无法处理多维度的结构化的上下文信息（图像）</p><h3 id="解决">解决</h3><p>提出了 lambda layer，通过将上下文转化为线性函数（lambda）来捕获关联性，输入不同这些线性函数也是不同的，以此来避免了attention maps，这样可以应用到长序列或者高分辨率的图象上。</p><p>由多个lambda layer组成的神经网络叫做Lambda Networks ，并且认为<strong>lambda layer可以有效替代attention机制</strong></p><p>优点： 计算简单且高效（small memory cost），且在流行数据集上表现不错</p><p><img src="https://i.loli.net/2020/11/19/PZfBu3tOpUmvQn7.png"></p><h4 id="lambda-layer-的大体介绍">lambda layer 的大体介绍</h4><p><img src="https://i.loli.net/2020/11/19/CNXsc9DRwtBaJj5.png"></p><h3 id="模型">模型</h3><h4 id="模型架构">模型架构</h4><p><img src="https://i.loli.net/2020/11/19/NDaA9MWiZ2rjsBJ.png"></p><p>图中每个pixel（query）都可以类比于NLP中的一个token，要attention to local context （pixel 所在的框)，如中间图所示，如果是attention机制，那么attent to每一个pixel，就会形成很大的attention map，而这只是一个pixel所形成的attention map ，当框在图像上滑动，计算每一个pixel的时候，内存消耗是巨大的。</p><p>图右采取的是 lambda函数的思想，也就是对于每一个pixel（query），都会计算一个线性的lambda函数，同时也是个矩阵，再和query相乘，就得到了此query对应的输出向量y。由于是线性的，所以消耗资源很少，可以处理高分辨率的图像</p><p>lambda layer 可以捕获全局的或者局部的关系</p><p><img src="https://i.loli.net/2020/11/19/EAaSbCwmdPeYO3y.png"></p><p>lambda network中对于各种参数的定义，基本和transformer一致</p><p><img src="https://i.loli.net/2020/11/19/jn9coamK7Ns46Qy.png"></p><blockquote><p>注：</p><p>C和X一般是一样的东西</p><p>n : local context pixels （ n = 225*225 ）</p></blockquote><p><img src="https://i.loli.net/2020/11/19/pD6N5er9HnLdOI3.png"></p><blockquote><p>lambda layer 主要数据流向：</p><p>input: X,C -&gt; 生成线性函数，应用到对应的query中</p><p>output: Y</p></blockquote><p>具体流程如下：</p><p>传统的self-attention</p><p><img src="https://i.loli.net/2020/11/19/NzglXkjLWhxB745.png"></p><p>lambda network attention</p><p><img src="https://i.loli.net/2020/11/19/s3GpwT5IAKYohzc.png"></p><p>不太懂key和自己做attention有什么意义呢？</p><p>lambda layer 中涉及的公式</p><p><img src="https://i.loli.net/2020/11/19/sCkquJxQLDgX9wd.png"></p><p><img src="https://i.loli.net/2020/11/19/t8HSnIKipmzWuLs.png" alt="image-20201119175615608"></p><h3 id="实验">实验</h3><p><img src="https://i.loli.net/2020/11/19/LwR5yKYzBUrHqAW.png"></p><h3 id="代码">代码</h3><p>代码实际上实现的很简单，在官方给出的代码中只是写了lambda layer的实现，于是我又在paper with code上了另一个torch实现的模型代码（数据集用的是CIFAR10），实现的是Lambda ResNets。就是在ResNets的框架下，在每一层使用了lambda layer进行计算</p><p>代码中应用的<code>einsum函数</code>很简洁方便并且还高效，可以表示点积、外积、转置、矩阵-向量乘法、矩阵-矩阵乘法等一些常见的矩阵运算，推荐！</p><h3 id="总结">总结</h3><p>感觉论文写的很不容易阅读，也算看过不少transformer的论文了，但是没有见过这篇论文这样的拗口的表达方式。明明很简单的概念，按照transformer类似表达就可以了，非要整出来一套奇怪的符号。作者是一个匿名作者，但是我估计可能是别的专业转行过来深度学习的</p><p>总体来说，这篇论文的写作和表达方式上对于后来的研究者来说很不友好！不过既然模型效果不错，那么在用attention处理图像问题的时候，可以考虑借鉴本文的思路</p><h3 id="pdf做过笔记的论文pdf">PDF（做过笔记的论文pdf）</h3><div class="pdfobject-container" data-target="/papers/LAMBDANETWORKS.pdf" data-height="500px"></div><p><a href="/papers/LAMBDANETWORKS.pdf">点击下载文件</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      记录值得分享的论文;
《Lambda network》: 提出了 lambda layer，通过将上下文转化为线性函数（lambda）来捕获关联性，以此避免attention maps，这样可以应用到长序列或者高分辨率的图象上。

    
    </summary>
    
    
      <category term="论文分享" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-11-16-使用预训练模型</title>
    <link href="http://yoursite.com/2020/11/16/2020-11-16-%E4%BD%BF%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"/>
    <id>http://yoursite.com/2020/11/16/2020-11-16-%E4%BD%BF%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/</id>
    <published>2020-11-16T11:02:19.000Z</published>
    <updated>2020-11-19T08:42:42.803Z</updated>
    
    <content type="html"><![CDATA[<h4 id="预训练模型">预训练模型</h4><blockquote><p><strong>Hugging face 简介</strong></p></blockquote><p>Hugging face 是一家总部位于纽约的聊天机器人初创服务商，开发的应用在青少年中颇受欢迎，相比于其他公司，Hugging Face更加注重产品带来的情感以及环境因素。官网链接在此 <a href="https://link.zhihu.com/?target=https%3A//huggingface.co/">https://huggingface.co/</a> 。</p><p>但更令它广为人知的是Hugging Face专注于NLP技术，拥有大型的开源社区。尤其是在github上开源的自然语言处理，预训练模型库 Transformers，已被下载超过一百万次，github上超过<strong>24000</strong>个star。Transformers 提供了NLP领域大量state-of-art的 预训练语言模型结构的模型和调用框架。以下是repo的链接（<a href="https://link.zhihu.com/?target=https%3A//github.com/huggingface/transformers">https://github.com/huggingface/transformers</a>）</p><p>这个库最初的名称是<strong>pytorch-pretrained-bert</strong>，它随着BERT一起应运而生。Google2018年10月底在 <a href="https://link.zhihu.com/?target=https%3A//github.com/google-research/bert">https://github.com/google-research/bert</a> 开源了BERT的tensorflow实现。当时，BERT以其强劲的性能，引起NLPer的广泛关注。几乎与此同时，pytorch-pretrained-bert也开始了它的第一次提交。pytorch-pretrained-bert 用当时已有大量支持者的pytorch框架复现了BERT的性能，并提供预训练模型的下载，使没有足够算力的开发者们也能够在几分钟内就实现 state-of-art-fine-tuning。</p><p><img src="E:\myBlog\source_posts\v2-43d31689c936d9f721eed2a2ccd51d7a_720w.png" alt="img"></p><p>因为pytorch框架的友好，BERT的强大，以及pytorch-pretrained-bert的简单易用，使这个repo也是受到大家的喜爱，不到10天就突破了1000个star。在2018年11月17日，repo就实现了BERT的基本功能，发布了版本0.1.2。接下来他们也没闲着，又开始将GPT等模型也往repo上搬。在2019年2月11日release的 0.5.0版本中，已经添加上了OpenAI GPT模型，以及Google的TransformerXL。</p><p>直到2019年7月16日，在repo上已经有了包括BERT，GPT，GPT-2，Transformer-XL，XLNET，XLM在内六个预训练语言模型，这时候名字再叫pytorch-pretrained-bert就不合适了，于是改成了pytorch-transformers，势力范围扩大了不少。这还没完！2019年6月Tensorflow2的beta版发布，Huggingface也闻风而动。为了立于不败之地，又实现了TensorFlow 2.0和PyTorch模型之间的深层互操作性，可以在TF2.0/PyTorch框架之间随意迁移模型。在2019年9月也发布了2.0.0版本，同时正式更名为 transformers 。到目前为止，transformers 提供了超过100种语言的，32种预训练语言模型，简单，强大，高性能，是新手入门的不二选择。</p><blockquote><p><strong>Transfromers中BERT简单运用</strong></p></blockquote><p>前几期里，一直在分享论文的阅读心得，虽然不是第一次看，但不知道大家是不是和我一样又有所收获。本期我们一起来看看如何使用Transformers包实现简单的BERT模型调用。</p><p>安装过程不再赘述，比如安装2.2.0版本 pip install transformers==2.2.0 即可，让我们看看如何调用BERT。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertModel, BertTokenizer</span><br><span class="line"><span class="comment"># 这里我们调用bert-base模型，同时模型的词典经过小写处理</span></span><br><span class="line">model_name = <span class="string">'bert-base-uncased'</span></span><br><span class="line"><span class="comment"># 读取模型对应的tokenizer</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(model_name)</span><br><span class="line"><span class="comment"># 载入模型</span></span><br><span class="line">model = BertModel.from_pretrained(model_name)</span><br><span class="line"><span class="comment"># 输入文本</span></span><br><span class="line">input_text = <span class="string">"Here is some text to encode"</span></span><br><span class="line"><span class="comment"># 通过tokenizer把文本变成 token_id</span></span><br><span class="line">input_ids = tokenizer.encode(input_text, add_special_tokens=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># input_ids: [101, 2182, 2003, 2070, 3793, 2000, 4372, 16044, 102]</span></span><br><span class="line">input_ids = torch.tensor([input_ids])</span><br><span class="line"><span class="comment"># 获得BERT模型最后一个隐层结果</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    last_hidden_states = model(input_ids)[<span class="number">0</span>]  <span class="comment"># Models outputs are now tuples</span></span><br><span class="line"></span><br><span class="line"><span class="string">""" tensor([[[-0.0549,  0.1053, -0.1065,  ..., -0.3550,  0.0686,  0.6506],</span></span><br><span class="line"><span class="string">         [-0.5759, -0.3650, -0.1383,  ..., -0.6782,  0.2092, -0.1639],</span></span><br><span class="line"><span class="string">         [-0.1641, -0.5597,  0.0150,  ..., -0.1603, -0.1346,  0.6216],</span></span><br><span class="line"><span class="string">         ...,</span></span><br><span class="line"><span class="string">         [ 0.2448,  0.1254,  0.1587,  ..., -0.2749, -0.1163,  0.8809],</span></span><br><span class="line"><span class="string">         [ 0.0481,  0.4950, -0.2827,  ..., -0.6097, -0.1212,  0.2527],</span></span><br><span class="line"><span class="string">         [ 0.9046,  0.2137, -0.5897,  ...,  0.3040, -0.6172, -0.1950]]]) </span></span><br><span class="line"><span class="string">shape: (1, 9, 768)     </span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></tbody></table></figure><p>可以看到，包括import在内的不到十行代码，我们就实现了读取一个预训练过的BERT模型，来encode我们指定的一个文本，对文本的每一个token生成768维的向量。如果是二分类任务，我们接下来就可以把第一个token也就是[CLS]的768维向量，接一个linear层，预测出分类的logits，或者根据标签进行训练。</p><p>如果你想在一些NLP常用数据集上复现BERT的效果，Transformers上也有现成的代码和方法，只要把数据配置好，运行命令即可，而且finetune的任务可以根据你的需要切换，非常方便。</p><p><img src="E:\myBlog\source_posts\v2-e26fa3a9005015c5cd6fa9ddedc6b2bd_720w.jpg" alt="img"></p><blockquote><p><strong>BERT configuration</strong></p></blockquote><p>接下来，我们进一步看下Transformers的源码，我们首先进入代码的路径src/transformers 下，其中有很多的python代码文件。</p><p><img src="E:\myBlog\source_posts\v2-3f94e3668496b5fe2275407e5cbbd440_720w.jpg" alt="img"></p><p>以 <strong>configuration</strong> 开头的都是各个模型的配置代码，比如 configuration_bert.py。在这个文件里我们能够看到，主要是一个继承自 PretrainedConfig 的类 BertConfig的定义，以及不同BERT模型的config文件的下载路径，下方显示前三个。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">BERT_PRETRAINED_CONFIG_ARCHIVE_MAP = {</span><br><span class="line">    <span class="string">"bert-base-uncased"</span>: <span class="string">"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json"</span>,</span><br><span class="line">    <span class="string">"bert-large-uncased"</span>: <span class="string">"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-config.json"</span>,</span><br><span class="line">    <span class="string">"bert-base-cased"</span>: <span class="string">"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json"</span>,</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>我们打开第一个的链接，就能下载到bert-base-uncased的模型的配置，其中包括dropout, hidden_size, num_hidden_layers, vocab_size 等等。比如bert-base-uncased的配置它是12层的，词典大小30522等等，甚至可以在config里利用output_hidden_states配置是否输出所有hidden_state。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">{</span><br><span class="line">  <span class="string">"architectures"</span>: [</span><br><span class="line">    <span class="string">"BertForMaskedLM"</span></span><br><span class="line">  ],</span><br><span class="line">  <span class="string">"attention_probs_dropout_prob"</span>: <span class="number">0.1</span>,</span><br><span class="line">  <span class="string">"hidden_act"</span>: <span class="string">"gelu"</span>,</span><br><span class="line">  <span class="string">"hidden_dropout_prob"</span>: <span class="number">0.1</span>,</span><br><span class="line">  <span class="string">"hidden_size"</span>: <span class="number">768</span>,</span><br><span class="line">  <span class="string">"initializer_range"</span>: <span class="number">0.02</span>,</span><br><span class="line">  <span class="string">"intermediate_size"</span>: <span class="number">3072</span>,</span><br><span class="line">  <span class="string">"max_position_embeddings"</span>: <span class="number">512</span>,</span><br><span class="line">  <span class="string">"num_attention_heads"</span>: <span class="number">12</span>,</span><br><span class="line">  <span class="string">"num_hidden_layers"</span>: <span class="number">12</span>,</span><br><span class="line">  <span class="string">"type_vocab_size"</span>: <span class="number">2</span>,</span><br><span class="line">  <span class="string">"vocab_size"</span>: <span class="number">30522</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><blockquote><p><strong>BERT tokenization</strong></p></blockquote><p>以<strong>tokenization</strong>开头的都是跟vocab有关的代码，比如在 tokenization_bert.py 中有函数如whitespace_tokenize，还有不同的tokenizer的类。同时也有各个模型对应的vocab.txt。从第一个链接进去就是bert-base-uncased的词典，这里面有30522个词，对应着config里面的vocab_size。</p><p>其中，第0个token是[pad]，第101个token是[CLS]，第102个token是[SEP]，所以之前我们encode得到的 [101, 2182, 2003, 2070, 3793, 2000, 4372, 16044, 102] ，其实tokenize后convert前的token就是 ['[CLS]', 'here', 'is', 'some', 'text', 'to', 'en', '##code', '[SEP]']，经过之前BERT论文的介绍，大家应该都比较熟悉了。</p><blockquote><p>其中值得一提的是，BERT的vocab预留了不少unused token，如果我们会在文本中使用特殊字符，在vocab中没有，这时候就可以通过替换vacab中的unused token，实现对新的token的embedding进行训练。</p></blockquote><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">PRETRAINED_VOCAB_FILES_MAP = {</span><br><span class="line">    <span class="string">"vocab_file"</span>: {</span><br><span class="line">        <span class="string">"bert-base-uncased"</span>: <span class="string">"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt"</span>,</span><br><span class="line">        <span class="string">"bert-large-uncased"</span>: <span class="string">"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt"</span>,</span><br><span class="line">        <span class="string">"bert-base-cased"</span>: <span class="string">"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt"</span>,</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><blockquote><p><strong>BERT modeling</strong></p></blockquote><p>以modeling开头的就是我们最关心的模型代码，比如 modeling_bert.py。同样的，文件中有<strong>许多不同的预训练模型</strong>以供下载，我们可以按需获取。</p><p>代码中我们可以重点关注BertModel类，它就是BERT模型的基本代码。我们可以看到它的类定义中，由embedding，encoder，pooler组成，forward时顺序经过三个模块，输出output。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertModel</span><span class="params">(BertPreTrainedModel)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        super().__init__(config)</span><br><span class="line">        self.config = config</span><br><span class="line"></span><br><span class="line">        self.embeddings = BertEmbeddings(config)</span><br><span class="line">        self.encoder = BertEncoder(config)</span><br><span class="line">        self.pooler = BertPooler(config)</span><br><span class="line"></span><br><span class="line">        self.init_weights()</span><br><span class="line">        </span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        self, input_ids=None, attention_mask=None, token_type_ids=None,</span></span></span><br><span class="line"><span class="function"><span class="params">        position_ids=None, head_mask=None, inputs_embeds=None,</span></span></span><br><span class="line"><span class="function"><span class="params">        encoder_hidden_states=None, encoder_attention_mask=None,</span></span></span><br><span class="line"><span class="function"><span class="params">    )</span>:</span></span><br><span class="line">    <span class="string">""" 省略部分代码 """</span></span><br><span class="line">    </span><br><span class="line">        embedding_output = self.embeddings(</span><br><span class="line">            input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds</span><br><span class="line">        )</span><br><span class="line">        encoder_outputs = self.encoder(</span><br><span class="line">            embedding_output,</span><br><span class="line">            attention_mask=extended_attention_mask,</span><br><span class="line">            head_mask=head_mask,</span><br><span class="line">            encoder_hidden_states=encoder_hidden_states,</span><br><span class="line">            encoder_attention_mask=encoder_extended_attention_mask,</span><br><span class="line">        )</span><br><span class="line">        sequence_output = encoder_outputs[<span class="number">0</span>]</span><br><span class="line">        pooled_output = self.pooler(sequence_output)</span><br><span class="line"></span><br><span class="line">        outputs = (sequence_output, pooled_output,) + encoder_outputs[</span><br><span class="line">            <span class="number">1</span>:</span><br><span class="line">        ]  <span class="comment"># add hidden_states and attentions if they are here</span></span><br><span class="line">        <span class="keyword">return</span> outputs  <span class="comment"># sequence_output, pooled_output, (hidden_states), (attentions)</span></span><br></pre></td></tr></tbody></table></figure><p>BertEmbeddings这个类中可以清楚的看到，embedding由三种embedding相加得到，经过layernorm 和 dropout后输出。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=<span class="number">0</span>)</span><br><span class="line">        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)</span><br><span class="line">        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)</span><br><span class="line">        <span class="comment"># self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load</span></span><br><span class="line">        <span class="comment"># any TensorFlow checkpoint file</span></span><br><span class="line">        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)</span><br><span class="line">        self.dropout = nn.Dropout(config.hidden_dropout_prob)</span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None)</span>:</span></span><br><span class="line">        <span class="string">""" 省略 embedding生成过程 """</span></span><br><span class="line">          </span><br><span class="line">        embeddings = inputs_embeds + position_embeddings + token_type_embeddings</span><br><span class="line">        embeddings = self.LayerNorm(embeddings)</span><br><span class="line">        embeddings = self.dropout(embeddings)</span><br><span class="line">        <span class="keyword">return</span> embeddings</span><br></pre></td></tr></tbody></table></figure><p>BertEncoder主要将embedding的输出，逐个经过每一层Bertlayer的处理，得到各层hidden_state，再根据config的参数，来决定最后是否所有的hidden_state都要输出，BertLayer的内容展开的话，篇幅过长，读者感兴趣可以自己一探究竟。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertEncoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.output_attentions = config.output_attentions</span><br><span class="line">        self.output_hidden_states = config.output_hidden_states</span><br><span class="line">        self.layer = nn.ModuleList([BertLayer(config) <span class="keyword">for</span> _ <span class="keyword">in</span> range(config.num_hidden_layers)])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        self,</span></span></span><br><span class="line"><span class="function"><span class="params">        hidden_states,</span></span></span><br><span class="line"><span class="function"><span class="params">        attention_mask=None,</span></span></span><br><span class="line"><span class="function"><span class="params">        head_mask=None,</span></span></span><br><span class="line"><span class="function"><span class="params">        encoder_hidden_states=None,</span></span></span><br><span class="line"><span class="function"><span class="params">        encoder_attention_mask=None,</span></span></span><br><span class="line"><span class="function"><span class="params">    )</span>:</span></span><br><span class="line">        all_hidden_states = ()</span><br><span class="line">        all_attentions = ()</span><br><span class="line">        <span class="keyword">for</span> i, layer_module <span class="keyword">in</span> enumerate(self.layer):</span><br><span class="line">            <span class="keyword">if</span> self.output_hidden_states:</span><br><span class="line">                all_hidden_states = all_hidden_states + (hidden_states,)</span><br><span class="line"></span><br><span class="line">            layer_outputs = layer_module(</span><br><span class="line">                hidden_states, attention_mask, head_mask[i], encoder_hidden_states, encoder_attention_mask</span><br><span class="line">            )</span><br><span class="line">            hidden_states = layer_outputs[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> self.output_attentions:</span><br><span class="line">                all_attentions = all_attentions + (layer_outputs[<span class="number">1</span>],)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Add last layer</span></span><br><span class="line">        <span class="keyword">if</span> self.output_hidden_states:</span><br><span class="line">            all_hidden_states = all_hidden_states + (hidden_states,)</span><br><span class="line"></span><br><span class="line">        outputs = (hidden_states,)</span><br><span class="line">        <span class="keyword">if</span> self.output_hidden_states:</span><br><span class="line">            outputs = outputs + (all_hidden_states,)</span><br><span class="line">        <span class="keyword">if</span> self.output_attentions:</span><br><span class="line">            outputs = outputs + (all_attentions,)</span><br><span class="line">        <span class="keyword">return</span> outputs  <span class="comment"># last-layer hidden state, (all hidden states), (all attentions)</span></span><br></pre></td></tr></tbody></table></figure><p>Bertpooler 其实就是将BERT的[CLS]的hidden_state 取出，经过一层DNN和Tanh计算后输出。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertPooler</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.dense = nn.Linear(config.hidden_size, config.hidden_size)</span><br><span class="line">        self.activation = nn.Tanh()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, hidden_states)</span>:</span></span><br><span class="line">        <span class="comment"># We "pool" the model by simply taking the hidden state corresponding</span></span><br><span class="line">        <span class="comment"># to the first token.</span></span><br><span class="line">        first_token_tensor = hidden_states[:, <span class="number">0</span>]</span><br><span class="line">        pooled_output = self.dense(first_token_tensor)</span><br><span class="line">        pooled_output = self.activation(pooled_output)</span><br><span class="line">        <span class="keyword">return</span> pooled_output</span><br></pre></td></tr></tbody></table></figure><p>在这个文件中还有上述基础的BertModel的进一步的变化，比如BertForMaskedLM，BertForNextSentencePrediction这些是Bert加了预训练头的模型，还有BertForSequenceClassification， BertForQuestionAnswering 这些加上了特定任务头的模型。</p><ol type="1"><li>"Here is some text to encode"加上CLS和SEP也才八个,因为Bert的tokenizer使用了wordpiece 算法，</li></ol><p>这句话在tokenize了以后是下面这样的['here', 'is', 'some', 'text', 'to', 'en', '##code'],加上[CLS]和[SEP]就变成了9个token。</p><ol start="2" type="1"><li><p>with torch.no_grad(): last_hidden_states = model(input_ids)[0]</p><p>这里的[0]， 原因是模型输出是 seq_output, pooled_output，这里取的是seq_output。</p><p>sequence_output = encoder_outputs[0]</p><p>这边的[0]是因为 取得是模型最后一层的state（因为在元组里，最后一层的state放在最上面了）</p><p>first_token_tensor = hidden_states[:, 0]</p><p>pooler里面这边的0是因为 想要取的是 整个序列第一个token也就是[CLS] token的state</p></li></ol><h4 id="基本原理">基本原理</h4><p>使用的基本原理也非常简单，from_pretrained的参数pretrained_model_name_or_path，可以接受的参数有几种，short-cut name（缩写名称，类似于gpt2这种）、identifier name（类似于microsoft/DialoGPT-small这种）、文件夹、文件。</p><p><strong>对于short-cut name或identifier name，这种情况下，本地有文件，可以使用本地的，本地没有文件，则下载。</strong>(上面的例子就是没有下载文件，直接用的是short-cut name)。一些常用的short-cut name，可以从这个链接查看：<a href="https://link.zhihu.com/?target=https%3A//huggingface.co/transformers/pretrained_models.html">https://huggingface.co/transformers/pretrained_models.html</a>。</p><p>对于文件夹，则会从文件夹中找vocab.json、pytorch_model.bin、tf_model.h5、merges.txt、special_tokens_map.json、added_tokens.json、tokenizer_config.json、sentencepiece.bpe.model等进行加载。所以这也是为什么下载的时候，一定要保证这些名称是这几个，不能变。</p><p>对于文件，则会直接加载文件。</p><p>官方给的样例，通常都是short-cut name，这里操作就是替换成下载好文件的文件夹。至此，我们完成了模型、词典等各种文件的本地加载。</p><p>具体操作如下：</p><blockquote><p>模型库</p></blockquote><p>官网的模型库的地址如下：<a href="https://link.zhihu.com/?target=https%3A//huggingface.co/models">https://huggingface.co/models</a></p><p><img src="E:\myBlog\source_posts\v2-b06d68ff284848b750e38eb5a450e661_720w.jpg" alt="img"></p><blockquote><p>使用模型</p></blockquote><p>首先需要安装<code>transformers</code>库，使用以下命令安装：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install transformers</span><br></pre></td></tr></tbody></table></figure><p>接下来在代码中调用<code>AutoTokenizer.from_pretrained</code>和<code>AutoModel.from_pretrained</code>即可例如：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> *</span><br><span class="line">model_name = <span class="string">'hfl/chinese-xlnet-base'</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_name)</span><br><span class="line">model = AutoModel.from_pretrained(model_name)</span><br></pre></td></tr></tbody></table></figure><p>运行后系统会自动下载相关的模型文件并存放在电脑中：</p><p><img src="E:\myBlog\source_posts\v2-550ec0db9dbec7dd23afd33c925c019f_720w.jpg" alt="img"></p><p>使用Windows模型保存的路径在<code>C:\Users\[用户名]\.cache\torch\transformers\</code>目录下，根据模型的不同下载的东西也不相同</p><p>使用Linux模型保存的路径在<code>~/.cache/torch/transformers/</code>目录下</p><p><img src="E:\myBlog\source_posts\v2-a9751b1feebbfabc5615ce594dad816f_720w.jpg" alt="img"></p><blockquote><p>存在的问题</p></blockquote><blockquote><p><code>这些前提是你的电脑有网络可以直接使用代码下载相应的模型文件，但是问题是有些机器是没有外网连接权限或者下载速度非常慢。</code></p><p><code>这时候就需要把模型文件下载后在导入代码中，还是以刚才的hfl/chinese-xlnet-base模型为例，直接在官网搜索模型，点击进入模型的详情界面</code></p></blockquote><p><img src="E:\myBlog\source_posts\v2-6c04926b05ec59131e7bc3e018d1255a_720w.jpg" alt="img"></p><p>在界面中找到<code>List all files in model</code></p><p><img src="E:\myBlog\source_posts\v2-46b282a4b5ff9a9fe445e95d403f2c07_720w.jpg" alt="img"></p><p>把弹窗内的文件全部下载下来</p><p><img src="E:\myBlog\source_posts\v2-09353001fd2ab6a1ffbb210f675381dc_720w.jpg" alt="img"></p><p>我们假设文件保存在<code>E:\models\hfl\chinese-xlnet-base\</code>目录下</p><p><img src="E:\myBlog\source_posts\v2-b6a23972b45d1b9cc88701a55bcc159a_720w.jpg" alt="img"></p><p>我们只需要把<code>model_name</code>修改为下载的文件夹即可</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> *</span><br><span class="line">model_name = <span class="string">'E:/models/hfl/chinese-xlnet-base/'</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_name)</span><br><span class="line">model = AutoModel.from_pretrained(model_name)</span><br></pre></td></tr></tbody></table></figure><h3 id="参考">参考</h3><blockquote><p>使用transformers预训练模型</p><p><span class="github-emoji" style="display:inline;vertical-align:middle;color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8">⭐</span> <a href="https://zhuanlan.zhihu.com/p/120315111" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/120315111</a></p><p><span class="github-emoji" style="display:inline;vertical-align:middle;color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8">⭐</span><a href="https://zhuanlan.zhihu.com/p/147144376" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/147144376</a></p><p><a href="https://zhuanlan.zhihu.com/p/274509234" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/274509234</a></p><p>关于hugging face/transformers的介绍</p><p><a href="https://zhuanlan.zhihu.com/p/141527015" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/141527015</a></p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      在NLP领域越来越依赖预训练模型，那么如何高效使用已经预训练好的模型到自己的模型中呢？本文参考其它博客，做个总结
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-11-12-从attention到BERT-family</title>
    <link href="http://yoursite.com/2020/11/12/2020-11-12-%E4%BB%8Eattention%E5%88%B0BERT-family/"/>
    <id>http://yoursite.com/2020/11/12/2020-11-12-%E4%BB%8Eattention%E5%88%B0BERT-family/</id>
    <published>2020-11-12T04:54:33.000Z</published>
    <updated>2020-11-14T12:29:50.995Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言">前言</h3><p>做NLP领域的应该没人不知道BERT，之前开组会的时候讲到过BERT的一篇改进论文，导师建议对transformer、BERT以及预训练模型发展做个总结综述，作为下次汇报的内容。于是这几天在网上参考了一些博客，还有大佬邱锡鹏老师和刘群老师关于预训练模型的报告，以及结合自己读的一些相关论文，对整个模型的发展演变梳理了一下，汇总如下</p><h3 id="attention以及self-attention">attention以及self-attention</h3><h4 id="提出">提出</h4><p>借鉴了人类的注意力机制</p><p><img src="E:\myBlog\source_posts\516P2JBYrgOToyU.jpg" alt="人类的视觉注意力"></p><p>人类视觉通过快速扫描全局图像，获得注意力焦点，而后对这一区域投入更多注意力资源，以获取更多所需要关注目标的细节信息，而抑制其他无用信息。这是人类利用有限的注意力资源从大量信息中快速筛选出高价值信息的手段</p><p>深度学习中的注意力机制从本质上讲和人类的选择性视觉注意力机制类似，核心目标也是从众多信息中选择出对当前任务目标更关键的信息。</p><h4 id="encoder-decoder框架">Encoder-Decoder框架</h4><p>一般的seq2seq模型</p><p><img src="E:\myBlog\source_posts\ohBVdz1KDn4ZcOb.png" alt="抽象的文本处理领域的Encoder-Decoder框架"></p><p>对于句子对&lt;Source,Target&gt;，我们的目标是给定输入句子Source，期待通过Encoder-Decoder框架来生成目标句子Target。Source和Target分别由各自的单词序列构成：</p><p><img src="E:\myBlog\source_posts\qrCwtas6iRVKYbA.png" alt="img"></p><p>Encoder对输入句子Source进行编码，将输入句子通过非线性变换转化为中间语义表示C：</p><p><img src="E:\myBlog\source_posts\M7EXx2KPgeHFQhL.png" alt="img"></p><p>对于解码器Decoder来说，其任务是根据句子Source的中间语义表示C和之前已经生成的历史信息 y1,y2,y3...yi-1来生成i时刻要生成的单词yi：</p><p><img src="E:\myBlog\source_posts\6uHkShXDNKOlBQ3.png" alt="img"></p><p>每个yi都依次这么产生，那么看起来就是整个系统根据输入句子Source生成了目标句子Target。机器翻译、文本摘要、问答系统都是encoder-decoder框架。</p><h4 id="问题">问题</h4><p><img src="E:\myBlog\source_posts\KNWS6Ax1uI285cH.png" alt="img"></p><p>在生成目标句子的单词时，不论生成哪个单词，它们使用的输入句子Source的语义编码C都是一样的，没有任何区别。</p><p>而语义编码C是由句子Source的每个单词经过Encoder编码产生的，这意味着不论是生成哪个单词，y1,y2还是y3，其实句子Source中任意单词对生成某个目标单词yi来说影响力都是相同的，这是为何说这个模型没有体现出注意力的缘由。这类似于人类看到眼前的画面，但是眼中却没有注意焦点</p><p>没有引入注意力的模型在输入句子比较短的时候问题不大，但是如果输入句子比较长，此时所有语义完全通过一个中间语义向量来表示，单词自身的信息已经消失，可想而知会丢失很多细节信息，这也是为何要引入注意力模型的重要原因。</p><h4 id="attention">attention</h4><p>目标句子中的每个单词都应该学会其对应的源语句子中单词的注意力分配概率信息。这意味着在生成每个单词yi的时候，原先都是相同的中间语义表示C会被替换成根据当前生成单词而不断变化的Ci（C是动态的，根据生成次词的不同，C也是不同的）。</p><p><strong>即由固定的中间语义表示C换成了根据当前输出单词来调整成加入注意力模型的变化的Ci</strong>。</p><p><img src="E:\myBlog\source_posts\oe6E7C9sUHrVuNZ.png" alt="引入注意力模型的Encoder-Decoder框架"></p><p>即生成目标句子单词的过程成了下面的形式：</p><p><img src="E:\myBlog\source_posts\q8ZFEu4BSI1lo9z.png" alt="img"></p><p>而每个Ci可能对应着不同的源语句子单词的注意力分配概率分布</p><p><img src="E:\myBlog\source_posts\TBg8KZhoyiOlUst.png" alt="img"></p><p>实际中，Tom、chase、jerry都是被编码成512维的向量，所以权重相加之后Ctom应该也是一个向量</p><p>其中，f2函数代表Encoder对输入英文单词的某种变换函数，比如如果Encoder是用的RNN模型的话，这个f2函数的结果往往是某个时刻输入xi后隐层节点的状态值；g代表Encoder根据单词的中间表示合成整个句子中间语义表示的变换函数，一般的做法中，g函数就是对构成元素加权求和，即下列公式：</p><p><img src="E:\myBlog\source_posts\W9SnjkNw3BVasdc.png" alt="img"></p><p>其中，Lx代表输入句子Source的长度，aij代表在Target输出第i个单词时Source输入句子中第j个单词的注意力分配系数，而hj则是Source输入句子中第j个单词的语义编码。假设下标i就是上面例子所说的“ 汤姆” ，那么Lx就是3，h1=f(“Tom”)，h2=f(“Chase”),h3=f(“Jerry”)分别是输入句子每个单词的语义编码，对应的注意力模型权值则分别是0.6,0.2,0.2，所以g函数本质上就是个加权求和函数。如果形象表示的话，翻译中文单词“汤姆”的时候，数学公式对应的中间语义表示Ci的形成过程类似图4。</p><p><img src="E:\myBlog\source_posts\LRJZMaUbN9VuxGr.png" alt="Attention的形成过程"></p><p>如何得到单词注意力分配概率Ｃ呢？</p><p>为了便于说明，我们假设对图2的非Attention模型的Encoder-Decoder框架进行细化，Encoder采用RNN模型，Decoder也采用RNN模型，这是比较常见的一种模型配置，则图2的框架转换为图5。</p><p><img src="E:\myBlog\source_posts\ebTh6NzIdlVG1L2.png" alt="RNN作为具体模型的Encoder-Decoder框架"></p><p>那么用下图可以较为便捷地说明注意力分配概率分布值的通用计算过程。</p><p><img src="E:\myBlog\source_posts\TEXoxdzgrHOuiMQ.jpg" alt="注意力分配概率计算"></p><p><span class="github-emoji" style="display:inline;vertical-align:middle;color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8">⭐</span> 利用的是i-1时刻的隐状态（作为query）去和h1，h2，h3求相似性</p><p>对于采用RNN的Decoder来说，在时刻i，如果要生成yi单词，我们是可以知道Target在生成Yi之前的时刻i-1时，隐层节点i-1时刻的输出值Hi-1的，而我们的目的是要计算<strong>生成Yi时</strong>输入句子中的单词对Yi来说的注意力分配概率分布，那么<strong>可以用Target输出句子i-1时刻的隐层节点状态Hi-1去一一和输入句子Source中每个单词对应的RNN隐层节点状态hj进行对比</strong>，然后函数F的输出经过Softmax进行归一化就得到了符合概率分布取值区间的注意力分配概率分布数值。</p><h4 id="attention机制的本质思想">Attention机制的本质思想</h4><p><img src="E:\myBlog\source_posts\y3BToK96rsgUZfF.png" alt="Attention机制的本质思想"></p><p>我们可以这样来看待Attention机制：将Source中的构成元素想象成是由一系列的&lt;Key,Value&gt;数据对构成，此时给定Target中的某个元素Query，通过计算Query和各个Key的相似性或者相关性，得到每个Key对应Value的权重系数，然后对Value进行加权求和，即得到了最终的Attention数值。</p><p>所以本质上Attention机制是对Source中元素的Value值进行加权求和，而Query和Key用来计算对应Value的权重系数。即可以将其本质思想改写为如下公式：</p><p><img src="E:\myBlog\source_posts\ipGlzuFcmS8n2VR.png" alt="img"></p><p>其中，Lx=||Source||代表Source的长度，公式含义即如上所述。上文所举的机器翻译的例子里，因为在计算Attention的过程中，Source中的Key和Value合二为一，指向的是同一个东西，也即输入句子中每个单词对应的语义编码，所以可能不容易看出这种能够体现本质思想的结构。</p><p>当然，从概念上理解，把Attention仍然理解为从大量信息中有选择地筛选出少量重要信息并聚焦到这些重要信息上，忽略大多不重要的信息，这种思路仍然成立。聚焦的过程体现在权重系数的计算上，权重越大越聚焦于其对应的Value值上，即权重代表了信息的重要性，而Value是其对应的信息。</p><h4 id="计算过程">计算过程</h4><p>至于Attention机制的具体计算过程，如果对目前大多数方法进行抽象的话，可以将其归纳为两个过程：第一个过程是根据Query和Key计算权重系数，第二个过程根据权重系数对Value进行加权求和。而第一个过程又可以细分为两个阶段：第一个阶段根据Query和Key计算两者的相似性或者相关性；第二个阶段对第一阶段的原始分值进行归一化处理；这样，可以将Attention的计算过程抽象为如图10展示的三个阶段。</p><p><img src="E:\myBlog\source_posts\tzO9fhXUeBblmVG.png" alt="三阶段计算Attention过程"></p><p>在第一个阶段，可以引入不同的函数和计算机制，根据Query和某个Key_i，计算两者的相似性或者相关性，最常见的方法包括：求两者的向量点积、求两者的向量Cosine相似性或者通过再引入额外的神经网络来求值，即如下方式：</p><p><img src="E:\myBlog\source_posts\9xpPOa7ohFf3u1d.png" alt="img"></p><p>第一阶段产生的分值根据具体产生的方法不同其数值取值范围也不一样，第二阶段引入类似SoftMax的计算方式对第一阶段的得分进行数值转换，一方面可以进行归一化，将原始计算分值整理成所有元素权重之和为1的概率分布；另一方面也可以通过SoftMax的内在机制更加突出重要元素的权重。即一般采用如下公式计算：</p><p><img src="E:\myBlog\source_posts\XFW5tcSGjqBnIyN.png" alt="img"></p><p>第二阶段的计算结果a_i即为value_i对应的权重系数，然后进行加权求和即可得到Attention数值：</p><p><img src="E:\myBlog\source_posts\soa1M9LIPGi3krC.png" alt="img"></p><p>通过如上三个阶段的计算，即可求出针对Query的Attention数值，目前绝大多数具体的注意力机制计算方法都符合上述的三阶段抽象计算过程。</p><h4 id="self-attention模型">Self Attention模型</h4><p>在attention机制中，query是来自外在的张量，而Self Attention的query则是自己本身</p><p>但是Self Attention在计算过程中会直接将句子中任意两个单词的联系通过一个计算步骤直接联系起来，所以远距离依赖特征之间的距离被极大缩短，有利于有效地利用这些特征。除此外，Self Attention对于增加计算的并行性也有直接帮助作用。这是为何Self Attention逐渐被广泛使用的主要原因。</p><h3 id="transformer">transformer</h3><p>完全基于self-attention的模型</p><p><img src="E:\myBlog\source_posts\image-20201113233224742.png" alt="image-20201113233224742"></p><h3 id="transformer的改进模型">transformer的改进模型</h3><h3 id="预训练模型">预训练模型</h3><h3 id="bert">BERT</h3><h3 id="bert的改进模型">BERT的改进模型</h3><h3 id="总结">总结</h3><hr><h4 id="总结-1">总结</h4><p>Transformer相比LSTM的优点</p><ol type="1"><li><p><strong>完全的并行计算</strong>，Transformer的attention和feed-forward，均可以并行计算。而LSTM则依赖上一时刻，必须串行</p></li><li><p><strong>减少长程依赖</strong>，利用self-attention将每个字之间距离缩短为1，大大缓解了长距离依赖问题</p></li><li><p><strong>提高网络深度</strong>。由于大大缓解了长程依赖梯度衰减问题，Transformer网络可以很深，基于Transformer的BERT甚至可以做到24层。而LSTM一般只有2层或者4层。网络越深，高阶特征捕获能力越好，模型performance也可以越高。</p><p>缺点：</p></li><li><p>文本长度很长时，比如篇章级别，<strong>计算量爆炸</strong>。self-attention的计算量为O(n^2), n为文本长度。</p></li><li><p>Transformer参数量较大，在大规模数据集上，效果远好于LSTM。但在<strong>小规模数据集</strong>上，如果不是利用pretrain models，效果不一定有LSTM好。</p></li></ol><h3 id="transformer家族2----编码长度优化transformer-xllongformer">🚀Transformer家族2 -- 编码长度优化（Transformer-XL、Longformer）</h3><h4 id="背景">1 背景</h4><ol type="1"><li>Transformer利用self-attention机制进行建模，使得任何两个位置token距离都为1。如果没有内存和算力的限制，Transformer理论上可以编码无限长的文本。但由于attention计算量十分大，而且计算复杂度和序列长度为O(n^2)关系，导致序列长度增加，内存和计算量消耗飞快增加。实际中由于内存和算力有限，一般只能编码一定长度，例如512。</li></ol><p>为了提升模型的长程编码能力，从而提升模型在长文本，特别是document-level语料上的效果，我们必须对Transformer编码长度进行优化。</p><h4 id="transformer-xl">2 Transformer-XL</h4><p><img src="E:\myBlog\source_posts\ctFMhR8IEdPVSZ1.png" alt="在这里插入图片描述"> 论文信息：2019年01月，谷歌 &amp; CMU，ACL 2019 论文地址 https://arxiv.org/abs/1901.02860 代码和模型地址 https://github.com/kimiyoung/transformer-xl</p><h5 id="为什么需要transformer-xl">2.1 为什么需要Transformer-XL</h5><p>为了解决长文本编码问题，原版Transformer采用了固定编码长度的方案，例如512个token。将长文本按照固定长度，切分为多个segment。每个segment内部单独编码，segment之间不产生交互信息。这种方式的问题如下</p><ol type="1"><li>模型无法建模超过固定编码长度的文本</li><li>segment之间没有交互信息，导致了文本碎片化。长语句的编码效果有待提升。</li><li>predict阶段，decoder每生成一个字，就往后挪一个，没有重复利用之前信息，导致计算量爆炸</li></ol><p>train和evaluate过程如下<img src="E:\myBlog\source_posts\8YW7R4Q5J6zbmD1.png" alt="在这里插入图片描述"></p><h5 id="实现方法">2.2 实现方法</h5><h6 id="segment-level-recurrence-with-state-reuse-片段级递归和信息复用">2.2.1 Segment-Level Recurrence with State Reuse 片段级递归和信息复用</h6><p>Transformer-XL在编码后一个segment时，将前一个segment的隐层缓存下来。后一个segment的self-attention计算，会使用到前一个segment的隐层。后一个segment的第n+1层，对前一个segment的第n层隐层进行融合。故最大编码长度理论上为O(N × L)。</p><p>在预测阶段，由于对segment隐层使用了缓存，故每预测一个词，不需要重新对之前的文本进行计算。大大提升了预测速度，最大可达到原始Transformer的1800倍。如下图所示 <img src="E:\myBlog\source_posts\KmazXviordxyZuw.png" alt="在这里插入图片描述"></p><h6 id="relative-positional-encodings-相对位置编码">2.2.2 Relative Positional Encodings 相对位置编码</h6><p>segment递归中有个比较大的问题，就是如何区分不同segment中的相同位置。如果采用原版Transformer中的绝对编码方案，两者是无法区分的。如下 <img src="E:\myBlog\source_posts\38l7XhVjZnqyuBe.png" alt="gh">不同segment中的相同位置，其position encoding会相同。这显然是有问题的。Transformer-XL将绝对位置编码改为了q和k之间的相对位置编码，代表了两个token之间的相对位置。从语义上讲，是make sense的。我们来看看具体实现方式。</p><p>绝对位置编码的attention计算如下 <img src="E:\myBlog\source_posts\IKCVE7riwaQ9Zl1.png" alt="在这里插入图片描述">分为四部分</p><ol type="1"><li>query的token encoding和 key的token encoding，之间的关联信息</li><li>query的token encoding和 key的position encoding，之间的关联信息。Uj为绝对位置j的编码向量</li><li>query的position encoding和 key的token encoding，之间的关联信息。Ui为绝对位置i的编码向量</li><li>query的position encoding和 key的position encoding，之间的关联信息</li></ol><p>而采用相对位置编码后，attention计算如下 <img src="E:\myBlog\source_posts\Id9fV3bxSnmHyu2.png" alt="在这里插入图片描述">同样包含四部分，仍然为二者token encoding和position encoding之间的关联关系。区别在于</p><ol type="1"><li>Ri-j为i和j之间相对位置编码，其中R为相对位置编码矩阵</li><li>u和v为query的位置编码，采用一个固定向量。因为采用相对位置编码后，无法对单个绝对位置进行编码了。文中称为global content bias，和global positional bias</li></ol><p>也有其他文章，采用了不同的相对位置编码方案。比如"Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.2018. Self-attention with relative position representations. arXiv preprint arXiv:1803.02155." 中只有a和b两部分，丢掉了c和d。Transformer-XL对两种方案进行了对比实验，证明前一种好。</p><h5 id="实验结果">2.3 实验结果</h5><h6 id="长文本编码效果">长文本编码效果</h6><p><img src="E:\myBlog\source_posts\IyoR924J3BQWrOu.png" alt="在这里插入图片描述"> 在WikiText-103上的实验结果。WikiText-103包含词级别的超长文本，平均每篇文章长度为3.6K token。利用它可以验证模型的长文本编码能力。实验结果表明Transformer-XL large的PPL最低，效果最好。同时作者在One Billion Word、enwik8、text8上均进行了实验，都表明Transformer-XL效果最好。</p><h6 id="有效编码长度">有效编码长度</h6><p><img src="E:\myBlog\source_posts\oKQv3akDcXWOZ7B.png" alt="在这里插入图片描述"> 模型可编码的有效长度如上，r为top-r难度样本上的表现。Transformer-XL比RNN长80%，比Transformer长450%。证明Transformer-XL可编码长度最长，长程捕获能力最强。</p><h6 id="预测速度">预测速度</h6><p><img src="E:\myBlog\source_posts\g2TyZhWtRSEA7pB.png" alt="在这里插入图片描述"> 在不同segment长度下，模型预测速度的对比。和原始Transformer相比，预测速度最大可以提升1874倍。</p><h6 id="消融分析">消融分析</h6><p>文章对片段级递归和相对位置编码两个Method进行了消融分析，如下 <img src="E:\myBlog\source_posts\4EeY78JcQojSuhD.png" alt="在这里插入图片描述"> 两个改进点均有作用，其中片段级递归作用更大。</p><h4 id="longformer">3 Longformer</h4><p><img src="E:\myBlog\source_posts\KhcS2VIyvNEAwrg.png" alt="在这里插入图片描述"> 论文信息：2020年04月，allenai 论文地址 https://arxiv.org/abs/2004.05150 代码和模型地址 https://github.com/allenai/longformer</p><h5 id="改进方法">3.1 改进方法</h5><h6 id="attention稀疏化">3.1.1 attention稀疏化</h6><p>Transformer不能捕获长距离信息，本质原因还是因为计算量过大导致的。那我们通过降低attention计算量，是不是就可以提升长距离编码能力呢。答案是肯定的，LongFormer提出了三种attention稀疏化的方法，来降低计算量。 <img src="E:\myBlog\source_posts\hDmpknyqTSIUEW7.png" alt="在这里插入图片描述"> a是原始的全连接方式的attention。后面三种为文章使用的稀疏attention。</p><ol type="1"><li>Sliding Window attention。滑窗方式的attention。假设序列长度为n，滑窗大小w，则每个位置的attention只和滑窗范围内的token进行计算，复杂度从O(n^2)变为了O(n * w)。当w &lt;&lt; n时，可以大大降低计算量。</li><li>Dilated Sliding Window attention。受到空洞卷积的启发，提出了空洞滑窗attention。看下面这张图就明白了。</li></ol><p><img src="E:\myBlog\source_posts\ZwAy1H3kB7SjIP5.png" alt="在这里插入图片描述"></p><ol type="1"><li>Global Attention + sliding window。某些关键位置采用全局attention，这些位置的attention和所有token进行计算。而其他相对不关键的位置则采用滑窗attention。那什么叫关键位置呢？作者举例，分类问题中[CLS]为关键位置，需要计算全局attention。QA中question每个位置均为关键位置，同样计算全局attention。</li></ol><h6 id="tensor-virtual-machine-tvm">3.1.2 Tensor Virtual Machine (TVM)</h6><p>作者使用了TVM构建CUDA kernel，加快了longformer的速度，并降低了显存需求。这个又是另一个模型加速方面的话题，我们就先不展开了。</p><h5 id="实验结果-1">3.2 实验结果</h5><h6 id="大小模型效果">大小模型效果</h6><p><img src="E:\myBlog\source_posts\ZThJDNXByQp2Lod.png" alt="在这里插入图片描述">作者在大小模型上均实验了LongFormer的效果。小模型为12 layers，512 hidden。大模型为30 layers，512 hidden。在text8和enwik8数据集上。小模型达到了SOTA。大模型比18层的Transformer-XL好，虽然不如Adaptive-span-Transformer和Compressive，但胜在可以pretrain-finetune</p><h6 id="消融分析-1">消融分析</h6><p><img src="E:\myBlog\source_posts\pMCoW2ZLv6gq3j4.png" alt="在这里插入图片描述">消融分析中，可以发现</p><ol type="1"><li>Dilation空洞，有一定的收益</li><li>top layer滑窗大小比bottom layer大时，效果好一些。这个也是make sense的。因为top layer捕获高维语义，关联信息距离相对较远，窗口应该尽量大一些。</li></ol><h6 id="语料长度">语料长度</h6><p><img src="E:\myBlog\source_posts\IjGfH7Mi1LK2hsC.png" alt="在这里插入图片描述"> 从上表中我们发现，语料都是特别长的长文本。LongFormer真的是document级别的Transformer。</p><h6 id="下游任务finetune效果">下游任务finetune效果</h6><p><img src="E:\myBlog\source_posts\JdZUrh6L3QFBHIn.png" alt="在这里插入图片描述"> <img src="E:\myBlog\source_posts\WlOZR3iaJSA2nIU.png" alt="在这里插入图片描述"> 第一个table为RoBERTa和LongFormer在问答、指代消解、分类任务中的对比。第二个table为这几个任务数据集平均文本长度。每项任务都是超出RoBERTa，当文本长度大于512时，performance提升特别明显。更加说明了长程捕获能力在NLP中的重要性。</p><h3 id="transformer家族3----计算效率优化adaptive-spanreformerlite-transformer">🚀Transformer家族3 -- 计算效率优化（Adaptive-Span、Reformer、Lite-Transformer）</h3><h4 id="背景-1">1 背景</h4><p>上文我们从编码长度优化的角度，分析了如何对Transformer进行优化。Transformer-XL、LongFormer等模型，通过片段递归和attention稀疏化等方法，将长文本编码能力提升到了很高的高度。基本已经克服了Transformer长文本捕获能力偏弱的问题，使得下游任务模型performance得到了较大提升，特别是文本较长（大于512）的任务上。</p><p>但Transformer计算量和内存消耗过大的问题，还亟待解决。事实上，Transformer-XL、LongFormer已经大大降低了内存和算力消耗。毕竟Transformer之所以长距离编码能力偏弱，就是因为其计算量是序列长度的平方关系，对算力需求过大，导致当前GPU/TPU不能满足需求。编码长度优化和计算量优化，二者是相辅相成的。但着眼于论文的出发点，我们还是分为两个不同的章节进行分析。毕竟总不能所有模型都放在一个章节吧（_）。</p><p>本文我们带来Adaptive-Span Transformer、Reformer、Lite-Transformer等几篇文章</p><h4 id="adaptive-span-transformer">2 Adaptive-Span Transformer</h4><p><img src="E:\myBlog\source_posts\SZIDO7KcoGHqvhF.png" alt="在这里插入图片描述"> 论文信息：2019年5月，FaceBook，ACL2019 论文地址 https://arxiv.org/pdf/1905.07799.pdf 代码和模型地址 https://github.com/facebookresearch/adaptive-span</p><h5 id="为什么需要adaptive-span">2.1 为什么需要Adaptive-Span</h5><p>之前Transformer-XL将长文本编码能力提升到了较高高度，但是否每个layer的每个head，都需要这么长的attention呢？尽管使用了多种优化手段，长距离attention毕竟还是需要较大的内存和算力。研究发现，大部分head只需要50左右的attention长度，只有少部分head需要较长的attention。这个是make sense的，大部分token只和它附近的token有关联。如下图 <img src="E:\myBlog\source_posts\nOstVdScKG68CDz.png" alt="在这里插入图片描述">我们是否可以实现attention span长度的自适应呢？让不同的layer的不同的head，自己去学习自己的attention span长度呢？Adaptive-Span Transformer给出了肯定答案。</p><h5 id="实现方案">2.2 实现方案</h5><p>文章设定每个attention head内的token计算，都使用同一个span长度。我们就可以利用attention mask来实现自适应span。对每个head都添加一个attention mask，mask为0的位置不进行attention计算。文章设计的mask函数如下 <img src="E:\myBlog\source_posts\yRVqrjfJPgBKlIk.png" alt="在这里插入图片描述"> R为超参，控制曲线平滑度。其为单调递减函数，如下图。 <img src="E:\myBlog\source_posts\hvoFmDIp1BjgV5f.png" alt="在这里插入图片描述"></p><h5 id="实验结果-2">2.3 实验结果</h5><p><img src="E:\myBlog\source_posts\EDsBxfipL4HcOwF.png" alt="在这里插入图片描述"> 和Transformer家族其他很多模型一样，Adaptive-span也在字符级别的语言模型上进行了验证，数据集为text8。如上，Transformer注意力长度固定为512，结论如下</p><ol type="1"><li>Transformer-XL长程编码能力确实很强，平均span可达3800。</li><li>注意力长度确实不需要总那么长，Adaptive-Span大模型上，平均长度只有245</li><li>Adaptive-Span在算力需求很小（只有XL的1/3）的情况下，效果可以达到SOTA。</li></ol><p><img src="E:\myBlog\source_posts\IDjAR2ucfELsegw.png" alt="在这里插入图片描述">上面是在enwik8上的结果。Adaptive-Span又一次在算力很小的情况下，达到了最优效果。值得注意的是，64层的Transformer居然需要120G的计算量，又一次证明了原版Transformer是多么的吃计算资源。另外Transformer-XL在节省计算资源上，其实也算可圈可点。</p><h4 id="reformer">3 Reformer</h4><p><img src="E:\myBlog\source_posts\G8AVxR7fuCZdPHn.png" alt="在这里插入图片描述"> 论文信息：2020年1月，谷歌，ICLR2020 论文地址 https://arxiv.org/abs/2001.04451 代码和模型地址 https://github.com/google/trax/tree/master/trax/models/reformer</p><h5 id="为什么需要reformer">3.1 为什么需要Reformer</h5><p>Transformer内存和计算量消耗大的问题，一直以来广为诟病，并导致其一直不能在长文本上进行应用。（BERT、RoBERTa均设置最大长度为512）。Reformer认为Transformer有三大问题</p><ol type="1"><li>attention层计算量和序列长度为平方关系，导致无法进行长距离编码</li><li>内存占用和模型层数呈N倍关系，导致加深Transformer层数，消耗的内存特别大</li><li>feed-forward的dff比隐层dmodel一般大很多，导致FF层占用的内存特别大</li></ol><p>针对这几个问题，Reformer创新性的提出了三点改进方案</p><ol type="1"><li>LOCALITY-SENSITIVE HASHING 局部敏感hash，使得计算量从 O(L^2)降低为O(L log L) ,L为序列长度</li><li>Reversible Transformer 可逆Transformer，使得N层layers内存消耗变为只需要一层，从而使得模型加深不会受内存限制。</li><li>Feed-forward Chunking 分块全连接，大大降低了feed-forward层的内存消耗。</li></ol><p>Reformer是Transformer家族中最为关键的几个模型之一（去掉之一貌似都可以，顶多Transformer-XL不答应），其创新新也特别新颖，很多思想值得我们深入思考和借鉴。其效果也是特别明显，大大提高了内存和计算资源效率，编码长度可达64k。下面针对它的三点改进方案进行分析，有点难懂哦。</p><h5 id="实现方案-1">3.2 实现方案</h5><h6 id="locality-sensitive-hashing-局部敏感hash">3.2.1 LOCALITY-SENSITIVE HASHING 局部敏感hash</h6><p>局部敏感hash有点难懂，Reformer针对Transformer结构进行了深度灵魂拷问</p><h6 id="query和key必须用两套吗">Query和Key必须用两套吗</h6><p>Transformer主体结构为attention，原版attention计算方法如下 <img src="E:\myBlog\source_posts\Z1fYDpO6BnLyHj4.png" alt="在这里插入图片描述"> 每个token，利用其query向量，和其他token的key向量进行点乘，从而代表两个token之间的相关性。归一化后，利用得到的相关性权重，对每个token的value向量进行加权求和。首先一个问题就是，query和key向量可以是同一套吗？我们可否利用key向量去和其他token的key计算相关性呢？</p><p>为此文章进行实验分析，证明是可行的。个人认为这一点也是make sense的。 <img src="E:\myBlog\source_posts\YyTZDG1Bohkswit.png" alt="在这里插入图片描述">在文本和图像上，Q=K的attention，和普通attention，效果差别不大。</p><h6 id="必须和每个token计算相关性吗">必须和每个token计算相关性吗</h6><p>原版attention中，一个token必须和序列中其他所有token计算相关性，导致计算量随序列长度呈平方关系增长，大大制约了可编码最大长度。那必须和每个token计算相关性吗？其实之前Adaptive-Span Transformer也深度拷问过这个话题。它得出的结论是，对于大部分layer的multi-head，长度50范围内进行attention就已经足够了。不过Adaptive-Span采取的方法还是简单粗暴了一点，它约定每个head的attention span长度是固定的，并且attention span为当前token附近的其他token。</p><p>Adaptive-Span Transformer的这种方法显然还是没有抓住Attention计算冗余的痛点。Attention本质是加权求和，权重为两个token间的相关性。最终结果取决于较大的topk权重，其他权值较小的基本就是炮灰。并且softmax归一化更是加剧了这一点。小者更小，大者更大。为了减少计算冗余，我们可以只对相关性大的其他token的key向量计算Attention。</p><h6 id="怎么找到相关性大的向量呢">怎么找到相关性大的向量呢</h6><p>我们现在要从序列中找到与本token相关性最大的token，也就是当前key向量与哪些key向量相关性大。极端例子，如果两个向量完全相同，他们的相关性是最高的。确定两个高维向量的相关性确实比较困难，好在我们可以利用向量Hash来计算。</p><p>Reformer采用了局部敏感hash。我们让两个key向量在随机向量上投影，将它们划分到投影区间内。 <img src="E:\myBlog\source_posts\gSHlhWRaoqfk9pN.png" alt="在这里插入图片描述"> 如图所示，划分了四个区间（4个桶bucket），进行了三次Hash。第一次Hash，使得上面两个向量分别归入bucket0和bucket3中，下面两个向量都归入bucket0。第二次Hash，上面两个向量和下面两个，均归入到bucket2中了。我们可以发现</p><ol type="1"><li>相似的向量，也就是相关性大的，容易归入到一个bucket中</li><li>局部敏感Hash还是有一定的错误率的，我们可以利用多轮Hash来缓解。这也是Reformer的做法，它采取了4轮和8轮的Hash。</li></ol><h6 id="整个流程">整个流程</h6><p>经过局部敏感Hash后，我们可以将相关性大的key归入同一个bucket中。这样只用在bucket内进行普通Attention即可，大大降低了计算冗余度。为了实现并行计算，考虑到每个bucket包含的向量数目可能不同，实际处理中需要多看一个bucket。整个流程如下 <img src="E:\myBlog\source_posts\ZS5dCT8nVcaiob4.png" alt="在这里插入图片描述"></p><ol type="1"><li>让query等于key</li><li>局部敏感Hash（LSH）分桶。上图同一颜色的为同一个桶，共4个桶</li><li>桶排序，将相同的桶放在一起</li><li>为了实现并行计算，将所有桶分块（chunk），每个chunk大小相同</li><li>桶内计算Attention，由于之前做了分块操作，所以需要多看一个块。</li></ol><h6 id="多轮lsh">多轮LSH</h6><p>为了减少分桶错误率，文章采用了多次分桶，计算LSH Attention，Multi-round LSH attention。可以提升整体准确率。如下表。<img src="E:\myBlog\source_posts\L4BDoiwTfaRjInv.png" alt="在这里插入图片描述"></p><h6 id="reversible-transformer-可逆transformer">3.2.2 REVERSIBLE TRANSFORMER 可逆Transformer</h6><p>LSH局部敏感Hash确实比较难理解，可逆Transformer相对好懂一些。这个方案是为了解决Transformer内存占用量，随layers层数线性增长的问题。为什么会线性增长呢？原因是反向传播中，梯度会从top layer向bottom layer传播，所以必须保存住每一层的Q K V向量，也就导致N层就需要N套Q K V。</p><p>那有没有办法不保存每一层的Q K V呢？可逆Transformer正是这个思路。它利用时间换空间的思想，只保留一层的向量，反向传播时，实时计算出之前层的向量。所以叫做Reversible。Reformer每一层分为两部分，x1和x2。输出也两部分，y1和y2。计算如下</p><p><img src="E:\myBlog\source_posts\gKZT1mRFhEj48nU.png" alt="image-20201028214552153"></p><p>采用可逆残差连接后，模型效果基本没有下降。这也是make sense的，毕竟可逆是从计算角度来解决问题的，对模型本身没有改变。 <img src="E:\myBlog\source_posts\hElWr4uHtUomVYG.png" alt="在这里插入图片描述"></p><h6 id="feed-forward-chunking-ff层分块">3.2.3 Feed-Forward chunking FF层分块</h6><p>针对fead-forward层内存消耗过大的问题，Reformer也给出了解决方案，就是FF层分块。如下 <img src="E:\myBlog\source_posts\rpzwVXOSUavEs18.png" alt="在这里插入图片描述"></p><h5 id="实验结果-3">3.3 实验结果</h5><h6 id="内存和时间复杂度">内存和时间复杂度</h6><p>Reformer三个创新点，大大降低了内存和时间复杂度，消融分析如下 <img src="E:\myBlog\source_posts\5ixhnpvKeQHrBw1.png" alt="在这里插入图片描述"></p><h6 id="模型效果">模型效果</h6><p>如下为在机器翻译上的效果。Reformer减少了算力消耗，同时也大大增加了长文本编码能力，故模型效果也得到了提升。如下。 <img src="E:\myBlog\source_posts\8EtHVoBsfYxnmhO.png" alt="在这里插入图片描述"></p><h4 id="lite-transformer">4 Lite Transformer</h4><p><img src="E:\myBlog\source_posts\9UfkunTVEpL5XBG.png" alt="在这里插入图片描述"> 论文信息：2020年4月，MIT &amp; 上海交大，ICLR2020 论文地址 https://arxiv.org/abs/2004.11886 代码和模型地址 https://github.com/mit-han-lab/lite-transformer</p><h5 id="为什么要做lite-transformer">4.1 为什么要做Lite Transformer</h5><p>主要出发点仍然是Transformer计算量太大，计算冗余过多的问题。跟Adaptive-Span Transformer和Reformer想法一样，Lite Transformer也觉得没必要做Full Attention，很多Attention连接是冗余的。不一样的是，它通过压缩Attention通道的方式实现，将多头减少了一半。与Base Transformer相比，计算量减少了2.5倍。并且文章使用了量化和剪枝技术，使得模型体积减小了18.2倍。</p><h5 id="实现方案-2">4.2 实现方案</h5><p>实现方案很简单，仍然采用了原版Transformer的seq2seq结构，创新点为</p><ol type="1"><li>multiHead self-attention变为了两路并行，分别为一半的通道数（多头）。如下图a所示。其中左半部分为正常的fully attention，它用来捕获全局信息。右半部分为CNN卷积，用来捕获布局信息。最终二者通过FFN层融合。这个架构称为LONG-SHORT RANGE ATTENTION (LSRA)，长短期Attention。</li><li>为了进一步降低计算量，作者将CNN转变为了一个depth wise卷积和一个线性全连接。dw卷积在mobileNet中有讲过，不清楚可自行谷歌。</li></ol><p><img src="E:\myBlog\source_posts\WQtJvpbP9rUczhs.png" alt="在这里插入图片描述"></p><h5 id="实验结果-4">4.3 实验结果</h5><h6 id="计算复杂度">计算复杂度</h6><p><img src="E:\myBlog\source_posts\w6b1LlqIpZgt8fX.png" alt="在这里插入图片描述"> 如上图，在文本摘要任务上，Lite Transformer计算量相比Base Transformer，减少了2.5倍。同时Rouge指标基本没变。</p><h6 id="模型体积">模型体积</h6><p><img src="E:\myBlog\source_posts\JYzeocGBUr6jy4W.png" alt="在这里插入图片描述"> Lite Transformer模型体积只有Transformer的2.5分之一，通过8bit量化和剪枝，最终模型体积下降了18.2倍。</p><h3 id="邱锡鹏教授nlp预训练模型综述">🚀邱锡鹏教授：NLP预训练模型综述</h3><h4 id="引言"><strong>1.引言</strong></h4><p>随深度学习的发展，多种神经网络都被应用在 NLP 任务中，比如 CNN、RNN、GNN 和 attention 机制等，但由于现有的数据集对于大部分有监督 NLP 任务来说都很小，因此，早期的模型对 NLP 任务来说都很“浅”，往往只包含 1-3 层。</p><p>而预训练模型（Pre-trained Models, PTMs）的出现将NLP带入一个新的时代，更“深”的模型和训练技巧的增强也使得 PTMs 由“浅”变“深”，在多项任务都达到了 SOTA 性能。</p><h4 id="背景-2"><strong>2.背景</strong></h4><h5 id="语言表示学习"><strong>2.1 语言表示学习</strong></h5><p>对于语言来说，一个好的表示应当描绘语言的内在规则比如词语含义、句法结构、语义角色甚至语用。</p><p><img src="E:\myBlog\source_posts\ZmF3yXaiHPME1LQ.png" alt="img"></p><p>而分布式表示的核心思想就是通过低维实值向量来描述一段文本的意义，而向量的每一个维度都没有对于意义，整体则代表一个具体的概念。图 1 是 NLP 的通用神经体系架构。</p><p>有两种 embedding（词嵌入）方式：上下文嵌入和非上下文嵌入，两者的区别在于词的 embedding 是否根据词出现的上下文动态地改变。</p><p><strong>非上下文嵌入：</strong>表示语言的第一步就是将分离的语言符号映射到分布式嵌入空间中。也就是对于词汇表中的每个单词（词根），通过 lookup table 映射到一个向量。</p><p>这种嵌入方式有两个局限：一是一个词通过这种方法获得的词嵌入总是静态且与上下文无关的，无法处理多义词；二是难以解决不在词汇表中的词（针对这个问题，很多 NLP 任务提出了字符级或词根级的词表示，如 CharCNN、FastText、Byte-Pair Encoding (BPE)）。</p><p><strong>上下文嵌入：</strong>为解决多义性和上下文相关的问题，将词在不同上下文的语义做区分。通过对词（词根）的 token 加一层 Neural Contextual Encoder（神经上下文编码器）得到词的上下文嵌入。</p><p>有了上下文嵌入，就可以代入到具体的NLP任务中</p><p>以下是NLP领域主要模型的发展</p><p>随着BERT的出现，模型的规模越来越大，参数越来越多，训练的无标签数据集越来越大。因为随着参数增大，模型的性能并没有出现饱和的状态。GPT3有1750亿个参数。</p><p><img src="E:\myBlog\source_posts\image-20201114154032790.png" alt="image-20201114154032790"></p><p>在问答任务AQuAD 2.0 中，预训练模型精度已经和人类差不多了</p><p><img src="E:\myBlog\source_posts\image-20201114164427442.png" alt="image-20201114164427442"></p><p>felf-attention 机制 ，得到“the”的上下文表示</p><p><img src="E:\myBlog\source_posts\image-20201114154601028.png" alt="image-20201114154601028"></p><h5 id="为什么要预训练"><strong>2.3 为什么要预训练？</strong></h5><p>模型参数的数量增长迅速，而为了训练这些参数，就需要更大的数据集来避免过拟合，但是标注的数据集太少，大规模的标注数据集成本非常高，很多时候需要专家来进行标注。 而相比之下，大规模未标注的语料却很容易构建。</p><p>为了利用大量的未标注文本数据，我们可以先从其中学习一个好的表示，再将这些表示用在别的任务中。这一通过 PTMs 从未标注大规模数据集中提取表示的预训练过程在很多 NLP 任务中都取得了很好的表现。</p><p>预训练的优点可以总结为以下三点：1 在大规模语料上通过预训练学习通用语言表示对下游任务很有帮助，更好地泛化到不同的任务；2) 预训练提供了更好的模型初始化参数，使得在目标任务上有更好的泛化性能和更快的收敛速度；3) 预训练是一种有效的正则化方法，能够避免复杂模型在小数据集上过拟合。</p><h5 id="预训练任务汇总">预训练任务汇总</h5><p>自监督学习：去⼈为地构造⼀些任务，这些 任务在实际中可能不存在 , 但是可以通过这些任务, 去学习到⼀些隐含 的知识</p><p><img src="E:\myBlog\source_posts\image-20201114160905548.png" alt="image-20201114160905548"></p><h4 id="elmo模型">ELMo模型</h4><p><img src="E:\myBlog\source_posts\image-20201114202903029.png" alt="image-20201114202903029"></p><p>ELMo的预训练过程</p><h5 id="如何压缩ptms"><strong>3.3 如何压缩PTMs</strong></h5><p>预训练模型往往包含至少几千万个参数，这也使得模型难以部署到生活中的线上服务以及资源有限的设备上，这就使得模型压缩成为一条可能能够压缩模型尺寸并提高计算效率的方法。</p><p>压缩 PTMs 一般有四个方法：</p><ul><li><p><strong>剪枝（pruning）：</strong>去除不那么重要的参数（e.g. 权重、层数、通道数、attention heads）</p></li><li><p><strong>量化（weight quantization）：</strong>使用占位更少（低精度）的参数</p></li><li><p><strong>参数共享（parameter sharing）：</strong>相似模型单元间共享参数</p></li><li><p><strong>知识蒸馏（knowledge diistillation）：</strong>用一些优化目标从大型 teacher 模型学习一个小的 student 模型，一些利用知识蒸馏的 PTMs 见表 3</p></li></ul><p><img src="E:\myBlog\source_posts\5tdJqBHve3XYuCo.png" alt="img"></p><h4 id="如何将ptms应用至下游任务"><strong>4.如何将PTMs应用至下游任务</strong></h4><p><img src="E:\myBlog\source_posts\image-20201114165642819.png" alt="image-20201114165642819"></p><p>预训练完模型数据集不要了，只保留模型以及参数，迁移到下游任务，在目标数据集上进行微调</p><h4 id="未来方向"><strong>7.未来方向</strong></h4><h5 id="ptms的上界"><strong>7.1 PTMs的上界</strong></h5><p>随 BERT 的出现，我们可以发现，很多模型都可以通过更长的训练步长不在和更大的语料来提升性能</p><p>PTMs 的共同目标都是学习语言的本质通用知识(或者说是世界的知识)，然而，随着模型的不断加深，语料的不断增大，训练模型的花销也越来越大。一种更可行的解决方案是设计更有效的模型架构、自监督预训练任务、优化器和软硬件方面的技巧等。</p><h5 id="面向任务的预训练与模型压缩"><strong>7.2 面向任务的预训练与模型压缩</strong></h5><p>任务定向</p><p>模型精简</p><p>在实践中，不同的下游任务要求 PTMs 拥有不同的功能。而 PTMs 与下游目标任务间的差异通常表现在两方面：模型架构与数据分布。较大的 PTMs 通常情况下会有更好的性能，但实际问题是如何在低容量设备和低时延应用上使用如此庞大的 PTM。</p><p>除此之外，我们可以通过模型压缩来将通用 PTMs 教给面向对象的 PTM。尽管 CV 中对 CNNs 的压缩已经非常成熟，但 Tansformer 的全连接结构使得模型压缩非常具有挑战性。</p><hr><p>BERT模型从模型创新角度看一般，创新不算大。但是效果太好了，基本刷新了很多NLP的任务的最好性能，有些任务还被刷爆了，这个才是关键。</p><p>另外一点是Bert具备广泛的通用性，就是说绝大部分NLP任务都可以采用类似的两阶段模式直接去提升效果，这个第二关键。客观的说，把Bert当做最近两年NLP重大进展的集大成者更符合事实。</p><p>预训练最初是应用在图像领域的</p><p>模型中大量参数通过大的训练集合比如 ImageNet预先训练好直接拿来初始化大部分网络结构参数，然后再用下游任务上 Fine-tuning过程去调整参数让它们更适合解决下游任务。</p><p>CNN结构来说，不同层级的神经元学习到了不同类型的图像特征，由底向上特征形成层级结构预训练好的网络参数，尤其是底层的网络参数抽取出特征跟具体任务越无关，越具备任务的通用性，所以这是为何一般用底层预训练好的参数初始化新任务 网络参数的原因</p><h3 id="nlp领域的预训练模型">NLP领域的预训练模型</h3><h4 id="word-embedding考古史">Word Embedding考古史</h4><p><img src="E:\myBlog\source_posts\4aFBLDCEQmgXp9Z.jpg" alt="img"></p><p>什么是语言模型？其实看上面这张PPT上扣下来的图就明白了，为了能够量化地衡量哪个句子更像一句人话，可以设计如上图所示函数，核心函数P的思想是根据句子里面前面的一系列前导单词预测后面跟哪个单词的概率大小（理论上除了上文之外，也可以引入单词的下文联合起来预测单词出现概率）。句子里面每个单词都有个根据上文预测自己的过程，把所有这些单词的产生概率乘起来，数值越大代表这越像一句人话。语言模型压下暂且不表，我隐约预感到我这么讲你可能还是不太会明白，但是大概这个意思，不懂的可以去网上找，资料多得一样地汗牛冲动。</p><p>假设现在让你设计一个神经网络结构，去做这个语言模型的任务，就是说给你很多语料做这个事情，训练好一个神经网络，训练好之后，以后输入一句话的前面几个单词，要求这个网络输出后面紧跟的单词应该是哪个，你会怎么做？</p><p><img src="E:\myBlog\source_posts\SUI2jF7xEsaz9Z4.jpg" alt="img"></p><p>你可以像上图这么设计这个网络结构，这其实就是大名鼎鼎的中文人称“神经网络语言模型”，英文小名NNLM的网络结构，用来做语言模型。这个工作有年头了，是个陈年老工作，是Bengio 在2003年发表在JMLR上的论文。它生于2003，火于2013，以后是否会不朽暂且不知，但是不幸的是出生后应该没有引起太大反响，沉寂十年终于时来运转沉冤得雪，在2013年又被NLP考古工作者从海底湿淋淋地捞出来了祭入神殿。为什么会发生这种技术奇遇记？你要想想2013年是什么年头，是深度学习开始渗透NLP领域的光辉时刻，万里长征第一步，而NNLM可以算是南昌起义第一枪。在深度学习火起来之前，极少有人用神经网络做NLP问题，如果你10年前坚持用神经网络做NLP，估计别人会认为你这人神经有问题。所谓红尘滚滚，谁也挡不住历史发展趋势的车轮，这就是个很好的例子。</p><p>上面是闲话，闲言碎语不要讲，我们回来讲一讲NNLM的思路。先说训练过程，现在看其实很简单，见过RNN、LSTM、CNN后的你们回头再看这个网络甚至显得有些简陋。学习任务是输入某个句中单词Wt=“Bert”前面句子的t-1个单词，要求网络正确预测单词Bert，即最大化：</p><p><img src="https://i.loli.net/2020/10/30/k8XpuUwT47WOo5Q.png" alt="image-20201030202319906" style="zoom:50%;"></p><p>前面任意单词Wi用Onehot编码（比如：0001000）作为原始单词输入，之后乘以矩阵Q后获得向量C（Wi），每个单词的 C（Wi）拼接，上接隐层，然后接softmax去预测后面应该后续接哪个单词。这个 C（Wi）是什么？这其实就是单词对应的Word Embedding值，那个矩阵Q包含V行，V代表词典大小，每一行内容代表对应单词的Word embedding值。只不过Q的内容也是网络参数，需要学习获得，训练刚开始用随机值初始化矩阵Q，当这个网络训练好之后，矩阵Q的内容被正确赋值，每一行代表一个单词对应的Word embedding值。所以你看，通过这个网络学习语言模型任务，这个网络不仅自己能够根据上文预测后接单词是什么，同时获得一个副产品，就是那个矩阵Q，这就是单词的Word Embedding是被如何学会的。</p><p>2013年最火的用语言模型做Word Embedding的工具是Word2Vec，后来又出了Glove，Word2Vec是怎么工作的呢？看下图。</p><p><img src="E:\myBlog\source_posts\bXq5RvhS3niaTxB.jpg" alt="img"></p><p>Word2Vec的网络结构其实和NNLM是基本类似的，只是这个图长得清晰度差了点，看上去不像，其实它们是亲兄弟。不过这里需要指出：尽管网络结构相近，而且也是做语言模型任务，但是其训练方法不太一样。Word2Vec有两种训练方法，一种叫CBOW，核心思想是从一个句子里面把一个词抠掉，用这个词的上文和下文去预测被抠掉的这个词；第二种叫做Skip-gram，和CBOW正好反过来，输入某个单词，要求网络预测它的上下文单词。而你回头看看，NNLM是怎么训练的？是输入一个单词的上文，去预测这个单词。这是有显著差异的。为什么Word2Vec这么处理？原因很简单，因为Word2Vec和NNLM不一样，NNLM的主要任务是要学习一个解决语言模型任务的网络结构，语言模型就是要看到上文预测下文，而word embedding只是无心插柳的一个副产品。但是Word2Vec目标不一样，它单纯就是要word embedding的，这是主产品，所以它完全可以随性地这么去训练网络。</p><p>为什么要讲Word2Vec呢？这里主要是要引出CBOW的训练方法，BERT其实跟它有关系，后面会讲它们之间是如何的关系，当然它们的关系BERT作者没说，是我猜的，至于我猜的对不对，后面你看后自己判断。</p><p><img src="E:\myBlog\source_posts\aM9lpsNOS7vrmnq.jpg" alt="img"></p><p>使用Word2Vec或者Glove，通过做语言模型任务，就可以获得每个单词的Word Embedding，那么这种方法的效果如何呢？上图给了网上找的几个例子，可以看出有些例子效果还是很不错的，一个单词表达成Word Embedding后，很容易找出语义相近的其它词汇。</p><p>我们的主题是预训练，那么问题是Word Embedding这种做法能算是预训练吗？这其实就是标准的预训练过程。要理解这一点要看看学会Word Embedding后下游任务是怎么用它的。</p><p><img src="E:\myBlog\source_posts\YEf95lnIW28OGRa.jpg" alt="img"></p><p>假设如上图所示，我们有个NLP的下游任务，比如QA，就是问答问题，所谓问答问题，指的是给定一个问题X，给定另外一个句子Y,要判断句子Y是否是问题X的正确答案。问答问题假设设计的网络结构如上图所示，这里不展开讲了，懂得自然懂，不懂的也没关系，因为这点对于本文主旨来说不关键，关键是网络如何使用训练好的Word Embedding的。它的使用方法其实和前面讲的NNLM是一样的，句子中每个单词以Onehot形式作为输入，然后乘以学好的Word Embedding矩阵Q，就直接取出单词对应的Word Embedding了。这乍看上去好像是个查表操作，不像是预训练的做法是吧？其实不然，那个Word Embedding矩阵Q其实就是网络Onehot层到embedding层映射的网络参数矩阵。所以你看到了，使用Word Embedding等价于什么？等价于把Onehot层到embedding层的网络用预训练好的参数矩阵Q初始化了。这跟前面讲的图像领域的低层预训练过程其实是一样的，区别无非Word Embedding只能初始化第一层网络参数，再高层的参数就无能为力了。下游NLP任务在使用Word Embedding的时候也类似图像有两种做法，一种是Frozen，就是Word Embedding那层网络参数固定不动；另外一种是Fine-Tuning，就是Word Embedding这层参数使用新的训练集合训练也需要跟着训练过程更新掉。</p><p>上面这种做法就是18年之前NLP领域里面采用预训练的典型做法，之前说过，Word Embedding其实对于很多下游NLP任务是有帮助的，只是帮助没有大到闪瞎忘记戴墨镜的围观群众的双眼而已。那么新问题来了，为什么这样训练及使用Word Embedding的效果没有期待中那么好呢？答案很简单，因为Word Embedding有问题呗。这貌似是个比较弱智的答案，关键是Word Embedding存在什么问题？这其实是个好问题。</p><p><img src="E:\myBlog\source_posts\U3YwNJ1RmydDukM.jpg" alt="img"></p><p>这片在Word Embedding头上笼罩了好几年的乌云是什么？是多义词问题。我们知道，多义词是自然语言中经常出现的现象，也是语言灵活性和高效性的一种体现。多义词对Word Embedding来说有什么负面影响？如上图所示，比如多义词Bank，有两个常用含义，但是Word Embedding在对bank这个单词进行编码的时候，是区分不开这两个含义的，因为它们尽管上下文环境中出现的单词不同，但是在用语言模型训练的时候，不论什么上下文的句子经过word2vec，都是预测相同的单词bank，而同一个单词占的是同一行的参数空间，这导致两种不同的上下文信息都会编码到相同的word embedding空间里去。所以word embedding无法区分多义词的不同语义，这就是它的一个比较严重的问题。</p><p>你可能觉得自己很聪明，说这可以解决啊，确实也有很多研究人员提出很多方法试图解决这个问题，但是从今天往回看，这些方法看上去都成本太高或者太繁琐了，有没有简单优美的解决方案呢？</p><p>ELMO提供了一种简洁优雅的解决方案。</p><h3 id="从word-embedding到elmo">从Word Embedding到ELMO</h3><p>ELMO是“Embedding from Language Models”的简称，其实这个名字并没有反应它的本质思想，提出ELMO的论文题目：“Deep contextualized word representation”更能体现其精髓，而精髓在哪里？在deep contextualized这个短语，一个是deep，一个是context，其中context更关键。在此之前的Word Embedding本质上是个静态的方式，所谓静态指的是训练好之后每个单词的表达就固定住了，以后使用的时候，不论新句子上下文单词是什么，这个单词的Word Embedding不会跟着上下文场景的变化而改变，所以对于比如Bank这个词，它事先学好的Word Embedding中混合了几种语义 ，在应用中来了个新句子，即使从上下文中（比如句子包含money等词）明显可以看出它代表的是“银行”的含义，但是对应的Word Embedding内容也不会变，它还是混合了多种语义。这是为何说它是静态的，这也是问题所在。ELMO的本质思想是：我事先用语言模型学好一个单词的Word Embedding，此时多义词无法区分，不过这没关系。在我实际使用Word Embedding的时候，单词已经具备了特定的上下文了，这个时候我可以根据上下文单词的语义去调整单词的Word Embedding表示，这样经过调整后的Word Embedding更能表达在这个上下文中的具体含义，自然也就解决了多义词的问题了。所以ELMO本身是个根据当前上下文对Word Embedding动态调整的思路。</p><p><img src="E:\myBlog\source_posts\m6NvFoRGhWbji83.jpg" alt="img"></p><p>ELMO采用了典型的两阶段过程，第一个阶段是利用语言模型进行预训练；第二个阶段是在做下游任务时，从预训练网络中提取对应单词的网络各层的Word Embedding作为新特征补充到下游任务中。上图展示的是其预训练过程，它的网络结构采用了双层双向LSTM，目前语言模型训练的任务目标是根据单词 Wi 的上下文去正确预测单词 Wi ， Wi 之前的单词序列Context-before称为上文，之后的单词序列Context-after称为下文。图中左端的前向双层LSTM代表正方向编码器，输入的是从左到右顺序的除了预测单词外 Wi 的上文Context-before；右端的逆向双层LSTM代表反方向编码器，输入的是从右到左的逆序的句子下文Context-after；每个编码器的深度都是两层LSTM叠加。这个网络结构其实在NLP中是很常用的。使用这个网络结构利用大量语料做语言模型任务就能预先训练好这个网络，如果训练好这个网络后，输入一个新句子 Snew ，句子中每个单词都能得到对应的三个Embedding:最底层是单词的Word Embedding，往上走是第一层双向LSTM中对应单词位置的Embedding，这层编码单词的句法信息更多一些；再往上走是第二层LSTM中对应单词位置的Embedding，这层编码单词的语义信息更多一些。也就是说，ELMO的预训练过程不仅仅学会单词的Word Embedding，还学会了一个双层双向的LSTM网络结构，而这两者后面都有用。</p><p><img src="E:\myBlog\source_posts\ymSXKwFh8WBcEd5.jpg" alt="img"></p><p>上面介绍的是ELMO的第一阶段：预训练阶段。那么预训练好网络结构后，如何给下游任务使用呢？上图展示了下游任务的使用过程，比如我们的下游任务仍然是QA问题，此时对于问句X，我们可以先将句子X作为预训练好的ELMO网络的输入，这样句子X中每个单词在ELMO网络中都能获得对应的三个Embedding，之后给予这三个Embedding中的每一个Embedding一个权重a，这个权重可以学习得来，根据各自权重累加求和，将三个Embedding整合成一个。然后将整合后的这个Embedding作为X句在自己任务的那个网络结构中对应单词的输入，以此作为补充的新特征给下游任务使用。对于上图所示下游任务QA中的回答句子Y来说也是如此处理。因为ELMO给下游提供的是每个单词的特征形式，所以这一类预训练的方法被称为“Feature-based Pre-Training”。至于为何这么做能够达到区分多义词的效果，你可以想一想，其实比较容易想明白原因。</p><p><img src="E:\myBlog\source_posts\b8ToQxv5BPgELI1.jpg" alt="img"></p><p>上面这个图是TagLM采用类似ELMO的思路做命名实体识别任务的过程，其步骤基本如上述ELMO的思路，所以此处不展开说了。TagLM的论文发表在2017年的ACL会议上，作者就是AllenAI里做ELMO的那些人，所以可以将TagLM看做ELMO的一个前导工作。前几天这个PPT发出去后有人质疑说FastAI的在18年4月提出的ULMFiT才是抛弃传统Word Embedding引入新模式的开山之作，我深不以为然。首先TagLM出现的更早而且模式基本就是ELMO的思路；另外ULMFiT使用的是三阶段模式，在通用语言模型训练之后，加入了一个领域语言模型预训练过程，而且论文重点工作在这块，方法还相对比较繁杂，这并不是一个特别好的主意，因为领域语言模型的限制是它的规模往往不可能特别大，精力放在这里不太合适，放在通用语言模型上感觉更合理；再者，尽管ULFMiT实验做了6个任务，但是都集中在分类问题相对比较窄，不如ELMO验证的问题领域广，我觉得这就是因为第二步那个领域语言模型带来的限制。所以综合看，尽管ULFMiT也是个不错的工作，但是重要性跟ELMO比至少还是要差一档，当然这是我个人看法。每个人的学术审美口味不同，我个人一直比较赞赏要么简洁有效体现问题本质要么思想特别游离现有框架脑洞开得异常大的工作，所以ULFMiT我看论文的时候就感觉看着有点难受，觉得这工作没抓住重点而且特别麻烦，但是看ELMO论文感觉就赏心悦目，觉得思路特别清晰顺畅，看完暗暗点赞，心里说这样的文章获得NAACL2018最佳论文当之无愧，比ACL很多最佳论文也好得不是一点半点，这就是好工作带给一个有经验人士的一种在读论文时候就能产生的本能的感觉，也就是所谓的这道菜对上了食客的审美口味。</p><p><img src="E:\myBlog\source_posts\6y7VvCDm9NRpHJx.jpg" alt="img"></p><p>前面我们提到静态Word Embedding无法解决多义词的问题，那么ELMO引入上下文动态调整单词的embedding后多义词问题解决了吗？解决了，而且比我们期待的解决得还要好。上图给了个例子，对于Glove训练出的Word Embedding来说，多义词比如play，根据它的embedding找出的最接近的其它单词大多数集中在体育领域，这很明显是因为训练数据中包含play的句子中体育领域的数量明显占优导致；而使用ELMO，根据上下文动态调整后的embedding不仅能够找出对应的“演出”的相同语义的句子，而且还可以保证找出的句子中的play对应的词性也是相同的，这是超出期待之处。之所以会这样，是因为我们上面提到过，第一层LSTM编码了很多句法信息，这在这里起到了重要作用。</p><p><img src="E:\myBlog\source_posts\YFAVkuIxemlaHPN.jpg" alt="img"></p><p>ELMO经过这般操作，效果如何呢？实验效果见上图，6个NLP任务中性能都有幅度不同的提升，最高的提升达到25%左右，而且这6个任务的覆盖范围比较广，包含句子语义关系判断，分类任务，阅读理解等多个领域，这说明其适用范围是非常广的，普适性强，这是一个非常好的优点。</p><p><img src="E:\myBlog\source_posts\LpMSFe5kX7Qxroh.jpg" alt="img"></p><p>那么站在现在这个时间节点看，ELMO有什么值得改进的缺点呢？首先，一个非常明显的缺点在特征抽取器选择方面，ELMO使用了LSTM而不是新贵Transformer，Transformer是谷歌在17年做机器翻译任务的“Attention is all you need”的论文中提出的，引起了相当大的反响，很多研究已经证明了Transformer提取特征的能力是要远强于LSTM的。如果ELMO采取Transformer作为特征提取器，那么估计Bert的反响远不如现在的这种火爆场面。另外一点，ELMO采取双向拼接这种融合特征的能力可能比Bert一体化的融合特征方式弱，但是，这只是一种从道理推断产生的怀疑，目前并没有具体实验说明这一点。</p><p>我们如果把ELMO这种预训练方法和图像领域的预训练方法对比，发现两者模式看上去还是有很大差异的。除了以ELMO为代表的这种基于特征融合的预训练方法外，NLP里还有一种典型做法，这种做法和图像领域的方式就是看上去一致的了，一般将这种方法称为“基于Fine-tuning的模式”，而GPT就是这一模式的典型开创者。</p><h3 id="从word-embedding到gpt">从Word Embedding到GPT</h3><p><img src="E:\myBlog\source_posts\IDl2hH8j3JVdx6F.jpg" alt="img"></p><p>GPT是“Generative Pre-Training”的简称，从名字看其含义是指的生成式的预训练。GPT也采用两阶段过程，第一个阶段是利用语言模型进行预训练，第二阶段通过Fine-tuning的模式解决下游任务。上图展示了GPT的预训练过程，其实和ELMO是类似的，主要不同在于两点：首先，特征抽取器不是用的RNN，而是用的Transformer，上面提到过它的特征抽取能力要强于RNN，这个选择很明显是很明智的；其次，GPT的预训练虽然仍然是以语言模型作为目标任务，但是采用的是单向的语言模型，所谓“单向”的含义是指：语言模型训练的任务目标是根据 Wi 单词的上下文去正确预测单词 Wi ， Wi 之前的单词序列Context-before称为上文，之后的单词序列Context-after称为下文。ELMO在做语言模型预训练的时候，预测单词 Wi 同时使用了上文和下文，而GPT则只采用Context-before这个单词的上文来进行预测，而抛开了下文。这个选择现在看不是个太好的选择，原因很简单，它没有把单词的下文融合进来，这限制了其在更多应用场景的效果，比如阅读理解这种任务，在做任务的时候是可以允许同时看到上文和下文一起做决策的。如果预训练时候不把单词的下文嵌入到Word Embedding中，是很吃亏的，白白丢掉了很多信息。</p><p>这里强行插入一段简单提下Transformer，尽管上面提到了，但是说的还不完整，补充两句。首先，Transformer是个叠加的“自注意力机制（Self Attention）”构成的深度网络，是目前NLP里最强的特征提取器，注意力这个机制在此被发扬光大，从任务的配角不断抢戏，直到Transformer一跃成为踢开RNN和CNN传统特征提取器，荣升头牌，大红大紫。注意力机制可以参考“<a href="https://zhuanlan.zhihu.com/p/37601161" target="_blank" rel="noopener">深度学习中的注意力模型</a>”，补充下相关基础知识，如果不了解注意力机制你肯定会落后时代的发展。而介绍Transformer比较好的文章可以参考以下两篇文章：一个是Jay Alammar可视化地介绍Transformer的博客文章<a href="https://link.zhihu.com/?target=https%3A//jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a> ，非常容易理解整个机制，建议先从这篇看起；然后可以参考哈佛大学NLP研究组写的“<a href="https://link.zhihu.com/?target=http%3A//nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer.</a> ”，代码原理双管齐下，讲得非常清楚。我相信上面两个文章足以让你了解Transformer了，所以这里不展开介绍。</p><p>其次，我的判断是Transformer在未来会逐渐替代掉RNN成为主流的NLP工具，RNN一直受困于其并行计算能力，这是因为它本身结构的序列性依赖导致的，尽管很多人在试图通过修正RNN结构来修正这一点，但是我不看好这种模式，因为给马车换轮胎不如把它升级到汽车，这个道理很好懂，更何况目前汽车的雏形已经出现了，干嘛还要执着在换轮胎这个事情呢？是吧？再说CNN，CNN在NLP里一直没有形成主流，CNN的最大优点是易于做并行计算，所以速度快，但是在捕获NLP的序列关系尤其是长距离特征方面天然有缺陷，不是做不到而是做不好，目前也有很多改进模型，但是特别成功的不多。综合各方面情况，很明显Transformer同时具备并行性好，又适合捕获长距离特征，没有理由不在赛跑比赛中跑不过RNN和CNN。</p><p>好了，题外话结束，我们再回到主题，接着说GPT。上面讲的是GPT如何进行第一阶段的预训练，那么假设预训练好了网络模型，后面下游任务怎么用？它有自己的个性，和ELMO的方式大有不同。</p><p><img src="E:\myBlog\source_posts\3Gr9vqoPHkSfcAg.jpg" alt="img"></p><p>上图展示了GPT在第二阶段如何使用。首先，对于不同的下游任务来说，本来你可以任意设计自己的网络结构，现在不行了，你要向GPT的网络结构看齐，把任务的网络结构改造成和GPT的网络结构是一样的。然后，在做下游任务的时候，利用第一步预训练好的参数初始化GPT的网络结构，这样通过预训练学到的语言学知识就被引入到你手头的任务里来了，这是个非常好的事情。再次，你可以用手头的任务去训练这个网络，对网络参数进行Fine-tuning，使得这个网络更适合解决手头的问题。就是这样。看到了么？这有没有让你想起最开始提到的图像领域如何做预训练的过程（请参考上图那句非常容易暴露年龄的歌词）？对，这跟那个模式是一模一样的。</p><p>这里引入了一个新问题：对于NLP各种花样的不同任务，怎么改造才能靠近GPT的网络结构呢？</p><p><img src="E:\myBlog\source_posts\iJqb8TYLwCvdSVk.jpg" alt="img"></p><p>GPT论文给了一个改造施工图如上，其实也很简单：对于分类问题，不用怎么动，加上一个起始和终结符号即可；对于句子关系判断问题，比如Entailment，两个句子中间再加个分隔符即可；对文本相似性判断问题，把两个句子顺序颠倒下做出两个输入即可，这是为了告诉模型句子顺序不重要；对于多项选择问题，则多路输入，每一路把文章和答案选项拼接作为输入即可。从上图可看出，这种改造还是很方便的，不同任务只需要在输入部分施工即可。</p><p><img src="E:\myBlog\source_posts\qnLcVGo5IK6riYh.jpg" alt="img"></p><p>GPT的效果是非常令人惊艳的，在12个任务里，9个达到了最好的效果，有些任务性能提升非常明显。</p><p><img src="E:\myBlog\source_posts\96zdAXvcOmJTuP2.jpg" alt="img"></p><p>那么站在现在的时间节点看，GPT有什么值得改进的地方呢？其实最主要的就是那个单向语言模型，如果改造成双向的语言模型任务估计也没有Bert太多事了。当然，即使如此GPT也是非常非常好的一个工作，跟Bert比，其作者炒作能力亟待提升。</p><h3 id="bert的诞生">Bert的诞生</h3><p><img src="E:\myBlog\source_posts\rSJAqOMB4sathDg.jpg" alt="img"></p><p>我们经过跋山涉水，终于到了目的地Bert模型了。</p><p>Bert采用和GPT完全相同的两阶段模型，首先是语言模型预训练；其次是使用Fine-Tuning模式解决下游任务。和GPT的最主要不同在于在预训练阶段采用了类似ELMO的双向语言模型，当然另外一点是语言模型的数据规模要比GPT大。所以这里Bert的预训练过程不必多讲了。</p><p><img src="E:\myBlog\source_posts\61JpWKSZ5fF3tNk.jpg" alt="img"></p><p>第二阶段，Fine-Tuning阶段，这个阶段的做法和GPT是一样的。当然，它也面临着下游任务网络结构改造的问题，在改造任务方面Bert和GPT有些不同，下面简单介绍一下。</p><p><img src="E:\myBlog\source_posts\UTQdhtVA7PzcIlF.jpg" alt="img"></p><p>在介绍Bert如何改造下游任务之前，先大致说下NLP的几类问题，说这个是为了强调Bert的普适性有多强。通常而言，绝大部分NLP问题可以归入上图所示的四类任务中：一类是序列标注，这是最典型的NLP任务，比如中文分词，词性标注，命名实体识别，语义角色标注等都可以归入这一类问题，它的特点是句子中每个单词要求模型根据上下文都要给出一个分类类别。第二类是分类任务，比如我们常见的文本分类，情感计算等都可以归入这一类。它的特点是不管文章有多长，总体给出一个分类类别即可。第三类任务是句子关系判断，比如Entailment，QA，语义改写，自然语言推理等任务都是这个模式，它的特点是给定两个句子，模型判断出两个句子是否具备某种语义关系；第四类是生成式任务，比如机器翻译，文本摘要，写诗造句，看图说话等都属于这一类。它的特点是输入文本内容后，需要自主生成另外一段文字。</p><p><img src="E:\myBlog\source_posts\mxJybVWl2OatfUc.jpg" alt="img"></p><p>对于种类如此繁多而且各具特点的下游NLP任务，Bert如何改造输入输出部分使得大部分NLP任务都可以使用Bert预训练好的模型参数呢？上图给出示例，对于句子关系类任务，很简单，和GPT类似，加上一个起始和终结符号，句子之间加个分隔符即可。对于输出来说，把第一个起始符号对应的Transformer最后一层位置上面串接一个softmax分类层即可。对于分类问题，与GPT一样，只需要增加起始和终结符号，输出部分和句子关系判断任务类似改造；对于序列标注问题，输入部分和单句分类是一样的，只需要输出部分Transformer最后一层每个单词对应位置都进行分类即可。从这里可以看出，上面列出的NLP四大任务里面，除了生成类任务外，Bert其它都覆盖到了，而且改造起来很简单直观。尽管Bert论文没有提，但是稍微动动脑子就可以想到，其实对于机器翻译或者文本摘要，聊天机器人这种生成式任务，同样可以稍作改造即可引入Bert的预训练成果。只需要附着在S2S结构上，encoder部分是个深度Transformer结构，decoder部分也是个深度Transformer结构。根据任务选择不同的预训练数据初始化encoder和decoder即可。这是相当直观的一种改造方法。当然，也可以更简单一点，比如直接在单个Transformer结构上加装隐层产生输出也是可以的。不论如何，从这里可以看出，NLP四大类任务都可以比较方便地改造成Bert能够接受的方式。这其实是Bert的非常大的优点，这意味着它几乎可以做任何NLP的下游任务，具备普适性，这是很强的。</p><p><img src="E:\myBlog\source_posts\e7tSMGZjDHmY1Ck.jpg" alt="img"></p><p>Bert采用这种两阶段方式解决各种NLP任务效果如何？在11个各种类型的NLP任务中达到目前最好的效果，某些任务性能有极大的提升。一个新模型好不好，效果才是王道。</p><p><img src="E:\myBlog\source_posts\RniS8uQhpDmcHN6.jpg" alt="img"></p><p>到这里我们可以再梳理下几个模型之间的演进关系。从上图可见，Bert其实和ELMO及GPT存在千丝万缕的关系，比如如果我们把GPT预训练阶段换成双向语言模型，那么就得到了Bert；而如果我们把ELMO的特征抽取器换成Transformer，那么我们也会得到Bert。所以你可以看出：Bert最关键两点，一点是特征抽取器采用Transformer；第二点是预训练的时候采用双向语言模型。</p><p>那么新问题来了：对于Transformer来说，怎么才能在这个结构上做双向语言模型任务呢？乍一看上去好像不太好搞。我觉得吧，其实有一种很直观的思路，怎么办？看看ELMO的网络结构图，只需要把两个LSTM替换成两个Transformer，一个负责正向，一个负责反向特征提取，其实应该就可以。当然这是我自己的改造，Bert没这么做。那么Bert是怎么做的呢？我们前面不是提过Word2Vec吗？我前面肯定不是漫无目的地提到它，提它是为了在这里引出那个CBOW训练方法，所谓写作时候埋伏笔的“草蛇灰线，伏脉千里”，大概就是这个意思吧？前面提到了CBOW方法，它的核心思想是：在做语言模型任务的时候，我把要预测的单词抠掉，然后根据它的上文Context-Before和下文Context-after去预测单词。其实Bert怎么做的？Bert就是这么做的。从这里可以看到方法间的继承关系。当然Bert作者没提Word2Vec及CBOW方法，这是我的判断，Bert作者说是受到完形填空任务的启发，这也很可能，但是我觉得他们要是没想到过CBOW估计是不太可能的。</p><p>从这里可以看出，在文章开始我说过Bert在模型方面其实没有太大创新，更像一个最近几年NLP重要技术的集大成者，原因在于此，当然我不确定你怎么看，是否认同这种看法，而且我也不关心你怎么看。其实Bert本身的效果好和普适性强才是最大的亮点。</p><p><img src="E:\myBlog\source_posts\LO9j7cIxEJCe2Ay.jpg" alt="img"></p><p>那么Bert本身在模型和方法角度有什么创新呢？就是论文中指出的Masked 语言模型和Next Sentence Prediction。而Masked语言模型上面讲了，本质思想其实是CBOW，但是细节方面有改进。</p><p><img src="E:\myBlog\source_posts\75DVNACdHgtRbS9.jpg" alt="img"></p><p>Masked双向语言模型向上图展示这么做：随机选择语料中15%的单词，把它抠掉，也就是用[Mask]掩码代替原始单词，然后要求模型去正确预测被抠掉的单词。但是这里有个问题：训练过程大量看到[mask]标记，但是真正后面用的时候是不会有这个标记的，这会引导模型认为输出是针对[mask]这个标记的，但是实际使用又见不到这个标记，这自然会有问题。为了避免这个问题，Bert改造了一下，15%的被上天选中要执行[mask]替身这项光荣任务的单词中，只有80%真正被替换成[mask]标记，10%被狸猫换太子随机替换成另外一个单词，10%情况这个单词还待在原地不做改动。这就是Masked双向语音模型的具体做法。</p><p><img src="E:\myBlog\source_posts\MTCajrZPKuF51Ne.jpg" alt="img"></p><p>至于说“Next Sentence Prediction”，指的是做语言模型预训练的时候，分两种情况选择两个句子，一种是选择语料中真正顺序相连的两个句子；另外一种是第二个句子从语料库中抛色子，随机选择一个拼到第一个句子后面。我们要求模型除了做上述的Masked语言模型任务外，附带再做个句子关系预测，判断第二个句子是不是真的是第一个句子的后续句子。之所以这么做，是考虑到很多NLP任务是句子关系判断任务，单词预测粒度的训练到不了句子关系这个层级，增加这个任务有助于下游句子关系判断任务。所以可以看到，它的预训练是个多任务过程。这也是Bert的一个创新。</p><p><img src="E:\myBlog\source_posts\XQ17TqJcYpPoA3i.jpg" alt="img"></p><p>上面这个图给出了一个我们此前利用微博数据和开源的Bert做预训练时随机抽出的一个中文训练实例，从中可以体会下上面讲的masked语言模型和下句预测任务。训练数据就长这种样子。</p><p><img src="E:\myBlog\source_posts\E1vcQhzTsgbLqkF.jpg" alt="img"></p><p>顺带讲解下Bert的输入部分，也算是有些特色。它的输入部分是个线性序列，两个句子通过分隔符分割，最前面和最后增加两个标识符号。每个单词有三个embedding:位置信息embedding，这是因为NLP中单词顺序是很重要的特征，需要在这里对位置信息进行编码；单词embedding,这个就是我们之前一直提到的单词embedding；第三个是句子embedding，因为前面提到训练数据都是由两个句子构成的，那么每个句子有个句子整体的embedding项对应给每个单词。把单词对应的三个embedding叠加，就形成了Bert的输入。</p><p><img src="E:\myBlog\source_posts\wHfCcyhaiXPMx1d.jpg" alt="img"></p><p>至于Bert在预训练的输出部分如何组织，可以参考上图的注释。</p><p><img src="E:\myBlog\source_posts\B5wItXbYpPr619Z.jpg" alt="img"></p><p>我们说过Bert效果特别好，那么到底是什么因素起作用呢？如上图所示，对比试验可以证明，跟GPT相比，双向语言模型起到了最主要的作用，对于那些需要看到下文的任务来说尤其如此。而预测下个句子来说对整体性能来说影响不算太大，跟具体任务关联度比较高。</p><p><img src="E:\myBlog\source_posts\u9mKAEGjp4fa7ys.jpg" alt="img"></p><p>最后，我讲讲我对Bert的评价和看法，我觉得Bert是NLP里里程碑式的工作，对于后面NLP的研究和工业应用会产生长久的影响，这点毫无疑问。但是从上文介绍也可以看出，从模型或者方法角度看，Bert借鉴了ELMO，GPT及CBOW，主要提出了Masked 语言模型及Next Sentence Prediction，但是这里Next Sentence Prediction基本不影响大局，而Masked LM明显借鉴了CBOW的思想。所以说Bert的模型没什么大的创新，更像最近几年NLP重要进展的集大成者，这点如果你看懂了上文估计也没有太大异议，如果你有大的异议，杠精这个大帽子我随时准备戴给你。如果归纳一下这些进展就是：首先是两阶段模型，第一阶段双向语言模型预训练，这里注意要用双向而不是单向，第二阶段采用具体任务Fine-tuning或者做特征集成；第二是特征抽取要用Transformer作为特征提取器而不是RNN或者CNN；第三，双向语言模型可以采取CBOW的方法去做（当然我觉得这个是个细节问题，不算太关键，前两个因素比较关键）。Bert最大的亮点在于效果好及普适性强，几乎所有NLP任务都可以套用Bert这种两阶段解决思路，而且效果应该会有明显提升。可以预见的是，未来一段时间在NLP应用领域，Transformer将占据主导地位，而且这种两阶段预训练方法也会主导各种应用。</p><p>另外，我们应该弄清楚预训练这个过程本质上是在做什么事情，本质上预训练是通过设计好一个网络结构来做语言模型任务，然后把大量甚至是无穷尽的无标注的自然语言文本利用起来，预训练任务把大量语言学知识抽取出来编码到网络结构中，当手头任务带有标注信息的数据有限时，这些先验的语言学特征当然会对手头任务有极大的特征补充作用，因为当数据有限的时候，很多语言学现象是覆盖不到的，泛化能力就弱，集成尽量通用的语言学知识自然会加强模型的泛化能力。如何引入先验的语言学知识其实一直是NLP尤其是深度学习场景下的NLP的主要目标之一，不过一直没有太好的解决办法，而ELMO/GPT/Bert的这种两阶段模式看起来无疑是解决这个问题自然又简洁的方法，这也是这些方法的主要价值所在。</p><p>对于当前NLP的发展方向，我个人觉得有两点非常重要，一个是需要更强的特征抽取器，目前看Transformer会逐渐担当大任，但是肯定还是不够强的，需要发展更强的特征抽取器；第二个就是如何优雅地引入大量无监督数据中包含的语言学知识，注意我这里强调地是优雅，而不是引入，此前相当多的工作试图做各种语言学知识的嫁接或者引入，但是很多方法看着让人牙疼，就是我说的不优雅。目前看预训练这种两阶段方法还是很有效的，也非常简洁，当然后面肯定还会有更好的模型出现。</p><p>完了，这就是自然语言模型预训练的发展史。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      对最近一段时间读的论文以及自己的对NLP领域transformer系列模型的了解做一个总结
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-11-11-论文分享</title>
    <link href="http://yoursite.com/2020/11/11/2020-11-11-%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"/>
    <id>http://yoursite.com/2020/11/11/2020-11-11-%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/</id>
    <published>2020-11-11T07:04:42.000Z</published>
    <updated>2020-11-23T02:13:58.638Z</updated>
    
    <content type="html"><![CDATA[<p><img src="E:\myBlog\source_posts\image-20201111153229465.png" alt="image-20201111153229465"></p><blockquote><p>ICLR 2020</p><p>code url (official tf) : <a href="https://github.com/google-research/ALBERT" target="_blank" rel="noopener" class="uri">https://github.com/google-research/ALBERT</a></p><p>code url (unofficial torch): <a href="https://github.com/huggingface/transformers" target="_blank" rel="noopener" class="uri">https://github.com/huggingface/transformers</a></p><p>cited ：646</p></blockquote><h3 id="问题">问题</h3><p>BERT参数量有点多，消耗算力较大，能否去压缩BERT模型，构建一个轻量级的BERT？</p><h3 id="解决">解决</h3><p>提出两个降低参数的策略 + 自监督损失（SOP）</p><h4 id="降低参数">降低参数</h4><p>降低参数1：factorized embedding parameterization ，将大型的token 嵌入矩阵分解为两个小矩阵。使得</p><p>降低参数2：cross-layer parameter sharing ， 以防随着层数加深参数增加 。 之前的参数共享策略只是关注于transformer这种的encoder-decoder模型中而不是BERT这种预训练-微调模型中。</p><p>这两个策略在减少大量参数且提高参数效率的同时，对BERT的性能影响不大</p><p>具体参考如下：</p><p><img src="E:\myBlog\source_posts\image-20201121140455423.png" alt="image-20201121140455423"></p><h4 id="自损失监督sop">自损失监督(SOP)</h4><p>代替BERT中的NSP策略，使得模型更加关注句子内的一致性</p><h4 id="结果">结果</h4><p>模型ALBERT：提高训练速度，减少内存消耗</p><p>在语言理解任务上达到SOTA水平，具体参考如下：</p><p><img src="E:\myBlog\source_posts\image-20201121142012520.png" alt="image-20201121142012520"></p><h3 id="模型">模型</h3><h4 id="config-设置">config 设置</h4><p>ALBERT模型的config设置和BERT基本一致</p><p><img src="E:\myBlog\source_posts\image-20201121143009555.png"></p><h4 id="factorized-embedding-parameterization">factorized embedding parameterization</h4><p>之前的BERT模型都是Embedding size = hiden size ， （是次优的）</p><p><img src="E:\myBlog\source_posts\image-20201122094115198.png" alt="image-20201122094115198"></p><p><img src="E:\myBlog\source_posts\image-20201122094556848.png" alt="image-20201122094556848"></p><h3 id="实验">实验</h3><h3 id="总结">总结</h3><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      记录值得分享的论文;
《ALBERT》: 一个轻量级的BERT

    
    </summary>
    
    
      <category term="论文分享" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-11-09-pytorch反向传播以及参数更新理解</title>
    <link href="http://yoursite.com/2020/11/09/2020-11-09-pytorch%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%BB%A5%E5%8F%8A%E5%8F%82%E6%95%B0%E6%9B%B4%E6%96%B0%E7%90%86%E8%A7%A3/"/>
    <id>http://yoursite.com/2020/11/09/2020-11-09-pytorch%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%BB%A5%E5%8F%8A%E5%8F%82%E6%95%B0%E6%9B%B4%E6%96%B0%E7%90%86%E8%A7%A3/</id>
    <published>2020-11-09T07:26:05.000Z</published>
    <updated>2020-11-10T10:43:40.718Z</updated>
    
    <content type="html"><![CDATA[<h3 id="反向传播以及更新">反向传播以及更新</h3><h4 id="方法一手动计算变量">方法一：手动计算变量</h4><p>这种方法不常用，因为一般的模型参数太多了</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="comment"># 定义参数</span></span><br><span class="line">w1 = Variable(torch.FloatTensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]),requires_grad = <span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 定义输出</span></span><br><span class="line">d = torch.mean(w1)</span><br><span class="line"><span class="comment"># 反向求导</span></span><br><span class="line">d.backward()</span><br><span class="line"><span class="comment"># 定义学习率等参数</span></span><br><span class="line">lr = <span class="number">0.001</span></span><br><span class="line"><span class="comment"># 手动更新参数</span></span><br><span class="line">w1.data.zero_() <span class="comment"># BP求导更新参数之前,需先对导数置0</span></span><br><span class="line">w1.data.sub_(lr*w1.grad.data)<span class="number">12345678910111213</span></span><br></pre></td></tr></tbody></table></figure><blockquote><p><em>一个网络中通常有很多变量,如果按照上述的方法手动求导,然后更新参数,是很麻烦的,这个时候可以调用torch.optim</em></p></blockquote><h4 id="方法二使用torch.optim">方法二:使用torch.optim</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="comment"># 这里假设我们定义了一个网络,为net</span></span><br><span class="line">steps = <span class="number">10000</span></span><br><span class="line"><span class="comment"># 定义一个optim对象</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr = <span class="number">0.01</span>)</span><br><span class="line"><span class="comment"># 在for循环中更新参数</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(steps):</span><br><span class="line">  optimizer.zero_grad() <span class="comment"># 对网络中参数当前的导数置0，清零梯度缓存</span></span><br><span class="line">  output = net(input) <span class="comment"># 网络前向计算</span></span><br><span class="line">  loss = criterion(output, target) <span class="comment"># 通过定义损失函数：criterion，计算误差，得到网络的损失值：loss；</span></span><br><span class="line">  loss.backward() <span class="comment">#　通过loss.backward()完成误差的反向传播，通过pytorch的内在机制完成自动求导得到每个参数的梯度。</span></span><br><span class="line">  optimizer.step() <span class="comment"># 更新参数123456789101112131415</span></span><br></pre></td></tr></tbody></table></figure><blockquote><p>torch.optim只用于参数更新和对参数的梯度置０，不能计算参数的梯度，在使用torch.optim进行参数更新之前，需要写前向与反向传播求导的代码</p></blockquote><h4 id="注">注</h4><p>loss是反向传播整个计算图/模型（有一条传播路径）的节点参数，其中一个模型可以认为是一个连通图，是由数据传播的，比如encoder和decoder之间会有隐藏向量Z进行连接，那么就是一个计算图，那么loss反向传播就会更新所有的参数。参数在定义时默认就是可动态更新的。</p><h3 id="variable-parameter的区别">Variable &amp; Parameter的区别</h3><p>之所以有Variable这个数据结构，是为了引入计算图（自动求导），方便构建神经网络。也就是一般模型网络（计算图）的输入是Variable类型的，是要外部给值的，返回的是tensor类型。</p><p>不同于Parameter，Parameter一般是随机初始化，然后根据loss反向传播被动更新值</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x = Variable(torch.Tensor(array), requires_grad = True) #可以自求导更新，若一个节点requires_grad被设置为True，那么计图中所有依赖它求得的节点的requires_grad都为True</span><br></pre></td></tr></tbody></table></figure><p>Pytorch主要通过引入<code>nn.Parameter</code>类型的变量和<code>optimizer机制</code>来解决自动更新多个参数的问题。</p><p>Parameter是Variable的子类，本质上和后者一样，只不过<strong>parameter默认是求梯度的</strong>，同时一个网络net中的parameter变量是可以通过 <code>net.parameters()</code> 来很方便地访问到的，只需将网络中所有需要训练更新的参数定义为Parameter类型，再佐以optimizer，就能够完成所有参数的更新了</p><p>Parameter是torch.autograd.Variable的一个字类，常被用于Module的参数。例如权重和偏置。自动加入参数列表，可以进行保存恢复。和Variable具有相同的运算。</p><p>Parameter的require_grad默认设置为true。Varaible默认设置为False.</p><p>Parameters类是<a href="https://pytorch.apachecn.org/docs/1.0/tensors.html#torch.Tensor" target="_blank" rel="noopener"><code>Tensor</code></a> 的子类, 不过相对于它的父类，Parameters类有一个很重要的特性就是当其在 <a href="https://pytorch.apachecn.org/docs/1.0/nn.html#torch.nn.Module" target="_blank" rel="noopener"><code>Module</code></a>类中被使用并被当做这个<a href="https://pytorch.apachecn.org/docs/1.0/nn.html#torch.nn.Module" target="_blank" rel="noopener"><code>Module</code></a>类的模块属性的时候，那么这个Parameters对象会被自动地添加到这个<a href="https://pytorch.apachecn.org/docs/1.0/nn.html#torch.nn.Module" target="_blank" rel="noopener"><code>Module</code></a>类的参数列表(list of parameters)之中，同时也就会被添加入此<a href="https://pytorch.apachecn.org/docs/1.0/nn.html#torch.nn.Module" target="_blank" rel="noopener"><code>Module</code></a>类的 <a href="https://pytorch.apachecn.org/docs/1.0/nn.html#torch.nn.Module.parameters" target="_blank" rel="noopener"><code>parameters()</code></a>方法所返回的参数迭代器中。而Parameters类的父类Tensor类也可以被用为构建模块的属性，但不会被加入参数列表。这样主要是因为，有时可能需要在模型中存储一些非模型参数的临时状态，比如RNN中的最后一个隐状态。而通过使用非<a href="https://pytorch.apachecn.org/docs/1.0/nn.html#torch.nn.Parameter" target="_blank" rel="noopener"><code>Parameter</code></a>的Tensor类，可以将这些临时变量注册(register)为模型的属性的同时使其不被加入参数列表。</p><p><strong>我们可以这样简单区分，在计算图中，数据（包括输入数据和计算过程中产生的feature map等）是 variable 类型，该类型不会被保存到模型中。 </strong></p><p><strong>网络的权重是 parameter 类型，在计算过程中会被更新，将会被保存到模型中。</strong></p><blockquote><p><a href="https://www.jianshu.com/p/cb739922ce88" target="_blank" rel="noopener" class="uri">https://www.jianshu.com/p/cb739922ce88</a></p><p><a href="https://zhoef.com/2019/08/12/16_Pytorch_Basic/" target="_blank" rel="noopener" class="uri">https://zhoef.com/2019/08/12/16_Pytorch_Basic/</a></p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      pytorch反向传播以及参数更新理解
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-11-09-安装jupyter远程访问服务器</title>
    <link href="http://yoursite.com/2020/11/09/2020-11-09-%E5%AE%89%E8%A3%85jupyter%E8%BF%9C%E7%A8%8B%E8%AE%BF%E9%97%AE%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
    <id>http://yoursite.com/2020/11/09/2020-11-09-%E5%AE%89%E8%A3%85jupyter%E8%BF%9C%E7%A8%8B%E8%AE%BF%E9%97%AE%E6%9C%8D%E5%8A%A1%E5%99%A8/</id>
    <published>2020-11-09T06:19:07.000Z</published>
    <updated>2020-11-19T08:42:52.995Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言">前言</h3><p>想要去用jupyter远程访问我们实验室的服务器，于是参考网上教程如下：</p><h3 id="具体操作">具体操作</h3><h4 id="一.-ubuntu下安装jupyter-notebook">一. Ubuntu下安装jupyter notebook</h4><h5 id="使用anaconda安装">1. 使用Anaconda安装</h5><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install jupyter notebook</span><br></pre></td></tr></tbody></table></figure><h5 id="使用pip安装">2. 使用pip安装</h5><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install jupyter notebook</span><br></pre></td></tr></tbody></table></figure><h4 id="二.-jupyter-notebook-配置">二. Jupyter notebook 配置</h4><h5 id="生成配置文件">1. 生成配置文件</h5><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter notebook --generate-config</span><br></pre></td></tr></tbody></table></figure><h5 id="创建密码">2. 创建密码</h5><p>使用python中的<code>passwd()</code>创建密码，终端输入<code>ipython</code>打开ipython并输入:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">from</span> notebook.auth <span class="keyword">import</span> passwd</span><br><span class="line">In [<span class="number">2</span>]: passwd()</span><br><span class="line">Enter password: ******</span><br><span class="line">Verify password: ******</span><br><span class="line">Out [<span class="number">2</span>]: <span class="string">'sha1:...'</span>  <span class="comment">#应该是密钥</span></span><br></pre></td></tr></tbody></table></figure><p>复制Out [2] 显示的密码（'sha1:...' 包括引号）。</p><h5 id="修改jupyter-notebook的配置文件">3. 修改jupyter notebook的配置文件</h5><ul><li>打开配置文件</li></ul><figure class="highlight vim"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">vim</span> ~/.jupyter/jupyter_notebook_config.<span class="keyword">py</span></span><br></pre></td></tr></tbody></table></figure><ul><li>在该文件中做如下修改或直接在文件尾端添加：</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">c.NotebookApp.allow_remote_access = <span class="literal">True</span> <span class="comment">#允许远程连接</span></span><br><span class="line">c.NotebookApp.ip=<span class="string">'*'</span> <span class="comment"># 设置所有ip皆可访问</span></span><br><span class="line">c.NotebookApp.password = <span class="string">u'sha:..'</span> <span class="comment">#之前复制的密码</span></span><br><span class="line">c.NotebookApp.open_browser = <span class="literal">False</span> <span class="comment"># 禁止自动打开浏览器</span></span><br><span class="line">c.NotebookApp.port =<span class="number">23333</span> <span class="comment">#任意指定一个端口 ，我们指定的是23333</span></span><br></pre></td></tr></tbody></table></figure><h4 id="启动jupyter-notebook">4. 启动jupyter notebook</h4><p>终端输入：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter notebook</span><br></pre></td></tr></tbody></table></figure><p>或使用<code>nohup</code>后台运行 jupyter notebook:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup jupyter notebook &gt;~/jupyter.log <span class="number">2</span>&gt;&amp;<span class="number">1</span> &amp;  <span class="comment">#后台挂起，并且将输出重定向到jupyter.log文件中</span></span><br></pre></td></tr></tbody></table></figure><h4 id="远程访问jupyter-notebook">5. 远程访问jupyter notebook</h4><p>本地浏览器输入<code>http://(服务器地址):(配置文件中设定的端口)</code>； 假设服务器地址为210.30.97.69，配置的端口为23333，这里的浏览器输入地址应为<code>http://210.30.97.69:23333</code>； 即可访问jupyter notebook。</p><h3 id="注">注</h3><p>只能通过和服务器所在的局域网来访问，也就是校园网可以正常访问服务器，外网不可。试过用电脑连接我手机热点，无法访问</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      安装jupyter远程访问服务器
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-11-07-论文分享</title>
    <link href="http://yoursite.com/2020/11/07/2020-11-07-%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"/>
    <id>http://yoursite.com/2020/11/07/2020-11-07-%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/</id>
    <published>2020-11-07T13:48:05.000Z</published>
    <updated>2020-11-09T06:32:04.995Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2020/11/09/7bSkvNlUTsB6VoC.png" alt="image-20201107220502247"></p><blockquote><p>ACL 2020</p><p>code url (official torch) : <a href="https://github.com/namisan/mt-dnn" target="_blank" rel="noopener" class="uri">https://github.com/namisan/mt-dnn</a></p><p>被引用次数：11</p></blockquote><h3 id="背景">背景</h3><p>泛化性和鲁棒性对于机器学习来说是很重要的，对抗训练可以增强鲁棒性，但是同时也会使泛化性受到损失；</p><p>BERT等大型自然语言模型已经在泛化性方面取得了巨大的进步，然而这种预训练模型容易受到对抗攻击</p><h3 id="问题">问题</h3><p>如何使得大型NLP模型兼得泛化性和鲁棒性？</p><h3 id="解决">解决</h3><p>提出了一种通用算法ALUM (Adversarial training for large neural Language Models), 把对抗训练用到了预训练和微调两个阶段，通过对抗训练来提高模型的泛化性和鲁棒性。</p><p>对抗训练的方法是针对embedding space，通过最大化对抗损失、最小化模型损失的方式进行对抗，在下游任务上取得了一致的效果提升。</p><p>这种对抗训练方法不仅能够在BERT上有提高，而且在RoBERTa这种已经预训练好的模型上也能有所提高，说明对抗训练的确可以帮助模型纠正易错点。</p><p>算法可以应用在任何基于transformer的语言模型中</p><h3 id="贡献">贡献</h3><p><img src="https://i.loli.net/2020/11/09/sTFuUcA7MvNkre5.png" alt="image-20201108095152820"></p><h3 id="模型">模型</h3><h4 id="准备">准备</h4><p>tokenization使用的是BPE（Byte-PairEncoding）</p><p>模型基于BERT和 RoBERTa模型，但是在训练策略上与前两者有所改动如下：</p><p>在一个epoch中，掩码率以每经过20%的epoch，增加5%掩码率的增速使得掩码率从5%增加到25%</p><p><img src="https://i.loli.net/2020/11/09/gd1sfkUao5wqhrc.png" alt="image-20201108100621048"></p><h4 id="标准训练目标函数">标准训练目标函数</h4><p>标准的预训练和微调函数都可以认为是在训练数据上进行最小化标准差</p><p><img src="https://i.loli.net/2020/11/09/2C9JdbBunrHYxTl.png" alt="image-20201108101520342"></p><h4 id="对抗训练">对抗训练</h4><p><img src="https://i.loli.net/2020/11/09/iLw28ZgrYTVWbnU.png" alt="image-20201108133249939"></p><h4 id="alum算法">ALUM算法</h4><p>基于几个关键想法：</p><ol type="1"><li><p>扰动embedding空间，优于直接对输入文本应用扰动。</p></li><li><p>通过虚拟对抗训练为标准目标添加正则化项。比传统的对抗训练有效果，尤其是在标签有噪声时。</p></li></ol><p><img src="https://i.loli.net/2020/11/09/O5fCEFvolWSPiDZ.png" alt="img"></p><p>其中超参α用于调节标准差和鲁棒差的平衡</p><p>（预训练α = 10，微调α = 1）</p><ul><li>因为有最大化操作，所以训练昂贵。</li><li>有利于embedding邻域的标签平滑。</li></ul><h4 id="算法流程">算法流程</h4><p>首先使用标准目标（1）训练模型；然后使用虚拟对抗训练（3）继续训练。</p><p><img src="https://i.loli.net/2020/11/09/mYZrkBKpTVelFsa.png" alt="image-20201108103125543"></p><p><img src="https://i.loli.net/2020/11/09/mdbNu7MXOc5WZy6.png" alt="image-20201108133930095"></p><h3 id="总结">总结</h3><p>本文提出了一种通用的对抗性训练算法ALUM：</p><ul><li><p>对抗预训练可以显著提高泛化能力和鲁棒性。</p></li><li><p>ALUM大大提高了BERT和RoBERTa在各种NLP任务中的准确性，并且可以与对抗微调相结合以获得进一步的收益。</p></li><li><p>未来的发展方向：</p><ul><li>进一步研究对抗性预训练在提高泛化和鲁棒性方面的作用；</li><li>对抗性训练加速；</li><li>将ALUM应用于其他领域。</li></ul></li></ul><p>论文提出了一个通用的模型无关的对抗训练算法架构，可以应用在任何基于transformer的语言模型中。可以尝试去结合模型</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      记录值得分享的论文;
《Adversarial Training for Large Neural Language Models》: 提出一个对抗训练算法ALUM，用于提高模型的鲁棒性。此算法可以应用在任何基于transformer的语言模型中

    
    </summary>
    
    
      <category term="论文分享" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-11-06-论文分享</title>
    <link href="http://yoursite.com/2020/11/06/2020-11-06-%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"/>
    <id>http://yoursite.com/2020/11/06/2020-11-06-%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/</id>
    <published>2020-11-06T01:56:14.000Z</published>
    <updated>2020-11-11T07:13:54.487Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2020/11/06/dEebUnIiFygwDWk.png" alt="image-20201106095828031"></p><blockquote><p>EMNLP 2020</p><p>code url (official torch) : <a href="https://github.com/ChunyuanLI/Optimus" target="_blank" rel="noopener" class="uri">https://github.com/ChunyuanLI/Optimus</a></p><p>被引用次数：9</p></blockquote><h3 id="背景">背景</h3><p>在NLP领域中，VAE是一个很有效的生成模型和表示学习框架</p><p>PLM（Pre-trained language models）一般可以分为两种。（1）基于transformer的encoder的BERT， 用于自然语言理解任务。它能够输出一个上下文的表示信息，用于下游任务；（2）基于transformer的decoder的GPT-2，用于自然语言生成任务。它能够以自回归的方式产生文本序列（机器翻译）</p><h3 id="问题">问题</h3><p>前人想着去结合语言理解任务和语言生成任务，如UniLM、T5模型，效果有提升，但是这些模型缺少一种在紧密（compact）空间（低维空间）中显式的建模，导致很难在一个abstract level去控制语言的生成和表示</p><p><img src="https://i.loli.net/2020/11/06/3hC7D1c2vMRGubZ.png" alt="image-20201106102922590"></p><p>VAE可以克服这种局限性，可以生成higher-level 的句子表示，从而控制low-level 的word-by-word generation。</p><p>但是目前VAE都是应用在浅层的模型中，例如two-layer LSTMs ，这限制了模型的表现</p><h3 id="解决">解决</h3><p>提出了OPTIMUS， the first large-scale pre-trained deep latent variable models for natural language. ，一个统一的潜在编码空间在大型文本库（large text corpus）训练完之后，在多个下游任务（自然语言理解、自然语言生成）中进行微调</p><p>以下是OPTIMUS的优点，它结合了BERT和GPT-2的优势，用于处理自然语言任务，同时相比于BERT和GPT-2，克服了它们的局限性</p><p><img src="https://i.loli.net/2020/11/06/1pkceDbh7qXGxzK.png" alt="image-20201106103607562"></p><p><img src="https://i.loli.net/2020/11/06/BbtwAayhgp8frzD.png" alt="image-20201106103657363"></p><h3 id="贡献">贡献</h3><ul><li><p>提出首个大规模预训练隐变量生成模型OPTIMUS；</p></li><li><p>高效地将隐变量和预训练的GPT-2相结合（ Latent vector injection），提出两种结合方法；</p></li><li><p>发现大规模预训练可以减缓KL Vanishing的问题；</p></li><li><p>在多个任务上取得显著的效果。</p></li></ul><h3 id="模型">模型</h3><h4 id="目标函数">目标函数</h4><p>一般的自然语言模型（如GPT-2）的生成目标，依靠前面的输出的token来预测后面的token，通常训练是通过maximum likelihood estimate (MLE). 但是这样也有局限性，前面已经提到了</p><p><img src="https://i.loli.net/2020/11/06/EN3LTgnCxiowcJ8.png" alt="image-20201106105452675"></p><p>在生成阶段（训练阶段），模型的生成基于隐变量z，对于给定的文本x，VAE的生成目标，相比于公式（1）多了一个条件z，即显式地依赖z。</p><p><strong>z是高层次的语义特征，来指导生成低层次的x，即句法和词汇</strong></p><p><img src="https://i.loli.net/2020/11/06/piNRVdgaPS3yxLI.png" alt="image-20201106105946819"></p><p>这里θ表示的是用于文本生成的<strong>解码器</strong>。而隐变量是通过一个<strong>编码器</strong>得到的，可以形式化为<img src="https://i.loli.net/2020/11/06/NMvZcI8k6R4KxUV.png" alt="image-20201106115812296"></p><p>此时的证据下界（ELBO）就是</p><p><img src="https://i.loli.net/2020/11/06/6rVTSBlHvmYnqLi.png" alt="image-20201106115847275"></p><p>在本文中，添加了一个超参β， 用于控制训练过程。</p><p>所以目标函数可以转化为如下形式，<code>Lβ</code></p><p><img src="https://i.loli.net/2020/11/06/fWcLjZQ1iEOty5F.png" alt="image-20201106120223123"></p><p><code>LE</code>是重构损失（ (or negative log-likelihood(NLL)）， <code>LR</code>是KL散度（正则项），用于让生成的z逼近先验<code>p(z)</code></p><h4 id="模型架构">模型架构</h4><p><img src="https://i.loli.net/2020/11/06/eQYDwAVuWnOs25d.png" alt="image-20201106120651825"></p><p>可以看出来，模型架构比较简单，但是一些细节也要考虑一下</p><p>基本流程如下：</p><p>1.使用预训练好的BERT和GPT-2参数，用于OPTIMUS模型encoder和decoder参数的初始化；</p><p>此时BERT(L=12,H=768,A=12,Total Parameters=110M) and GPT-2 (L=12,H=768, A=12，,Total Parameters=117M），其中L表示transformer block的层数；H表示中间隐藏层的维度；A表示自注意力头的个数</p><blockquote><p>可以发现，BERT和GPT-2的超参L、H、A都是一样的</p></blockquote><ol start="2" type="1"><li>对于初始化后的OPTIMUS，在大型文本库（large text corpus）的训练集下进行预训练</li><li>预训练完OPTIMUS，再在具体的下游任务上进行微调</li></ol><h4 id="connecting-bert-gpt-2">Connecting BERT &amp; GPT-2</h4><p>同时在连接BERT和GPT-2中，存在一些问题</p><h5 id="问题1-tokenization如何分词">问题1 Tokenization（如何分词）</h5><p>BERT和GPT-2采取了不同的分词方法，如何去表示句子？</p><h5 id="解决-1">解决</h5><p>在BERT中用的是Word Piece Embeddings (WPE)分词方法，在GPT-2中, 用的是 Byte Pair Encoding (BPE)，</p><p>本文中同时采取了两种方法</p><p><img src="https://i.loli.net/2020/11/06/rXBTJk6bOqMG5Iy.png" alt="image-20201106122651781"></p><p>等于没说。。。</p><p>是否可以统一分词方法 ？</p><h5 id="问题2-融合隐变量和gpt-2">问题2 融合隐变量和GPT-2</h5><p>如何高效地将Z融合进GPT-2中？</p><h5 id="解决-2">解决</h5><h5 id="section"></h5><p><img src="https://i.loli.net/2020/11/06/SPL4c7rhjRyznHx.png" alt="image-20201106104604195"></p><p>该如何把隐变量<code>z</code>提供给解码器呢？</p><p>本文提供两种方法，分别为记忆（Memory）和嵌入（Embedding）：</p><p><img src="https://i.loli.net/2020/11/06/2cu9DGXztrTRFLd.png" alt="image-20201106114441841"></p><p>经过实验验证，使用Memory比Embedding方法更有效，作者给出理由如下，就是Memory能够使得decoder在每一层都能直接获得潜在信息，而Embedding只能在输入输出才能获得信息。 Memory能从潜在信息中获得更多的信息用于生成任务</p><p><img src="https://i.loli.net/2020/11/06/qebzUvKSh1AOC2N.png" alt="image-20201106114209547"></p><blockquote><p><strong>在本文中，默认Memory和Embedding方法一起使用</strong></p></blockquote><h4 id="optimus的预训练">OPTIMUS的预训练</h4><p>存在一个问题，就是当VAE和auto regressive models在一起训练时，会出现“KL-vanishing problem”，或者说是“posterior collapse”</p><p><img src="https://i.loli.net/2020/11/06/pi7UOIJXy5dQqFg.png" alt="image-20201106130104016"></p><p>ELBO 包含reconstruction loss和KL loss两部分。我们的目标是最大化ELBO，等价于最小化KL项并最大化reconstruction项。存在如下问题：</p><p>问题1.对于reconstruction部分，当扮演p(x|z)角色的decoder足够强大，仅凭自己就可以model q(x)分布，那么也没有必要去依赖z。</p><p>问题2.对于KL项，如果简单的将data x和latent variable z无关并让q(z|x) = q(z) = p(z)，即posterior退化为和prior一样的高斯，<strong>KL就可以取得最小值0</strong>。</p><p>所以针对上述问题，本文在预训练的时候做了如下优化：</p><p><img src="https://i.loli.net/2020/11/06/fWcLjZQ1iEOty5F.png" alt="image-20201106120223123"></p><ul><li><p>对β使用循环调度（cyclical schedule），一共10个 periods 来加强β的作用；</p></li><li><p>每一个 period中，在训练的前一半，设置β=0 只训练encoder，避免了上述问题1；对后一半的前一半，将其增长到1，对最后四分之一，固定为1；</p></li><li><p>当β≠0的时候，加入KL thresholding scheme （KL 阈值），保证KL项始终大于一个常数 λ，这样可以避免了上述问题2，避免LR项取得最小值0。此时LR被替换为 hinge loss</p><p><img src="https://i.loli.net/2020/11/06/n3ZKDu7pEOSWaiN.png" alt="image-20201106131315827"></p></li></ul><p><img src="https://i.loli.net/2020/11/06/1VFDdL2fcsTexPj.png" alt="image-20201106130006529"></p><h3 id="实验">实验</h3><p>预训练OPTIMUS之后，剩下的就是对不同的任务进行微调了。本文在三类任务上实验：语言模型、受限文本生成和自然语言理解</p><p><img src="https://i.loli.net/2020/11/06/2YQ7lmnkhc618pV.png" alt="image-20201106110800369"></p><h3 id="总结">总结</h3><p>VAE和PLM在自然语言处理中都是很重要的部分，自然会想到将二者结合起来</p><p>总体来说，论文没太多创新点，而且在正文部分有些故弄玄虚，明明一个很简单的概念，说的让人无法理解，非要绕个弯子，可能这样会让人觉得更高深些吧，但是对于后来的研究者来说很痛苦</p><p>代码还没有看，准备再读读代码，加深模型的理解</p><h4 id="更改方向">更改方向</h4><p>1.用更好的方法去解决KL vanishing 问题</p><p>2.统一分词方法是否有更好的结果</p><p>3.使用最新的VAE方法/PLM方法去解决问题</p><p>4.结合T-CVAE框架</p><p>5.因为NLP模型普遍对于对抗攻击很敏感，所以增加对抗思想，提高模型的鲁棒性。结合论文《Adversarial Training for Large Neural Language Models》（ACL 2020）中的通用算法ALUM去解决对抗攻击的问题</p><h3 id="参考">参考</h3><blockquote><p>OPTIMUS <a href="https://zhuanlan.zhihu.com/p/143517152" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/143517152</a></p><p>KL vanishing <a href="https://zhuanlan.zhihu.com/p/64071467" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/64071467</a></p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      记录值得分享的论文;
《OPTIMUS》: 结合VAE和BERT、GPT-2,提出首个大规模预训练隐变量生成模型OPTIMUS，解决自然语言生成和理解任务

    
    </summary>
    
    
      <category term="论文分享" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-11-04-论文分享</title>
    <link href="http://yoursite.com/2020/11/04/2020-11-04-%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"/>
    <id>http://yoursite.com/2020/11/04/2020-11-04-%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/</id>
    <published>2020-11-04T13:59:43.000Z</published>
    <updated>2020-11-07T14:00:20.434Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2020/11/05/d4IuoeCYcaxO8v9.png" alt="image-20201104220110364"></p><blockquote><p>IJCAI 2019</p><p>code url (official tf ，无torch版本) : <a href="https://github.com/sodawater/T-CVAE" target="_blank" rel="noopener" class="uri">https://github.com/sodawater/T-CVAE</a></p><p>被引用次数：12</p></blockquote><h3 id="背景">背景</h3><p>故事补全是一个非常具有挑战性的任务，即为一个不完整的故事生成缺失的情节。</p><p>它涉及两个方面：<strong>理解和生成</strong>。故事理解包括识别人物角色等。生成是基于理解的下一步，即根据给定故事中的线索进行推理。一个好的故事情节应该是有意义的和连贯的上下文。此外，输入文本的不连续性使得理解和生成更加困难。</p><p>本模型的任务：给定一个故事的任何四个句子，目标是生成缺失的句子，即缺失的情节，来完成这个故事。 （基于ROCStories的常识故事语料库）</p><p><img src="https://i.loli.net/2020/11/05/iSonHTZ7I5g9Atz.png" alt="根据缺失的故事，不同的补全句子"></p><h3 id="问题">问题</h3><p>前人的研究都关注于为不完整的故事选择或产生一个合理的结局。这些任务是我们故事完成任务的特殊化，因此先前的方法不适合生成故事的开始或中间情节。并且倾向于生成泛型和非连贯性的情节。</p><p>1.如何去补全缺失中间部分的故事 ？</p><p>2.怎样使补全的句子是<strong>有意义的、连贯的、多样的</strong> ？</p><h3 id="解决">解决</h3><p>我们提出了一种新的<strong>基于transformer的条件变量自动编码模型</strong>（T-CVAE）</p><p>1.Transformer：作为模型基础，并采用了一个改进的<strong>具有共享自我注意层</strong>的Transformer，这种共享自我注意层能够使解码器同时关注到编码器和解码器的状态，以此能够使模型获取更多的上下文线索。</p><p>2.条件变分自编码模型：提高生成的多样性和一致性</p><h3 id="贡献">贡献</h3><p><img src="https://i.loli.net/2020/11/05/aH6sB72CvqQTPoj.png" alt="贡献"></p><p>可以看出，并没有太多的贡献。</p><h3 id="模型">模型</h3><h4 id="模型流程">模型流程</h4><p><img src="https://i.loli.net/2020/11/05/3HsRKVLjtPhfcx2.png" alt="image-20201105104914786"></p><p>数据在encoder和decoder的流动</p><p><img src="https://i.loli.net/2020/11/05/iJFKlqYbGe3CnjO.png" alt="image-20201105105933096"></p><h4 id="后验网络-pzxy">后验网络 P(z|x,y)</h4><p><img src="https://i.loli.net/2020/11/05/b7exp5tuoq4W6V3.png" alt="image-20201105105357954"></p><h4 id="先验网络pzx">先验网络P(z|x)</h4><p><img src="https://i.loli.net/2020/11/05/TxVdC3bBXFpjnUY.png" alt="image-20201105105419533"></p><h4 id="组合层combination-layer">组合层（combination layer）</h4><p>并不是利用z来直接初始化decoder的状态，而是利用了组合层。之后再经过Linear层和softmax层输出最终的预测</p><p><img src="https://i.loli.net/2020/11/05/owdjV8YQah6eqpB.png" alt="image-20201105105628307"></p><h4 id="目标函数">目标函数</h4><p>由于z上的积分是难以解决的，因此我们应用变分推理并优化相应的证据下限（ELBO）：</p><p><img src="https://i.loli.net/2020/11/05/I3iZ2EebhXOzyTk.png" alt="image-20201105104715312"></p><p><img src="https://i.loli.net/2020/11/05/ACP2tKvEyraOoJh.png" alt="image-20201105104837678"></p><p>训练目标：</p><p>1.最大化重构y的概率，这样可以使得后验网络和情节生成器 ( p(y|x,z) ) 做出的预测更加接近于标准值 ；</p><p>2.最小化z的先验分布和后验分布的KL散度，这样当标准值不存在的时候（推理），可以促使先验网络去产生合理的概率分布</p><h3 id="实验">实验</h3><p><img src="https://i.loli.net/2020/11/05/2KNSnML9rlE6dvT.png" alt="image-20201104222356336"></p><p><img src="https://i.loli.net/2020/11/05/9z3pS2onE6vlBgy.png" alt="消融实验"></p><p><img src="https://i.loli.net/2020/11/05/ahGK4kXudPfMU25.png" alt="image-20201104222411554"></p><h3 id="总结">总结</h3><p>总体来说，论文最大的贡献就是将CVAE和transformer结合了起来，处理故事补全的问题，transformer模型上并没有太大的创新。但是VAE和transformer系列模型结合的思路值得我去借鉴。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      记录值得分享的论文;
《T-CVAE》 :　结合VAE和transformer，提出基于transformer的条件变量自动编码模型（T-CVAE），用于解决故事补全的任务

    
    </summary>
    
    
      <category term="论文分享" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-10-31-注意力机制总结</title>
    <link href="http://yoursite.com/2020/10/31/2020-10-31-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2020/10/31/2020-10-31-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%80%BB%E7%BB%93/</id>
    <published>2020-10-31T02:19:17.000Z</published>
    <updated>2020-10-31T08:27:16.216Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言">前言</h3><p>注意力模型最近几年在深度学习各个领域被广泛使用，无论是图像处理、语音识别还是自然语言处理的各种不同类型的任务中，都很容易遇到注意力模型的身影。所以，了解注意力机制的工作原理对于关注深度学习技术发展的技术人员来说有很大的必要。</p><blockquote><p>RNN做机器翻译有它自身的弱点，Attention正是为了克服这个弱点而出现的。所以，要理解Attention，就要搞明白两件事： - RNN在做机器翻译时有什么弱点 - Attention是如何克服这个弱点的</p></blockquote><h3 id="rnn做机器翻译的经典思路-encoder-decoder">RNN做机器翻译的经典思路 encoder-decoder</h3><p>用RNN做机器翻译时，通常需要两个RNN网络，一个用来将接收待翻译语句，对其进行编码，最后输出一个vector，这个网络叫encoder。然后，该vector会作为输入，传给另一个RNN网络，该网络用来根据vector产生目标语言的翻译语句，这个网络叫做decoder。如下图所示：</p><p><img src="https://i.loli.net/2020/10/31/UOF2qxEoRB7ftsP.png" alt="image-20201031104117636"></p><p>上图中间的Context就是我们这里说的第一个RNN产生的vector</p><h3 id="encoder-decoder的缺点在哪里">encoder-decoder的缺点在哪里？</h3><p>encoder-decoder最大的缺点是，encoder接收了不管多长的语句，最后输出的只是最后一个vector，当语句很长时，这个vector能否有效地表示该语句是很值得怀疑的。 如何解决这个问题呢？我们很自然会想到，第一个RNN其实在中间会产生很多输出，这些输出都被我们抛弃了，我们只用了最后的一个。如果能利用上中间的输出，兴许可以解决问题。Attention正是利用上了这些中间的输出。</p><h3 id="attention是如何利用中间的输出的">Attention是如何利用中间的输出的</h3><p>先上图，再来解释：</p><p><img src="https://i.loli.net/2020/10/31/ixs6baohzDmjfPn.png" alt="image-20201031104152681"></p><p>上图中的A是我们的encoder， B是我们的decoder。 可以想象，A网络接收了一个四个字的句子，对每个字都产生了一个输出（这些输出都是一个vector），我们称其为s1，s2，s3，s4。</p><p>我们看上图的B网络，在第一个B产生的hidden state（称其为h1）除了传给下一个cell外，还传到了A网络，这里就是Attention发挥作用的地方，我们来看看发生了什么。</p><p><strong>第一步</strong>： h1 分别与s1，s2，s3，s4做点积，产生了四个数，称其为m1，m2，m3，m4（这些都是标量，不是向量了！）</p><p><strong>第二步</strong>： m1，m2，m3，m4 传到一个softmax层，产生一个概率分布a1，a2，a3， a4。</p><p><strong>第三步</strong>： 将a1，a2，a3， a4 与s1，s2，s3，s4分别相乘，再相加，得到得到一个vector，称其为Attention vector。</p><p><strong>第四步</strong>：</p><p>Attention vector 将作为输入传到B网络的第二个cell中，参与预测。</p><p>以上就是Attention机制的基本思想了。我们看到，Attention vector 实际上融合了s1，s2，s3，s4的信息，具体的融合是用一个概率分布来达到的，而这个概率分布又是通过B网络上一个cell的hidden state与s1，s2，s3，s4进行点乘得到的。 Attention vector实际上达到了让B网络聚焦于A网络输出的某一部分的作用。</p><h3 id="attention中产生概率分布的两种方法">Attention中产生概率分布的两种方法</h3><p>在第3部分中，我们的概率分布来自于h与s的点积再做softmax，这只是最基本的方式。在实际中，我们可以有不同的方法来产生这个概率分布，每一种方法都代表了一种具体的Attention机制。</p><ul><li><h4 id="加法attention">1 加法Attention</h4><p>在加法Attention中，我们不再让h与s做点积，而是做如下的运算：</p><p><img src="https://i.loli.net/2020/10/31/dOAr8BHlK25vQuU.png" alt="image-20201031104221351"></p></li></ul><p>va和Wa都是可以训练的参数。h与s之间的分号表示将二者接到一起产生一个更长的vector。这样产生的数再送往softmax层，进而产生一个概率分布。</p><p>当然，我们还可以这么做：</p><p><img src="https://i.loli.net/2020/10/31/Hu4eWOfmQLk9DrK.png" alt="image-20201031104229311"></p><p>这里只是不再把h与s接到一起而已，本质上没有什么区别的。</p><ul><li><h4 id="乘法attention">2 乘法Attention</h4><p>乘法Attention将h与s做如下的运算：</p><p><img src="https://i.loli.net/2020/10/31/V87BZWl3sMUeGvb.png" alt="image-20201031104239592"></p></li></ul><p>显然，乘法Attention的参数更少，效率自然也会更高一些。</p><h3 id="attention机制的扩展">Attention机制的扩展</h3><p>Attention机制的核心在于对一个序列数据进行聚焦，这个聚焦是通过一个概率分布来实现的。这种机制其实有很强的普适性，可以用在各个方面。</p><p>比如，根据图片产生描述该图片的文字， 首先，图片会经过CNN进行特征的提取，提取的数据会输入到产生描述文字的RNN中，这里，我们可以引入Attention机制，让我们在产生下一个文字时，聚焦于我们正在描述的图片部位。</p><p>其次，在句子表示中，self Attention机制是成功扩展的Attention的范例。其基本原理如下：</p><p>假如我们用一个RNN读入了一个句子，产生了h1， h2，h3，h4四个hidden state。 为了得到该句子的摘要，我们可以这样做： 对每一个h计算一个分数：</p><p><img src="https://i.loli.net/2020/10/31/7ltIKSuAZFiBzQn.png" alt="image-20201031104249976"></p><p>四个h共产生了4个分数，将这四个分数送入一个softmax层，产生一个概率分布，根据这个概率分布对四个h进行加和，得到句子摘要的第一个vector。如下图所示：</p><p><img src="https://i.loli.net/2020/10/31/x2DGOb9jBoNJweq.png" alt="image-20201031104257776"></p><p>为了得到更多的vector，我们可以把上面图中的小写va换成一个矩阵，然后，我们的a也就变成了多个概率分布组成的矩阵，每个概率分布都可以用来与h进行加和产生一个vector，这样我们就产生了摘要的多个vector，如下图所示：</p><p><img src="https://i.loli.net/2020/10/31/9esynl6cDKfmEiS.png" alt="image-20201031104320465"></p><h3 id="人类的视觉注意力">人类的视觉注意力</h3><p>从注意力模型的命名方式看，很明显其借鉴了人类的注意力机制，因此，我们首先简单介绍人类视觉的选择性注意力机制。</p><p><img src="https://i.loli.net/2020/10/31/516P2JBYrgOToyU.jpg" alt="人类的视觉注意力"></p><p>视觉注意力机制是人类视觉所特有的大脑信号处理机制。人类视觉通过快速扫描全局图像，获得需要重点关注的目标区域，也就是一般所说的注意力焦点，而后对这一区域投入更多注意力资源，以获取更多所需要关注目标的细节信息，而抑制其他无用信息。</p><p>这是人类利用有限的注意力资源从大量信息中快速筛选出高价值信息的手段，是人类在长期进化中形成的一种生存机制，人类视觉注意力机制极大地提高了视觉信息处理的效率与准确性。</p><p>图1形象化展示了人类在看到一副图像时是如何高效分配有限的注意力资源的，其中红色区域表明视觉系统更关注的目标，很明显对于图1所示的场景，人们会把注意力更多投入到人的脸部，文本的标题以及文章首句等位置。</p><p>深度学习中的注意力机制从本质上讲和人类的选择性视觉注意力机制类似，核心目标也是从众多信息中选择出对当前任务目标更关键的信息。</p><h3 id="encoder-decoder框架">Encoder-Decoder框架</h3><p>要了解深度学习中的注意力模型，就不得不先谈Encoder-Decoder框架，因为目前大多数注意力模型附着在Encoder-Decoder框架下，当然，其实注意力模型可以看作一种通用的思想，本身并不依赖于特定框架，这点需要注意。</p><p>Encoder-Decoder框架可以看作是一种深度学习领域的研究模式，应用场景异常广泛。图2是文本处理领域里常用的Encoder-Decoder框架最抽象的一种表示。</p><p><img src="https://i.loli.net/2020/10/31/ohBVdz1KDn4ZcOb.png" alt="抽象的文本处理领域的Encoder-Decoder框架"></p><p>文本处理领域的Encoder-Decoder框架可以这么直观地去理解：可以把它看作适合处理由一个句子（或篇章）生成另外一个句子（或篇章）的通用处理模型。对于句子对&lt;Source,Target&gt;，我们的目标是给定输入句子Source，期待通过Encoder-Decoder框架来生成目标句子Target。Source和Target可以是同一种语言，也可以是两种不同的语言。而Source和Target分别由各自的单词序列构成：</p><p><img src="https://i.loli.net/2020/10/31/qrCwtas6iRVKYbA.png" alt="img"></p><p>Encoder顾名思义就是对输入句子Source进行编码，将输入句子通过非线性变换转化为中间语义表示C：</p><p><img src="https://i.loli.net/2020/10/31/M7EXx2KPgeHFQhL.png" alt="img"></p><p>对于解码器Decoder来说，其任务是根据句子Source的中间语义表示C和之前已经生成的历史信息 y1,y2,y3...yi-1来生成i时刻要生成的单词yi：</p><p><img src="https://i.loli.net/2020/10/31/6uHkShXDNKOlBQ3.png" alt="img"></p><p>每个yi都依次这么产生，那么看起来就是整个系统根据输入句子Source生成了目标句子Target。如果Source是中文句子，Target是英文句子，那么这就是解决机器翻译问题的Encoder-Decoder框架；如果Source是一篇文章，Target是概括性的几句描述语句，那么这是文本摘要的Encoder-Decoder框架；如果Source是一句问句，Target是一句回答，那么这是问答系统或者对话机器人的Encoder-Decoder框架。由此可见，在文本处理领域，Encoder-Decoder的应用领域相当广泛。</p><p>Encoder-Decoder框架不仅仅在文本领域广泛使用，在语音识别、图像处理等领域也经常使用。比如对于语音识别来说，图2所示的框架完全适用，区别无非是Encoder部分的输入是语音流，输出是对应的文本信息；而对于“图像描述”任务来说，Encoder部分的输入是一副图片，Decoder的输出是能够描述图片语义内容的一句描述语。一般而言，文本处理和语音识别的Encoder部分通常采用RNN模型，图像处理的Encoder一般采用CNN模型。</p><h3 id="attention模型">Attention模型</h3><p>本节先以机器翻译作为例子讲解最常见的Soft Attention模型的基本原理，之后抛离Encoder-Decoder框架抽象出了注意力机制的本质思想，然后简单介绍最近广为使用的Self Attention的基本思路。</p><h4 id="soft-attention模型">Soft Attention模型</h4><p>图2中展示的Encoder-Decoder框架是没有体现出“注意力模型”的，所以可以把它看作是注意力不集中的分心模型。为什么说它注意力不集中呢？请观察下目标句子Target中每个单词的生成过程如下：</p><p><img src="https://i.loli.net/2020/10/31/KNWS6Ax1uI285cH.png" alt="img"></p><p>其中f是Decoder的非线性变换函数。从这里可以看出，在生成目标句子的单词时，不论生成哪个单词，它们使用的输入句子Source的语义编码C都是一样的，没有任何区别。</p><p>而语义编码C是由句子Source的每个单词经过Encoder</p><p>编码产生的，这意味着不论是生成哪个单词，y1,y2还是y3，其实句子Source中任意单词对生成某个目标单词yi来说影响力都是相同的，这是为何说这个模型没有体现出注意力的缘由。这类似于人类看到眼前的画面，但是眼中却没有注意焦点一样。</p><p>如果拿机器翻译来解释这个分心模型的Encoder-Decoder框架更好理解，比如输入的是英文句子：Tom chase Jerry，Encoder-Decoder框架逐步生成中文单词：“汤姆”，“追逐”，“杰瑞”。</p><p>在翻译“杰瑞”这个中文单词的时候，分心模型里面的每个英文单词对于翻译目标单词“杰瑞”贡献是相同的，很明显这里不太合理，显然“Jerry”对于翻译成“杰瑞”更重要，但是分心模型是无法体现这一点的，这就是为何说它没有引入注意力的原因。</p><p>没有引入注意力的模型在输入句子比较短的时候问题不大，但是如果输入句子比较长，此时所有语义完全通过一个中间语义向量来表示，单词自身的信息已经消失，可想而知会丢失很多细节信息，这也是为何要引入注意力模型的重要原因。</p><p>上面的例子中，如果引入Attention模型的话，应该在翻译“杰瑞”的时候，体现出英文单词对于翻译当前中文单词不同的影响程度，比如给出类似下面一个概率分布值：</p><p>（Tom,0.3）(Chase,0.2) (Jerry,0.5)</p><p>每个英文单词的概率代表了翻译当前单词“杰瑞”时，注意力分配模型分配给不同英文单词的注意力大小。这对于正确翻译目标语单词肯定是有帮助的，因为引入了新的信息。</p><p>同理，目标句子中的每个单词都应该学会其对应的源语句子中单词的注意力分配概率信息。这意味着在生成每个单词yi的时候，原先都是相同的中间语义表示C会被替换成根据当前生成单词而不断变化的Ci。理解Attention模型的关键就是这里，即由固定的中间语义表示C换成了根据当前输出单词来调整成加入注意力模型的变化的Ci。增加了注意力模型的Encoder-Decoder框架理解起来如图3所示。</p><p><img src="https://i.loli.net/2020/10/31/oe6E7C9sUHrVuNZ.png" alt="引入注意力模型的Encoder-Decoder框架"></p><p>即生成目标句子单词的过程成了下面的形式：</p><p><img src="https://i.loli.net/2020/10/31/q8ZFEu4BSI1lo9z.png" alt="img"></p><p>而每个Ci可能对应着不同的源语句子单词的注意力分配概率分布，比如对于上面的英汉翻译来说，其对应的信息可能如下：</p><p><img src="https://i.loli.net/2020/10/31/TBg8KZhoyiOlUst.png" alt="img"></p><p>其中，f2函数代表Encoder对输入英文单词的某种变换函数，比如如果Encoder是用的RNN模型的话，这个f2函数的结果往往是某个时刻输入xi后隐层节点的状态值；g代表Encoder根据单词的中间表示合成整个句子中间语义表示的变换函数，一般的做法中，g函数就是对构成元素加权求和，即下列公式：</p><p><img src="https://i.loli.net/2020/10/31/W9SnjkNw3BVasdc.png" alt="img"></p><p>其中，Lx代表输入句子Source的长度，aij代表在Target输出第i个单词时Source输入句子中第j个单词的注意力分配系数，而hj则是Source输入句子中第j个单词的语义编码。假设下标i就是上面例子所说的“ 汤姆” ，那么Lx就是3，h1=f(“Tom”)，h2=f(“Chase”),h3=f(“Jerry”)分别是输入句子每个单词的语义编码，对应的注意力模型权值则分别是0.6,0.2,0.2，所以g函数本质上就是个加权求和函数。如果形象表示的话，翻译中文单词“汤姆”的时候，数学公式对应的中间语义表示Ci的形成过程类似图4。</p><p><img src="https://i.loli.net/2020/10/31/LRJZMaUbN9VuxGr.png" alt="Attention的形成过程"></p><p>这里还有一个问题：生成目标句子某个单词，比如“汤姆”的时候，如何知道Attention模型所需要的输入句子单词注意力分配概率分布值呢？就是说“汤姆”对应的输入句子Source中各个单词的概率分布：(Tom,0.6)(Chase,0.2) (Jerry,0.2) 是如何得到的呢？</p><p>为了便于说明，我们假设对图2的非Attention模型的Encoder-Decoder框架进行细化，Encoder采用RNN模型，Decoder也采用RNN模型，这是比较常见的一种模型配置，则图2的框架转换为图5。</p><p><img src="https://i.loli.net/2020/10/31/ebTh6NzIdlVG1L2.png" alt="RNN作为具体模型的Encoder-Decoder框架"></p><p>那么用下图可以较为便捷地说明注意力分配概率分布值的通用计算过程。</p><p><img src="https://i.loli.net/2020/10/31/TEXoxdzgrHOuiMQ.jpg" alt="注意力分配概率计算"></p><p>对于采用RNN的Decoder来说，在时刻i，如果要生成yi单词，我们是可以知道Target在生成Yi之前的时刻i-1时，隐层节点i-1时刻的输出值Hi-1的，而我们的目的是要计算生成Yi时输入句子中的单词“Tom”、“Chase”、“Jerry”对Yi来说的注意力分配概率分布，那么可以用Target输出句子i-1时刻的隐层节点状态Hi-1去一一和输入句子Source中每个单词对应的RNN隐层节点状态hj进行对比，即通过函数F(hj,Hi-1)来获得目标单词yi和每个输入单词对应的对齐可能性，这个F函数在不同论文里可能会采取不同的方法，然后函数F的输出经过Softmax进行归一化就得到了符合概率分布取值区间的注意力分配概率分布数值。</p><p>绝大多数Attention模型都是采取上述的计算框架来计算注意力分配概率分布信息，区别只是在F的定义上可能有所不同。图7可视化地展示了在英语-德语翻译系统中加入Attention机制后，Source和Target两个句子每个单词对应的注意力分配概率分布。</p><p><img src="https://i.loli.net/2020/10/31/T1tNMbc6DE3HkGQ.png" alt="英语-德语翻译的注意力概率分布"></p><p>上述内容就是经典的Soft Attention模型的基本思想，那么怎么理解Attention模型的物理含义呢？一般在自然语言处理应用里会把Attention模型看作是输出Target句子中某个单词和输入Source句子每个单词的对齐模型，这是非常有道理的。</p><p>目标句子生成的每个单词对应输入句子单词的概率分布可以理解为输入句子单词和这个目标生成单词的对齐概率，这在机器翻译语境下是非常直观的：传统的统计机器翻译一般在做的过程中会专门有一个短语对齐的步骤，而注意力模型其实起的是相同的作用。</p><p><img src="https://i.loli.net/2020/10/31/zYmvXW8pHiO2VUo.jpg" alt="Google 神经网络机器翻译系统结构图"></p><p>图8所示即为Google于2016年部署到线上的基于神经网络的机器翻译系统，相对传统模型翻译效果有大幅提升，翻译错误率降低了60%，其架构就是上文所述的加上Attention机制的Encoder-Decoder框架，主要区别无非是其Encoder和Decoder使用了8层叠加的LSTM模型。</p><h4 id="attention机制的本质思想">Attention机制的本质思想</h4><p>如果把Attention机制从上文讲述例子中的Encoder-Decoder框架中剥离，并进一步做抽象，可以更容易看懂Attention机制的本质思想。</p><p><img src="https://i.loli.net/2020/10/31/y3BToK96rsgUZfF.png" alt="Attention机制的本质思想"></p><p>我们可以这样来看待Attention机制（参考图9）：将Source中的构成元素想象成是由一系列的&lt;Key,Value&gt;数据对构成，此时给定Target中的某个元素Query，通过计算Query和各个Key的相似性或者相关性，得到每个Key对应Value的权重系数，然后对Value进行加权求和，即得到了最终的Attention数值。所以本质上Attention机制是对Source中元素的Value值进行加权求和，而Query和Key用来计算对应Value的权重系数。即可以将其本质思想改写为如下公式：</p><p><img src="https://i.loli.net/2020/10/31/ipGlzuFcmS8n2VR.png" alt="img"></p><p>其中，Lx=||Source||代表Source的长度，公式含义即如上所述。上文所举的机器翻译的例子里，因为在计算Attention的过程中，Source中的Key和Value合二为一，指向的是同一个东西，也即输入句子中每个单词对应的语义编码，所以可能不容易看出这种能够体现本质思想的结构。</p><p>当然，从概念上理解，把Attention仍然理解为从大量信息中有选择地筛选出少量重要信息并聚焦到这些重要信息上，忽略大多不重要的信息，这种思路仍然成立。聚焦的过程体现在权重系数的计算上，权重越大越聚焦于其对应的Value值上，即权重代表了信息的重要性，而Value是其对应的信息。</p><p>从上图可以引出另外一种理解，也可以将Attention机制看作一种软寻址（Soft Addressing）:Source可以看作存储器内存储的内容，元素由地址Key和值Value组成，当前有个Key=Query的查询，目的是取出存储器中对应的Value值，即Attention数值。通过Query和存储器内元素Key的地址进行相似性比较来寻址，之所以说是软寻址，指的不像一般寻址只从存储内容里面找出一条内容，而是可能从每个Key地址都会取出内容，取出内容的重要性根据Query和Key的相似性来决定，之后对Value进行加权求和，这样就可以取出最终的Value值，也即Attention值。所以不少研究人员将Attention机制看作软寻址的一种特例，这也是非常有道理的。</p><p>至于Attention机制的具体计算过程，如果对目前大多数方法进行抽象的话，可以将其归纳为两个过程：第一个过程是根据Query和Key计算权重系数，第二个过程根据权重系数对Value进行加权求和。而第一个过程又可以细分为两个阶段：第一个阶段根据Query和Key计算两者的相似性或者相关性；第二个阶段对第一阶段的原始分值进行归一化处理；这样，可以将Attention的计算过程抽象为如图10展示的三个阶段。</p><p><img src="https://i.loli.net/2020/10/31/tzO9fhXUeBblmVG.png" alt="三阶段计算Attention过程"></p><p>在第一个阶段，可以引入不同的函数和计算机制，根据Query和某个Key_i，计算两者的相似性或者相关性，最常见的方法包括：求两者的向量点积、求两者的向量Cosine相似性或者通过再引入额外的神经网络来求值，即如下方式：</p><p><img src="https://i.loli.net/2020/10/31/9xpPOa7ohFf3u1d.png" alt="img"></p><p>第一阶段产生的分值根据具体产生的方法不同其数值取值范围也不一样，第二阶段引入类似SoftMax的计算方式对第一阶段的得分进行数值转换，一方面可以进行归一化，将原始计算分值整理成所有元素权重之和为1的概率分布；另一方面也可以通过SoftMax的内在机制更加突出重要元素的权重。即一般采用如下公式计算：</p><p><img src="https://i.loli.net/2020/10/31/XFW5tcSGjqBnIyN.png" alt="img"></p><p>第二阶段的计算结果a_i即为value_i对应的权重系数，然后进行加权求和即可得到Attention数值：</p><p><img src="https://i.loli.net/2020/10/31/soa1M9LIPGi3krC.png" alt="img"></p><p>通过如上三个阶段的计算，即可求出针对Query的Attention数值，目前绝大多数具体的注意力机制计算方法都符合上述的三阶段抽象计算过程。</p><h4 id="self-attention模型">Self Attention模型</h4><p>通过上述对Attention本质思想的梳理，我们可以更容易理解本节介绍的Self Attention模型。Self Attention也经常被称为intra Attention（内部Attention），最近一年也获得了比较广泛的使用，比如Google最新的机器翻译模型内部大量采用了Self Attention模型。</p><p>在一般任务的Encoder-Decoder框架中，输入Source和输出Target内容是不一样的，比如对于英-中机器翻译来说，Source是英文句子，Target是对应的翻译出的中文句子，Attention机制发生在Target的元素Query和Source中的所有元素之间。而Self</p><p>Attention顾名思义，指的不是Target和Source之间的Attention机制，而是Source内部元素之间或者Target内部元素之间发生的Attention机制，也可以理解为Target=Source这种特殊情况下的注意力计算机制。其具体计算过程是一样的，只是计算对象发生了变化而已，所以此处不再赘述其计算过程细节。</p><p>如果是常规的Target不等于Source情形下的注意力计算，其物理含义正如上文所讲，比如对于机器翻译来说，本质上是目标语单词和源语单词之间的一种单词对齐机制。那么如果是Self Attention机制，一个很自然的问题是：通过Self Attention到底学到了哪些规律或者抽取出了哪些特征呢？或者说引入Self Attention有什么增益或者好处呢？我们仍然以机器翻译中的Self Attention来说明，图11和图12是可视化地表示Self Attention在同一个英语句子内单词间产生的联系。</p><p><img src="https://i.loli.net/2020/10/31/BqbvNSUWyrnTjt4.jpg" alt="可视化Self Attention实例"></p><p><img src="https://i.loli.net/2020/10/31/q2iCXflnh5oH1WE.jpg" alt="可视化Self Attention实例"></p><p>从两张图（图11、图12）可以看出，Self Attention可以捕获同一个句子中单词之间的一些句法特征（比如图11展示的有一定距离的短语结构）或者语义特征（比如图12展示的its的指代对象Law）。</p><p>很明显，引入Self Attention后会更容易捕获句子中长距离的相互依赖的特征，因为如果是RNN或者LSTM，需要依次序序列计算，对于远距离的相互依赖的特征，要经过若干时间步步骤的信息累积才能将两者联系起来，而距离越远，有效捕获的可能性越小。</p><p>但是Self Attention在计算过程中会直接将句子中任意两个单词的联系通过一个计算步骤直接联系起来，所以远距离依赖特征之间的距离被极大缩短，有利于有效地利用这些特征。除此外，Self Attention对于增加计算的并行性也有直接帮助作用。这是为何Self Attention逐渐被广泛使用的主要原因。</p><h4 id="attention机制的应用">Attention机制的应用</h4><p>前文有述，Attention机制在深度学习的各种应用领域都有广泛的使用场景。上文在介绍过程中我们主要以自然语言处理中的机器翻译任务作为例子，下面分别再从图像处理领域和语音识别选择典型应用实例来对其应用做简单说明。</p><p><img src="https://i.loli.net/2020/10/31/Yog7WcVFCXbRv3U.jpg" alt="图片-描述任务的Encoder-Decoder框架"></p><p>图片描述（Image-Caption）是一种典型的图文结合的深度学习应用，输入一张图片，人工智能系统输出一句描述句子，语义等价地描述图片所示内容。很明显这种应用场景也可以使用Encoder-Decoder框架来解决任务目标，此时Encoder输入部分是一张图片，一般会用CNN来对图片进行特征抽取，Decoder部分使用RNN或者LSTM来输出自然语言句子（参考上图）。</p><p>此时如果加入Attention机制能够明显改善系统输出效果，Attention模型在这里起到了类似人类视觉选择性注意的机制，在输出某个实体单词的时候会将注意力焦点聚焦在图片中相应的区域上。图14给出了根据给定图片生成句子“A person is standing on a beach with a surfboard.”过程时每个单词对应图片中的注意力聚焦区域。</p><p><img src="https://i.loli.net/2020/10/31/RGVXYj8W2yQsqtz.jpg" alt="图片生成句子中每个单词时的注意力聚焦区域"></p><p>下图给出了另外四个例子形象地展示了这种过程，每个例子上方左侧是输入的原图，下方句子是人工智能系统自动产生的描述语句，上方右侧图展示了当AI系统产生语句中划横线单词的时候，对应图片中聚焦的位置区域。比如当输出单词dog的时候，AI系统会将注意力更多地分配给图片中小狗对应的位置。</p><p><img src="https://i.loli.net/2020/10/31/v7Eubye6A4dSLZp.jpg" alt="图像描述任务中Attention机制的聚焦作用"></p><p><img src="https://i.loli.net/2020/10/31/Yt5fPBJWRAaUHNv.jpg" alt="语音识别中音频序列和输出字符之间的Attention"></p><p>语音识别的任务目标是将语音流信号转换成文字，所以也是Encoder-Decoder的典型应用场景。Encoder部分的Source输入是语音流信号，Decoder部分输出语音对应的字符串流。</p><p>上图可视化地展示了在Encoder-Decoder框架中加入Attention机制后，当用户用语音说句子 how much would a woodchuck chuck 时，输入部分的声音特征信号和输出字符之间的注意力分配概率分布情况，颜色越深代表分配到的注意力概率越高。从图中可以看出，在这个场景下，Attention机制起到了将输出字符和输入语音信号进行对齐的功能。</p><p>上述内容仅仅选取了不同AI领域的几个典型Attention机制应用实例，Encoder-Decoder加Attention架构由于其卓越的实际效果，目前在深度学习领域里得到了广泛的使用，了解并熟练使用这一架构对于解决实际问题会有极大帮助。</p><h3 id="总结">总结</h3><p>通过以上的内容，我们了解到，Attention机制最初用来克服RNN做机器翻译时的缺点，然后，人们发现，Attention机制具有广泛的适用性，于是它又被扩展到了产生图片描述，做句子摘要等任务上。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      注意力机制模型的总结
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-10-30-从embedding到BERT预训练模型</title>
    <link href="http://yoursite.com/2020/10/30/2020-10-30-%E4%BB%8Eembedding%E5%88%B0BERT%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"/>
    <id>http://yoursite.com/2020/10/30/2020-10-30-%E4%BB%8Eembedding%E5%88%B0BERT%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/</id>
    <published>2020-10-30T11:53:49.000Z</published>
    <updated>2020-10-30T12:38:47.914Z</updated>
    
    <content type="html"><![CDATA[<p>Bert最近很火，应该是最近最火爆的AI进展，网上的评价很高，那么Bert值得这么高的评价吗？我个人判断是值得。那为什么会有这么高的评价呢？是因为它有重大的理论或者模型创新吗？其实并没有，从模型创新角度看一般，创新不算大。但是架不住效果太好了，基本刷新了很多NLP的任务的最好性能，有些任务还被刷爆了，这个才是关键。另外一点是Bert具备广泛的通用性，就是说绝大部分NLP任务都可以采用类似的两阶段模式直接去提升效果，这个第二关键。客观的说，把Bert当做最近两年NLP重大进展的集大成者更符合事实。</p><p>本文的主题是自然语言处理中的预训练过程，会大致说下NLP中的预训练技术是一步一步如何发展到Bert模型的，从中可以很自然地看到Bert的思路是如何逐渐形成的，Bert的历史沿革是什么，继承了什么，创新了什么，为什么效果那么好，主要原因是什么，以及为何说模型创新不算太大，为何说Bert是近年来NLP重大进展的集大成者。我们一步一步来讲，而串起来这个故事的脉络就是自然语言的预训练过程，但是落脚点还是在Bert身上。要讲自然语言的预训练，得先从图像领域的预训练说起。</p><h3 id="图像领域的预训练">图像领域的预训练</h3><p>自从深度学习火起来后，预训练过程就是做图像或者视频领域的一种比较常规的做法，有比较长的历史了，而且这种做法很有效，能明显促进应用的效果。</p><p><img src="https://i.loli.net/2020/10/30/HSZgfVhv5cAO2Qt.jpg" alt="img"></p><p>那么图像领域怎么做预训练呢，上图展示了这个过程，我们设计好网络结构以后，对于图像来说一般是CNN的多层叠加网络结构，可以先用某个训练集合比如训练集合A或者训练集合B对这个网络进行预先训练，在A任务上或者B任务上学会网络参数，然后存起来以备后用。假设我们面临第三个任务C，网络结构采取相同的网络结构，在比较浅的几层CNN结构，网络参数初始化的时候可以加载A任务或者B任务学习好的参数，其它CNN高层参数仍然随机初始化。之后我们用C任务的训练数据来训练网络，此时有两种做法，一种是浅层加载的参数在训练C任务过程中不动，这种方法被称为“Frozen”;另外一种是底层网络参数尽管被初始化了，在C任务训练过程中仍然随着训练的进程不断改变，这种一般叫“Fine-Tuning”，顾名思义，就是更好地把参数进行调整使得更适应当前的C任务。一般图像或者视频领域要做预训练一般都这么做。</p><p>这么做有几个好处，首先，如果手头任务C的训练集合数据量较少的话，现阶段的好用的CNN比如Resnet/Densenet/Inception等网络结构层数很深，几百万上千万参数量算起步价，上亿参数的也很常见，训练数据少很难很好地训练这么复杂的网络，但是如果其中大量参数通过大的训练集合比如ImageNet预先训练好直接拿来初始化大部分网络结构参数，然后再用C任务手头比较可怜的数据量上Fine-tuning过程去调整参数让它们更适合解决C任务，那事情就好办多了。这样原先训练不了的任务就能解决了，即使手头任务训练数据也不少，加个预训练过程也能极大加快任务训练的收敛速度，所以这种预训练方式是老少皆宜的解决方案，另外疗效又好，所以在做图像处理领域很快就流行开来。</p><p>那么新的问题来了，为什么这种预训练的思路是可行的？</p><p><img src="https://i.loli.net/2020/10/30/hnsrCeotdP4jLSq.jpg" alt="img"></p><p>目前我们已经知道，对于层级的CNN结构来说，不同层级的神经元学习到了不同类型的图像特征，由底向上特征形成层级结构，如上图所示，如果我们手头是个人脸识别任务，训练好网络后，把每层神经元学习到的特征可视化肉眼看一看每层学到了啥特征，你会看到最底层的神经元学到的是线段等特征，图示的第二个隐层学到的是人脸五官的轮廓，第三层学到的是人脸的轮廓，通过三步形成了特征的层级结构，越是底层的特征越是所有不论什么领域的图像都会具备的比如边角线弧线等底层基础特征，越往上抽取出的特征越与手头任务相关。正因为此，所以预训练好的网络参数，尤其是底层的网络参数抽取出特征跟具体任务越无关，越具备任务的通用性，所以这是为何一般用底层预训练好的参数初始化新任务网络参数的原因。而高层特征跟任务关联较大，实际可以不用使用，或者采用Fine-tuning用新数据集合清洗掉高层无关的特征抽取器。</p><p><img src="https://i.loli.net/2020/10/30/RrSMsuboYdOzvKl.jpg" alt="img"></p><p>一般我们喜欢用ImageNet来做网络的预训练，主要有两点，一方面ImageNet是图像领域里有超多事先标注好训练数据的数据集合，分量足是个很大的优势，量越大训练出的参数越靠谱；另外一方面因为ImageNet有1000类，类别多，算是通用的图像数据，跟领域没太大关系，所以通用性好，预训练完后哪哪都能用，是个万金油。分量足的万金油当然老少通吃，人人喜爱。</p><p>听完上述话，如果你是具备研究素质的人，也就是说具备好奇心，你一定会问下面这个问题：”既然图像领域预训练这么好用，那干嘛自然语言处理不做这个事情呢？是不是搞NLP的人比搞CV的傻啊？就算你傻，你看见人家这么做，有样学样不就行了吗？这不就是创新吗，也许能成，万一成了，你看，你的成功来得就是这么突然!”</p><p>嗯，好问题，其实搞NLP的人一点都不比你傻，早就有人尝试过了，不过总体而言不太成功而已。听说过word embedding吗？2003年出品，陈年技术，馥郁芳香。word embedding其实就是NLP里的早期预训练技术。当然也不能说word embedding不成功，一般加到下游任务里，都能有1到2个点的性能提升，只是没有那么耀眼的成功而已。</p><p>没听过？那下面就把这段陈年老账讲给你听听。</p><h3 id="word-embedding考古史">Word Embedding考古史</h3><p>这块大致讲讲Word Embedding的故事，很粗略，因为网上关于这个技术讲的文章太多了，汗牛冲动，我不属牛，此刻更没有流汗，所以其实丝毫没有想讲Word Embedding的冲动和激情，但是要说预训练又得从这开始，那就粗略地讲讲，主要是引出后面更精彩的部分。在说Word Embedding之前，先更粗略地说下语言模型，因为一般NLP里面做预训练一般的选择是用语言模型任务来做。</p><p><img src="https://i.loli.net/2020/10/30/4aFBLDCEQmgXp9Z.jpg" alt="img"></p><p>什么是语言模型？其实看上面这张PPT上扣下来的图就明白了，为了能够量化地衡量哪个句子更像一句人话，可以设计如上图所示函数，核心函数P的思想是根据句子里面前面的一系列前导单词预测后面跟哪个单词的概率大小（理论上除了上文之外，也可以引入单词的下文联合起来预测单词出现概率）。句子里面每个单词都有个根据上文预测自己的过程，把所有这些单词的产生概率乘起来，数值越大代表这越像一句人话。语言模型压下暂且不表，我隐约预感到我这么讲你可能还是不太会明白，但是大概这个意思，不懂的可以去网上找，资料多得一样地汗牛冲动。</p><p>假设现在让你设计一个神经网络结构，去做这个语言模型的任务，就是说给你很多语料做这个事情，训练好一个神经网络，训练好之后，以后输入一句话的前面几个单词，要求这个网络输出后面紧跟的单词应该是哪个，你会怎么做？</p><p><img src="https://i.loli.net/2020/10/30/SUI2jF7xEsaz9Z4.jpg" alt="img"></p><p>你可以像上图这么设计这个网络结构，这其实就是大名鼎鼎的中文人称“神经网络语言模型”，英文小名NNLM的网络结构，用来做语言模型。这个工作有年头了，是个陈年老工作，是Bengio 在2003年发表在JMLR上的论文。它生于2003，火于2013，以后是否会不朽暂且不知，但是不幸的是出生后应该没有引起太大反响，沉寂十年终于时来运转沉冤得雪，在2013年又被NLP考古工作者从海底湿淋淋地捞出来了祭入神殿。为什么会发生这种技术奇遇记？你要想想2013年是什么年头，是深度学习开始渗透NLP领域的光辉时刻，万里长征第一步，而NNLM可以算是南昌起义第一枪。在深度学习火起来之前，极少有人用神经网络做NLP问题，如果你10年前坚持用神经网络做NLP，估计别人会认为你这人神经有问题。所谓红尘滚滚，谁也挡不住历史发展趋势的车轮，这就是个很好的例子。</p><p>上面是闲话，闲言碎语不要讲，我们回来讲一讲NNLM的思路。先说训练过程，现在看其实很简单，见过RNN、LSTM、CNN后的你们回头再看这个网络甚至显得有些简陋。学习任务是输入某个句中单词Wt=“Bert”前面句子的t-1个单词，要求网络正确预测单词Bert，即最大化：</p><p><img src="https://i.loli.net/2020/10/30/k8XpuUwT47WOo5Q.png" alt="image-20201030202319906" style="zoom:50%;"></p><p>前面任意单词Wi用Onehot编码（比如：0001000）作为原始单词输入，之后乘以矩阵Q后获得向量C（Wi），每个单词的 C（Wi）拼接，上接隐层，然后接softmax去预测后面应该后续接哪个单词。这个 C（Wi）是什么？这其实就是单词对应的Word Embedding值，那个矩阵Q包含V行，V代表词典大小，每一行内容代表对应单词的Word embedding值。只不过Q的内容也是网络参数，需要学习获得，训练刚开始用随机值初始化矩阵Q，当这个网络训练好之后，矩阵Q的内容被正确赋值，每一行代表一个单词对应的Word embedding值。所以你看，通过这个网络学习语言模型任务，这个网络不仅自己能够根据上文预测后接单词是什么，同时获得一个副产品，就是那个矩阵Q，这就是单词的Word Embedding是被如何学会的。</p><p>2013年最火的用语言模型做Word Embedding的工具是Word2Vec，后来又出了Glove，Word2Vec是怎么工作的呢？看下图。</p><p><img src="https://i.loli.net/2020/10/30/bXq5RvhS3niaTxB.jpg" alt="img"></p><p>Word2Vec的网络结构其实和NNLM是基本类似的，只是这个图长得清晰度差了点，看上去不像，其实它们是亲兄弟。不过这里需要指出：尽管网络结构相近，而且也是做语言模型任务，但是其训练方法不太一样。Word2Vec有两种训练方法，一种叫CBOW，核心思想是从一个句子里面把一个词抠掉，用这个词的上文和下文去预测被抠掉的这个词；第二种叫做Skip-gram，和CBOW正好反过来，输入某个单词，要求网络预测它的上下文单词。而你回头看看，NNLM是怎么训练的？是输入一个单词的上文，去预测这个单词。这是有显著差异的。为什么Word2Vec这么处理？原因很简单，因为Word2Vec和NNLM不一样，NNLM的主要任务是要学习一个解决语言模型任务的网络结构，语言模型就是要看到上文预测下文，而word embedding只是无心插柳的一个副产品。但是Word2Vec目标不一样，它单纯就是要word embedding的，这是主产品，所以它完全可以随性地这么去训练网络。</p><p>为什么要讲Word2Vec呢？这里主要是要引出CBOW的训练方法，BERT其实跟它有关系，后面会讲它们之间是如何的关系，当然它们的关系BERT作者没说，是我猜的，至于我猜的对不对，后面你看后自己判断。</p><p><img src="https://i.loli.net/2020/10/30/aM9lpsNOS7vrmnq.jpg" alt="img"></p><p>使用Word2Vec或者Glove，通过做语言模型任务，就可以获得每个单词的Word Embedding，那么这种方法的效果如何呢？上图给了网上找的几个例子，可以看出有些例子效果还是很不错的，一个单词表达成Word Embedding后，很容易找出语义相近的其它词汇。</p><p>我们的主题是预训练，那么问题是Word Embedding这种做法能算是预训练吗？这其实就是标准的预训练过程。要理解这一点要看看学会Word Embedding后下游任务是怎么用它的。</p><p><img src="https://i.loli.net/2020/10/30/YEf95lnIW28OGRa.jpg" alt="img"></p><p>假设如上图所示，我们有个NLP的下游任务，比如QA，就是问答问题，所谓问答问题，指的是给定一个问题X，给定另外一个句子Y,要判断句子Y是否是问题X的正确答案。问答问题假设设计的网络结构如上图所示，这里不展开讲了，懂得自然懂，不懂的也没关系，因为这点对于本文主旨来说不关键，关键是网络如何使用训练好的Word Embedding的。它的使用方法其实和前面讲的NNLM是一样的，句子中每个单词以Onehot形式作为输入，然后乘以学好的Word Embedding矩阵Q，就直接取出单词对应的Word Embedding了。这乍看上去好像是个查表操作，不像是预训练的做法是吧？其实不然，那个Word Embedding矩阵Q其实就是网络Onehot层到embedding层映射的网络参数矩阵。所以你看到了，使用Word Embedding等价于什么？等价于把Onehot层到embedding层的网络用预训练好的参数矩阵Q初始化了。这跟前面讲的图像领域的低层预训练过程其实是一样的，区别无非Word Embedding只能初始化第一层网络参数，再高层的参数就无能为力了。下游NLP任务在使用Word Embedding的时候也类似图像有两种做法，一种是Frozen，就是Word Embedding那层网络参数固定不动；另外一种是Fine-Tuning，就是Word Embedding这层参数使用新的训练集合训练也需要跟着训练过程更新掉。</p><p>上面这种做法就是18年之前NLP领域里面采用预训练的典型做法，之前说过，Word Embedding其实对于很多下游NLP任务是有帮助的，只是帮助没有大到闪瞎忘记戴墨镜的围观群众的双眼而已。那么新问题来了，为什么这样训练及使用Word Embedding的效果没有期待中那么好呢？答案很简单，因为Word Embedding有问题呗。这貌似是个比较弱智的答案，关键是Word Embedding存在什么问题？这其实是个好问题。</p><p><img src="https://i.loli.net/2020/10/30/U3YwNJ1RmydDukM.jpg" alt="img"></p><p>这片在Word Embedding头上笼罩了好几年的乌云是什么？是多义词问题。我们知道，多义词是自然语言中经常出现的现象，也是语言灵活性和高效性的一种体现。多义词对Word Embedding来说有什么负面影响？如上图所示，比如多义词Bank，有两个常用含义，但是Word Embedding在对bank这个单词进行编码的时候，是区分不开这两个含义的，因为它们尽管上下文环境中出现的单词不同，但是在用语言模型训练的时候，不论什么上下文的句子经过word2vec，都是预测相同的单词bank，而同一个单词占的是同一行的参数空间，这导致两种不同的上下文信息都会编码到相同的word embedding空间里去。所以word embedding无法区分多义词的不同语义，这就是它的一个比较严重的问题。</p><p>你可能觉得自己很聪明，说这可以解决啊，确实也有很多研究人员提出很多方法试图解决这个问题，但是从今天往回看，这些方法看上去都成本太高或者太繁琐了，有没有简单优美的解决方案呢？</p><p>ELMO提供了一种简洁优雅的解决方案。</p><h3 id="从word-embedding到elmo">从Word Embedding到ELMO</h3><p>ELMO是“Embedding from Language Models”的简称，其实这个名字并没有反应它的本质思想，提出ELMO的论文题目：“Deep contextualized word representation”更能体现其精髓，而精髓在哪里？在deep contextualized这个短语，一个是deep，一个是context，其中context更关键。在此之前的Word Embedding本质上是个静态的方式，所谓静态指的是训练好之后每个单词的表达就固定住了，以后使用的时候，不论新句子上下文单词是什么，这个单词的Word Embedding不会跟着上下文场景的变化而改变，所以对于比如Bank这个词，它事先学好的Word Embedding中混合了几种语义 ，在应用中来了个新句子，即使从上下文中（比如句子包含money等词）明显可以看出它代表的是“银行”的含义，但是对应的Word Embedding内容也不会变，它还是混合了多种语义。这是为何说它是静态的，这也是问题所在。ELMO的本质思想是：我事先用语言模型学好一个单词的Word Embedding，此时多义词无法区分，不过这没关系。在我实际使用Word Embedding的时候，单词已经具备了特定的上下文了，这个时候我可以根据上下文单词的语义去调整单词的Word Embedding表示，这样经过调整后的Word Embedding更能表达在这个上下文中的具体含义，自然也就解决了多义词的问题了。所以ELMO本身是个根据当前上下文对Word Embedding动态调整的思路。</p><p><img src="https://i.loli.net/2020/10/30/m6NvFoRGhWbji83.jpg" alt="img"></p><p>ELMO采用了典型的两阶段过程，第一个阶段是利用语言模型进行预训练；第二个阶段是在做下游任务时，从预训练网络中提取对应单词的网络各层的Word Embedding作为新特征补充到下游任务中。上图展示的是其预训练过程，它的网络结构采用了双层双向LSTM，目前语言模型训练的任务目标是根据单词 Wi 的上下文去正确预测单词 Wi ， Wi 之前的单词序列Context-before称为上文，之后的单词序列Context-after称为下文。图中左端的前向双层LSTM代表正方向编码器，输入的是从左到右顺序的除了预测单词外 Wi 的上文Context-before；右端的逆向双层LSTM代表反方向编码器，输入的是从右到左的逆序的句子下文Context-after；每个编码器的深度都是两层LSTM叠加。这个网络结构其实在NLP中是很常用的。使用这个网络结构利用大量语料做语言模型任务就能预先训练好这个网络，如果训练好这个网络后，输入一个新句子 Snew ，句子中每个单词都能得到对应的三个Embedding:最底层是单词的Word Embedding，往上走是第一层双向LSTM中对应单词位置的Embedding，这层编码单词的句法信息更多一些；再往上走是第二层LSTM中对应单词位置的Embedding，这层编码单词的语义信息更多一些。也就是说，ELMO的预训练过程不仅仅学会单词的Word Embedding，还学会了一个双层双向的LSTM网络结构，而这两者后面都有用。</p><p><img src="https://i.loli.net/2020/10/30/ymSXKwFh8WBcEd5.jpg" alt="img"></p><p>上面介绍的是ELMO的第一阶段：预训练阶段。那么预训练好网络结构后，如何给下游任务使用呢？上图展示了下游任务的使用过程，比如我们的下游任务仍然是QA问题，此时对于问句X，我们可以先将句子X作为预训练好的ELMO网络的输入，这样句子X中每个单词在ELMO网络中都能获得对应的三个Embedding，之后给予这三个Embedding中的每一个Embedding一个权重a，这个权重可以学习得来，根据各自权重累加求和，将三个Embedding整合成一个。然后将整合后的这个Embedding作为X句在自己任务的那个网络结构中对应单词的输入，以此作为补充的新特征给下游任务使用。对于上图所示下游任务QA中的回答句子Y来说也是如此处理。因为ELMO给下游提供的是每个单词的特征形式，所以这一类预训练的方法被称为“Feature-based Pre-Training”。至于为何这么做能够达到区分多义词的效果，你可以想一想，其实比较容易想明白原因。</p><p><img src="https://i.loli.net/2020/10/30/b8ToQxv5BPgELI1.jpg" alt="img"></p><p>上面这个图是TagLM采用类似ELMO的思路做命名实体识别任务的过程，其步骤基本如上述ELMO的思路，所以此处不展开说了。TagLM的论文发表在2017年的ACL会议上，作者就是AllenAI里做ELMO的那些人，所以可以将TagLM看做ELMO的一个前导工作。前几天这个PPT发出去后有人质疑说FastAI的在18年4月提出的ULMFiT才是抛弃传统Word Embedding引入新模式的开山之作，我深不以为然。首先TagLM出现的更早而且模式基本就是ELMO的思路；另外ULMFiT使用的是三阶段模式，在通用语言模型训练之后，加入了一个领域语言模型预训练过程，而且论文重点工作在这块，方法还相对比较繁杂，这并不是一个特别好的主意，因为领域语言模型的限制是它的规模往往不可能特别大，精力放在这里不太合适，放在通用语言模型上感觉更合理；再者，尽管ULFMiT实验做了6个任务，但是都集中在分类问题相对比较窄，不如ELMO验证的问题领域广，我觉得这就是因为第二步那个领域语言模型带来的限制。所以综合看，尽管ULFMiT也是个不错的工作，但是重要性跟ELMO比至少还是要差一档，当然这是我个人看法。每个人的学术审美口味不同，我个人一直比较赞赏要么简洁有效体现问题本质要么思想特别游离现有框架脑洞开得异常大的工作，所以ULFMiT我看论文的时候就感觉看着有点难受，觉得这工作没抓住重点而且特别麻烦，但是看ELMO论文感觉就赏心悦目，觉得思路特别清晰顺畅，看完暗暗点赞，心里说这样的文章获得NAACL2018最佳论文当之无愧，比ACL很多最佳论文也好得不是一点半点，这就是好工作带给一个有经验人士的一种在读论文时候就能产生的本能的感觉，也就是所谓的这道菜对上了食客的审美口味。</p><p><img src="https://i.loli.net/2020/10/30/6y7VvCDm9NRpHJx.jpg" alt="img"></p><p>前面我们提到静态Word Embedding无法解决多义词的问题，那么ELMO引入上下文动态调整单词的embedding后多义词问题解决了吗？解决了，而且比我们期待的解决得还要好。上图给了个例子，对于Glove训练出的Word Embedding来说，多义词比如play，根据它的embedding找出的最接近的其它单词大多数集中在体育领域，这很明显是因为训练数据中包含play的句子中体育领域的数量明显占优导致；而使用ELMO，根据上下文动态调整后的embedding不仅能够找出对应的“演出”的相同语义的句子，而且还可以保证找出的句子中的play对应的词性也是相同的，这是超出期待之处。之所以会这样，是因为我们上面提到过，第一层LSTM编码了很多句法信息，这在这里起到了重要作用。</p><p><img src="https://i.loli.net/2020/10/30/YFAVkuIxemlaHPN.jpg" alt="img"></p><p>ELMO经过这般操作，效果如何呢？实验效果见上图，6个NLP任务中性能都有幅度不同的提升，最高的提升达到25%左右，而且这6个任务的覆盖范围比较广，包含句子语义关系判断，分类任务，阅读理解等多个领域，这说明其适用范围是非常广的，普适性强，这是一个非常好的优点。</p><p><img src="https://i.loli.net/2020/10/30/LpMSFe5kX7Qxroh.jpg" alt="img"></p><p>那么站在现在这个时间节点看，ELMO有什么值得改进的缺点呢？首先，一个非常明显的缺点在特征抽取器选择方面，ELMO使用了LSTM而不是新贵Transformer，Transformer是谷歌在17年做机器翻译任务的“Attention is all you need”的论文中提出的，引起了相当大的反响，很多研究已经证明了Transformer提取特征的能力是要远强于LSTM的。如果ELMO采取Transformer作为特征提取器，那么估计Bert的反响远不如现在的这种火爆场面。另外一点，ELMO采取双向拼接这种融合特征的能力可能比Bert一体化的融合特征方式弱，但是，这只是一种从道理推断产生的怀疑，目前并没有具体实验说明这一点。</p><p>我们如果把ELMO这种预训练方法和图像领域的预训练方法对比，发现两者模式看上去还是有很大差异的。除了以ELMO为代表的这种基于特征融合的预训练方法外，NLP里还有一种典型做法，这种做法和图像领域的方式就是看上去一致的了，一般将这种方法称为“基于Fine-tuning的模式”，而GPT就是这一模式的典型开创者。</p><h3 id="从word-embedding到gpt">从Word Embedding到GPT</h3><p><img src="https://i.loli.net/2020/10/30/IDl2hH8j3JVdx6F.jpg" alt="img"></p><p>GPT是“Generative Pre-Training”的简称，从名字看其含义是指的生成式的预训练。GPT也采用两阶段过程，第一个阶段是利用语言模型进行预训练，第二阶段通过Fine-tuning的模式解决下游任务。上图展示了GPT的预训练过程，其实和ELMO是类似的，主要不同在于两点：首先，特征抽取器不是用的RNN，而是用的Transformer，上面提到过它的特征抽取能力要强于RNN，这个选择很明显是很明智的；其次，GPT的预训练虽然仍然是以语言模型作为目标任务，但是采用的是单向的语言模型，所谓“单向”的含义是指：语言模型训练的任务目标是根据 Wi 单词的上下文去正确预测单词 Wi ， Wi 之前的单词序列Context-before称为上文，之后的单词序列Context-after称为下文。ELMO在做语言模型预训练的时候，预测单词 Wi 同时使用了上文和下文，而GPT则只采用Context-before这个单词的上文来进行预测，而抛开了下文。这个选择现在看不是个太好的选择，原因很简单，它没有把单词的下文融合进来，这限制了其在更多应用场景的效果，比如阅读理解这种任务，在做任务的时候是可以允许同时看到上文和下文一起做决策的。如果预训练时候不把单词的下文嵌入到Word Embedding中，是很吃亏的，白白丢掉了很多信息。</p><p>这里强行插入一段简单提下Transformer，尽管上面提到了，但是说的还不完整，补充两句。首先，Transformer是个叠加的“自注意力机制（Self Attention）”构成的深度网络，是目前NLP里最强的特征提取器，注意力这个机制在此被发扬光大，从任务的配角不断抢戏，直到Transformer一跃成为踢开RNN和CNN传统特征提取器，荣升头牌，大红大紫。注意力机制可以参考“<a href="https://zhuanlan.zhihu.com/p/37601161" target="_blank" rel="noopener">深度学习中的注意力模型</a>”，补充下相关基础知识，如果不了解注意力机制你肯定会落后时代的发展。而介绍Transformer比较好的文章可以参考以下两篇文章：一个是Jay Alammar可视化地介绍Transformer的博客文章<a href="https://link.zhihu.com/?target=https%3A//jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a> ，非常容易理解整个机制，建议先从这篇看起；然后可以参考哈佛大学NLP研究组写的“<a href="https://link.zhihu.com/?target=http%3A//nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer.</a> ”，代码原理双管齐下，讲得非常清楚。我相信上面两个文章足以让你了解Transformer了，所以这里不展开介绍。</p><p>其次，我的判断是Transformer在未来会逐渐替代掉RNN成为主流的NLP工具，RNN一直受困于其并行计算能力，这是因为它本身结构的序列性依赖导致的，尽管很多人在试图通过修正RNN结构来修正这一点，但是我不看好这种模式，因为给马车换轮胎不如把它升级到汽车，这个道理很好懂，更何况目前汽车的雏形已经出现了，干嘛还要执着在换轮胎这个事情呢？是吧？再说CNN，CNN在NLP里一直没有形成主流，CNN的最大优点是易于做并行计算，所以速度快，但是在捕获NLP的序列关系尤其是长距离特征方面天然有缺陷，不是做不到而是做不好，目前也有很多改进模型，但是特别成功的不多。综合各方面情况，很明显Transformer同时具备并行性好，又适合捕获长距离特征，没有理由不在赛跑比赛中跑不过RNN和CNN。</p><p>好了，题外话结束，我们再回到主题，接着说GPT。上面讲的是GPT如何进行第一阶段的预训练，那么假设预训练好了网络模型，后面下游任务怎么用？它有自己的个性，和ELMO的方式大有不同。</p><p><img src="https://i.loli.net/2020/10/30/3Gr9vqoPHkSfcAg.jpg" alt="img"></p><p>上图展示了GPT在第二阶段如何使用。首先，对于不同的下游任务来说，本来你可以任意设计自己的网络结构，现在不行了，你要向GPT的网络结构看齐，把任务的网络结构改造成和GPT的网络结构是一样的。然后，在做下游任务的时候，利用第一步预训练好的参数初始化GPT的网络结构，这样通过预训练学到的语言学知识就被引入到你手头的任务里来了，这是个非常好的事情。再次，你可以用手头的任务去训练这个网络，对网络参数进行Fine-tuning，使得这个网络更适合解决手头的问题。就是这样。看到了么？这有没有让你想起最开始提到的图像领域如何做预训练的过程（请参考上图那句非常容易暴露年龄的歌词）？对，这跟那个模式是一模一样的。</p><p>这里引入了一个新问题：对于NLP各种花样的不同任务，怎么改造才能靠近GPT的网络结构呢？</p><p><img src="https://i.loli.net/2020/10/30/iJqb8TYLwCvdSVk.jpg" alt="img"></p><p>GPT论文给了一个改造施工图如上，其实也很简单：对于分类问题，不用怎么动，加上一个起始和终结符号即可；对于句子关系判断问题，比如Entailment，两个句子中间再加个分隔符即可；对文本相似性判断问题，把两个句子顺序颠倒下做出两个输入即可，这是为了告诉模型句子顺序不重要；对于多项选择问题，则多路输入，每一路把文章和答案选项拼接作为输入即可。从上图可看出，这种改造还是很方便的，不同任务只需要在输入部分施工即可。</p><p><img src="https://i.loli.net/2020/10/30/qnLcVGo5IK6riYh.jpg" alt="img"></p><p>GPT的效果是非常令人惊艳的，在12个任务里，9个达到了最好的效果，有些任务性能提升非常明显。</p><p><img src="https://i.loli.net/2020/10/30/96zdAXvcOmJTuP2.jpg" alt="img"></p><p>那么站在现在的时间节点看，GPT有什么值得改进的地方呢？其实最主要的就是那个单向语言模型，如果改造成双向的语言模型任务估计也没有Bert太多事了。当然，即使如此GPT也是非常非常好的一个工作，跟Bert比，其作者炒作能力亟待提升。</p><h3 id="bert的诞生">Bert的诞生</h3><p><img src="https://i.loli.net/2020/10/30/rSJAqOMB4sathDg.jpg" alt="img"></p><p>我们经过跋山涉水，终于到了目的地Bert模型了。</p><p>Bert采用和GPT完全相同的两阶段模型，首先是语言模型预训练；其次是使用Fine-Tuning模式解决下游任务。和GPT的最主要不同在于在预训练阶段采用了类似ELMO的双向语言模型，当然另外一点是语言模型的数据规模要比GPT大。所以这里Bert的预训练过程不必多讲了。</p><p><img src="https://i.loli.net/2020/10/30/61JpWKSZ5fF3tNk.jpg" alt="img"></p><p>第二阶段，Fine-Tuning阶段，这个阶段的做法和GPT是一样的。当然，它也面临着下游任务网络结构改造的问题，在改造任务方面Bert和GPT有些不同，下面简单介绍一下。</p><p><img src="https://i.loli.net/2020/10/30/UTQdhtVA7PzcIlF.jpg" alt="img"></p><p>在介绍Bert如何改造下游任务之前，先大致说下NLP的几类问题，说这个是为了强调Bert的普适性有多强。通常而言，绝大部分NLP问题可以归入上图所示的四类任务中：一类是序列标注，这是最典型的NLP任务，比如中文分词，词性标注，命名实体识别，语义角色标注等都可以归入这一类问题，它的特点是句子中每个单词要求模型根据上下文都要给出一个分类类别。第二类是分类任务，比如我们常见的文本分类，情感计算等都可以归入这一类。它的特点是不管文章有多长，总体给出一个分类类别即可。第三类任务是句子关系判断，比如Entailment，QA，语义改写，自然语言推理等任务都是这个模式，它的特点是给定两个句子，模型判断出两个句子是否具备某种语义关系；第四类是生成式任务，比如机器翻译，文本摘要，写诗造句，看图说话等都属于这一类。它的特点是输入文本内容后，需要自主生成另外一段文字。</p><p><img src="https://i.loli.net/2020/10/30/mxJybVWl2OatfUc.jpg" alt="img"></p><p>对于种类如此繁多而且各具特点的下游NLP任务，Bert如何改造输入输出部分使得大部分NLP任务都可以使用Bert预训练好的模型参数呢？上图给出示例，对于句子关系类任务，很简单，和GPT类似，加上一个起始和终结符号，句子之间加个分隔符即可。对于输出来说，把第一个起始符号对应的Transformer最后一层位置上面串接一个softmax分类层即可。对于分类问题，与GPT一样，只需要增加起始和终结符号，输出部分和句子关系判断任务类似改造；对于序列标注问题，输入部分和单句分类是一样的，只需要输出部分Transformer最后一层每个单词对应位置都进行分类即可。从这里可以看出，上面列出的NLP四大任务里面，除了生成类任务外，Bert其它都覆盖到了，而且改造起来很简单直观。尽管Bert论文没有提，但是稍微动动脑子就可以想到，其实对于机器翻译或者文本摘要，聊天机器人这种生成式任务，同样可以稍作改造即可引入Bert的预训练成果。只需要附着在S2S结构上，encoder部分是个深度Transformer结构，decoder部分也是个深度Transformer结构。根据任务选择不同的预训练数据初始化encoder和decoder即可。这是相当直观的一种改造方法。当然，也可以更简单一点，比如直接在单个Transformer结构上加装隐层产生输出也是可以的。不论如何，从这里可以看出，NLP四大类任务都可以比较方便地改造成Bert能够接受的方式。这其实是Bert的非常大的优点，这意味着它几乎可以做任何NLP的下游任务，具备普适性，这是很强的。</p><p><img src="https://i.loli.net/2020/10/30/e7tSMGZjDHmY1Ck.jpg" alt="img"></p><p>Bert采用这种两阶段方式解决各种NLP任务效果如何？在11个各种类型的NLP任务中达到目前最好的效果，某些任务性能有极大的提升。一个新模型好不好，效果才是王道。</p><p><img src="https://i.loli.net/2020/10/30/RniS8uQhpDmcHN6.jpg" alt="img"></p><p>到这里我们可以再梳理下几个模型之间的演进关系。从上图可见，Bert其实和ELMO及GPT存在千丝万缕的关系，比如如果我们把GPT预训练阶段换成双向语言模型，那么就得到了Bert；而如果我们把ELMO的特征抽取器换成Transformer，那么我们也会得到Bert。所以你可以看出：Bert最关键两点，一点是特征抽取器采用Transformer；第二点是预训练的时候采用双向语言模型。</p><p>那么新问题来了：对于Transformer来说，怎么才能在这个结构上做双向语言模型任务呢？乍一看上去好像不太好搞。我觉得吧，其实有一种很直观的思路，怎么办？看看ELMO的网络结构图，只需要把两个LSTM替换成两个Transformer，一个负责正向，一个负责反向特征提取，其实应该就可以。当然这是我自己的改造，Bert没这么做。那么Bert是怎么做的呢？我们前面不是提过Word2Vec吗？我前面肯定不是漫无目的地提到它，提它是为了在这里引出那个CBOW训练方法，所谓写作时候埋伏笔的“草蛇灰线，伏脉千里”，大概就是这个意思吧？前面提到了CBOW方法，它的核心思想是：在做语言模型任务的时候，我把要预测的单词抠掉，然后根据它的上文Context-Before和下文Context-after去预测单词。其实Bert怎么做的？Bert就是这么做的。从这里可以看到方法间的继承关系。当然Bert作者没提Word2Vec及CBOW方法，这是我的判断，Bert作者说是受到完形填空任务的启发，这也很可能，但是我觉得他们要是没想到过CBOW估计是不太可能的。</p><p>从这里可以看出，在文章开始我说过Bert在模型方面其实没有太大创新，更像一个最近几年NLP重要技术的集大成者，原因在于此，当然我不确定你怎么看，是否认同这种看法，而且我也不关心你怎么看。其实Bert本身的效果好和普适性强才是最大的亮点。</p><p><img src="https://i.loli.net/2020/10/30/LO9j7cIxEJCe2Ay.jpg" alt="img"></p><p>那么Bert本身在模型和方法角度有什么创新呢？就是论文中指出的Masked 语言模型和Next Sentence Prediction。而Masked语言模型上面讲了，本质思想其实是CBOW，但是细节方面有改进。</p><p><img src="https://i.loli.net/2020/10/30/75DVNACdHgtRbS9.jpg" alt="img"></p><p>Masked双向语言模型向上图展示这么做：随机选择语料中15%的单词，把它抠掉，也就是用[Mask]掩码代替原始单词，然后要求模型去正确预测被抠掉的单词。但是这里有个问题：训练过程大量看到[mask]标记，但是真正后面用的时候是不会有这个标记的，这会引导模型认为输出是针对[mask]这个标记的，但是实际使用又见不到这个标记，这自然会有问题。为了避免这个问题，Bert改造了一下，15%的被上天选中要执行[mask]替身这项光荣任务的单词中，只有80%真正被替换成[mask]标记，10%被狸猫换太子随机替换成另外一个单词，10%情况这个单词还待在原地不做改动。这就是Masked双向语音模型的具体做法。</p><p><img src="https://i.loli.net/2020/10/30/MTCajrZPKuF51Ne.jpg" alt="img"></p><p>至于说“Next Sentence Prediction”，指的是做语言模型预训练的时候，分两种情况选择两个句子，一种是选择语料中真正顺序相连的两个句子；另外一种是第二个句子从语料库中抛色子，随机选择一个拼到第一个句子后面。我们要求模型除了做上述的Masked语言模型任务外，附带再做个句子关系预测，判断第二个句子是不是真的是第一个句子的后续句子。之所以这么做，是考虑到很多NLP任务是句子关系判断任务，单词预测粒度的训练到不了句子关系这个层级，增加这个任务有助于下游句子关系判断任务。所以可以看到，它的预训练是个多任务过程。这也是Bert的一个创新。</p><p><img src="https://i.loli.net/2020/10/30/XQ17TqJcYpPoA3i.jpg" alt="img"></p><p>上面这个图给出了一个我们此前利用微博数据和开源的Bert做预训练时随机抽出的一个中文训练实例，从中可以体会下上面讲的masked语言模型和下句预测任务。训练数据就长这种样子。</p><p><img src="https://i.loli.net/2020/10/30/E1vcQhzTsgbLqkF.jpg" alt="img"></p><p>顺带讲解下Bert的输入部分，也算是有些特色。它的输入部分是个线性序列，两个句子通过分隔符分割，最前面和最后增加两个标识符号。每个单词有三个embedding:位置信息embedding，这是因为NLP中单词顺序是很重要的特征，需要在这里对位置信息进行编码；单词embedding,这个就是我们之前一直提到的单词embedding；第三个是句子embedding，因为前面提到训练数据都是由两个句子构成的，那么每个句子有个句子整体的embedding项对应给每个单词。把单词对应的三个embedding叠加，就形成了Bert的输入。</p><p><img src="https://i.loli.net/2020/10/30/wHfCcyhaiXPMx1d.jpg" alt="img"></p><p>至于Bert在预训练的输出部分如何组织，可以参考上图的注释。</p><p><img src="https://i.loli.net/2020/10/30/B5wItXbYpPr619Z.jpg" alt="img"></p><p>我们说过Bert效果特别好，那么到底是什么因素起作用呢？如上图所示，对比试验可以证明，跟GPT相比，双向语言模型起到了最主要的作用，对于那些需要看到下文的任务来说尤其如此。而预测下个句子来说对整体性能来说影响不算太大，跟具体任务关联度比较高。</p><p><img src="https://i.loli.net/2020/10/30/u9mKAEGjp4fa7ys.jpg" alt="img"></p><p>最后，我讲讲我对Bert的评价和看法，我觉得Bert是NLP里里程碑式的工作，对于后面NLP的研究和工业应用会产生长久的影响，这点毫无疑问。但是从上文介绍也可以看出，从模型或者方法角度看，Bert借鉴了ELMO，GPT及CBOW，主要提出了Masked 语言模型及Next Sentence Prediction，但是这里Next Sentence Prediction基本不影响大局，而Masked LM明显借鉴了CBOW的思想。所以说Bert的模型没什么大的创新，更像最近几年NLP重要进展的集大成者，这点如果你看懂了上文估计也没有太大异议，如果你有大的异议，杠精这个大帽子我随时准备戴给你。如果归纳一下这些进展就是：首先是两阶段模型，第一阶段双向语言模型预训练，这里注意要用双向而不是单向，第二阶段采用具体任务Fine-tuning或者做特征集成；第二是特征抽取要用Transformer作为特征提取器而不是RNN或者CNN；第三，双向语言模型可以采取CBOW的方法去做（当然我觉得这个是个细节问题，不算太关键，前两个因素比较关键）。Bert最大的亮点在于效果好及普适性强，几乎所有NLP任务都可以套用Bert这种两阶段解决思路，而且效果应该会有明显提升。可以预见的是，未来一段时间在NLP应用领域，Transformer将占据主导地位，而且这种两阶段预训练方法也会主导各种应用。</p><p>另外，我们应该弄清楚预训练这个过程本质上是在做什么事情，本质上预训练是通过设计好一个网络结构来做语言模型任务，然后把大量甚至是无穷尽的无标注的自然语言文本利用起来，预训练任务把大量语言学知识抽取出来编码到网络结构中，当手头任务带有标注信息的数据有限时，这些先验的语言学特征当然会对手头任务有极大的特征补充作用，因为当数据有限的时候，很多语言学现象是覆盖不到的，泛化能力就弱，集成尽量通用的语言学知识自然会加强模型的泛化能力。如何引入先验的语言学知识其实一直是NLP尤其是深度学习场景下的NLP的主要目标之一，不过一直没有太好的解决办法，而ELMO/GPT/Bert的这种两阶段模式看起来无疑是解决这个问题自然又简洁的方法，这也是这些方法的主要价值所在。</p><p>对于当前NLP的发展方向，我个人觉得有两点非常重要，一个是需要更强的特征抽取器，目前看Transformer会逐渐担当大任，但是肯定还是不够强的，需要发展更强的特征抽取器；第二个就是如何优雅地引入大量无监督数据中包含的语言学知识，注意我这里强调地是优雅，而不是引入，此前相当多的工作试图做各种语言学知识的嫁接或者引入，但是很多方法看着让人牙疼，就是我说的不优雅。目前看预训练这种两阶段方法还是很有效的，也非常简洁，当然后面肯定还会有更好的模型出现。</p><p>完了，这就是自然语言模型预训练的发展史。</p><h3 id="参考">参考</h3><blockquote><p><a href="https://zhuanlan.zhihu.com/p/49271699" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/49271699</a></p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      从embedding到BERT预训练模型
    
    </summary>
    
    
    
  </entry>
  
</feed>
