<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>思建的NLP之旅</title>
  
  <subtitle>沉淀自己</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-08-10T16:16:20.533Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>李思建</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>2020-08-09-常用chrome快捷键</title>
    <link href="http://yoursite.com/2020/08/09/2020-08-09-%E5%B8%B8%E7%94%A8chrome%E5%BF%AB%E6%8D%B7%E9%94%AE/"/>
    <id>http://yoursite.com/2020/08/09/2020-08-09-%E5%B8%B8%E7%94%A8chrome%E5%BF%AB%E6%8D%B7%E9%94%AE/</id>
    <published>2020-08-09T06:43:06.000Z</published>
    <updated>2020-08-10T16:16:20.533Z</updated>
    
    <content type="html"><![CDATA[<p>chrome是我日常使用的浏览器，平时也会使用快捷键来提高效率。chrome的快捷键真的很好使，可以</p><p>摆脱很多不必要的鼠标点击，键盘直接搞定。总结一下我常用的快捷键。</p><table><thead><tr><th align="center">描述</th><th align="center">快捷键</th></tr></thead><tbody><tr><td align="center">打开新窗口</td><td align="center">Ctrl + n</td></tr><tr><td align="center">在隐身模式下打开新窗口</td><td align="center">Ctrl + Shift + n</td></tr><tr><td align="center">—–</td><td align="center"></td></tr><tr><td align="center">打开新的标签页，并跳转到该标签页</td><td align="center">Ctrl + t</td></tr><tr><td align="center">恢复已关闭的标签页</td><td align="center">Ctrl + Shift + t</td></tr><tr><td align="center">跳转到下一个标签页</td><td align="center">Ctrl + Tab</td></tr><tr><td align="center">跳转到上一个标签页</td><td align="center">Ctrl + Shift + Tab</td></tr><tr><td align="center">关闭当前标签页</td><td align="center">Ctrl + w</td></tr><tr><td align="center">——</td><td align="center"></td></tr><tr><td align="center">打开当前标签页浏览记录中的上一个页面</td><td align="center">alt ＋左箭头</td></tr><tr><td align="center">打开当前标签页浏览记录中的下一个页面</td><td align="center">alt ＋右箭头</td></tr><tr><td align="center">——</td><td align="center"></td></tr><tr><td align="center">保存当前标签页为书签</td><td align="center">Ctrl + d</td></tr><tr><td align="center">将所有打开的标签页以书签的形式保存在新文件夹中</td><td align="center">Ctrl + Shift + d</td></tr><tr><td align="center">——</td><td align="center"></td></tr><tr><td align="center">跳转到与查找栏中搜索字词相匹配的下一条内容</td><td align="center">Ctrl + g</td></tr><tr><td align="center">跳转到与查找栏中搜索字词相匹配的上一条内容</td><td align="center">Ctrl + Shift + g</td></tr><tr><td align="center">浏览下一个可点击项</td><td align="center">Tab</td></tr><tr><td align="center">浏览上一个可点击项</td><td align="center">Shift + Tab</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      记录常用chrome快捷键
    
    </summary>
    
    
      <category term="技术" scheme="http://yoursite.com/categories/%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="技术" scheme="http://yoursite.com/tags/%E6%8A%80%E6%9C%AF/"/>
    
      <category term="chrome" scheme="http://yoursite.com/tags/chrome/"/>
    
  </entry>
  
  <entry>
    <title>2020-08-08-hexo新建page</title>
    <link href="http://yoursite.com/2020/08/08/2020-08-08-hexo%E6%96%B0%E5%BB%BApage/"/>
    <id>http://yoursite.com/2020/08/08/2020-08-08-hexo%E6%96%B0%E5%BB%BApage/</id>
    <published>2020-08-08T11:10:50.000Z</published>
    <updated>2020-08-08T11:52:04.652Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>在博客中需要一些个性化设置，添加一些page等 ，记录下我的操作</p><h3 id="添加page-界面"><a href="#添加page-界面" class="headerlink" title="添加page 界面"></a>添加page 界面</h3><p>我想要添加一个“一句话感想”的page，于是可以这样操作</p><p>step 1.hexo新建新的page界面</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new page onesentence  <span class="comment"># onesentence 是新建page的名称 （最好是英文名）</span></span><br></pre></td></tr></table></figure><p>这时候在博客的source文件夹里会有一个onesentence的文件夹，并且里面生成了一个index.md文件，用于写一句话感想的内容</p><p>step 2.在主题的配置文件 _config.yml 文件中的 menu 中进行匹配，如下图，添加一个onesentence项，<code>/onesentence</code>表示挂接到上述的新建文件夹里，</p><p>在这里也可以设置图标，在fontawesome网站里找，我找了一个保龄球:bowling:的图标，和page主题没啥联系，就是看着顺眼 :laughing:</p><p>此时<code>hexo s -g</code> 就可以看到已经有了这个界面，不过是英文的文件名，所以此时还要设置一下此文件名的中文名映射</p><p><img src="https://i.loli.net/2020/08/08/jiUOEvuzWnT7Kmp.png" alt="image-20200808192748174"></p><p>step 3.   打开<strong>themes\next\languages</strong>，我用的是zh-CN，打开此文件，在menu下添加<code>onesentence: 一句话</code>，即可完成中文映射，</p><p>此时 hexo s -g ,就可以在本地服务器的侧边栏部分看到新添加的“一句话”page</p><p><img src="https://i.loli.net/2020/08/08/cCymB2KXMh1OPN5.png" alt="image-20200808193652117"></p><img src="https://i.loli.net/2020/08/08/ZrQkcR8IPspHMdi.png" alt="image-20200808194045722" style="zoom: 67%;" /><p>step 4.  编辑“一句话”页面下的md文件，部署就能看到内容</p>]]></content>
    
    <summary type="html">
    
      新建page页
    
    </summary>
    
    
      <category term="hexo" scheme="http://yoursite.com/categories/hexo/"/>
    
    
      <category term="hexo" scheme="http://yoursite.com/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>2020-08-08-next缺少custom.styl的问题</title>
    <link href="http://yoursite.com/2020/08/08/2020-08-08-%E7%BC%BA%E5%B0%91custom-styl%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <id>http://yoursite.com/2020/08/08/2020-08-08-%E7%BC%BA%E5%B0%91custom-styl%E7%9A%84%E9%97%AE%E9%A2%98/</id>
    <published>2020-08-08T10:56:09.000Z</published>
    <updated>2020-08-10T07:20:00.982Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>在 next7.x 版本中没有custom.styl文件。如果我们想要在博客中添加自己的css样式，可以在此文件中添加，下面介绍一下</p><h3 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h3><p>step1 ：添加custom.styl文件</p><p>文件路径：<code>~\themes\next\source\css</code> ,添加<code>_custom</code>文件夹。然后在<code>_custom</code>中创建<code>custom.styl</code>文件。我们自己的样式就可以在此文件中添加</p><p>step2： 添加引用</p><p>在<code>~\themes\next\source\css</code>中的<code>main.styl</code>文件末尾加入引用即可</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;My Layer</span><br><span class="line">@import &quot;_custom&#x2F;custom.styl&quot;;</span><br></pre></td></tr></table></figure><p>step3： 添加样式</p><p>用vscode打开<code>custom.styl</code>，博客背景以及前页的不透明度等等，就可以更换样式了。</p><p>对于网页的组件，F12打开调试界面，就可以知道每个组件的名称等信息，便于更改样式</p>]]></content>
    
    <summary type="html">
    
      next7.x 版本没有custom.styl文件
    
    </summary>
    
    
      <category term="next" scheme="http://yoursite.com/categories/next/"/>
    
    
      <category term="故障排除" scheme="http://yoursite.com/tags/%E6%95%85%E9%9A%9C%E6%8E%92%E9%99%A4/"/>
    
      <category term="next" scheme="http://yoursite.com/tags/next/"/>
    
  </entry>
  
  <entry>
    <title>2020-08-08-vim常见操作</title>
    <link href="http://yoursite.com/2020/08/08/2020-08-08-vim%E5%B8%B8%E8%A7%81%E6%93%8D%E4%BD%9C/"/>
    <id>http://yoursite.com/2020/08/08/2020-08-08-vim%E5%B8%B8%E8%A7%81%E6%93%8D%E4%BD%9C/</id>
    <published>2020-08-08T05:22:42.000Z</published>
    <updated>2020-08-10T16:16:14.191Z</updated>
    
    <content type="html"><![CDATA[<h3 id="常规操作"><a href="#常规操作" class="headerlink" title="常规操作"></a>常规操作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">vim a.txt  <span class="comment"># 创建a.txt文件并进入编辑状态 。 如果a.txt 已经存在，则直接进入编辑状态</span></span><br><span class="line"></span><br><span class="line"><span class="number">1.</span> 按下i键，下端显示 –INSERT–。可以进行插入，输入文本 </span><br><span class="line"></span><br><span class="line"><span class="number">2.</span> 输入了之后 按Esc键退出编辑状态</span><br><span class="line"></span><br><span class="line"><span class="number">3.</span> 键入 :wq! 强制保存文件并退出  <span class="comment"># !是强制执行，注：有些文件设置了只读，一般不是修改文件的，但是如果你是`文件的owner或者root的话，通过wq!还是能保存文件退出。:wq不可以</span></span><br><span class="line"></span><br><span class="line">   :w 在编辑的过程中保存文件,相当于word中的ctrl+s    </span><br><span class="line"></span><br><span class="line">   :wq 保存文件并退出 <span class="comment">#一般使用这个命令退出</span></span><br></pre></td></tr></table></figure><blockquote><p>注：以<strong><code>:</code></strong>和<strong><code>/</code></strong>开头的命令都有历史纪录，可以首先键入:或/然后按<strong>上下箭头</strong>来选择某个历史命令</p></blockquote><h3 id="Vim模式"><a href="#Vim模式" class="headerlink" title="Vim模式"></a>Vim模式</h3><p>(都是在英文输入环境下操作)</p><ul><li><strong>Normal</strong> 模式：进入Vim后的一般模式。</li><li><strong>Insert</strong> 模式：按下<code>i</code>键后进入插入模式，可以修改文档。</li><li><strong>Visual</strong> 模式：按下<code>v</code>键后进入选择模式，可以选择文档内容。</li></ul><h3 id="Vim打开和切换文件"><a href="#Vim打开和切换文件" class="headerlink" title="Vim打开和切换文件"></a>Vim打开和切换文件</h3><ul><li><code>:ls</code>显示打开的文件，可以使用<code>:bn</code>在文件间切换( n也可以换成<code>:ls</code>里给出的文件序号 )。</li><li>在终端<code>vim -o file1 file2 ...</code>可以打开多个文件(横向分隔屏幕)。</li><li>终端<code>vim -O file1 file2 ...</code>可以打开多个文件(纵向分隔屏幕)。 :star:</li><li><code>Ctrl</code>+<code>w</code>+<code>方向键</code>在窗口间切换光标</li></ul><h3 id="Vim退出"><a href="#Vim退出" class="headerlink" title="Vim退出"></a>Vim退出</h3><ul><li><p><code>:q</code>：退出。</p></li><li><p><code>:q!</code>：强制退出，放弃所有修改。</p></li><li><p><code>:wq</code>：保存修改并退出。:star:</p></li></ul><h3 id="常用快捷键"><a href="#常用快捷键" class="headerlink" title="常用快捷键"></a>常用快捷键</h3><ul><li><p><code>gg</code>到文档首行，<code>G</code>（shift+g）到文档结尾。</p></li><li><p><code>pageUp</code>下一页，<code>pageDown</code>上一页。</p></li><li><p><code>ctrl + d</code>    向下翻半页(down)， <code>ctrl + u</code>    向上翻半页(up)  :star:</p></li><li><p><code>H</code>将光标移动到屏幕首行，<code>M</code>将光标移动到屏幕中间行，<code>L</code>将光标移动到屏幕最后一行。</p></li><li><p><code>q:</code>显示<strong>命令行历史记录</strong>（显示开头为:的历史命令行）窗口，可以选择命令行执行。若是<code>q/</code>,则会显示开头为/的历史命令行 </p></li><li><p><code>u</code> 撤销  (undo) :star:</p></li><li><p><code>w</code>    下一个单词     word</p></li><li><p><code>b</code>     前一个单词     behind</p></li><li><p><code>e</code>    本单词末尾     end</p></li><li><p><code>:set nu</code>    显示行号   (number )      　</p></li><li><p><code>:set nonu</code>   隐藏行号   ( number)</p></li><li><p><code>:98</code>跳转到第98行。</p></li><li><p><code>:5,10d</code>    //回车后，第5~10行被删除</p></li><li><p><code>:5,$d</code>    //回车后，第5~最后一行被删除</p></li><li><p><code>:5,10y</code>    //回车后，第5~10行被复制 </p></li></ul><h3 id="复制粘贴"><a href="#复制粘贴" class="headerlink" title="复制粘贴"></a>复制粘贴</h3><ul><li>在<strong>Visual</strong>模式下选择文档内容后按<code>y</code>键，复制被选择内容。主要用于<strong>多行文字</strong>（复制完之后vim自动退出Visual模式）</li><li>在<strong>Visual</strong>模式下选择文档内容后按<code>d</code>删除</li><li>按<code>p</code>键粘贴，注意粘贴从<strong>紧跟光标后的那个字符</strong>之后才开始。（不需要进入Visual模式）</li><li><code>yy</code>复制当前行，<code>dd</code>删除(剪贴)当前行。 用于<strong>一行文字</strong></li><li><code>:5,10y</code>    //回车后，第5~10行被复制 </li></ul><p>如果在vim外的其它文件里复制内容到vim里，则无法使用<code>p</code>进行粘贴，此时右键粘贴即可（无需进入inset模式）</p><h3 id="查找"><a href="#查找" class="headerlink" title="查找"></a>查找</h3><ul><li>在<strong>Normal</strong>模式下，按<code>/</code>进入查找模式，输入<code>/word</code>后回车，高亮显示所有文档<code>word</code>，按<code>n</code>跳到下一个<code>word</code>,按<code>N</code>跳到上一个。（默认大小写敏感）</li><li>若输入<code>/word\c</code>代表大小写不敏感查找，<code>\C</code>代表大小写敏感。</li><li>在<strong>Normal</strong>模式下按<code>q</code>+<code>/</code>显示<strong>查找历史记录</strong>窗口。</li><li>如果一个词很长，键入麻烦，可以将光标移动到该词上，按<code>*</code>键即可以该单词进行搜索，相当于/搜索。</li></ul><p><img src="https://i.loli.net/2020/08/11/GZszjJB9uIUMFTq.png" alt="img"></p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><blockquote><p>vim的常用操作  <a href="https://www.cnblogs.com/doseoer/p/6241443.html" target="_blank" rel="noopener">https://www.cnblogs.com/doseoer/p/6241443.html</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      vim常见操作的总结
    
    </summary>
    
    
      <category term="linux" scheme="http://yoursite.com/categories/linux/"/>
    
    
      <category term="linux" scheme="http://yoursite.com/tags/linux/"/>
    
      <category term="vim" scheme="http://yoursite.com/tags/vim/"/>
    
  </entry>
  
  <entry>
    <title>2020-08-07-transformerXL解读</title>
    <link href="http://yoursite.com/2020/08/07/2020-08-07-transformerXL%E8%A7%A3%E8%AF%BB/"/>
    <id>http://yoursite.com/2020/08/07/2020-08-07-transformerXL%E8%A7%A3%E8%AF%BB/</id>
    <published>2020-08-07T10:20:34.000Z</published>
    <updated>2020-08-10T16:15:46.034Z</updated>
    
    <content type="html"><![CDATA[<p> Transformer最大的问题：在语言建模时的设置受到固定长度上下文的限制。</p><p>本文提出的Transformer-XL，使学习不再仅仅依赖于定长，且不破坏时间的相关性。</p><p>Transformer-XL包含segment-level 循环机制和positional编码框架。不仅可以捕捉长时依赖，还可以解决上下文断片问题 fragmentation problem。可以学到比RNNs长80%的依赖，比vanilla Transformers长450%。在长短序列上都取得了更好的结果。与vanilla Transformer相比，Transformer-XL的另一个优势是它可以被用于单词级和字符级的语言建模。</p>]]></content>
    
    <summary type="html">
    
      最近读的论文transformer-XL
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-08-06-pytorch函数之nn.Linear</title>
    <link href="http://yoursite.com/2020/08/06/2020-08-06-pytorch%E5%87%BD%E6%95%B0%E4%B9%8BLinear/"/>
    <id>http://yoursite.com/2020/08/06/2020-08-06-pytorch%E5%87%BD%E6%95%B0%E4%B9%8BLinear/</id>
    <published>2020-08-06T02:29:09.000Z</published>
    <updated>2020-08-08T06:51:12.414Z</updated>
    
    <content type="html"><![CDATA[<h1 id=""><a href="#" class="headerlink" title=""></a></h1><p>class torch.nn.Linear（in_features，out_features，bias = True ）</p><p>对传入数据应用线性变换：y = A x+ b</p><p>参数：</p><p>in_features - 每个输入样本的大小</p><p>out_features - 每个输出样本的大小</p><p>bias - 如果设置为False，则图层不会学习附加偏差。默认值：True</p><p>代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">m &#x3D; nn.Linear(20, 30)</span><br><span class="line"></span><br><span class="line">input &#x3D; autograd.Variable(torch.randn(128, 20))</span><br><span class="line"></span><br><span class="line">output &#x3D; m(input)</span><br><span class="line"></span><br><span class="line">print(output.size())</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([128, 30])</span><br></pre></td></tr></table></figure><p>分析:</p><p>output.size()=矩阵size(128,20)*矩阵size（20,30）=(128,30)</p>]]></content>
    
    <summary type="html">
    
      Linear函数
    
    </summary>
    
    
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>2020-08-05-BPE算法</title>
    <link href="http://yoursite.com/2020/08/05/2020-08-05-%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2020/08/05/2020-08-05-%E7%AE%97%E6%B3%95/</id>
    <published>2020-08-05T08:35:26.000Z</published>
    <updated>2020-08-10T16:15:43.534Z</updated>
    
    <content type="html"><![CDATA[<h3 id="总说"><a href="#总说" class="headerlink" title="总说"></a><strong>总说</strong></h3><p>BPE，（byte pair encoder）字节对编码，也可以叫做digram coding双字母组合编码，<code>主要目的是为了数据压缩</code>，算法描述为<code>字符串里频率最常见的一对字符被一个没有在这个字符中出现的字符代替的层层迭代过程</code>。具体在下面描述。</p><h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><ol><li>准备足够大的训练语料</li><li>确定期望的<strong>subword词表大小</strong></li><li>将单词拆分为字符序列并在<strong>末尾添加后缀“ &lt;/ w&gt;”</strong>，统计单词频率。 本阶段的subword的粒度是字符。 例如，“ low”的频率为5，那么我们将其改写为“ l o w &lt;/ w&gt;”：5</li><li>统计每一个连续字节对的出现频率，选择最高频者合并成新的subword</li><li>重复第4步直到达到第2步设定的subword词表大小或下一个最高频的字节对出现频率为1</li></ol><p>停止符”</w>“的意义在于表示subword是词后缀。举例来说：”st”字词不加”</w>“可以出现在词首如”st ar”，加了”</w>“表明改字词位于词尾，如”wide st</w>“，二者意义截然不同。</p><p>每次合并后词表可能出现3种变化：</p><ul><li>+1，表明加入合并后的新字词，同时原来的2个子词还保留（2个字词不是完全同时连续出现）</li><li>+0，表明加入合并后的新字词，同时原来的2个子词中一个保留，一个被消解（一个字词完全随着另一个字词的出现而紧跟着出现）</li><li>-1，表明加入合并后的新字词，同时原来的2个子词都被消解（2个字词同时连续出现）</li></ul><p>实际上，随着合并的次数增加，词表大小通常先增加后减小。</p><h4 id="例子1"><a href="#例子1" class="headerlink" title="例子1"></a><strong>例子1</strong></h4><p>输入：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;'l o w &lt;/w&gt;': 5, 'l o w e r &lt;/w&gt;': 2, 'n e w e s t &lt;/w&gt;': 6, 'w i d e s t &lt;/w&gt;': 3&#125;</span><br></pre></td></tr></table></figure><p>Iter 1, 最高频连续字节对”e”和”s”出现了6+3=9次，合并成”es”。输出：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;'l o w &lt;/w&gt;': 5, 'l o w e r &lt;/w&gt;': 2, 'n e w es t &lt;/w&gt;': 6, 'w i d es t &lt;/w&gt;': 3&#125;</span><br></pre></td></tr></table></figure><p>Iter 2, 最高频连续字节对”es”和”t”出现了6+3=9次, 合并成”est”。输出：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;'l o w &lt;/w&gt;': 5, 'l o w e r &lt;/w&gt;': 2, 'n e w est &lt;/w&gt;': 6, 'w i d est &lt;/w&gt;': 3&#125;</span><br></pre></td></tr></table></figure><p>Iter 3, 以此类推，最高频连续字节对为”est”和”</w>“ 输出：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;'l o w &lt;/w&gt;': 5, 'l o w e r &lt;/w&gt;': 2, 'n e w est&lt;/w&gt;': 6, 'w i d est&lt;/w&gt;': 3&#125;</span><br></pre></td></tr></table></figure><p>……</p><p>Iter n, 继续迭代<strong>直到达到预设的subword词表大小或下一个最高频的字节对出现频率为1</strong>。</p><h3 id="BPE实现"><a href="#BPE实现" class="headerlink" title="BPE实现"></a>BPE实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re, collections</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_stats</span><span class="params">(vocab)</span>:</span></span><br><span class="line">    pairs = collections.defaultdict(int)</span><br><span class="line">    <span class="keyword">for</span> word, freq <span class="keyword">in</span> vocab.items():</span><br><span class="line">        symbols = word.split()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(symbols)<span class="number">-1</span>):</span><br><span class="line">            pairs[symbols[i],symbols[i+<span class="number">1</span>]] += freq</span><br><span class="line">    <span class="keyword">return</span> pairs</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge_vocab</span><span class="params">(pair, v_in)</span>:</span></span><br><span class="line">    v_out = &#123;&#125;</span><br><span class="line">    bigram = re.escape(<span class="string">' '</span>.join(pair))</span><br><span class="line">    p = re.compile(<span class="string">r'(?&lt;!\S)'</span> + bigram + <span class="string">r'(?!\S)'</span>)</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> v_in:</span><br><span class="line">        w_out = p.sub(<span class="string">''</span>.join(pair), word)</span><br><span class="line">        v_out[w_out] = v_in[word]</span><br><span class="line">    <span class="keyword">return</span> v_out</span><br><span class="line"></span><br><span class="line">vocab = &#123;<span class="string">'l o w &lt;/w&gt;'</span>: <span class="number">5</span>, <span class="string">'l o w e r &lt;/w&gt;'</span>: <span class="number">2</span>, <span class="string">'n e w e s t &lt;/w&gt;'</span>: <span class="number">6</span>, <span class="string">'w i d e s t &lt;/w&gt;'</span>: <span class="number">3</span>&#125;</span><br><span class="line">num_merges = <span class="number">1000</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_merges):</span><br><span class="line">    pairs = get_stats(vocab)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> pairs:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    best = max(pairs, key=pairs.get)</span><br><span class="line">    vocab = merge_vocab(best, vocab)</span><br><span class="line">    print(best)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print output</span></span><br><span class="line"><span class="comment"># ('e', 's')</span></span><br><span class="line"><span class="comment"># ('es', 't')</span></span><br><span class="line"><span class="comment"># ('est', '&lt;/w&gt;')</span></span><br><span class="line"><span class="comment"># ('l', 'o')</span></span><br><span class="line"><span class="comment"># ('lo', 'w')</span></span><br><span class="line"><span class="comment"># ('n', 'e')</span></span><br><span class="line"><span class="comment"># ('ne', 'w')</span></span><br><span class="line"><span class="comment"># ('new', 'est&lt;/w&gt;')</span></span><br><span class="line"><span class="comment"># ('low', '&lt;/w&gt;')</span></span><br><span class="line"><span class="comment"># ('w', 'i')</span></span><br><span class="line"><span class="comment"># ('wi', 'd')</span></span><br><span class="line"><span class="comment"># ('wid', 'est&lt;/w&gt;')</span></span><br><span class="line"><span class="comment"># ('low', 'e')</span></span><br><span class="line"><span class="comment"># ('lowe', 'r')</span></span><br><span class="line"><span class="comment"># ('lower', '&lt;/w&gt;')</span></span><br></pre></td></tr></table></figure><h3 id="编码和解码"><a href="#编码和解码" class="headerlink" title="编码和解码"></a>编码和解码</h3><ul><li><h4 id="编码"><a href="#编码" class="headerlink" title="编码"></a>编码</h4></li></ul><p>在之前的算法中，我们已经得到了<strong>subword词表</strong>，<strong>对该词表按照子词长度由大到小排序</strong>。编码时，<strong>对于每个单词，遍历排好序的子词词表寻找是否有token是当前单词的子字符串，如果有，则该token是表示单词的tokens之一</strong>。</p><p>我们从最长的token迭代到最短的token，尝试将每个单词中的子字符串替换为token。 最终，我们将迭代所有tokens，并将所有子字符串替换为tokens。 如果仍然有子字符串没被替换但所有token都已迭代完毕，则将剩余的子词替换为特殊token，如<unk>。</p><h4 id="例子2"><a href="#例子2" class="headerlink" title="例子2"></a>例子2</h4><p>用得到subword词表去表示含有多个单词的句子</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 给定单词序列</span></span><br><span class="line">[“the&lt;/w&gt;”, “highest&lt;/w&gt;”, “mountain&lt;/w&gt;”]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设已有排好序的subword词表</span></span><br><span class="line">[“errrr&lt;/w&gt;”, “tain&lt;/w&gt;”, “moun”, “est&lt;/w&gt;”, “high”, “the&lt;/w&gt;”, “a&lt;/w&gt;”]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 迭代结果</span></span><br><span class="line">"the&lt;/w&gt;" -&gt; ["the&lt;/w&gt;"]</span><br><span class="line">"highest&lt;/w&gt;" -&gt; ["high", "est&lt;/w&gt;"]</span><br><span class="line">"mountain&lt;/w&gt;" -&gt; ["moun", "tain&lt;/w&gt;"]</span><br></pre></td></tr></table></figure><p>编码的计算量很大。 在实践中，我们可以pre-tokenize所有单词，并在词典中保存单词tokenize的结果。 如果我们看到字典中不存在的未知单词。 我们应用上述编码方法对单词进行tokenize，然后将新单词的tokenization添加到字典中备用。</p><ul><li><h4 id="解码"><a href="#解码" class="headerlink" title="解码"></a>解码</h4></li></ul><p><strong>将所有的tokens拼在一起</strong>。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 编码序列</span></span><br><span class="line">[“the&lt;/w&gt;”, “high”, “est&lt;/w&gt;”, “moun”, “tain&lt;/w&gt;”]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解码序列</span></span><br><span class="line">“the&lt;/w&gt; highest&lt;/w&gt; mountain&lt;/w&gt;”</span><br></pre></td></tr></table></figure><h4 id="例子3"><a href="#例子3" class="headerlink" title="例子3"></a>例子3</h4><p>比如我们想编码：</p><p>aaabdaaabac</p><p>我们会发现这里的aa出现的词数最高（我们这里只看两个字符的频率），那么用这里没有的字符Z来替代aa：</p><p>ZabdZabac</p><p>Z=aa</p><p>此时，又发现ab出现的频率最高，那么同样的，Y来代替ab：</p><p>ZYdZYac</p><p>Y=ab</p><p>Z=aa</p><p>同样的，ZY出现的频率大，我们用X来替代ZY：</p><p>XdXac</p><p>X=ZY</p><p>Y=ab</p><p>Z=aa</p><p>最后，连续两个字符的频率都为1了，也就结束了。就是这么简单。</p><p>解码的时候，就按照相反的顺序更新替换即可。</p>]]></content>
    
    <summary type="html">
    
      BPE是数据压缩，常用于NLP任务中
    
    </summary>
    
    
      <category term="算法" scheme="http://yoursite.com/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="算法" scheme="http://yoursite.com/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>2020-08-04-博客优化以及问题解决</title>
    <link href="http://yoursite.com/2020/08/04/2020-08-04-%E5%8D%9A%E5%AE%A2%E4%BC%98%E5%8C%96%E4%BB%A5%E5%8F%8A%E8%A7%A3%E5%86%B3%E9%97%AE%E9%A2%98/"/>
    <id>http://yoursite.com/2020/08/04/2020-08-04-%E5%8D%9A%E5%AE%A2%E4%BC%98%E5%8C%96%E4%BB%A5%E5%8F%8A%E8%A7%A3%E5%86%B3%E9%97%AE%E9%A2%98/</id>
    <published>2020-08-04T09:45:51.000Z</published>
    <updated>2020-08-10T13:19:31.332Z</updated>
    
    <content type="html"><![CDATA[<h3 id="修改Git-Bash的默认打开工作路径"><a href="#修改Git-Bash的默认打开工作路径" class="headerlink" title="修改Git Bash的默认打开工作路径"></a>修改Git Bash的默认打开工作路径</h3><p>我每次想在我的博客文件夹里进入git bash，必须要打开文件夹才能进入，操作繁琐，于是在桌面建立git bash 快捷方式，并将git bash 的默认打开路径更改为我的博客文件夹下，这样点击图标，即能进入本地git仓库</p><p>1.找到git bash，右键属性，可以看到目标栏及起始位置栏。</p><img src="https://i.loli.net/2020/08/04/tPL1uzsVApn5FvC.png" alt="img" style="zoom: 80%;" /><p>将目标栏中的 –cd-to-home 去掉；将起始位置中填写为本地git仓库的路径，即可完成操作。如下图所示，博客文件夹位置在<code>E:\myBlog</code></p><img src="https://i.loli.net/2020/08/04/Vw7Kg3U2OIZQ6R9.png" alt="image-20200804181206322" style="zoom: 50%;" /><p>注： 若在文件夹里进入 git bash，则然后按下<code>shift+F10</code> （激活右键菜单栏），再按<code>s</code>跳转到git bash，最后按下<code>enter</code>即可</p><h3 id="博客打开网站和更新不完全"><a href="#博客打开网站和更新不完全" class="headerlink" title="博客打开网站和更新不完全"></a>博客打开网站和更新不完全</h3><p>在这几天在本地文件夹更新完配置文件对博客进行个性化设置时，使用<code>localhost:4000</code>访问本地blog可以正常显示更改后的样式，但是在登录网站域名就会出现不一致的现象，有时会响应速度慢，延时高，甚至连接超时。</p><p>在整个过程中一直没发现问题，因为本地localhost和网站不一致就不能理解。后来才发现，我的hexo命令写错了。本应该是hexo clean ，我错写为hexo clear，导致不能轻触缓存，所以在网站中不能及时更新显示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo clean <span class="comment"># 清除缓存，网页正常情况下可以忽略此命令</span></span><br></pre></td></tr></table></figure><h3 id="博客无法连接"><a href="#博客无法连接" class="headerlink" title="博客无法连接"></a>博客无法连接</h3><p>本地服务器可以正常显示，但是博客连接不上</p><p>如何可以ping 通，则代表不是域名方面的问题，应该就是服务器的问题，可能是部署在github上，所以会有点慢，后续准备买一个阿里云的服务器。</p><p>解决：</p><p>1.博客正在加载， 等一段时间刷新</p><p>2.如果还是不行，则清理chrome的cookie缓存再刷新即可， 可以解决问题，但是操作麻烦。</p><p><img src="https://i.loli.net/2020/08/10/Iwpk2yb5OAjuoKf.png" alt="image-20200810152929992"></p><p>3.清除特定网站下的缓存：</p><p>打开开发者工具（F12），选择 Network——Disable cache 。需要清除某网站缓存时 F12 打开开发者工具就会自动清除这个网站的缓存，而不必清除所有网站的缓存了。</p><p>4.如果在文章标题中使用了当天的日期，可能无法及时得到页面更新。因为Github使用了格林尼治标准时间，也就是UTC。中国是东八时区，UTC+8，对于hexo来说是一个未来的时间，所以新的Posts不会被渲染。</p><p>在hexo配置文件<code>_config.yml</code>中设置<code>timezone: Asia/Shanghai</code>  (有效解决问题)  :star:</p><p>参考</p><blockquote><p>博客无法更新post文章  <a href="https://www.jianshu.com/p/b73c28e77760" target="_blank" rel="noopener">https://www.jianshu.com/p/b73c28e77760</a></p></blockquote><p>clone的时候无法clone next的内容</p>]]></content>
    
    <summary type="html">
    
      记录在优化自己博客的时候遇到的问题以及解决方案
    
    </summary>
    
    
      <category term="博客" scheme="http://yoursite.com/categories/%E5%8D%9A%E5%AE%A2/"/>
    
    
      <category term="故障排除" scheme="http://yoursite.com/tags/%E6%95%85%E9%9A%9C%E6%8E%92%E9%99%A4/"/>
    
  </entry>
  
  <entry>
    <title>2020-08-01-hexo+next个性化设置</title>
    <link href="http://yoursite.com/2020/08/01/2020-08-10-hexo-next%E4%B8%AA%E6%80%A7%E5%8C%96%E8%AE%BE%E7%BD%AE/"/>
    <id>http://yoursite.com/2020/08/01/2020-08-10-hexo-next%E4%B8%AA%E6%80%A7%E5%8C%96%E8%AE%BE%E7%BD%AE/</id>
    <published>2020-08-01T09:28:10.000Z</published>
    <updated>2020-08-10T11:27:23.435Z</updated>
    
    <content type="html"><![CDATA[<p>一些基本的个性化设置可以参考其它博客，本文只记录在我完成 </p><h3 id="修改文章底部的那个带-号的标签"><a href="#修改文章底部的那个带-号的标签" class="headerlink" title="修改文章底部的那个带#号的标签"></a>修改文章底部的那个带#号的标签</h3><p>在原本next自带的标签格式如下所示：</p><p><img src="https://i.loli.net/2020/08/10/GLtxuMpYd7WXmaV.png" alt="image-20200810180543537"></p><p>前面的<code>#</code>不太好看，在这里可以添加<code>font awesome</code>的<code>icon</code>，个性化标签显示</p><p>修改模板 <code>/themes/next/layout/_macro/post.swig</code>，搜索 <code>rel=&quot;tag&quot;</code>，将<code>rel=&quot;tag&quot;&gt;</code>换成<code>rel=&quot;tag&quot;&lt;i class=&quot;fa fa-tag&quot;&gt;&lt;/i&gt;</code>  ，其中”fa fa-tag”可以根据<code>font awesome</code>里自己选择喜欢的<code>icon</code></p><p>因为在代码中不需要<code>tag_indicate</code>，所以可以将部分代码删去，如图中红框部分</p><p><img src="https://i.loli.net/2020/08/10/2zhlEdQjqyTcPRo.png" alt="image-20200810181951506"></p><p>个性化后如下所示：</p><p><img src="https://i.loli.net/2020/08/10/tuvBmHFRZ8JMk3S.png" alt="image-20200810180207065"></p><h3 id="hexo-文章加密"><a href="#hexo-文章加密" class="headerlink" title="hexo 文章加密"></a>hexo 文章加密</h3><blockquote><p><a href="https://vic.kim/2019/05/27/Hexo文章加密/" target="_blank" rel="noopener">https://vic.kim/2019/05/27/Hexo%E6%96%87%E7%AB%A0%E5%8A%A0%E5%AF%86/</a></p></blockquote><h3 id="在每篇文章末尾添加“本文结束”标记"><a href="#在每篇文章末尾添加“本文结束”标记" class="headerlink" title="在每篇文章末尾添加“本文结束”标记"></a>在每篇文章末尾添加“本文结束”标记</h3><p>修改模板 <code>/themes/next/layout/_macro/post.swig</code>，在<code></code>代码行中添加<code>&lt;div style=&quot;text-align:center;color: #ccc;font-size:14px;&quot;&gt;-------------本文结束&lt;i class=&quot;fa fa-paw&quot;&gt;&lt;/i&gt;感谢您的阅读-------------&lt;/div&gt;</code>即可完成设置，如下所示，红框内是添加内容</p><p><img src="https://i.loli.net/2020/08/10/TQEoIcbKLGaC5dr.png" alt="image-20200810182801218"></p><p>个性化如下所示：</p><p><img src="https://i.loli.net/2020/08/10/dCvbzpGBhs6excl.png" alt="image-20200810183022935"></p>]]></content>
    
    <summary type="html">
    
      记录在博客个性化设置过程
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-08-01-teacher-foring以及解决</title>
    <link href="http://yoursite.com/2020/08/01/2020-08-01-teacher-foring%E4%BB%A5%E5%8F%8A%E8%A7%A3%E5%86%B3/"/>
    <id>http://yoursite.com/2020/08/01/2020-08-01-teacher-foring%E4%BB%A5%E5%8F%8A%E8%A7%A3%E5%86%B3/</id>
    <published>2020-08-01T01:29:57.000Z</published>
    <updated>2020-08-01T02:18:22.987Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-07-30-pytorch使用手册</title>
    <link href="http://yoursite.com/2020/07/30/2020-07-30-pytorch%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/"/>
    <id>http://yoursite.com/2020/07/30/2020-07-30-pytorch%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/</id>
    <published>2020-07-30T09:26:32.000Z</published>
    <updated>2020-07-30T09:30:59.629Z</updated>
    
    <content type="html"><![CDATA[<h3 id="python中对于对象的拷贝分为浅拷贝-copy-和深拷贝-deepcopy-两种方式。其中浅拷贝由“-”完成。而深拷贝由copy模块中deepcopy-函数担任。"><a href="#python中对于对象的拷贝分为浅拷贝-copy-和深拷贝-deepcopy-两种方式。其中浅拷贝由“-”完成。而深拷贝由copy模块中deepcopy-函数担任。" class="headerlink" title="python中对于对象的拷贝分为浅拷贝(copy)和深拷贝(deepcopy)两种方式。其中浅拷贝由“=”完成。而深拷贝由copy模块中deepcopy()函数担任。"></a><strong>python中对于对象的拷贝分为浅拷贝(copy)和深拷贝(deepcopy)两种方式。其中浅拷贝由“=”完成。而深拷贝由copy模块中deepcopy()函数担任。</strong></h3><h3 id="浅拷贝和深拷贝的区别是：浅拷贝只是将原对象在内存中引用地址拷贝过来了。让新的对象指向这个地址。而深拷贝是将这个对象的所有内容遍历拷贝过来了，相当于跟原来没关系了，所以如果你这时候修改原来对象的值跟他没关系了，不会随之更改。"><a href="#浅拷贝和深拷贝的区别是：浅拷贝只是将原对象在内存中引用地址拷贝过来了。让新的对象指向这个地址。而深拷贝是将这个对象的所有内容遍历拷贝过来了，相当于跟原来没关系了，所以如果你这时候修改原来对象的值跟他没关系了，不会随之更改。" class="headerlink" title="\浅拷贝和深拷贝的区别是：浅拷贝只是将原对象在内存中引用地址拷贝过来了。让新的对象指向这个地址。而深拷贝是将这个对象的所有内容遍历拷贝过来了，相当于跟原来没关系了，所以如果你这时候修改原来对象的值跟他没关系了，不会随之更改。**"></a><strong><em>\</em>浅拷贝和深拷贝的区别是：浅拷贝只是将原对象在内存中引用地址拷贝过来了。让新的对象指向这个地址。而深拷贝是将这个对象的所有内容遍历拷贝过来了，相当于跟原来没关系了，所以如果你这时候修改原来对象的值跟他没关系了，不会随之更改。**</strong></h3><h3 id="1-浅拷贝”-”的使用"><a href="#1-浅拷贝”-”的使用" class="headerlink" title="1.浅拷贝”=”的使用"></a><strong>1.浅拷贝”=”的使用</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.使用=复制不可变对象的值，以及复制以后修改其值后的变化。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">val1 = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">val2 = val1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(<span class="string">"val1 is :&#123;0&#125;,val2 is :&#123;1&#125;"</span>.format(val1,val2))<span class="comment">#val1 is :1000,val2 is :1000</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(id(val1),id(val2))  <span class="comment">#34052192 34052192</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#这时候修改val1的值，尽管val2指向val1.但因为val1是不可变类型，修改其值，会重新给新值分配内存，然后指向他。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">val1 += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(val1,id(val1),val2,id(val2)) <span class="comment">#1001 10131616 1000 10131568  值不一样，内存地址也不一样了</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#1.使用=复制可变对象的值，以及复制以后修改其值后的变化。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ls1 =[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ls2 = ls1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(id(ls1),id(ls2)) <span class="comment">#43702792 43702792 直接使用=复制变量，内存地址一样，值也一样。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(ls1,ls2) <span class="comment">#[1, 2, 3, 4] [1, 2, 3, 4]直接使用=复制变量，内存地址一样，值也一样。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#这时候修改可变对的值,因为其值可变，所以只需要在原内存地址上修改即可。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ls1.append(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(id(ls1),id(ls2)) <span class="comment">#可变对象修改其值，内存引用不变</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(ls1,ls2) <span class="comment">#[1, 2, 3, 4, 5] [1, 2, 3, 4, 5] 因为两个变量的内存指向一样，所以值也一样。</span></span><br></pre></td></tr></table></figure><h3 id="2-深拷贝：copy-deepcopy-函数"><a href="#2-深拷贝：copy-deepcopy-函数" class="headerlink" title="2.深拷贝：copy.deepcopy()函数"></a>2.深拷贝：copy.deepcopy()函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.使用copy.deepcopy()拷贝不可变对象的值，以及复制以后修改其值后的变化。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">val1 = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">val2 = copy.deepcopy(val1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(<span class="string">"val1 is :&#123;0&#125;,val2 is :&#123;1&#125;"</span>.format(val1,val2))<span class="comment">#val1 is :1000,val2 is :1000</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(id(val1),id(val2))  <span class="comment">#33717408 33717408 对于不可变对象，深度拷贝内存地址没有修改。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">val1 += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(val1,id(val1),val2,id(val2)) <span class="comment">#1001 33717904 1000 33717408</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#1.使用copy.deepcopy()复制可变对象的值，以及复制以后修改其值后的变化。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ls1 =[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ls2 = copy.deepcopy(ls1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(id(ls1),id(ls2)) <span class="comment">#34628472 34628712 注意对于可变对象深度拷贝后内存地址都修改了。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(ls1,ls2) <span class="comment">#[1, 2, 3, 4] [1, 2, 3, 4]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ls1.append(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(id(ls1),id(ls2)) <span class="comment">#34628472 34628712</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(ls1,ls2) <span class="comment">#[1, 2, 3, 4, 5] [1, 2, 3, 4] #注意这个时候ls2的值没有随着ls1修改。</span></span><br></pre></td></tr></table></figure><h3 id="总结：其实对于浅拷贝和深拷贝来说，如果拷贝对象都是不可变对象的话，那么两者效果是一样的。如果是可变对象的话，“-”拷贝的方式，只是拷贝了内存中的地址引用，两个对象的地址引用一样，所以两个对象的值会随着一方的修改而修改。而对于deepcopy-来说，如果是可变对象的话，那么拷贝内容后新对象的内存地址也会重新分配，跟原来的内存地址不一样了。所以两者任意修改变量的内容不会对另一方造成影响。"><a href="#总结：其实对于浅拷贝和深拷贝来说，如果拷贝对象都是不可变对象的话，那么两者效果是一样的。如果是可变对象的话，“-”拷贝的方式，只是拷贝了内存中的地址引用，两个对象的地址引用一样，所以两个对象的值会随着一方的修改而修改。而对于deepcopy-来说，如果是可变对象的话，那么拷贝内容后新对象的内存地址也会重新分配，跟原来的内存地址不一样了。所以两者任意修改变量的内容不会对另一方造成影响。" class="headerlink" title="总结：其实对于浅拷贝和深拷贝来说，如果拷贝对象都是不可变对象的话，那么两者效果是一样的。如果是可变对象的话，“=”拷贝的方式，只是拷贝了内存中的地址引用，两个对象的地址引用一样，所以两个对象的值会随着一方的修改而修改。而对于deepcopy()来说，如果是可变对象的话，那么拷贝内容后新对象的内存地址也会重新分配，跟原来的内存地址不一样了。所以两者任意修改变量的内容不会对另一方造成影响。"></a>总结：其实对于浅拷贝和深拷贝来说，如果拷贝对象都是不可变对象的话，那么两者效果是一样的。如果是可变对象的话，“=”拷贝的方式，只是拷贝了内存中的地址引用，两个对象的地址引用一样，所以两个对象的值会随着一方的修改而修改。而对于deepcopy()来说，如果是可变对象的话，那么拷贝内容后新对象的内存地址也会重新分配，跟原来的内存地址不一样了。所以两者任意修改变量的内容不会对另一方造成影响。</h3><h3 id="3-注意一个特殊的copy-跟深浅拷贝都有区别，慎用。"><a href="#3-注意一个特殊的copy-跟深浅拷贝都有区别，慎用。" class="headerlink" title="3.注意一个特殊的copy(),跟深浅拷贝都有区别，慎用。"></a>3.注意一个特殊的copy(),跟深浅拷贝都有区别，慎用。</h3><ol><li>copy.copy对于可变类型，会进行浅拷贝</li><li>copy.copy对于不可变类型，不会拷贝，仅仅是指向</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span>使用copy()拷贝不可变对象</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">val1 = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">val2 = copy.copy(val1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(val1,val2)<span class="comment">##1000 1000</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(id(val1),id(val2))<span class="comment">#8551568 8551568</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="number">2.</span>使用copy（）拷贝可变对象</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ls1 =[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ls2 = copy.copy(ls1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ls1.append(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(ls1,ls2)  <span class="comment">#[1, 2, 3, 4, 5] [1, 2, 3, 4]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">看上去copy()函数效果和deepcopy()效果一样，可变对象拷贝后值也没有随着一个对象的修改而修改。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">然后真实情况真是这样嘛？请看下面的案例，同样是拷贝可变对象。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">origin = [<span class="number">1</span>, <span class="number">2</span>, [<span class="number">3</span>, <span class="number">4</span>]]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cop1 = copy.copy(origin)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cop2 = copy.deepcopy(origin)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">origin[<span class="number">2</span>][<span class="number">0</span>] = <span class="string">"hey!"</span>  <span class="comment">#修改数据源的值</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(cop1,cop2) <span class="comment">#[1, 2, ['hey!', 4]] [1, 2, [3, 4]]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">很显然这时copy（）函数拷贝的值随着原对象的值修改了，而deepcopy()的值没有随着原对象的值修改。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">主要是因为deepcopy会将复杂对象的每一层复制一个单独的个体出来对于copy（）函数要慎用，慎用。</span><br></pre></td></tr></table></figure><p>神经网络的典型处理如下所示：</p><h3 id="1-定义可学习参数的网络结构（堆叠各层和层的设计）；-2-数据集输入；-3-对输入进行处理（由定义的网络层进行处理）-主要体现在网络的前向传播；-4-计算loss-，由Loss层计算；-5-反向传播求梯度；-6-根据梯度改变参数值-最简单的实现方式（SGD）为-weight-weight-learning-rate-gradient"><a href="#1-定义可学习参数的网络结构（堆叠各层和层的设计）；-2-数据集输入；-3-对输入进行处理（由定义的网络层进行处理）-主要体现在网络的前向传播；-4-计算loss-，由Loss层计算；-5-反向传播求梯度；-6-根据梯度改变参数值-最简单的实现方式（SGD）为-weight-weight-learning-rate-gradient" class="headerlink" title="1. 定义可学习参数的网络结构（堆叠各层和层的设计）； 2. 数据集输入； 3. 对输入进行处理（由定义的网络层进行处理）,主要体现在网络的前向传播； 4. 计算loss ，由Loss层计算； 5. 反向传播求梯度； 6. 根据梯度改变参数值,最简单的实现方式（SGD）为:   weight = weight - learning_rate * gradient"></a><strong>1. 定义可学习参数的网络结构（堆叠各层和层的设计）； 2. 数据集输入； 3. 对输入进行处理（由定义的网络层进行处理）,主要体现在网络的前向传播； 4. 计算loss ，由Loss层计算； 5. 反向传播求梯度； 6. 根据梯度改变参数值,最简单的实现方式（SGD）为:</strong>   weight = weight - learning_rate * gradient</h3><p>下面是利用PyTorch定义深度网络层（Op）示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FeatureL2Norm</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        super(FeatureL2Norm, self).__init__()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, feature)</span>:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        epsilon = <span class="number">1e-6</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#        print(feature.size())</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#        print(torch.pow(torch.sum(torch.pow(feature,2),1)+epsilon,0.5).size())</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        norm = torch.pow(torch.sum(torch.pow(feature,<span class="number">2</span>),<span class="number">1</span>)+epsilon,<span class="number">0.5</span>).unsqueeze(<span class="number">1</span>).expand_as(feature)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> torch.div(feature,norm)</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FeatureRegression</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, output_dim=<span class="number">6</span>, use_cuda=True)</span>:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        super(FeatureRegression, self).__init__()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.conv = nn.Sequential(</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            nn.Conv2d(<span class="number">225</span>, <span class="number">128</span>, kernel_size=<span class="number">7</span>, padding=<span class="number">0</span>),</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            nn.BatchNorm2d(<span class="number">128</span>),</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            nn.Conv2d(<span class="number">128</span>, <span class="number">64</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">0</span>),</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.linear = nn.Linear(<span class="number">64</span> * <span class="number">5</span> * <span class="number">5</span>, output_dim)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> use_cuda:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            self.conv.cuda()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            self.linear.cuda()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        x = self.conv(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        x = self.linear(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>由上例代码可以看到，不论是在定义网络结构还是定义网络层的操作（Op），均需要定义forward函数，下面看一下<a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">PyTorch官网</a>对PyTorch的forward方法的描述：</p><p><img src="https://img-blog.csdnimg.cn/20181114105426553.PNG" alt="img"></p><p>那么调用forward方法的具体流程是什么样的呢？<a href="https://blog.csdn.net/u012436149/article/details/70145598" target="_blank" rel="noopener">具体流程是这样的：</a></p><h3 id="以一个Module为例：-1-调用module的call方法-2-module的call里面调用module的forward方法-3-forward里面如果碰到Module的子类，回到第1步，如果碰到的是Function的子类，继续往下-4-调用Function的call方法-5-Function的call方法调用了Function的forward方法。-6-Function的forward返回值-7-module的forward返回值-8-在module的call进行forward-hook操作，然后返回值。"><a href="#以一个Module为例：-1-调用module的call方法-2-module的call里面调用module的forward方法-3-forward里面如果碰到Module的子类，回到第1步，如果碰到的是Function的子类，继续往下-4-调用Function的call方法-5-Function的call方法调用了Function的forward方法。-6-Function的forward返回值-7-module的forward返回值-8-在module的call进行forward-hook操作，然后返回值。" class="headerlink" title="以一个Module为例： 1. 调用module的call方法 2. module的call里面调用module的forward方法 3. forward里面如果碰到Module的子类，回到第1步，如果碰到的是Function的子类，继续往下 4. 调用Function的call方法 5. Function的call方法调用了Function的forward方法。 6. Function的forward返回值 7. module的forward返回值 8. 在module的call进行forward_hook操作，然后返回值。"></a>以一个Module为例： <strong>1. 调用module的call方法 2. module的call里面调用module的forward方法 3. forward里面如果碰到Module的子类，回到第1步，如果碰到的是Function的子类，继续往下 4. 调用Function的call方法 5. Function的call方法调用了Function的forward方法。 6. Function的forward返回值 7. module的forward返回值 8. 在module的call进行forward_hook操作，然后返回值。</strong></h3><p>上述中“调用module的call方法”是指nn.Module 的<strong>call</strong>方法。定义<strong>call</strong>方法的类可以当作函数调用，具体参考Python的面向对象编程。也就是说，当把定义的网络模型model当作函数调用的时候就自动调用定义的网络模型的forward方法。nn.Module 的<strong>call</strong>方法部分源码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, *input, **kwargs)</span>:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   result = self.forward(*input, **kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   <span class="keyword">for</span> hook <span class="keyword">in</span> self._forward_hooks.values():</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">       <span class="comment">#将注册的hook拿出来用</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">       hook_result = hook(self, input, result)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   ...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure><p>可以看到，当执行model(x)的时候，底层自动调用forward方法计算结果。具体示例如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LeNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        super(LeNet, self).__init__()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">layer1 = nn.Sequential()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">layer1.add_module(<span class="string">'conv1'</span>, nn.Conv(<span class="number">1</span>, <span class="number">6</span>, <span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">layer1.add_moudle(<span class="string">'pool1'</span>, nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">self.layer1 = layer1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">layer2 = nn.Sequential()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">layer2.add_module(<span class="string">'conv2'</span>, nn.Conv(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">layer2.add_moudle(<span class="string">'pool2'</span>, nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">self.layer2 = layer2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">layer3 = nn.Sequential()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">layer3.add_module(<span class="string">'fc1'</span>, nn.Linear(<span class="number">400</span>, <span class="number">120</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">layer3.add_moudle(<span class="string">'fc2'</span>, nn.Linear(<span class="number">120</span>, <span class="number">84</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">layer3.add_moudle(<span class="string">'fc3'</span>, nn.Linear(<span class="number">84</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">self.layer3 = layer3</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = self.layer1(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = self.layer2(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = self.layer3(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h3 id="model-LeNet-y-model-x"><a href="#model-LeNet-y-model-x" class="headerlink" title="model = LeNet() y = model(x)"></a><strong>model = LeNet() y = model(x)</strong></h3><p>如上则调用网络模型定义的forward方法。</p>]]></content>
    
    <summary type="html">
    
      记录在阅读pytorch代码时遇到的知识点
    
    </summary>
    
    
      <category term="pytorch" scheme="http://yoursite.com/categories/pytorch/"/>
    
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>2020-07-30-待写博客</title>
    <link href="http://yoursite.com/2020/07/30/2020-07-30-%E5%BE%85%E5%86%99%E5%8D%9A%E5%AE%A2/"/>
    <id>http://yoursite.com/2020/07/30/2020-07-30-%E5%BE%85%E5%86%99%E5%8D%9A%E5%AE%A2/</id>
    <published>2020-07-30T08:34:51.000Z</published>
    <updated>2020-08-08T16:59:44.649Z</updated>
    
    <content type="html"><![CDATA[<h3 id="pytorch-中forward的使用以及原理-–pytorch使用"><a href="#pytorch-中forward的使用以及原理-–pytorch使用" class="headerlink" title="pytorch 中forward的使用以及原理   –pytorch使用"></a>pytorch 中forward的使用以及原理   –pytorch使用</h3><p><a href="https://blog.csdn.net/u011501388/article/details/84062483" target="_blank" rel="noopener">https://blog.csdn.net/u011501388/article/details/84062483</a></p><h4 id="阅读代码时的问题-记录"><a href="#阅读代码时的问题-记录" class="headerlink" title="阅读代码时的问题 记录"></a>阅读代码时的问题 记录</h4><h3 id=""><a href="#" class="headerlink" title=""></a></h3><h3 id="PyTorch里面的torch-nn-Parameter-详解"><a href="#PyTorch里面的torch-nn-Parameter-详解" class="headerlink" title="PyTorch里面的torch.nn.Parameter()详解"></a>PyTorch里面的torch.nn.Parameter()详解</h3><p><a href="https://cloud.tencent.com/developer/article/1608348" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1608348</a></p><h3 id="论文阅读的思维导图"><a href="#论文阅读的思维导图" class="headerlink" title="论文阅读的思维导图"></a>论文阅读的思维导图</h3><p>conda 安装新版本python之后，会覆盖之前的版本</p><h1 id="LINUX-杀死、暂停、继续、后台运行进程"><a href="#LINUX-杀死、暂停、继续、后台运行进程" class="headerlink" title="LINUX 杀死、暂停、继续、后台运行进程"></a>LINUX 杀死、暂停、继续、后台运行进程</h1><p>ctrl + z</p><p>可以将一个正在前台执行的命令放到后台，并且暂停</p><p>若想恢复到前台，则</p><ol><li>jobs  #查看当前有多少在后台运行的命令 会有序号 job号</li><li>fg 〔<em>job</em>号〕  将后台中的命令调至前台继续运行  如： fg %1</li></ol><p><a href="https://blog.csdn.net/QQ1910084514/article/details/80390671" target="_blank" rel="noopener">https://blog.csdn.net/QQ1910084514/article/details/80390671</a></p>]]></content>
    
    <summary type="html">
    
      记录等待写的博客
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-07-28-transformer解读-pytorch版本</title>
    <link href="http://yoursite.com/2020/07/28/2020-07-28-transformer-pytorch/"/>
    <id>http://yoursite.com/2020/07/28/2020-07-28-transformer-pytorch/</id>
    <published>2020-07-28T08:43:03.000Z</published>
    <updated>2020-08-10T13:20:19.914Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>最近几天都在阅读哈佛pytorch实现transformer的代码，代码风格很好，很值得参考和研读。和实验室师兄又在一起讨论了几次，代码思路和实现过程基本都了解了，对于原论文 <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">“Attention is All You Need”</a> 中关于transformer模型的理解又深入了许多。果然要想了解模型，还是要好好研读实现代码。以便于后面自己结合模型的研究。</p><p>本篇是对实现代码的注释，加上了自己的理解，也会有一些函数的介绍扩充。</p><h4 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h4><blockquote><p>解读的是哈佛的一篇transformer的pytorch版本实现</p><p><a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">http://nlp.seas.harvard.edu/2018/04/03/attention.html</a></p><p>参考另一篇博客</p><p><a href="http://fancyerii.github.io/2019/03/09/transformer-codes/" target="_blank" rel="noopener">http://fancyerii.github.io/2019/03/09/transformer-codes/</a></p><p>Transformer注解及PyTorch实现（上）</p><p><a href="https://www.jiqizhixin.com/articles/2018-11-06-10" target="_blank" rel="noopener">https://www.jiqizhixin.com/articles/2018-11-06-10</a></p><p>Transformer注解及PyTorch实现（下）</p><p><a href="https://www.jiqizhixin.com/articles/2018-11-06-18" target="_blank" rel="noopener">https://www.jiqizhixin.com/articles/2018-11-06-18</a></p><p>训练过程中的 Mask实现</p><p><a href="https://www.cnblogs.com/wevolf/p/12484972.html" target="_blank" rel="noopener">https://www.cnblogs.com/wevolf/p/12484972.html</a></p></blockquote><h3 id="The-Annotated-Transformer"><a href="#The-Annotated-Transformer" class="headerlink" title="The Annotated Transformer"></a>The Annotated Transformer</h3><p><img src="https://i.loli.net/2020/07/28/NUAyXWJ5DzHmjuv.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !pip install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl numpy matplotlib spacy torchtext seaborn </span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> math, copy, time</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn</span><br><span class="line">seaborn.set_context(context=<span class="string">"talk"</span>)</span><br></pre></td></tr></table></figure><p>Transformer使用了Self-Attention机制，它在编码每一词的时候都能够注意(attend to)整个句子，从而可以解决长距离依赖的问题，同时计算Self-Attention可以用矩阵乘法一次计算所有的时刻，因此可以充分利用计算资源(CPU/GPU上的矩阵运算都是充分优化和高度并行的)。</p><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>Most competitive neural sequence transduction models have an encoder-decoder structure <a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="noopener">(cite)</a>. Here, <code>the encoder maps an input sequence of symbol representations (x1,…,xn)(x1,…,xn) to a sequence of continuous representations z=(z1,…,zn)z=(z1,…,zn). Given z, the decoder then generates an output sequence (y1,…,ym)(y1,…,ym) of symbols one element at a time.</code> At each step the model is auto-regressive <a href="https://arxiv.org/abs/1308.0850" target="_blank" rel="noopener">(cite)</a>, consuming the previously generated symbols as additional input when generating the next.</p><p><strong>EncoderDecoder定义了一种通用的Encoder-Decoder架构</strong>，具体的Encoder、Decoder、src_embed、target_embed和generator都是构造函数传入的参数。这样我们<strong>做实验更换不同的组件就会更加方便</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderDecoder</span><span class="params">(nn.Module)</span>:</span> <span class="comment">#定义的是整个模型 ，不包括generator</span></span><br><span class="line">    </span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">   标准的Encoder-Decoder架构。这是很多模型的基础</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    class里， init函数是实例化一个对象的时候用于初始化对象用的</span></span><br><span class="line"><span class="string">    forward函数是在执行调用对象的时候使用， 需要传入正确的参数 </span></span><br><span class="line"><span class="string">    在执行时候调用__call__方法，然后再call里再调用forward</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, encoder, decoder, src_embed, tgt_embed, generator)</span>:</span></span><br><span class="line">        super(EncoderDecoder, self).__init__()</span><br><span class="line">        <span class="comment"># encoder和decoder都是构造的时候传入的，这样会非常灵活</span></span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line">        <span class="comment"># 源语言和目标语言的embedding，包括embedding层和position encode层</span></span><br><span class="line">        self.src_embed = src_embed <span class="comment">#源数据集的嵌入</span></span><br><span class="line">        self.tgt_embed = tgt_embed <span class="comment">#目标数据集的嵌入，作为decoder的输入</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        generator后面会讲到，就是根据Decoder的隐状态输出当前时刻的词</span></span><br><span class="line"><span class="string">    基本的实现就是隐状态输入一个全连接层，全连接层的输出大小是词的个数</span></span><br><span class="line"><span class="string">然后接一个softmax变成概率</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.generator = generator</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, src, tgt, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="comment">#首先调用encode方法对输入进行编码，然后调用decode方法解码</span></span><br><span class="line">        <span class="keyword">return</span> self.decode(self.encode(src, src_mask), src_mask,</span><br><span class="line">                            tgt, tgt_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, src, src_mask)</span>:</span></span><br><span class="line">        <span class="comment"># 调用encoder来进行编码，传入的参数embedding的src和src_mask</span></span><br><span class="line">        <span class="keyword">return</span> self.encoder(self.src_embed(src), src_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span><span class="params">(self, memory, src_mask, tgt, tgt_mask)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask) <span class="comment">#目标是输入的一部分</span></span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span>  <span class="comment">#decoder后面的linear+softmax</span></span><br><span class="line">    <span class="comment"># 根据Decoder的隐状态输出一个词</span></span><br><span class="line"><span class="comment"># d_model是Decoder输出的大小，vocab是词典大小 （数据语料有多少词 ）</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, vocab)</span>:</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        self.proj = nn.Linear(d_model, vocab) <span class="comment">#全连接，作为softmax的输入。</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(self.proj(x), dim=<span class="number">-1</span>) <span class="comment">#softmax的log值</span></span><br></pre></td></tr></table></figure><p>注：<code>Generator返回的是softmax的log值</code>。在PyTorch里为了计算交叉熵损失，有两种方法。第一种方法是使用<strong>nn.CrossEntropyLoss()</strong>，一种是使用<strong>NLLLoss()</strong>。很多开源代码里第二种更常见，</p><p>我们先看CrossEntropyLoss，它就是计算交叉熵损失函数，比如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">x = torch.randn(<span class="number">1</span>, <span class="number">5</span>)</span><br><span class="line">y = torch.empty(<span class="number">1</span>, dtype=torch.long).random_(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">loss = criterion(x, y)</span><br></pre></td></tr></table></figure><p>比如上面的代码，假设是5分类问题，x表示模型的输出logits(batch=1)，而y是真实分类的下标(0-4)。实际的计算过程为：<img src="https://i.loli.net/2020/08/06/KyPspa4Cqef6m8Q.png" alt="image-20200806000621448" style="zoom: 67%;" /></p><p>比如logits是[0,1,2,3,4]，真实分类是3，那么上式就是：</p><img src="https://i.loli.net/2020/08/06/i7mfUWAeHE5P1zd.png" alt="image-20200806000641945" style="zoom:67%;" /><p>因此我们也可以使用NLLLoss()配合F.log_softmax函数(或者nn.LogSoftmax，这不是一个函数而是一个Module了)来实现一样的效果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">m = nn.LogSoftmax(dim=<span class="number">1</span>)</span><br><span class="line">criterion = nn.NLLLoss()</span><br><span class="line">x = torch.randn(<span class="number">1</span>, <span class="number">5</span>)</span><br><span class="line">y = torch.empty(<span class="number">1</span>, dtype=torch.long).random_(<span class="number">5</span>)</span><br><span class="line">loss = criterion(m(x), y)</span><br></pre></td></tr></table></figure><p>NLLLoss(Negative Log Likelihood Loss)是计算负log似然损失。它输入的x是log_softmax之后的结果(长度为5的数组)，y是真实分类(0-4)，输出就是x[y]。因此上面的代码为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">criterion(m(x), y)=m(x)[y]</span><br></pre></td></tr></table></figure><p>The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.</p><p><img src="https://i.loli.net/2020/07/28/P3fSgRhrmFtlpxY.png" alt="png"></p><h3 id="Encoder-and-Decoder-Stacks"><a href="#Encoder-and-Decoder-Stacks" class="headerlink" title="Encoder and Decoder Stacks"></a>Encoder and Decoder Stacks</h3><h4 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h4><p>Encoder和Decoder都是由N个相同结构的Layer堆积(stack)而成。<strong>因此我们首先定义clones函数，用于克隆相同的SubLayer。</strong></p><p>这里使用了<strong>nn.ModuleList</strong>，ModuleList就像一个普通的Python的List，我们可以使用下标来访问它，它的好处是传入的ModuleList的所有Module都会注册的PyTorch里，这样Optimizer就能找到这里面的参数，从而能够用梯度下降更新这些参数。但是nn.ModuleList并不是Module(的子类)，因此它没有forward等方法，我们通常把它放到某个Module里。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clones</span><span class="params">(module, N)</span>:</span>  <span class="comment">#克隆N层，是个层数的列表。 copy.deepcopy是深复制， 一个改变不会影响另一个</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="keyword">for</span> _ <span class="keyword">in</span> range(N)]) <span class="comment">#复制N=6层</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span><span class="params">(nn.Module)</span>:</span>  <span class="comment">#定义编码器 </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#Encoder是N个EncoderLayer的stack</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layer, N)</span>:</span> <span class="comment"># 根据make_model定义，layer = encoderlayer （sublayer）</span></span><br><span class="line">        super(Encoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N) <span class="comment">#编码器有6层编码层，根据上述函数的定义，module=layer</span></span><br><span class="line">        self.norm = LayerNorm(layer.size) <span class="comment">#调用下面的LayerNorm。 分开定义是因为 LayerNorm = 2* layer</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask)</span>:</span> </span><br><span class="line">       <span class="comment">#逐层进行处理</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers: <span class="comment"># x 在每一层中传递</span></span><br><span class="line">            x = layer(x, mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x) <span class="comment">#最终encoder的返回值</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LayerNorm</span><span class="params">(nn.Module)</span>:</span> <span class="comment">#add &amp; norm部分  作为每一个子层的输出</span></span><br><span class="line">    <span class="string">"Construct a layernorm module (See citation for details)."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, features, eps=<span class="number">1e-6</span>)</span>:</span> <span class="comment">#feature = layer.size layer的形状</span></span><br><span class="line">        super(LayerNorm, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.a_2 = nn.Parameter(torch.ones(features))  <span class="comment">#将后面的tensor转换为可优化的参数</span></span><br><span class="line">        self.b_2 = nn.Parameter(torch.zeros(features))</span><br><span class="line">        self.eps = eps <span class="comment">#很小的值</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span> <span class="comment"># 平均值和标准差</span></span><br><span class="line">        mean = x.mean(<span class="number">-1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        std = x.std(<span class="number">-1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self.a_2 * (x - mean) / (std + self.eps) + self.b_2 <span class="comment">#输出</span></span><br></pre></td></tr></table></figure><p><strong>不管是Self-Attention还是全连接层，都首先是LayerNorm，然后是Self-Attention/Dense，然后是Dropout，最好是残差连接。这里面有很多可以重用的代码，我们把它封装成SublayerConnection。</strong></p><hr><p>That is, <code>the output of each sub-layer is LayerNorm(x+Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself.</code> We apply dropout <a href="http://jmlr.org/papers/v15/srivastava14a.html" target="_blank" rel="noopener">(cite)</a> to the output of each sub-layer, before it is added to the sub-layer input and normalized.</p><p>To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension <code>dmodel=512</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SublayerConnection</span><span class="params">(nn.Module)</span>:</span> <span class="comment">#每一个编码层中的两个子层之间的连接</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">LayerNorm + sublayer(Self-Attenion/Dense) + dropout + 残差连接</span></span><br><span class="line"><span class="string">为了简单，把LayerNorm放到了前面，这和原始论文稍有不同，原始论文LayerNorm在最后。</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, dropout)</span>:</span></span><br><span class="line">        super(SublayerConnection, self).__init__()</span><br><span class="line">        self.norm = LayerNorm(size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, sublayer)</span>:</span></span><br><span class="line">       <span class="comment">#sublayer是传入的参数，参考DecoderLayer，它可以当成函数调用，这个函数的有一个输入参数</span></span><br><span class="line">        <span class="keyword">return</span> x + self.dropout(sublayer(self.norm(x))) <span class="comment">#调用layernorm ，正则化之后再相加</span></span><br></pre></td></tr></table></figure><p>这个类会构造LayerNorm和Dropout，但是Self-Attention或者Dense并不在这里构造，还是放在了EncoderLayer里，在forward的时候由EncoderLayer传入。这样的好处是更加通用，比如Decoder也是类似的需要在Self-Attention、Attention或者Dense前面后加上LayerNorm和Dropout以及残差连接，我们就可以复用代码。但是这里要求传入的sublayer可以使用一个参数来调用的函数(或者有<strong>call</strong>)。</p><hr><p>forward调用sublayer[0] (这是SublayerConnection对象)的<strong>call</strong>方法，最终会调到它的forward方法，而这个方法需要两个参数，<strong>一个是输入Tensor，一个是一个callable，并且这个callable可以用一个参数来调用</strong>。而<strong>self_attn函数需要4个参数(Query的输入,Key的输入,Value的输入和Mask)</strong>，因此这里我们使用lambda的技巧把它变成一个参数x的函数(mask可以看成已知的数)。</p><p>  Callable 类型是可以被执行调用操作的类型。包含自定义函数等。自定义的函数比如使用def、lambda所定义的函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderLayer</span><span class="params">(nn.Module)</span>:</span> <span class="comment">#每一个编码层</span></span><br><span class="line">    <span class="string">"Encoder is made up of self-attn and feed forward (defined below)"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, self_attn, feed_forward, dropout)</span>:</span></span><br><span class="line">        super(EncoderLayer, self).__init__()</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">2</span>) <span class="comment">#每一层有2子层</span></span><br><span class="line">        self.size = size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask)</span>:</span></span><br><span class="line">      <span class="comment">#attention层，括号里面是参数。接收来自attention的输出</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">     lambda : atten()SublayerConnection里是作为sublayer出现的，而它的参数是norm(x),norm(x)的输出是一个向量x，</span></span><br><span class="line"><span class="string">   所以atten的参数是只有一个x， 而在muitihead里面，k、q、v在函数里是要被重新根据x计算的</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, mask)) </span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">1</span>](x, self.feed_forward) <span class="comment">#x是atten+norm之后的输出，再ff输出</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    可以理解为</span></span><br><span class="line"><span class="string">    z = lambda y: self.self_attn(y, y, y, mask)</span></span><br><span class="line"><span class="string">x = self.sublayer[0](x, z)</span></span><br><span class="line"><span class="string">    """</span></span><br></pre></td></tr></table></figure><h4 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h4><p>The decoder is also composed of a stack of <code>N=6</code> identical layers.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Generic N layer decoder with masking."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layer, N)</span>:</span></span><br><span class="line">        super(Decoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">     <span class="comment">#memory: 编码器的输出 x是输入</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, memory, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, memory, src_mask, tgt_mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderLayer</span><span class="params">(nn.Module)</span>:</span> <span class="comment">#每一层解码层</span></span><br><span class="line">    <span class="string">"Decoder is made of self-attn, src-attn, and feed forward (defined below)"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, self_attn, src_attn, feed_forward, dropout)</span>:</span></span><br><span class="line">        super(DecoderLayer, self).__init__()</span><br><span class="line">        self.size = size</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.src_attn = src_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">3</span>) <span class="comment">#每一层有3个子层</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, memory, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="string">"Follow Figure 1 (right) for connections."</span></span><br><span class="line">        m = memory</span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, tgt_mask)) <span class="comment">#第一子层</span></span><br><span class="line">        x = self.sublayer[<span class="number">1</span>](x, <span class="keyword">lambda</span> x: self.src_attn(x, m, m, src_mask)) <span class="comment">#第二子层 </span></span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">2</span>](x, self.feed_forward) <span class="comment">#第三子层</span></span><br></pre></td></tr></table></figure><p><strong>src-attn和self-attn的实现是一样的，只不过使用的Query，Key和Value的输入不同。</strong>普通的Attention(src-attn)的Query是下层输入进来的(来自self-attn的输出)，Key和Value是Encoder最后一层的输出memory；而Self-Attention的Query，Key和Value都是来自下层输入进来的。</p><hr><p>Decoder和Encoder有一个关键的不同：Decoder在解码第t个时刻的时候只能使用<strong>1…t时刻</strong>的输入，而不能使用t+1时刻及其之后的输入。因此我们需要一个函数来产生一个Mask矩阵，所以代码如下：</p><p>注意： t时刻包括t时刻的输入</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">subsequent_mask</span><span class="params">(size)</span>:</span>  <span class="comment">#将i后面的mask掉</span></span><br><span class="line">    <span class="string">"Mask out subsequent positions."</span></span><br><span class="line">    attn_shape = (<span class="number">1</span>, size, size)</span><br><span class="line">    subsequent_mask = np.triu(np.ones(attn_shape), k=<span class="number">1</span>).astype(<span class="string">'uint8'</span>) <span class="comment">#triu 上三角</span></span><br><span class="line">    <span class="keyword">return</span> torch.from_numpy(subsequent_mask) == <span class="number">0</span> <span class="comment">#将numpy格式转换为tensor格式，判断是否为0， 输出布尔值</span></span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2020/08/08/7brnPfDJxsLBtvh.png" alt="png"></p><p>它的输出：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">print(subsequent_mask(5))</span><br><span class="line"># 输出</span><br><span class="line">  1  0  0  0  0</span><br><span class="line">  1  1  0  0  0</span><br><span class="line">  1  1  1  0  0</span><br><span class="line">  1  1  1  1  0</span><br><span class="line">  1  1  1  1  1</span><br></pre></td></tr></table></figure><p>我们发现它输出的是一个方阵，对角线和下面都是1。<strong>第一行只有第一列是1，它的意思是时刻1只能attend to输入1</strong>，第三行说明时刻3可以attend to {1,2,3}而不能attend to{4,5}的输入，因为在真正Decoder的时候这是属于Future的信息。代码首先使用triu产生一个上三角阵：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">0 1 1 1 1</span><br><span class="line">0 0 1 1 1</span><br><span class="line">0 0 0 1 1</span><br><span class="line">0 0 0 0 1</span><br><span class="line">0 0 0 0 0</span><br></pre></td></tr></table></figure><p>然后需要把0变成1，把1变成0，这可以使用 matrix == 0来实现。</p><p>因为：布尔值True被索引求值为1，而False就等于0。</p><h4 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h4><p>An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.</p><p>We call our particular attention “<code>Scaled Dot-Product Attention</code>”. The input consists of queries and keys of dimension <code>dk</code>, and values of dimension <code>dv</code>. We compute the dot products of the query with all keys, divide each by <code>√dk</code>, and apply a softmax function to obtain the weights on the values.</p><p><img src="https://i.loli.net/2020/08/06/O3UNSGF7Poa1w4Q.png" alt="image-20200806015122441"></p><p><strong>Attention可以看成一个函数，它的输入是Query,Key,Value和Mask，输出是一个Tensor</strong>。其中输出是Value的加权平均，而权重来自Query和Key的计算。具体的计算如下图所示，计算公式为：</p><img src="https://i.loli.net/2020/07/28/WaSfHnNdt2L1AXU.png" alt="image-20200728212241453" style="zoom:50%;" /><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention</span><span class="params">(query, key, value, mask=None, dropout=None)</span>:</span></span><br><span class="line">    <span class="string">"Compute 'Scaled Dot Product Attention'"</span></span><br><span class="line">    d_k = query.size(<span class="number">-1</span>) <span class="comment"># query.size的最后一维</span></span><br><span class="line">    scores = torch.matmul(query, key.transpose(<span class="number">-2</span>, <span class="number">-1</span>)) / math.sqrt(d_k)</span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:<span class="comment"># 如果有mask</span></span><br><span class="line">        scores = scores.masked_fill(mask == <span class="number">0</span>, <span class="number">-1e9</span>)</span><br><span class="line">    p_attn = F.softmax(scores, dim = <span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: <span class="comment">#对p_attn进行dropout</span></span><br><span class="line">        p_attn = dropout(p_attn)</span><br><span class="line">    <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br></pre></td></tr></table></figure><p>我们知道, 在训练的时候, 我们是以 batch_size 为单位的, 那么就会有 padding, 一般我们取 pad == 0, 那么就会造成在 Attention 的时候, query 的值为 0, query 的值为 0, 所以我们计算的对应的 scores 的值也是 0, 那么就会导致 softmax 很可能分配给该单词一个相对不是很小的比例, 因此, 我们将 pad 对应的 score 取值为<strong>负无穷</strong>（普通的计算，score可以为负数？）, 以此来减小 pad 的影响. </p><p>很容易想到, 在 decoder, <strong>未预测的单词</strong>也是用 padding 的方式加入到 batch 的, 所以使用的mask 机制与 padding 时mask 的机制是相同的, 本质上都是query 的值为0, 只是 mask 矩阵不同, 我们可以根据 decoder 部分的代码发现这一点.</p><hr><p>我们使用一个<strong>实际的例子跟踪一些不同Tensor的shape</strong>，然后对照公式就很容易理解。比如<strong>Q是(30,8,33,64)，其中30是batch，8是head个数，33是序列长度，64是每个时刻的特征数（size）。K和Q的shape必须相同的，而V可以不同，但是这里的实现shape也是相同的。</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scores = torch.matmul(query, key.transpose(<span class="number">-2</span>, <span class="number">-1</span>)) \</span><br><span class="line">/ math.sqrt(d_k)</span><br></pre></td></tr></table></figure><p>上面的代码实现<img src="https://i.loli.net/2020/08/06/rLCJ7VFBAsmQb4x.png" alt="image-20200806014945713" style="zoom:50%;" />，和公式里稍微不同的是，这里的Q和K都是4d的Tensor，包括batch和head维度。<strong>matmul会把query和key的最后两维进行矩阵乘法</strong>，这样效率更高，如果我们要用标准的矩阵(二维Tensor)乘法来实现，那么需要遍历batch维和head维：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">batch_num = query.size(<span class="number">0</span>) <span class="comment"># query.size(0)返回的是0维的数</span></span><br><span class="line">head_num = query.size(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(batch_num):</span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(head_num):</span><br><span class="line">scores[i,j] = torch.matmul(query[i,j], key[i,j].transpose())</span><br></pre></td></tr></table></figure><p>而上面的写法一次完成所有这些循环，效率更高。<strong>输出的score是(30, 8, 33, 33)</strong>，前面两维不看，那<strong>么是一个(33, 33)的attention矩阵a，aij表示时刻 i关注 j 的得分</strong>(还没有经过softmax变成概率)。</p><p><strong>在编码器的attention中src_mask的作用！！！</strong></p><p>接下来是<code>scores.masked_fill(mask == 0, -1e9)</code>，用于<strong>把mask是0的变成一个很小的数</strong>，这样后面经过softmax之后的概率就很接近零(但是理论上还是用来很少一点点未来的信息)。</p><blockquote><p>masked_fill_(mask, value)：掩码操作<br>masked_fill方法有两个参数，maske和value，mask是一个pytorch张量（Tensor），<strong>元素是布尔值，value是要填充的值</strong>，填充规则是mask中取值为True位置对应于self的相应位置用value填充。</p><p>注：参数mask必须与score的size相同或者两者是可广播(broadcasting-semantics)的</p><p>pytorch masked_fill方法简单理解 </p><p> <a href="https://blog.csdn.net/jianyingyao7658/article/details/103382654" target="_blank" rel="noopener">https://blog.csdn.net/jianyingyao7658/article/details/103382654</a></p><p>pytorch 广播语义(Broadcasting semantics) </p><p> <a href="https://blog.csdn.net/qq_35012749/article/details/88308657" target="_blank" rel="noopener">https://blog.csdn.net/qq_35012749/article/details/88308657</a></p></blockquote><p>这里<strong>mask是(30, 1, 1, 33)的tensor</strong>，因为8个head的mask都是一样的，所有第二维是1，masked_fill时使用broadcasting就可以了。这里是self-attention的mask，所以每个时刻都可以attend到所有其它时刻，所有第三维也是1，也使用broadcasting。如果是普通的mask，那么mask的shape是(30, 1, 33, 33)。</p><p>这样讲有点抽象，我们可以举一个例子，为了简单，我们假设batch=2, head=8。第一个序列长度为3，第二个为4，那么self-attention的mask为(2, 1, 1, 4)，我们可以用两个向量表示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1 1 1 0</span><br><span class="line">1 1 1 1</span><br></pre></td></tr></table></figure><p>它的意思是在self-attention里，第一个序列的任一时刻可以attend to 前3个时刻(因为第4个时刻是padding的)；而第二个序列的可以attend to所有时刻的输入。而Decoder的src-attention的mask为(2, 1, 4, 4)，我们需要用2个矩阵表示：(一个序列对应一个一维src_mask（1×4），  一个序列对应一个二维的tgt_mask（4×4）)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">第一个序列的mask矩阵</span><br><span class="line">1 0 0 0</span><br><span class="line">1 1 0 0</span><br><span class="line">1 1 1 0</span><br><span class="line">1 1 1 0</span><br><span class="line"></span><br><span class="line">第二个序列的mask矩阵</span><br><span class="line">1 0 0 0</span><br><span class="line">1 1 0 0 </span><br><span class="line">1 1 1 0</span><br><span class="line">1 1 1 1</span><br></pre></td></tr></table></figure><p>接下来对score求softmax，把得分变成概率p_attn，如果有dropout还对p_attn进行Dropout(这也是原始论文没有的)。最后把p_attn和value相乘。p_attn是(30, 8, 33, 33)，value是(30, 8, 33, 64)，我们<strong>只看后两维，(33x33) x (33x64)最终得到33x64。</strong></p><hr><p>接下来就是输入怎么变成Q,K和V了，<strong>对于每一个Head，都使用三个矩阵WQ,WK,WV把输入转换成Q，K和V。</strong>然后<strong>分别用每一个Head进行Self-Attention的计算，最后把N个Head的输出拼接起来，最后用一个矩阵WO把输出压缩一下。</strong>具体计算过程为：</p><img src="https://i.loli.net/2020/08/06/1IbPcFJeK8tsHqN.png" alt="image-20200806023820900" style="zoom: 67%;" /><p>详细结构如下图所示，输入Q，K和V经过多个线性变换后得到N(8)组Query，Key和Value，然后使用Self-Attention计算得到N个向量，然后拼接起来，<strong>最后使用一个线性变换进行降维。</strong></p><img src="https://i.loli.net/2020/08/08/a2gozSYGn8NOkpH.png" alt="png" style="zoom:67%;" /><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadedAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, h, d_model, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        <span class="string">"Take in model size and number of heads."</span></span><br><span class="line">        super(MultiHeadedAttention, self).__init__()</span><br><span class="line">        <span class="keyword">assert</span> d_model % h == <span class="number">0</span>  <span class="comment"># 不能整除就报错</span></span><br><span class="line">        <span class="comment"># We assume d_v always equals d_k</span></span><br><span class="line">        self.d_k = d_model // h</span><br><span class="line">        self.h = h</span><br><span class="line">        self.linears = clones(nn.Linear(d_model, d_model), <span class="number">4</span>)</span><br><span class="line">        self.attn = <span class="literal">None</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, mask=None)</span>:</span></span><br><span class="line">        <span class="string">"Implements Figure 2"</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># # 所有h个head的mask都是相同的 </span></span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>) <span class="comment">#在维度为1的位置添加一个维度，数字为1</span></span><br><span class="line">        nbatches = query.size(<span class="number">0</span>) <span class="comment">#就是有多少batch的值</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 1) 首先使用线性变换，然后把d_model分配给h个Head，每个head为d_k=d_model/h </span></span><br><span class="line">        query, key, value = \</span><br><span class="line">            [l(x).view(nbatches, <span class="number">-1</span>, self.h, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">             <span class="keyword">for</span> l, x <span class="keyword">in</span> zip(self.linears, (query, key, value))]</span><br><span class="line">           <span class="comment">#.view()表示重构张量的维度</span></span><br><span class="line">         <span class="comment">#注：因为每个Linear学习到的参数是不一样的。所以qkv三个也是不一样的</span></span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">        <span class="comment"># 2)使用attention函数计算 </span></span><br><span class="line">        x, self.attn = attention(query, key, value, mask=mask, </span><br><span class="line">                                 dropout=self.dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3) 把8个head的64维向量拼接成一个512的向量。然后再使用一个线性变换(512,521)，shape不变。 </span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous() \</span><br><span class="line">             .view(nbatches, <span class="number">-1</span>, self.h * self.d_k)</span><br><span class="line">        <span class="keyword">return</span> self.linears[<span class="number">-1</span>](x)</span><br></pre></td></tr></table></figure><p>我们先看构造函数，这里<strong>d_model(512)是Multi-Head的输出大小</strong>，因为有h(8)个head，因此每个head的d_k=512/8=64。接着我们构造4个(d_model ， d_model)的矩阵，后面我们会看到它的用处。最后是构造一个Dropout层。</p><p>然后我们来看forward方法。<strong>输入的mask是(batch, 1, time)的，因为每个head的mask都是一样的，所以先用unsqueeze(1)变成(batch, 1, 1, time)</strong>，mask我们前面已经详细分析过了。</p><p>接下来是<strong>根据输入query，key和value计算变换后的Multi-Head的query，key和value</strong>。这是通过下面的语句来实现的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">query, key, value = \</span><br><span class="line">[l(x).view(nbatches, <span class="number">-1</span>, self.h, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"><span class="keyword">for</span> l, x <span class="keyword">in</span> zip(self.linears, (query, key, value))] <span class="comment"># l(x): 调用nn.Linear函数</span></span><br></pre></td></tr></table></figure><p><strong>zip(self.linears, (query, key, value))是把(self.linears[0],self.linears[1],self.linears[2])和(query, key, value)放到一起然后遍历。我们只看一个self.linears[0] (query)。根据构造函数的定义，self.linears[0]是一个(512, 512)的矩阵，而query是(batch, time, 512)，相乘之后得到的新query还是512(d_model)维的向量，然后用view把它变成(batch, time, 8, 64)。然后transponse成(batch, 8,time,64)，这是attention函数要求的shape。分别对应8个Head，每个Head的Query都是64维。</strong></p><blockquote><p>1.一般来说，矩阵相乘，[a,b] x [b,c] = [a,c]</p><p>所以不同维度要进行处理，必须降维。例如 A 矩阵 [a,b,c], B 矩阵是[c,d]</p><p>这个时候就需要将 A 矩阵看成是 [axb, c] 与 [c,d] 进行相乘，得到结果。</p><ol start="2"><li>Linear函数l(x)，应该就是 (batch<em>time,512)*</em>(512,512)</li></ol></blockquote><p>Key和Value的运算完全相同，因此我们也分别得到8个Head的64维的Key和64维的Value。接下来<strong>调用attention函数，得到x和self.attn。其中x的shape是(batch, 8, time, 64)，而attn是(batch, 8, time, time)。</strong></p><p><strong>x.transpose(1, 2)把x变成(batch, time, 8, 64)，然后把它view成(batch, time, 512)，其实就是把最后8个64维的向量拼接成512的向量。最后使用self.linears[-1]对x进行线性变换，self.linears[-1]是(512, 512)的，因此最终的输出还是(batch, time, 512)。我们最初构造了4个(512, 512)的矩阵，前3个用于对query，key和value进行变换，而最后一个对8个head拼接后的向量再做一次变换。</strong></p><h4 id="A0ttention在模型中的应用"><a href="#A0ttention在模型中的应用" class="headerlink" title="A0ttention在模型中的应用"></a>A0ttention在模型中的应用</h4><p>在Transformer里，有3个地方用到了MultiHeadedAttention：</p><ul><li><p>Encoder的Self-Attention层</p><p><strong>query，key和value都是相同的值</strong>，来自下层的输入。Mask都是1(当然padding的不算)。</p></li><li><p>Decoder的Self-Attention层</p><p><strong>query，key和value都是相同的值</strong>，来自下层的输入。但是Mask使得它不能访问未来的输入。</p></li><li><p>Encoder-Decoder的普通Attention</p><p><strong>query来自下层的输入，而key和value相同</strong>，是Encoder最后一层的输出，而Mask都是1。</p></li></ul><h3 id="Position-wise-前馈网络"><a href="#Position-wise-前馈网络" class="headerlink" title="Position-wise 前馈网络"></a>Position-wise 前馈网络</h3><p>In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. <code>This consists of two linear transformations with a ReLU activation in between.</code></p><p>全连接层有两个线性变换以及它们之间的ReLU激活组成：</p><img src="https://i.loli.net/2020/08/08/PU96rciRsWxOCKp.png" alt="image-20200728231445307" style="zoom:50%;" /><p>全连接层的输入和输出都是d_model(512)维的，中间隐单元的个数是d_ff(2048)维</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionwiseFeedForward</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Implements FFN equation."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, d_ff, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        super(PositionwiseFeedForward, self).__init__()</span><br><span class="line">        self.w_1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        self.w_2 = nn.Linear(d_ff, d_model)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.w_2(self.dropout(F.relu(self.w_1(x))))</span><br></pre></td></tr></table></figure><h3 id="Embeddings-和-Softmax"><a href="#Embeddings-和-Softmax" class="headerlink" title="Embeddings 和 Softmax"></a>Embeddings 和 Softmax</h3><p><strong>输入的词序列都是ID序列，我们需要Embedding</strong>。源语言和目标语言都需要Embedding，此外我们需要一个线性变换把隐变量变成输出概率，这可以通过前面的类Generator来实现。我们这里实现Embedding：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Embeddings</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, vocab)</span>:</span></span><br><span class="line">        super(Embeddings, self).__init__()</span><br><span class="line">        self.lut = nn.Embedding(vocab, d_model) <span class="comment">#将字典vocab大小映射到d_model大小</span></span><br><span class="line">        self.d_model = d_model</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.lut(x) * math.sqrt(self.d_model)</span><br></pre></td></tr></table></figure><p>注意的就是forward处理使用nn.Embedding对输入x进行Embedding之外，还除以了sqrt(d_model) （开方）</p><h3 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h3><p>位置编码的公式为：</p> <img src="https://i.loli.net/2020/08/08/WUpXhHsK3S1jCqn.png" alt="image-20200728232133981" style="zoom:50%;" /><img src="https://i.loli.net/2020/08/08/XOZPy89KVi1xjTh.png" alt="image-20200728232255029" style="zoom:50%;" /><p> where <code>pos</code> is the position and <code>i</code> is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. </p><p>假设输入是ID序列长度为10，<strong>如果输入Embedding之后是(10, 512)，那么位置编码的输出也是(10, 512)。</strong>上式中pos就是位置(0-9)，512维的偶数维使用sin函数，而奇数维使用cos函数。这种位置编码的好处是：PE_pos+k可以表示成 PE_pos的线性函数，这样网络就能容易的学到相对位置的关系。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">15</span>, <span class="number">5</span>))</span><br><span class="line">pe = PositionalEncoding(<span class="number">20</span>, <span class="number">0</span>)</span><br><span class="line">y = pe.forward(Variable(torch.zeros(<span class="number">1</span>, <span class="number">100</span>, <span class="number">20</span>)))</span><br><span class="line">plt.plot(np.arange(<span class="number">100</span>), y[<span class="number">0</span>, :, <span class="number">4</span>:<span class="number">8</span>].data.numpy())</span><br><span class="line">plt.legend([<span class="string">"dim %d"</span>%p <span class="keyword">for</span> p <span class="keyword">in</span> [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>]])</span><br></pre></td></tr></table></figure><p>图是一个示例，向量的大小d_model=20，我们这里画出来第4、5、6和7维(下标从零开始)维的图像，最大的位置是100。我们可以看到它们都是正弦(余弦)函数，而且周期越来越长。</p><p><img src="https://i.loli.net/2020/08/08/TfDHKvnM3emYysL.png" alt="png"></p><p>前面我们提到位置编码的好处是PE_pos+k可以表示成 P_Epos的线性函数，我们下面简单的验证一下。我们以第i维为例，为了简单，我们把<img src="https://i.loli.net/2020/08/06/iEoDOvKzB42N6Xe.png" alt="image-20200806104700979" style="zoom: 67%;" />记作Wi，这是一个常量。</p><img src="https://i.loli.net/2020/08/06/E9h2vXIDK1MAjUg.png" alt="image-20200806104725624" style="zoom:67%;" /><p>我们发现PE_pos+k 确实可以表示成 PE_pos的线性函数。</p><p>In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of <code>Pdrop=0.1</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Implement the PE function."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, dropout, max_len=<span class="number">5000</span>)</span>:</span></span><br><span class="line">        super(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute the positional encodings once in log space.</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        <span class="comment">#之所以用log再exp,可能是考虑到数值过大溢出的问题</span></span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) *</span><br><span class="line">                             -(math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        self.register_buffer(<span class="string">'pe'</span>, pe)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = x + Variable(self.pe[:, :x.size(<span class="number">1</span>)], </span><br><span class="line">                         requires_grad=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure><p>代码可以参考公式，调用了<code>Module.register_buffer函数</code>。这个函数的作用是创建一个buffer，比如这里把pe保存下来。register_buffer通常用于保存一些模型参数之外的值，比如在BatchNorm中，我们需要保存running_mean(Moving Average)，它不是模型的参数(不用梯度下降)，但是模型会修改它，而且在预测的时候也要使用它。这里也是类似的，pe是一个提前计算好的常量，我们在forward要用到它。我们在构造函数里并没有把pe保存到self里，但是在forward的时候我们却可以直接使用它(self.pe)。如果我们保存(序列化)模型到磁盘的话，PyTorch框架也会帮我们保存buffer里的数据到磁盘，这样反序列化的时候能恢复它们</p><h3 id="完整模型"><a href="#完整模型" class="headerlink" title="完整模型"></a>完整模型</h3><blockquote><p>Here we <code>define a function that takes in hyperparameters and produces a full model.</code></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_model</span><span class="params">(src_vocab, tgt_vocab, N=<span class="number">6</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">               d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, h=<span class="number">8</span>, dropout=<span class="number">0.1</span>)</span>:</span> <span class="comment">#d_ff： feedforward的维度</span></span><br><span class="line">    <span class="string">"Helper: Construct a model from hyperparameters."</span></span><br><span class="line">    c = copy.deepcopy</span><br><span class="line">    attn = MultiHeadedAttention(h, d_model)</span><br><span class="line">    ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line">    position = PositionalEncoding(d_model, dropout)</span><br><span class="line">    model = EncoderDecoder(</span><br><span class="line">        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),</span><br><span class="line">        Decoder(DecoderLayer(d_model, c(attn), c(attn), </span><br><span class="line">                             c(ff), dropout), N),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),</span><br><span class="line">        Generator(d_model, tgt_vocab))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># This was important from their code. </span></span><br><span class="line">    <span class="comment"># Initialize parameters with Glorot / fan_avg. 随机初始化参数，这非常重要</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">            nn.init.xavier_uniform(p)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例: 对model简单输入参数</span></span><br><span class="line">tmp_model = make_model(<span class="number">10</span>, <span class="number">10</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>首先把copy.deepcopy命名为c，这样使下面的代码简洁一点。然后构造MultiHeadedAttention，PositionwiseFeedForward和PositionalEncoding对象。接着就是构造EncoderDecoder对象。它需要5个参数：Encoder、Decoder、src-embed、tgt-embed和Generator。</p><p>我们先看后面三个简单的参数，Generator直接构造就行了，它的作用是把模型的隐单元变成输出词的概率。而src-embed是一个Embeddings层和一个位置编码层c(position)，tgt-embed也是类似的。</p><p>最后我们来看Decoder(Encoder和Decoder类似的)。Decoder由N个DecoderLayer组成，而DecoderLayer需要传入self-attn, src-attn，全连接层和Dropout。因为所有的MultiHeadedAttention都是一样的，因此我们直接deepcopy就行；同理所有的PositionwiseFeedForward也是一样的网络结果，我们可以deepcopy而不要再构造一个。</p><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>This section describes the training regime for our models.</p><blockquote><p>We stop for a quick interlude to introduce some of the tools needed to train a standard encoder decoder model. First <code>we define a batch object that holds the src and target sentences for training, as well as constructing the masks.</code></p></blockquote><h4 id="Batches-和-Masking"><a href="#Batches-和-Masking" class="headerlink" title="Batches 和 Masking"></a>Batches 和 Masking</h4><p><code>mask 矩阵来自 batch</code></p><p><code>self.src_mask = (src != pad).unsqueeze(-2)</code> 也就是说, 源语言的 <strong>mask 矩阵的维度是 (batch_size, 1, length)</strong>, 那么为什么 <code>attn_shape = (batch_size, size, size)</code> 呢? 可以这么解释, <strong>在 encoder 阶段的 Self_Attention 阶段, 所有的 Attention 是可以同时进行的, 把所有的 Attention_result 算出来, 然后用同一个 mask vector * Attention_result 就可以了</strong>, 但是在 decoder 阶段却不能这么做, 我们需要关注的问题是:</p><blockquote><p>根据已经预测出来的单词预测下面的单词, 这一过程<strong>是序列的</strong>,</p><p>而我们的计算是<strong>并行</strong>的, 所以这一过程中, 必须要引入矩阵. 也就是上面的 subsequent_mask() 函数获得的矩阵.</p></blockquote><p>这个矩阵也很形象, 分别表示已经预测的单词的个数为, 1, 2, 3, 4, 5.</p><p>然后我们将以上过程反过来过一篇, 就很明显了, 在 batch阶段获得 mask 矩阵, 然后和 batch 一起训练, 在 encoder 与 deocder 阶段实现 mask 机制.</p><blockquote><ul><li><p>mask在Batch中定义，src_mask.size (30,1,10) ,  trg_mask.size(30,10,10)</p></li><li><p>然后在MultiHeadedAttention中<code>mask = mask.unsqueeze(1)</code>又扩维了，</p><p>其中src_mask.size(30,1,1,10) ,trg_mask.size(30,1,10,10)</p></li><li><p>src_mask.size满足attention中的维度，所以可以对score进行mask</p><p> src_mask还在解码器的第1子层用到，相同的原理</p></li><li><p>trg_mask在解码器的第0子层用到，满足要求</p></li></ul></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Batch</span>:</span> <span class="comment">#定义每一个batch中的src、tgt、mask</span></span><br><span class="line">    <span class="comment">#trg = tgt: 真实的标签序列  out ： 预测的单词  </span></span><br><span class="line">    <span class="string">"Object for holding a batch of data with mask during training."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, src, trg=None, pad=<span class="number">0</span>)</span>:</span> </span><br><span class="line">        self.src = src</span><br><span class="line">        self.src_mask = (src != pad).unsqueeze(<span class="number">-2</span>) <span class="comment">#扩充维度 倒数第二维增加值为1 size=(30,1,10)</span></span><br><span class="line">        <span class="comment">#并且非零值全部赋值为1</span></span><br><span class="line">        </span><br><span class="line">         <span class="comment"># 在预测的时候是没有 tgt 的,此时为 None 此时trg是tgt的形参</span></span><br><span class="line">        <span class="keyword">if</span> trg <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.trg = trg[:, :<span class="number">-1</span>] <span class="comment">#trg.size(30,9) ，在预测中，会提前输入起始符到ys中</span></span><br><span class="line">            <span class="string">"""</span></span><br><span class="line"><span class="string">              trg.size(30,9) 这里去掉的最后一个单词, 不是真正的单词, 而是标志 '&lt;eos&gt;' , 输入与输出都还有一个 '&lt;sos&gt;' 在句子的开头,  是decoder的输入，</span></span><br><span class="line"><span class="string">            需要进行mask，使得Self-Attention不能访问未来的输入。最后一个词不需要用到trg</span></span><br><span class="line"><span class="string">            """</span></span><br><span class="line">        self.trg_y = trg[:, <span class="number">1</span>:] <span class="comment"># trg_y.size(30,9) </span></span><br><span class="line">            <span class="comment">#trg_y: 最后的结果。用于loss中的比较。 去掉开头的'&lt;sos&gt;'，是decoder的输出</span></span><br><span class="line">            self.trg_mask = \</span><br><span class="line">                self.make_std_mask(self.trg, pad)</span><br><span class="line">            self.ntokens = (self.trg_y != pad).data.sum() <span class="comment">#不为0的总数 30*9 = 270</span></span><br><span class="line">    </span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_std_mask</span><span class="params">(tgt, pad)</span>:</span> <span class="comment">#tgt_mask.size(30,9,9)，每一个序列都是一个9*9的矩阵</span></span><br><span class="line">        <span class="string">"Create a mask to hide padding and future words."</span></span><br><span class="line">        <span class="comment">#"创建Mask，使得我们不能attend to未来的词"</span></span><br><span class="line">        tgt_mask = (tgt != pad).unsqueeze(<span class="number">-2</span>)</span><br><span class="line">        tgt_mask = tgt_mask &amp; Variable(</span><br><span class="line">            subsequent_mask(tgt.size(<span class="number">-1</span>)).type_as(tgt_mask.data))</span><br><span class="line">        <span class="keyword">return</span> tgt_mask</span><br></pre></td></tr></table></figure><p>Batch构造函数的输入是src和trg，后者可以为None，因为再预测的时候是没有tgt的。</p><p>我们用一个例子来说明Batch的代码，这是训练阶段的一个Batch，<strong>src是(48, 20)</strong>，48是batch大小，而20是最长的句子长度，其它的不够长的都padding成20了。而<strong>trg是(48, 25)</strong>，表示翻译后的最长句子是25个词，不足的也padding过了。</p><p>我们首先看src_mask怎么得到，(src != pad)把src中大于0的时刻置为1，这样表示它可以attend to的范围。然后unsqueeze(-2)把src_mask变成(48/batch, 1, 20/time)。它的用法参考前面的attention函数。</p><p>对于训练来说(Teaching Forcing模式)，Decoder有一个输入和一个输出。<strong>比如句子”<sos> it is a good day <eos>”，输入会变成”<sos> it is a good day”，而输出为”it is a good day <eos>”。对应到代码里，self.trg就是输入，而self.trg_y就是输出。</strong>接着对输入self.trg进行mask，使得Self-Attention不能访问未来的输入。这是通过make_std_mask函数实现的，这个函数会调用我们之前详细介绍过的subsequent_mask函数。最终得到的<strong>trg_mask的shape是(48/batch, 24, 24)</strong>，表示24个时刻的Mask矩阵，这是一个对角线以及之下都是1的矩阵，前面已经介绍过了。</p><p>注意<strong>src_mask的shape是(batch, 1, time)</strong>，而<strong>trg_mask是(batch, time, time)</strong>。因为src_mask的每一个时刻都能attend to所有时刻(padding的除外)，一次只需要一个向量就行了，而trg_mask需要一个矩阵。</p><h4 id="Training-Loop"><a href="#Training-Loop" class="headerlink" title="Training Loop"></a>Training Loop</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_epoch</span><span class="params">(data_iter, model, loss_compute)</span>:</span> <span class="comment">#返回total_loss / total_tokens 。是一个数值，损失计算</span></span><br><span class="line">    <span class="comment">#遍历一个epoch的数据</span></span><br><span class="line">    <span class="string">"Standard Training and Logging Function"</span></span><br><span class="line">    start = time.time() <span class="comment">#开始时间，计算用时</span></span><br><span class="line">    total_tokens = <span class="number">0</span> </span><br><span class="line">    total_loss = <span class="number">0</span> </span><br><span class="line">    tokens = <span class="number">0</span> </span><br><span class="line">    <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(data_iter): <span class="comment">#每一步data_iter（gen_data），实例化batch数据用于学习.进行20次</span></span><br><span class="line">        <span class="comment">#gen_data返回的是20个Batch，通过enumerate实例化20个batch </span></span><br><span class="line">        out = model.forward(batch.src, batch.trg, </span><br><span class="line">                            batch.src_mask, batch.trg_mask) <span class="comment">#调用EncoderDecoder的实例化model，解码器作为输出</span></span><br><span class="line">        loss = loss_compute(out, batch.trg_y, batch.ntokens) <span class="comment">#计算出一个batch中的loss。 trg_y是标准值。ntokens作为norm</span></span><br><span class="line">        total_loss += loss <span class="comment">#loss叠加。进行20次</span></span><br><span class="line">        total_tokens += batch.ntokens </span><br><span class="line">        tokens += batch.ntokens</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">50</span> == <span class="number">1</span>: <span class="comment">#i从0开始的，当i=1的时候，进行了一次batch，所以这里计算的就是一次batch所用的时间。而要进行20次。  50是随机设置</span></span><br><span class="line">            elapsed = time.time() - start <span class="comment">#计算一共用时</span></span><br><span class="line">            print(<span class="string">"Epoch Step: %d Loss: %f Tokens per Sec: %f"</span> % </span><br><span class="line">                    (i, loss / batch.ntokens, tokens / elapsed)) <span class="comment">#所有batch中的loss和ntoken,即一个epoch中</span></span><br><span class="line">            start = time.time() <span class="comment"># 重置时间</span></span><br><span class="line">            tokens = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> total_loss / total_tokens</span><br></pre></td></tr></table></figure><p>它遍历一个epoch的数据，然后调用forward，接着用loss_compute函数计算梯度，更新参数并且返回loss。这里的loss_compute是一个函数，它的输入是模型的预测out，真实的标签序列batch.trg_y和batch的词个数。实际的实现是MultiGPULossCompute类，这是一个callable。本来计算损失和更新参数比较简单，但是这里为了实现多GPU的训练，这个类就比较复杂了。</p><h4 id="Training-Data-和-Batching"><a href="#Training-Data-和-Batching" class="headerlink" title="Training Data 和 Batching"></a>Training Data 和 Batching</h4><p>We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding, which has a shared source-target vocabulary of about 37000 tokens. For English- French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary.</p><p>Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.</p><blockquote><p>We will use torch text for batching. This is discussed in more detail below. Here we create batches in a torchtext function that ensures our batch size padded to the maximum batchsize does not surpass a threshold (25000 if we have 8 gpus).</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">global</span> max_src_in_batch, max_tgt_in_batch</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_size_fn</span><span class="params">(new, count, sofar)</span>:</span></span><br><span class="line">    <span class="string">"Keep augmenting batch and calculate total number of tokens + padding."</span></span><br><span class="line">    <span class="keyword">global</span> max_src_in_batch, max_tgt_in_batch</span><br><span class="line">    <span class="keyword">if</span> count == <span class="number">1</span>:</span><br><span class="line">        max_src_in_batch = <span class="number">0</span></span><br><span class="line">        max_tgt_in_batch = <span class="number">0</span></span><br><span class="line">    max_src_in_batch = max(max_src_in_batch,  len(new.src))</span><br><span class="line">    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + <span class="number">2</span>)</span><br><span class="line">    src_elements = count * max_src_in_batch</span><br><span class="line">    tgt_elements = count * max_tgt_in_batch</span><br><span class="line">    <span class="keyword">return</span> max(src_elements, tgt_elements)</span><br></pre></td></tr></table></figure><h4 id="硬件-和-训练进度"><a href="#硬件-和-训练进度" class="headerlink" title="硬件 和 训练进度"></a>硬件 和 训练进度</h4><p>We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models, step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).</p><h4 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h4><p>We used the <code>Adam optimizer</code> <a href="https://arxiv.org/abs/1412.6980" target="_blank" rel="noopener">(cite)</a> with β1=0.9β1=0.9, β2=0.98β2=0.98 and ϵ=10−9ϵ=10−9. We varied the learning rate over the course of training, according to the formula: lrate=d−0.5model⋅min(step_num−0.5,step_num⋅warmup_steps−1.5)lrate=dmodel−0.5⋅min(step_num−0.5,step_num⋅warmup_steps−1.5) This corresponds to increasing the learning rate linearly for the first warmupstepswarmupsteps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmupsteps=4000warmupsteps=4000.</p><blockquote><p>Note: This part is very important. Need to train with this setup of the model.</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NoamOpt</span>:</span></span><br><span class="line">    <span class="string">"Optim wrapper that implements rate."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, model_size, factor, warmup, optimizer)</span>:</span></span><br><span class="line">        self.optimizer = optimizer</span><br><span class="line">        self._step = <span class="number">0</span></span><br><span class="line">        self.warmup = warmup</span><br><span class="line">        self.factor = factor</span><br><span class="line">        self.model_size = model_size</span><br><span class="line">        self._rate = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"Update parameters and rate"</span></span><br><span class="line">        self._step += <span class="number">1</span></span><br><span class="line">        rate = self.rate()</span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> self.optimizer.param_groups:</span><br><span class="line">            p[<span class="string">'lr'</span>] = rate</span><br><span class="line">        self._rate = rate</span><br><span class="line">        self.optimizer.step()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">rate</span><span class="params">(self, step = None)</span>:</span></span><br><span class="line">        <span class="string">"Implement `lrate` above"</span></span><br><span class="line">        <span class="keyword">if</span> step <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            step = self._step</span><br><span class="line">        <span class="keyword">return</span> self.factor * \</span><br><span class="line">            (self.model_size ** (<span class="number">-0.5</span>) *</span><br><span class="line">            min(step ** (<span class="number">-0.5</span>), step * self.warmup ** (<span class="number">-1.5</span>)))</span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_std_opt</span><span class="params">(model)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> NoamOpt(model.src_embed[<span class="number">0</span>].d_model, <span class="number">2</span>, <span class="number">4000</span>,</span><br><span class="line">            torch.optim.Adam(model.parameters(), lr=<span class="number">0</span>, betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span>))</span><br></pre></td></tr></table></figure><blockquote><p>Example of the curves of this model for different model sizes and for optimization hyperparameters.</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Three settings of the lrate hyperparameters.</span></span><br><span class="line">opts = [NoamOpt(<span class="number">512</span>, <span class="number">1</span>, <span class="number">4000</span>, <span class="literal">None</span>), </span><br><span class="line">        NoamOpt(<span class="number">512</span>, <span class="number">1</span>, <span class="number">8000</span>, <span class="literal">None</span>),</span><br><span class="line">        NoamOpt(<span class="number">256</span>, <span class="number">1</span>, <span class="number">4000</span>, <span class="literal">None</span>)]</span><br><span class="line">plt.plot(np.arange(<span class="number">1</span>, <span class="number">20000</span>), [[opt.rate(i) <span class="keyword">for</span> opt <span class="keyword">in</span> opts] <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">20000</span>)])</span><br><span class="line">plt.legend([<span class="string">"512:4000"</span>, <span class="string">"512:8000"</span>, <span class="string">"256:4000"</span>])</span><br><span class="line"><span class="literal">None</span></span><br></pre></td></tr></table></figure><p><img src="http://nlp.seas.harvard.edu/images/the-annotated-transformer_69_0.png" alt="png"></p><h4 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h4><h5 id="Label-Smoothing"><a href="#Label-Smoothing" class="headerlink" title="Label Smoothing"></a>Label Smoothing</h5><p>During training, we employed label smoothing of value ϵls=0.1ϵls=0.1 <a href="https://arxiv.org/abs/1512.00567" target="_blank" rel="noopener">(cite)</a>. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.</p><blockquote><p>We implement label smoothing using the KL div loss. Instead of using a one-hot target distribution, we create a distribution that has <code>confidence</code> of the correct word and the rest of the <code>smoothing</code> mass distributed throughout the vocabulary.</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LabelSmoothing</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Implement label smoothing."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, padding_idx, smoothing=<span class="number">0.0</span>)</span>:</span></span><br><span class="line">        super(LabelSmoothing, self).__init__()</span><br><span class="line">        self.criterion = nn.KLDivLoss(size_average=<span class="literal">False</span>)  <span class="comment">#KL散度</span></span><br><span class="line">        self.padding_idx = padding_idx</span><br><span class="line">        self.confidence = <span class="number">1.0</span> - smoothing</span><br><span class="line">        self.smoothing = smoothing</span><br><span class="line">        self.size = size</span><br><span class="line">        self.true_dist = <span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, target)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> x.size(<span class="number">1</span>) == self.size</span><br><span class="line">        true_dist = x.data.clone()</span><br><span class="line">        true_dist.fill_(self.smoothing / (self.size - <span class="number">2</span>))</span><br><span class="line">        true_dist.scatter_(<span class="number">1</span>, target.data.unsqueeze(<span class="number">1</span>), self.confidence)</span><br><span class="line">        true_dist[:, self.padding_idx] = <span class="number">0</span></span><br><span class="line">        mask = torch.nonzero(target.data == self.padding_idx)</span><br><span class="line">        <span class="keyword">if</span> mask.dim() &gt; <span class="number">0</span>:</span><br><span class="line">            true_dist.index_fill_(<span class="number">0</span>, mask.squeeze(), <span class="number">0.0</span>)</span><br><span class="line">        self.true_dist = true_dist</span><br><span class="line">        <span class="keyword">return</span> self.criterion(x, Variable(true_dist, requires_grad=<span class="literal">False</span>))</span><br></pre></td></tr></table></figure><blockquote><p>Here we can see an example of how the mass is distributed to the words based on confidence.</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Example of label smoothing.</span></span><br><span class="line">crit = LabelSmoothing(<span class="number">5</span>, <span class="number">0</span>, <span class="number">0.4</span>)</span><br><span class="line">predict = torch.FloatTensor([[<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>],</span><br><span class="line">                             [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>], </span><br><span class="line">                             [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>]])</span><br><span class="line">v = crit(Variable(predict.log()), </span><br><span class="line">         Variable(torch.LongTensor([<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>])))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show the target distributions expected by the system.</span></span><br><span class="line">plt.imshow(crit.true_dist)</span><br><span class="line"><span class="literal">None</span></span><br></pre></td></tr></table></figure><p><img src="http://nlp.seas.harvard.edu/images/the-annotated-transformer_74_0.png" alt="png"></p><blockquote><p>Label smoothing actually starts to penalize the model if it gets very confident about a given choice.</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">crit = LabelSmoothing(<span class="number">5</span>, <span class="number">0</span>, <span class="number">0.1</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(x)</span>:</span></span><br><span class="line">    d = x + <span class="number">3</span> * <span class="number">1</span></span><br><span class="line">    predict = torch.FloatTensor([[<span class="number">0</span>, x / d, <span class="number">1</span> / d, <span class="number">1</span> / d, <span class="number">1</span> / d],</span><br><span class="line">                                 ])</span><br><span class="line">    <span class="comment">#print(predict)</span></span><br><span class="line">    <span class="keyword">return</span> crit(Variable(predict.log()),</span><br><span class="line">                 Variable(torch.LongTensor([<span class="number">1</span>]))).data[<span class="number">0</span>]</span><br><span class="line">plt.plot(np.arange(<span class="number">1</span>, <span class="number">100</span>), [loss(x) <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">100</span>)])</span><br><span class="line"><span class="literal">None</span></span><br></pre></td></tr></table></figure><p><img src="http://nlp.seas.harvard.edu/images/the-annotated-transformer_76_0.png" alt="png"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h3><p>transformer模型主要分为两大部分, 分别是编码器和解码器, 编码器负责把自然语言序列映射成为隐藏层(下图中第2步用九宫格比喻的部分), 含有自然语言序列的数学表达. 然后解码器把隐藏层再映射为自然语言序列, 从而使我们可以解决各种问题, 如情感分类, 命名实体识别, 语义关系抽取, 摘要生成, 机 器翻译等等, 下面我们简单说一下下图的每一步都做了什么:</p><blockquote><p>1.输入自然语言序列到编码器: Why do we work?(为什么要工作); </p><p>2.编码器输出的隐藏层, 再输入到解码器; </p><p>3.输入&lt;𝑠𝑡𝑎𝑟𝑡&gt;<start>(起始)符号到解码器; </p><p>4.得到第一个字”为”; </p><p>5.将得到的第一个字”为”落下来再输入到解码器; </p><p>6.得到第二个字”什”; </p><p>7.将得到的第二字再落下来, 直到解码器输出&lt;𝑒𝑛𝑑&gt;<end>(终止符), 即序列生成完成.</p></blockquote><img src="https://i.loli.net/2020/08/06/1pea3WSThisHBql.png" alt="image-20200806233205808" style="zoom:67%;" /><p><img src="https://i.loli.net/2020/08/07/ZGa1snNULJtjFWS.png" alt="transformer"></p><p>原始data数据是：(30,10)</p><p>src: (30,10)  trg:(30,10) </p><p>在encoder中，</p><p>embedding： 参数x就是 src （30,10） 经过处理之后， x:（30,10,512） -&gt; 即输入给encoder的x：(30,10,512)</p><p>经过encoder各个层处理之后，输出的（30，10,512）  memory是encoder的输出，但是为什么memory：（1,10,512） ??? 因为在预测时 ，src是（1,10），不是（30,10）所以memory是（1,10,512）</p><p>decoder中：输入来自 memory 和 trg_emd</p><p>embedding ： 参数x是trg（30,9），经过处理之后，x：（30,9,512)   </p><p>经过decoder各个层处理之后，输出的（30，9 , 512）  </p><p>再经过generator层之后，x：（30,9,11） </p><p>在预测的时候是（1，1,512），不是（1,9,512），在预测完generator之后，（1,11），选一个最大的。</p><p>因为是一个数字一个数字预测输出的，所以是1，不是9</p><h3 id="第一个例子"><a href="#第一个例子" class="headerlink" title="第一个例子"></a>第一个例子</h3><blockquote><p>We can begin by trying out a simple copy-task. Given a random set of input symbols from a small vocabulary, the goal is to generate back those same symbols.</p></blockquote><h4 id="Synthetic-Data"><a href="#Synthetic-Data" class="headerlink" title="Synthetic Data"></a>Synthetic Data</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_gen</span><span class="params">(V, batch, nbatches)</span>:</span> <span class="comment"># batch=30:一次输入多少， nbatch=20：输入多少次</span></span><br><span class="line">    <span class="string">"Generate random data for a src-tgt copy task."</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(nbatches): <span class="comment">#一共循环nbatches个，在每一个是一个batch</span></span><br><span class="line"><span class="comment">#from_numpy ： 将numpy数据转换为tensor</span></span><br><span class="line"><span class="comment">#注：生成返回的tensor会和ndarry共享数据，任何对tensor的操作都会影响到ndarry</span></span><br><span class="line">        data = torch.from_numpy(np.random.randint(<span class="number">1</span>, V, size=(batch, <span class="number">10</span>))) <span class="comment">#1是产生的最小值，V=11是最大值，size是形状（batch，10）。生成（batch，10）的矩阵，矩阵的每一个元素都是1~V-1之间  （取不到V）</span></span><br><span class="line">        data[:, <span class="number">0</span>] = <span class="number">1</span> <span class="comment">#将第0列的值赋值为1</span></span><br><span class="line">        <span class="comment"># Variable 就是一个存放值， 里面的值会不停的变化.  存放的是Torch 的 Tensor . 如果用一个 Variable 进行计算, 那返回的也是一个同类型的 Variable.  </span></span><br><span class="line">        <span class="comment">#requires_grad： 是否参与误差反向传播, 要不要计算梯度</span></span><br><span class="line">        src = Variable(data, requires_grad=<span class="literal">False</span>) <span class="comment">#size(batch,10) 和data的值完全一样</span></span><br><span class="line">        tgt = Variable(data, requires_grad=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">yield</span> Batch(src, tgt, <span class="number">0</span>)<span class="comment">#yield就是return一个值，并且记住这个返回的位置，下次迭代就从这个位置后(下一行)开始</span></span><br><span class="line">        <span class="comment">#batch返回的是trg_mask</span></span><br></pre></td></tr></table></figure><h4 id="Loss-Computation"><a href="#Loss-Computation" class="headerlink" title="Loss Computation"></a>Loss Computation</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleLossCompute</span>:</span> <span class="comment">#loss计算以及更新。调用LabelSmoothing，使用KL散度</span></span><br><span class="line">    <span class="string">"A simple loss compute and train function."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, generator, criterion, opt=None)</span>:</span></span><br><span class="line">        self.generator = generator <span class="comment">#解码器后的生成函数</span></span><br><span class="line">        self.criterion = criterion <span class="comment"># LabelSmoothing（计算loss KLDivLoss KL散度）的实例化</span></span><br><span class="line">        self.opt = opt <span class="comment"># NoamOpt（优化）的实例化</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, x, y, norm)</span>:</span></span><br><span class="line">        x = self.generator(x) <span class="comment">#解码器的输出</span></span><br><span class="line">        loss = self.criterion(x.contiguous().view(<span class="number">-1</span>, x.size(<span class="number">-1</span>)), </span><br><span class="line">                              y.contiguous().view(<span class="number">-1</span>)) / norm  <span class="comment">#计算loss</span></span><br><span class="line">        loss.backward() <span class="comment">#将loss反向传播。loss是标量，根据链式法则自动计算出叶子节点的梯度值</span></span><br><span class="line">        <span class="keyword">if</span> self.opt <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: <span class="comment">#存在优化</span></span><br><span class="line">            self.opt.step() <span class="comment">#调用opt的step函数。 adam优化，，更新参数</span></span><br><span class="line">            self.opt.optimizer.zero_grad() <span class="comment">#把梯度置零，也就是把loss关于weight的导数变成0.</span></span><br><span class="line">        <span class="keyword">return</span> loss.data[<span class="number">0</span>] * norm</span><br></pre></td></tr></table></figure><h4 id="Greedy-Decoding"><a href="#Greedy-Decoding" class="headerlink" title="Greedy Decoding"></a>Greedy Decoding</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Train the simple copy task.</span></span><br><span class="line">V = <span class="number">11</span></span><br><span class="line">criterion = LabelSmoothing(size=V, padding_idx=<span class="number">0</span>, smoothing=<span class="number">0.0</span>) <span class="comment">#LabelSmoothing是KL散度实现的</span></span><br><span class="line">model = make_model(V, V, N=<span class="number">2</span>) <span class="comment">#src_vocab=11, tgt_vocab=11，覆盖N=2</span></span><br><span class="line"><span class="comment"># 对模型参数进行更新优化，使用Adam优化</span></span><br><span class="line">model_opt = NoamOpt(model.src_embed[<span class="number">0</span>].d_model, <span class="number">1</span>, <span class="number">400</span>,</span><br><span class="line">        torch.optim.Adam(model.parameters(), lr=<span class="number">0</span>, betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#model.eval()，pytorch会自动把BN和DropOut固定住，不会取平均，而是用训练好的值。</span></span><br><span class="line"><span class="comment">#model.train() 让model变成训练模式，此时dropout和batch normalization的操作在训练起到防止网络过拟合的问题</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">10</span>): <span class="comment">#一共10大份， model.train()打印1行，model.eval()打印1行</span></span><br><span class="line">    model.train()</span><br><span class="line">    <span class="comment">#调用run_epoch(data_iter, model, loss_compute)函数</span></span><br><span class="line">    <span class="comment">#返回total_loss / total_tokens 。返回值可以没有接收，不会报错</span></span><br><span class="line">    run_epoch(data_gen(V, <span class="number">30</span>, <span class="number">20</span>), model, </span><br><span class="line">              SimpleLossCompute(model.generator, criterion, model_opt))</span><br><span class="line">    model.eval()</span><br><span class="line">    <span class="comment">#print接收run_epoch的返回值 在输出的第三行</span></span><br><span class="line">    print(run_epoch(data_gen(V, <span class="number">30</span>, <span class="number">5</span>), model, </span><br><span class="line">                    SimpleLossCompute(model.generator, criterion, <span class="literal">None</span>)))</span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">3.023465</span> Tokens per Sec: <span class="number">403.074173</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.920030</span> Tokens per Sec: <span class="number">641.689380</span></span><br><span class="line"><span class="number">1.9274832487106324</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.940011</span> Tokens per Sec: <span class="number">432.003378</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.699767</span> Tokens per Sec: <span class="number">641.979665</span></span><br><span class="line"><span class="number">1.657595729827881</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.860276</span> Tokens per Sec: <span class="number">433.320240</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.546011</span> Tokens per Sec: <span class="number">640.537198</span></span><br><span class="line"><span class="number">1.4888023376464843</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.682198</span> Tokens per Sec: <span class="number">432.092305</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.313169</span> Tokens per Sec: <span class="number">639.441857</span></span><br><span class="line"><span class="number">1.3485562801361084</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.278768</span> Tokens per Sec: <span class="number">433.568756</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.062384</span> Tokens per Sec: <span class="number">642.542067</span></span><br><span class="line"><span class="number">0.9853351473808288</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.269471</span> Tokens per Sec: <span class="number">433.388727</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">0.590709</span> Tokens per Sec: <span class="number">642.862135</span></span><br><span class="line"><span class="number">0.5686767101287842</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">0.997076</span> Tokens per Sec: <span class="number">433.009746</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">0.343118</span> Tokens per Sec: <span class="number">642.288427</span></span><br><span class="line"><span class="number">0.34273059368133546</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">0.459483</span> Tokens per Sec: <span class="number">434.594030</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">0.290385</span> Tokens per Sec: <span class="number">642.519464</span></span><br><span class="line"><span class="number">0.2612409472465515</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.031042</span> Tokens per Sec: <span class="number">434.557008</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">0.437069</span> Tokens per Sec: <span class="number">643.630322</span></span><br><span class="line"><span class="number">0.4323212027549744</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">0.617165</span> Tokens per Sec: <span class="number">436.652626</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">0.258793</span> Tokens per Sec: <span class="number">644.372296</span></span><br><span class="line"><span class="number">0.27331129014492034</span></span><br></pre></td></tr></table></figure><blockquote><p>This code predicts a translation using greedy decoding for simplicity.</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#预测过程</span></span><br><span class="line"><span class="comment">#预测的时候没有用tgt（标准值），而是每次解码器的输入都是ys，是预测的值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">greedy_decode</span><span class="params">(model, src, src_mask, max_len, start_symbol)</span>:</span></span><br><span class="line">    memory = model.encode(src, src_mask) <span class="comment">#memory是编码器的输出 。是一个矩阵</span></span><br><span class="line">    ys = torch.ones(<span class="number">1</span>, <span class="number">1</span>).fill_(start_symbol).type_as(src.data) <span class="comment">#填充输出开始符，和src的类型一样。对预测的句子进行初始化 ys =1 （1,1）</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(max_len<span class="number">-1</span>): <span class="comment">#0~8 对每一个词都进行预测</span></span><br><span class="line">        out = model.decode(memory, src_mask, </span><br><span class="line">                           Variable(ys), </span><br><span class="line">                           Variable(subsequent_mask(ys.size(<span class="number">1</span>))</span><br><span class="line">                                    .type_as(src.data)))</span><br><span class="line">         <span class="comment"># ys 的维度是 batch_size * times （固定的）,   所以target_mask 矩阵必须是ys.size(1),所以是 times * times</span></span><br><span class="line">        <span class="comment"># 根据 decoder 的训练步骤, 这里的 out 输出就应该是 batch_size * (times+1) 的矩阵</span></span><br><span class="line">        </span><br><span class="line">        prob = model.generator(out[:, <span class="number">-1</span>]) <span class="comment">#generator返回的是softmax</span></span><br><span class="line">          <span class="comment"># out[:, -1] 这里是最新的一个单词的 embedding 向量</span></span><br><span class="line">        <span class="comment"># generator 就是产生最后的 vocabulary 的概率, 是一个全连接层</span></span><br><span class="line">        </span><br><span class="line">        _, next_word = torch.max(prob, dim = <span class="number">1</span>) <span class="comment"># torch.max:按维度dim 返回最大值，并且会返回索引。next_data接收#索引</span></span><br><span class="line">        next_word = next_word.data[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 将句子拼接起来  .type_as: 将tensor强制转换为src.data 格式的</span></span><br><span class="line">        ys = torch.cat([ys, </span><br><span class="line">                        torch.ones(<span class="number">1</span>, <span class="number">1</span>).type_as(src.data).fill_(next_word)], dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> ys</span><br><span class="line"></span><br><span class="line">model.eval()</span><br><span class="line">src = Variable(torch.LongTensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>]]) )</span><br><span class="line">src_mask = Variable(torch.ones(<span class="number">1</span>, <span class="number">1</span>, <span class="number">10</span>) )</span><br><span class="line">print(greedy_decode(model, src, src_mask, max_len=<span class="number">10</span>, start_symbol=<span class="number">1</span>))</span><br><span class="line">    <span class="number">1</span>     <span class="number">2</span>     <span class="number">3</span>     <span class="number">4</span>     <span class="number">5</span>     <span class="number">6</span>     <span class="number">7</span>     <span class="number">8</span>     <span class="number">9</span>    <span class="number">10</span></span><br><span class="line">[torch.LongTensor of size <span class="number">1</span>x10]</span><br></pre></td></tr></table></figure><h3 id="真实例子"><a href="#真实例子" class="headerlink" title="真实例子"></a>真实例子</h3><blockquote><p>Now we consider a real-world example using the IWSLT German-English Translation task. This task is much smaller than the WMT task considered in the paper, but it illustrates the whole system. We also show how to use multi-gpu processing to make it really fast.</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!pip install torchtext spacy</span></span><br><span class="line"><span class="comment">#!python -m spacy download en</span></span><br><span class="line"><span class="comment">#!python -m spacy download de</span></span><br></pre></td></tr></table></figure><h4 id="Data-Loading"><a href="#Data-Loading" class="headerlink" title="Data Loading"></a>Data Loading</h4><blockquote><p>We will load the dataset using torchtext and spacy for tokenization.</p><p>用torchtext来加载数据集 ， 用spacy来分词</p></blockquote><img src="https://i.loli.net/2020/08/07/teSG1hufEjF4Zkv.png" alt="image-20200807001353729" style="zoom: 67%;" /><p>torchtext组件流程：</p><blockquote><ul><li>定义Field：声明如何处理数据，主要包含以下数据预处理的配置信息，比如指定分词方法，是否转成小写，起始字符，结束字符，补全字符以及词典等等</li><li>定义Dataset：用于得到数据集，继承自pytorch的Dataset。此时数据集里每一个样本是一个 经过 Field声明的预处理 预处理后的 wordlist</li><li>建立vocab：在这一步建立词汇表，词向量(word embeddings)</li><li>构造迭代器Iterator：: 主要是数据输出的模型的迭代器。构造迭代器，支持batch定制用来分批次训练模型。</li></ul></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># For data loading.</span></span><br><span class="line"><span class="keyword">from</span> torchtext <span class="keyword">import</span> data, datasets</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="keyword">import</span> spacy</span><br><span class="line">    spacy_de = spacy.load(<span class="string">'de'</span>) <span class="comment">#加载德语语言模型</span></span><br><span class="line">    spacy_en = spacy.load(<span class="string">'en'</span>) <span class="comment">#加载英语语言模型</span></span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">   在文本处理的过程中，spaCy首先对文本分词，原始文本在空格处分割，类似于text.split(' ')，然后分词器（Tokenizer）从左向右依次处理token</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tokenize_de</span><span class="params">(text)</span>:</span> <span class="comment">#Tokenizer:分词器  进行德语分词  </span></span><br><span class="line">        <span class="comment">#text：输入的段落句子  tok.text：分后的token词</span></span><br><span class="line">        <span class="keyword">return</span> [tok.text <span class="keyword">for</span> tok <span class="keyword">in</span> spacy_de.tokenizer(text)]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tokenize_en</span><span class="params">(text)</span>:</span> <span class="comment"># 进行英语分词</span></span><br><span class="line">        <span class="keyword">return</span> [tok.text <span class="keyword">for</span> tok <span class="keyword">in</span> spacy_en.tokenizer(text)]</span><br><span class="line"></span><br><span class="line">    BOS_WORD = <span class="string">'&lt;s&gt;'</span>  <span class="comment">#开始符</span></span><br><span class="line">    EOS_WORD = <span class="string">'&lt;/s&gt;'</span> <span class="comment">#终止符</span></span><br><span class="line">    BLANK_WORD = <span class="string">"&lt;blank&gt;"</span> <span class="comment">#空格</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 构建Filed对象，声明如何处理数据。主要包含以下数据预处理的配置信息，比如指定分词方法，是否转成小写，#起始字符，结束字符，补全字符以及词典等等</span></span><br><span class="line">    SRC = data.Field(tokenize=tokenize_de, pad_token=BLANK_WORD) <span class="comment">#得到源句子</span></span><br><span class="line">    TGT = data.Field(tokenize=tokenize_en, init_token = BOS_WORD,  </span><br><span class="line">                     eos_token = EOS_WORD, pad_token=BLANK_WORD)</span><br><span class="line"></span><br><span class="line">    MAX_LEN = <span class="number">100</span> <span class="comment">#最大长度</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># https://s3.amazonaws.com/opennmt-models/iwslt.pt 数据集</span></span><br><span class="line">    <span class="comment">#同时对训练集和验证集还有测试集的构建，此时数据集里每一个样本是一个 经过 Field声明的预处理 预处理后的 #wordlist</span></span><br><span class="line">    train, val, test = datasets.IWSLT.splits(</span><br><span class="line">        exts=(<span class="string">'.de'</span>, <span class="string">'.en'</span>)   <span class="comment"># 构建数据集所需的数据集</span></span><br><span class="line">        , fields=(SRC, TGT),  <span class="comment">#如何赋值给train那三个的？？？？</span></span><br><span class="line">        filter_pred=<span class="keyword">lambda</span> x: len(vars(x)[<span class="string">'src'</span>]) &lt;= MAX_LEN <span class="keyword">and</span> </span><br><span class="line">            len(vars(x)[<span class="string">'trg'</span>]) &lt;= MAX_LEN)  <span class="comment">#源句子和目标句子长度小于100的筛选出来</span></span><br><span class="line">    </span><br><span class="line">    MIN_FREQ = <span class="number">2</span> <span class="comment">#定义最小频率</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#建立词汇表，词向量(word embeddings)。即需要给每个单词编码，然后输入模型</span></span><br><span class="line">    <span class="comment">#bulid_vocab()方法中传入用于构建词表的数据集</span></span><br><span class="line">    SRC.build_vocab(train.src, min_freq=MIN_FREQ) </span><br><span class="line">    TGT.build_vocab(train.trg, min_freq=MIN_FREQ)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#一旦运行了这些代码行，SRC.vocab.stoi将是一个词典，其词汇表中的标记作为键，而其对应的索引作为值； #SRC.vocab.itos将是相同的字典，其中的键和值被交换。</span></span><br></pre></td></tr></table></figure><blockquote><p>批训练对于速度来说很重要。希望批次分割非常均匀并且填充最少。 要做到这一点，我们<strong>必须修改torchtext默认的批处理函数</strong>。 这部分代码修补其默认批处理函数，以确保我们搜索足够多的句子以构建紧密批处理。  一般来说直接调用<code>BucketIterator</code> （训练用）和 <code>Iterator</code>（测试用） 即可</p><p><code>BucketIterator</code>和<code>Iterator</code>的区别是，BucketIterator尽可能的把长度相似的句子放在一个batch里面。</p></blockquote><h4 id="Iterators"><a href="#Iterators" class="headerlink" title="Iterators"></a>Iterators</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">定义一个迭代器，该迭代器将相似长度的示例批处理在一起。 在为每个新纪元(epoch)生产新鲜改组的批次时，最大程度地减少所需的填充量。</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyIterator</span><span class="params">(data.Iterator)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_batches</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment">#在train的时候，要进行sort，尽量减少padding</span></span><br><span class="line">        <span class="comment">#目的是自动进行shuffle和padding，并且为了训练效率期间，尽量把句子长度相似的shuffle在一起。</span></span><br><span class="line">        <span class="keyword">if</span> self.train:</span><br><span class="line">            <span class="function"><span class="keyword">def</span> <span class="title">pool</span><span class="params">(d, random_shuffler)</span>:</span></span><br><span class="line">                <span class="keyword">for</span> p <span class="keyword">in</span> data.batch(d, self.batch_size * <span class="number">100</span>):</span><br><span class="line">                    p_batch = data.batch(</span><br><span class="line">                        sorted(p, key=self.sort_key), <span class="comment">#按照词的数大小排序</span></span><br><span class="line">                        self.batch_size, self.batch_size_fn)</span><br><span class="line">                    <span class="keyword">for</span> b <span class="keyword">in</span> random_shuffler(list(p_batch)):</span><br><span class="line">                        <span class="keyword">yield</span> b <span class="comment">#b就是batch， 类比上述的gen_data函数</span></span><br><span class="line">            self.batches = pool(self.data(), self.random_shuffler) <span class="comment">#调用pool</span></span><br><span class="line">            </span><br><span class="line">         <span class="comment">#在valid+test(验证集和测试集)的时候  和上面具体区别在哪？？？？</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.batches = []</span><br><span class="line">            <span class="keyword">for</span> b <span class="keyword">in</span> data.batch(self.data(), self.batch_size,</span><br><span class="line">                                          self.batch_size_fn):</span><br><span class="line">                self.batches.append(sorted(b, key=self.sort_key))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rebatch</span><span class="params">(pad_idx, batch)</span>:</span>  <span class="comment">#pad_idx：空格键</span></span><br><span class="line">    <span class="string">"Fix order in torchtext to match ours"</span></span><br><span class="line">    src, trg = batch.src.transpose(<span class="number">0</span>, <span class="number">1</span>), batch.trg.transpose(<span class="number">0</span>, <span class="number">1</span>)<span class="comment">#为什么要进行</span></span><br><span class="line">    <span class="keyword">return</span> Batch(src, trg, pad_idx) <span class="comment">#调用上述的Batch类   pad_idx就是pad</span></span><br></pre></td></tr></table></figure><h4 id="Multi-GPU-Training"><a href="#Multi-GPU-Training" class="headerlink" title="Multi-GPU Training"></a>Multi-GPU Training</h4><blockquote><p>最后为了真正地快速训练，将使用多个GPU。 这部分代码实现了多GPU字生成，它不是Transformer特有的。 其<strong>思想是将训练时的单词生成分成块，以便在许多不同的GPU上并行处理。</strong> 我们使用PyTorch并行原语来做到这一点：</p><ul><li>replicate -复制 - 将模块拆分到不同的GPU上</li><li>scatter -分散 - 将批次拆分到不同的GPU上</li><li>parallel_apply -并行应用 - 在不同GPU上将模块应用于批处理</li><li>gather - 聚集 - 将分散的数据聚集到一个GPU上</li><li>nn.DataParallel - 一个特殊的模块包装器，在评估之前调用它们。</li></ul></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Skip if not interested in multigpu.</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiGPULossCompute</span>:</span></span><br><span class="line">    <span class="string">"A multi-gpu loss compute and train function."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, generator, criterion, devices, opt=None, chunk_size=<span class="number">5</span>)</span>:</span></span><br><span class="line">        <span class="comment"># Send out to different gpus.</span></span><br><span class="line">        self.generator = generator</span><br><span class="line">        self.criterion = nn.parallel.replicate(criterion, </span><br><span class="line">                                               devices=devices)</span><br><span class="line">        self.opt = opt</span><br><span class="line">        self.devices = devices</span><br><span class="line">        self.chunk_size = chunk_size</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, out, targets, normalize)</span>:</span></span><br><span class="line">        total = <span class="number">0.0</span></span><br><span class="line">        generator = nn.parallel.replicate(self.generator, </span><br><span class="line">                                                devices=self.devices)</span><br><span class="line">        out_scatter = nn.parallel.scatter(out, </span><br><span class="line">                                          target_gpus=self.devices)</span><br><span class="line">        out_grad = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> out_scatter]</span><br><span class="line">        targets = nn.parallel.scatter(targets, </span><br><span class="line">                                      target_gpus=self.devices)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Divide generating into chunks.</span></span><br><span class="line">        chunk_size = self.chunk_size</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, out_scatter[<span class="number">0</span>].size(<span class="number">1</span>), chunk_size):</span><br><span class="line">            <span class="comment"># Predict distributions</span></span><br><span class="line">            out_column = [[Variable(o[:, i:i+chunk_size].data, </span><br><span class="line">                                    requires_grad=self.opt <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>)] </span><br><span class="line">                           <span class="keyword">for</span> o <span class="keyword">in</span> out_scatter]</span><br><span class="line">            gen = nn.parallel.parallel_apply(generator, out_column)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Compute loss. </span></span><br><span class="line">            y = [(g.contiguous().view(<span class="number">-1</span>, g.size(<span class="number">-1</span>)), </span><br><span class="line">                  t[:, i:i+chunk_size].contiguous().view(<span class="number">-1</span>)) </span><br><span class="line">                 <span class="keyword">for</span> g, t <span class="keyword">in</span> zip(gen, targets)]</span><br><span class="line">            loss = nn.parallel.parallel_apply(self.criterion, y)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Sum and normalize loss</span></span><br><span class="line">            l = nn.parallel.gather(loss, </span><br><span class="line">                                   target_device=self.devices[<span class="number">0</span>])</span><br><span class="line">            l = l.sum()[<span class="number">0</span>] / normalize</span><br><span class="line">            total += l.data[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Backprop loss to output of transformer</span></span><br><span class="line">            <span class="keyword">if</span> self.opt <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                l.backward()</span><br><span class="line">                <span class="keyword">for</span> j, l <span class="keyword">in</span> enumerate(loss):</span><br><span class="line">                    out_grad[j].append(out_column[j][<span class="number">0</span>].grad.data.clone())</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Backprop all loss through transformer.            </span></span><br><span class="line">        <span class="keyword">if</span> self.opt <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            out_grad = [Variable(torch.cat(og, dim=<span class="number">1</span>)) <span class="keyword">for</span> og <span class="keyword">in</span> out_grad]</span><br><span class="line">            o1 = out</span><br><span class="line">            o2 = nn.parallel.gather(out_grad, </span><br><span class="line">                                    target_device=self.devices[<span class="number">0</span>])</span><br><span class="line">            o1.backward(gradient=o2)</span><br><span class="line">            self.opt.step()</span><br><span class="line">            self.opt.optimizer.zero_grad()</span><br><span class="line">        <span class="keyword">return</span> total * normalize</span><br></pre></td></tr></table></figure><blockquote><p>Now we create our model, criterion, optimizer, data iterators, and paralelization</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GPUs to use</span></span><br><span class="line">devices = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line"><span class="keyword">if</span> <span class="literal">True</span>:</span><br><span class="line">    pad_idx = TGT.vocab.stoi[<span class="string">"&lt;blank&gt;"</span>]</span><br><span class="line">    model = make_model(len(SRC.vocab), len(TGT.vocab), N=<span class="number">6</span>)</span><br><span class="line">    model.cuda()</span><br><span class="line">    criterion = LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, smoothing=<span class="number">0.1</span>)</span><br><span class="line">    criterion.cuda()</span><br><span class="line">    BATCH_SIZE = <span class="number">12000</span></span><br><span class="line">    train_iter = MyIterator(train, batch_size=BATCH_SIZE, device=<span class="number">0</span>,</span><br><span class="line">                            repeat=<span class="literal">False</span>, sort_key=<span class="keyword">lambda</span> x: (len(x.src), len(x.trg)),</span><br><span class="line">                            batch_size_fn=batch_size_fn, train=<span class="literal">True</span>)</span><br><span class="line">    valid_iter = MyIterator(val, batch_size=BATCH_SIZE, device=<span class="number">0</span>,</span><br><span class="line">                            repeat=<span class="literal">False</span>, sort_key=<span class="keyword">lambda</span> x: (len(x.src), len(x.trg)),</span><br><span class="line">                            batch_size_fn=batch_size_fn, train=<span class="literal">False</span>)</span><br><span class="line">    model_par = nn.DataParallel(model, device_ids=devices)</span><br><span class="line"><span class="literal">None</span></span><br></pre></td></tr></table></figure><blockquote><p>Now we <strong>train the model</strong>. I will play with the warmup steps a bit, but everything else uses the default parameters. On an AWS p3.8xlarge with 4 Tesla V100s, this runs at ~27,000 tokens per second with a batch size of 12,000</p><p>在具有4个Tesla V100 GPU的AWS p3.8xlarge机器上，每秒运行约27,000个词，批训练大小为12,000。</p></blockquote><h4 id="Training-the-System"><a href="#Training-the-System" class="headerlink" title="Training the System"></a>Training the System</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!wget https://s3.amazonaws.com/opennmt-models/iwslt.pt</span></span><br><span class="line"><span class="comment">#进行train和eval</span></span><br><span class="line"><span class="keyword">if</span> <span class="literal">False</span>: <span class="comment"># false存在的意义在哪？？？ 使用GPU？</span></span><br><span class="line">    model_opt = NoamOpt(model.src_embed[<span class="number">0</span>].d_model, <span class="number">1</span>, <span class="number">2000</span>,</span><br><span class="line">            torch.optim.Adam(model.parameters(), lr=<span class="number">0</span>, betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span>))</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        model_par.train()</span><br><span class="line">        run_epoch((rebatch(pad_idx, b) <span class="keyword">for</span> b <span class="keyword">in</span> train_iter), </span><br><span class="line">                  model_par, </span><br><span class="line">                  MultiGPULossCompute(model.generator, criterion, </span><br><span class="line">                                      devices=devices, opt=model_opt))</span><br><span class="line">        model_par.eval()</span><br><span class="line">        loss = run_epoch((rebatch(pad_idx, b) <span class="keyword">for</span> b <span class="keyword">in</span> valid_iter), </span><br><span class="line">                          model_par, </span><br><span class="line">                          MultiGPULossCompute(model.generator, criterion, </span><br><span class="line">                          devices=devices, opt=<span class="literal">None</span>))</span><br><span class="line">        print(loss)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    model = torch.load(<span class="string">"iwslt.pt"</span>) <span class="comment">#加载所有的tensor到CPU</span></span><br></pre></td></tr></table></figure><blockquote><p>Once trained we can decode the model to produce a set of translations. Here we simply translate the first sentence in the validation set. This dataset is pretty small so the translations with greedy search are reasonably accurate.</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#类比于run_epoch函数  </span></span><br><span class="line"><span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(valid_iter):</span><br><span class="line">    src = batch.src.transpose(<span class="number">0</span>, <span class="number">1</span>)[:<span class="number">1</span>]</span><br><span class="line">    src_mask = (src != SRC.vocab.stoi[<span class="string">"&lt;blank&gt;"</span>]).unsqueeze(<span class="number">-2</span>)</span><br><span class="line">    out = greedy_decode(model, src, src_mask, </span><br><span class="line">                        max_len=<span class="number">60</span>, start_symbol=TGT.vocab.stoi[<span class="string">"&lt;s&gt;"</span>])</span><br><span class="line">    print(<span class="string">"Translation:"</span>, end=<span class="string">"\t"</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, out.size(<span class="number">1</span>)):</span><br><span class="line">        sym = TGT.vocab.itos[out[<span class="number">0</span>, i]]</span><br><span class="line">        <span class="keyword">if</span> sym == <span class="string">"&lt;/s&gt;"</span>: <span class="keyword">break</span></span><br><span class="line">        print(sym, end =<span class="string">" "</span>)</span><br><span class="line">    print()</span><br><span class="line">    print(<span class="string">"Target:"</span>, end=<span class="string">"\t"</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, batch.trg.size(<span class="number">0</span>)):</span><br><span class="line">        sym = TGT.vocab.itos[batch.trg.data[i, <span class="number">0</span>]]</span><br><span class="line">        <span class="keyword">if</span> sym == <span class="string">"&lt;/s&gt;"</span>: <span class="keyword">break</span></span><br><span class="line">        print(sym, end =<span class="string">" "</span>)</span><br><span class="line">    print()</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line">Translation:&lt;unk&gt; &lt;unk&gt; . In my language , that means , thank you very much . </span><br><span class="line">Gold:&lt;unk&gt; &lt;unk&gt; . It means <span class="keyword">in</span> my language , thank you very much .</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      transformer-pytorch
    
    </summary>
    
    
      <category term="transformer" scheme="http://yoursite.com/categories/transformer/"/>
    
    
      <category term="transformer" scheme="http://yoursite.com/tags/transformer/"/>
    
  </entry>
  
  <entry>
    <title>2020-07-28-pytorch安装</title>
    <link href="http://yoursite.com/2020/07/28/2020-07-28-pytorch%E5%AE%89%E8%A3%85/"/>
    <id>http://yoursite.com/2020/07/28/2020-07-28-pytorch%E5%AE%89%E8%A3%85/</id>
    <published>2020-07-27T17:21:43.000Z</published>
    <updated>2020-07-28T01:36:02.720Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Anaconda安装配置"><a href="#Anaconda安装配置" class="headerlink" title="Anaconda安装配置"></a>Anaconda安装配置</h3><p>由于墙的问题，用conda安装Pytorch过程中会连接失败，这是因为Anaconda.org的服务器在国外。在这里可以用清华TUNA镜像源，包含Anaconda仓库的镜像，将其加入conda的配置，配置如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#添加Anaconda的TUNA镜像</span></span><br><span class="line"></span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span><br><span class="line"></span><br><span class="line"><span class="comment">#TUNA的help中镜像地址加有引号，需要去掉</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#设置搜索时显示通道地址</span></span><br><span class="line"></span><br><span class="line">conda config --set show_channel_urls yes</span><br></pre></td></tr></table></figure><p>执行完上述命令后，会生成~/.condarc文件，记录着对conda的配置，直接手动创建、编辑该文件是相同的效果。</p><h3 id="Pytorch安装"><a href="#Pytorch安装" class="headerlink" title="Pytorch安装"></a>Pytorch安装</h3><p>在这里的安装，我采用conda安装：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pytorch torchvision -c soumith</span><br></pre></td></tr></table></figure><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>进入python模式下，看能否导入torch成功：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      pytorch的安装笔记
    
    </summary>
    
    
      <category term="pytorch" scheme="http://yoursite.com/categories/pytorch/"/>
    
    
      <category term="pytorch" scheme="http://yoursite.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>2020-07-27-linux的session解析</title>
    <link href="http://yoursite.com/2020/07/27/2020-07-27-linux%E7%9A%84session%E8%A7%A3%E6%9E%90/"/>
    <id>http://yoursite.com/2020/07/27/2020-07-27-linux%E7%9A%84session%E8%A7%A3%E6%9E%90/</id>
    <published>2020-07-27T12:43:11.000Z</published>
    <updated>2020-07-27T16:31:07.271Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>我们使用ssh连接服务器时，ssh的窗口突然断开了连接，那么在服务器上跑的程序就也跟着断掉了，之前所有跑的数据也将丢失，这样将会浪费我们大量的时间。</p><h3 id="为什么ssh一旦断开我们的进程也将会被杀掉？"><a href="#为什么ssh一旦断开我们的进程也将会被杀掉？" class="headerlink" title="为什么ssh一旦断开我们的进程也将会被杀掉？"></a>为什么ssh一旦断开我们的进程也将会被杀掉？</h3><p>元凶：SIGHUP 信号</p><p>让我们来看看为什么关掉窗口/断开连接会使得正在运行的程序死掉。</p><p>在Linux/Unix中，有这样几个概念：</p><p>进程组（process group）：一个或多个进程的集合，每一个进程组有唯一一个进程组ID，即进程组长进程的ID。</p><p>会话期（session）：一个或多个进程组的集合，有唯一一个会话期首进程（session leader）。会话期ID为首进程的ID。</p><p>会话期可以有一个单独的控制终端（controlling terminal）。与控制终端连接的会话期首进程叫做控制进程（controlling process）。当前与终端交互的进程称为前台进程组。其余进程组称为后台进程组。</p><p>根据POSIX.1定义：</p><p>挂断信号（SIGHUP）默认的动作是终止程序。</p><p>当终端接口检测到网络连接断开，将挂断信号发送给控制进程（会话期首进程）。</p><p>如果会话期首进程终止，则该信号发送到该会话期前台进程组。</p><p>一个进程退出导致一个孤儿进程组中产生时，如果任意一个孤儿进程组进程处于STOP状态，发送SIGHUP和SIGCONT信号到该进程组中所有进程。</p><p>因此当网络断开或终端窗口关闭后，控制进程收到SIGHUP信号退出，会导致该会话期内其他进程退出。</p><p><strong>这里我认为我们的进程被杀掉也就是因为ssh与服务器之间的通信断掉了，这个通信断掉之后linux程序就默认将该连接下的所有进程都杀掉</strong></p><h3 id="session-是什么？"><a href="#session-是什么？" class="headerlink" title="session 是什么？"></a>session 是什么？</h3><p>我们常见的 Linux session 一般是指 shell session。Shell session 是终端中当前的状态，在终端中只能有一个 session。<code>当我们打开一个新的终端时，总会创建一个新的 shell session。</code></p><p>就进程间的关系来说，session 由一个或多个进程组组成。一般情况下，来自单个登录的所有进程都属于同一个 session。我们可以通过下图来理解进程、进程组和 session 之间的关系：</p><p><img src="https://img2018.cnblogs.com/blog/952033/202001/952033-20200103182042686-2100862807.png" alt="img"></p><p><code>会话是由会话中的第一个进程创建的，一般情况下是打开终端时创建的 shell 进程。</code>该进程也叫 session 的领头进程。Session 中领头进程的 PID 也就是 session 的 SID。我们可以通过下面的命令查看 SID：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ps -o pid,ppid,pgid,sid,tty,comm</span><br></pre></td></tr></table></figure><p><img src="https://img2018.cnblogs.com/blog/952033/202001/952033-20200103182117962-99639442.png" alt="img"></p><p>Session 中的每个进程组被称为一个 job，有一个 job 会成为 session 的前台 job(foreground)，其它的 job 则是后台 job(background)。每个 session 连接一个控制终端(control terminal)，控制终端中的输入被发送给前台 job，从前台 job 产生的输出也被发送到控制终端上。同时由控制终端产生的信号，比如 ctrl + z 等都会传递给前台 job。</p><p>一般情况下 session 和终端是一对一的关系，当我们打开多个终端窗口时，实际上就创建了多个 session。</p><p><code>Session 的意义在于多个工作(job)在一个终端中运行，其中的一个为前台 job，它直接接收该终端的输入并把结果输出到该终端。其它的 job 则在后台运行。</code></p><h3 id="session-的诞生与消亡"><a href="#session-的诞生与消亡" class="headerlink" title="session 的诞生与消亡"></a>session 的诞生与消亡</h3><p>通常，新的 session 由系统登录程序创建，session 中的领头进程是运行用户登录 shell 的进程。<code>新创建的每个进程都会属于一个进程组，当创建一个进程时，它和父进程在同一个进程组、session 中。</code></p><p>将进程放入不同 session 的惟一方法是使用 setsid 函数使其成为新 session 的领头进程。这还会将 session 领头进程放入一个新的进程组中。</p><p><code>当 session 中的所有进程都结束时 session 也就消亡了</code>。如下两种：</p><p>1.实际使用中比如网络断开了，session 肯定是要消亡的。</p><p>2.正常的消亡，比如让 session 的领头进程退出。</p><p>一般情况下 session 的领头进程是 shell 进程，如果它处于前台，我们可以使用 <code>exit 命令或者是 ctrl + d</code> 让它退出。或者我们可以直接通过 kill 命令杀死 session 的领头进程。这里面的原理是：当系统检测到挂断(hangup)条件时，内核中的驱动会将 SIGHUP 信号发送到整个 session。通常情况下，这会杀死 session 中的所有进程。</p><p>session 与终端的关系<br>如果 session 关联的是伪终端，这个伪终端本身就是随着 session 的建立而创建的，session 结束，那么这个伪终端也会被销毁。<br>如果 session 关联的是 tty1-6，tty 则不会被销毁。因为该终端设备是在系统初始化的时候创建的，并不是依赖该会话建立的，所以当 session 退出，tty 仍然存在。只是 init 系统在 session 结束后，会重启 getty 来监听这个 tty。</p><h3 id="nohup"><a href="#nohup" class="headerlink" title="nohup"></a>nohup</h3><p><code>如果我们在 session 中执行了 nohup 等类似的命令，当 session 消亡时，相关的进程并不会随着 session 结束，原因是这些进程不再受 SIGHUP 信号的影响。</code>比如我们执行下面的命令：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ nohup sleep <span class="number">1000</span> &gt;/dev/null <span class="number">2</span>&gt;&amp;<span class="number">1</span> &amp;</span><br></pre></td></tr></table></figure><p><img src="https://img2018.cnblogs.com/blog/952033/202001/952033-20200103182343352-1366915632.png" alt="img"></p><p>此时 sleep 进程的 sid 和其它进程是相同的，还可以通过 pstree 命令看到进程间的父子关系：</p><p><img src="https://img2018.cnblogs.com/blog/952033/202001/952033-20200103182417115-817556079.png" alt="img"></p><p><code>如果我们退出当前 session 的领头进程(bash)，sleep 进程并不会退出，这样我们就可以放心的等待该进程运行结果了。</code><br>nohup 并不改变进程的 sid，同时也说明在这种情况中，虽然 session 的领头进程退出了，但是 session 依然没有被销毁(至少 sid 还在被引用)。重新建立连接，通过下面的命令查看 sleep 进程的信息，发现进程的 sid 依然是 7837：</p><p><img src="https://img2018.cnblogs.com/blog/952033/202001/952033-20200103182448160-376880623.png" alt="img"></p><p>但是<code>此时的 sleep 已经被系统的 1 号进程 systemd 收养了</code>：</p><p><img src="https://img2018.cnblogs.com/blog/952033/202001/952033-20200103182521953-1574746082.png" alt="img"></p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><blockquote><p><a href="https://www.cnblogs.com/sparkdev/p/12146305.html" target="_blank" rel="noopener">https://www.cnblogs.com/sparkdev/p/12146305.html</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      关于ssh连接终端和session
    
    </summary>
    
    
      <category term="linux" scheme="http://yoursite.com/categories/linux/"/>
    
    
      <category term="linux" scheme="http://yoursite.com/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>2020-07-26-picgo上传失败原因</title>
    <link href="http://yoursite.com/2020/07/26/2020-07-26-picgo%E4%B8%8A%E4%BC%A0%E5%A4%B1%E8%B4%A5%E5%8E%9F%E5%9B%A0/"/>
    <id>http://yoursite.com/2020/07/26/2020-07-26-picgo%E4%B8%8A%E4%BC%A0%E5%A4%B1%E8%B4%A5%E5%8E%9F%E5%9B%A0/</id>
    <published>2020-07-26T12:53:58.000Z</published>
    <updated>2020-07-27T10:26:59.176Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>在使用后PicGo上传图片至github时总是会显示上传失败，以下是我的解决方案经验，我的情况是可以上传图片，但是偶尔会失败</p><h3 id="case-1-检查服务及端口配置"><a href="#case-1-检查服务及端口配置" class="headerlink" title="case 1 检查服务及端口配置"></a>case 1 检查服务及端口配置</h3><p>择相应的选项“设置server”，以下操作</p><p><img src="https://i.loli.net/2020/07/27/lf7Bw2jDUqgIxMk.png" alt=""></p><p>我们可以选择将开关先关闭，然后打开，确定后再重启软件，一般会成功。</p><p>或者修改端口号： 如修改为36688</p><p>记住，如果不行，那就直接关闭软件，然后等2分钟后，再打开picgo软件就可以上传成功了。</p><h3 id="case-2-查看日志"><a href="#case-2-查看日志" class="headerlink" title="case 2 查看日志"></a>case 2 查看日志</h3><p>找到“设置日志文件”，然后打开日志文件，检查相应的日志，了解上传失败的原因。</p><p><img src="https://i.loli.net/2020/07/27/421CU9GzgKkauxt.png" alt=""></p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://www.shopee6.com/web/web-tutorial/picgo-github-fail.html" target="_blank" rel="noopener">https://www.shopee6.com/web/web-tutorial/picgo-github-fail.html</a></p>]]></content>
    
    <summary type="html">
    
      解决picgo间歇性上传失败
    
    </summary>
    
    
      <category term="杂" scheme="http://yoursite.com/categories/%E6%9D%82/"/>
    
    
      <category term="杂" scheme="http://yoursite.com/tags/%E6%9D%82/"/>
    
  </entry>
  
  <entry>
    <title>2020-07-26-tensorflow函数解析</title>
    <link href="http://yoursite.com/2020/07/26/2020-07-26-tensorflow%E5%87%BD%E6%95%B0%E8%A7%A3%E6%9E%90/"/>
    <id>http://yoursite.com/2020/07/26/2020-07-26-tensorflow%E5%87%BD%E6%95%B0%E8%A7%A3%E6%9E%90/</id>
    <published>2020-07-26T07:10:21.000Z</published>
    <updated>2020-07-26T15:53:06.265Z</updated>
    
    <content type="html"><![CDATA[<p>tf.manul</p>]]></content>
    
    <summary type="html">
    
      tensorflow函数解析
    
    </summary>
    
    
      <category term="tensorflow" scheme="http://yoursite.com/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="http://yoursite.com/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>2020-07-26-numpy函数解析</title>
    <link href="http://yoursite.com/2020/07/26/2020-07-26-numpy%E5%87%BD%E6%95%B0%E8%A7%A3%E6%9E%90/"/>
    <id>http://yoursite.com/2020/07/26/2020-07-26-numpy%E5%87%BD%E6%95%B0%E8%A7%A3%E6%9E%90/</id>
    <published>2020-07-26T03:13:52.000Z</published>
    <updated>2020-08-04T13:04:46.725Z</updated>
    
    <content type="html"><![CDATA[<h3 id="newaxis用法"><a href="#newaxis用法" class="headerlink" title="newaxis用法"></a>newaxis用法</h3><p>newaxis表示增加一个新的坐标轴</p><ul><li>x[:, np.newaxis] ，放在后面，会给列上增加维度</li><li>x[np.newaxis, :] ，放在前面，会给行上增加维度</li></ul><p><strong>用途：</strong> 通常用它将一维的数据转换成一个矩阵，与代码后面的权重矩阵进行相乘。</p><h4 id="第一个程序"><a href="#第一个程序" class="headerlink" title="第一个程序"></a>第一个程序</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">a &#x3D; np.array([1,2,3])</span><br><span class="line">print (a.shape,&#39;\n&#39;,a)</span><br></pre></td></tr></table></figure><p>结果为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(3,)</span><br><span class="line">[1 2 3]</span><br></pre></td></tr></table></figure><h4 id="第二个程序"><a href="#第二个程序" class="headerlink" title="第二个程序"></a>第二个程序</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a &#x3D; np.array([1,2,3])[:,np.newaxis]</span><br><span class="line">print (a.shape,&#39;\n&#39;,a)</span><br></pre></td></tr></table></figure><p>结果为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(3, 1)</span><br><span class="line">[[1]</span><br><span class="line">[2]</span><br><span class="line">[3]]</span><br></pre></td></tr></table></figure><p>和第一个程序相比，a的shape为（3，）现在为（3，1）变为二维数组了，之前为[1,2,3]，现在变为</p><p>[[1]<br>[2]<br>[3]]</p><h4 id="第三个程序"><a href="#第三个程序" class="headerlink" title="第三个程序"></a>第三个程序</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a &#x3D; np.array([1,2,3])[np.newaxis,:]</span><br><span class="line">print (a.shape,&#39;\n&#39;,a)</span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(1, 3)</span><br><span class="line">[[1 2 3]]</span><br></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(1, 3)</span><br><span class="line">[[1 2 3]]</span><br></pre></td></tr></table></figure><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>np.newaxis的作用就是在原来的数组上增加一个维度。[np.newaxis,:]这个地方np.newaxis放的位置有关，第二个程序放在[:,]的后面，相当于在原来的后面增加一个维度，所以变为(3,1)，而第三个则放在前面，则为(1,3)。<code>加到哪一维，那一维就为1</code></p><h3 id="concatenate用法"><a href="#concatenate用法" class="headerlink" title="concatenate用法"></a>concatenate用法</h3><p>-用于进行数组拼接</p><p>函数定义：</p><p><code>numpy.concatenate</code>((a1, a2, …), axis=0, out=None)</p><ul><li>axis=0: 合并行</li><li>axis=1: 合并列</li></ul><p>例子如下：</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a=np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line"> &gt;&gt;&gt; b=np.array([[<span class="number">11</span>,<span class="number">21</span>,<span class="number">31</span>],[<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]])</span><br><span class="line"> <span class="meta"># 合并行</span></span><br><span class="line"> &gt;&gt;&gt; np.concatenate((a,b,c),axis=<span class="number">0</span>)  <span class="meta"># 默认情况下，axis=0可以不写</span></span><br><span class="line"> array([[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">  [<span class="meta"> 4,  5,  6</span>],</span><br><span class="line">  [<span class="meta">11, 21, 31</span>],</span><br><span class="line">  [<span class="meta"> 7,  8,  9</span>]])</span><br><span class="line"> <span class="meta"># 合并列</span></span><br><span class="line"> &gt;&gt;&gt; np.concatenate((a,b),axis=<span class="number">1</span>) </span><br><span class="line">  array([[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>, <span class="number">11</span>, <span class="number">21</span>, <span class="number">31</span>],</span><br><span class="line">  [<span class="meta"> 4,  5,  6,  7,  8,  9</span>]])</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      用于记录numpy中的函数解析
    
    </summary>
    
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>2020-07-26-python切片疑惑解析</title>
    <link href="http://yoursite.com/2020/07/26/2020-07-26-python%E5%88%87%E7%89%87%E7%96%91%E6%83%91%E8%A7%A3%E6%9E%90/"/>
    <id>http://yoursite.com/2020/07/26/2020-07-26-python%E5%88%87%E7%89%87%E7%96%91%E6%83%91%E8%A7%A3%E6%9E%90/</id>
    <published>2020-07-25T16:20:11.000Z</published>
    <updated>2020-08-02T17:12:59.709Z</updated>
    
    <content type="html"><![CDATA[<p>Python中对于数组和列表进行切片操作是很频繁的，我主要简单总结一下常用集中索引化方式</p><h3 id="一维：-n-、-m-、-1-、-1"><a href="#一维：-n-、-m-、-1-、-1" class="headerlink" title="一维： [ : n]、[m : ] 、[-1]、[::-1]"></a>一维： [ : n]、[m : ] 、[-1]、[::-1]</h3><p><strong>[m : ]</strong> ：代表列表中的第m项到最后一项 （从0开始）</p><p><strong>[ : n]</strong> ：代表列表中的第0项到第n-1项  （含左不含右）</p><p><strong>[-1]：</strong>取最后一个元素</p><p><strong>[::-1]</strong>：取从后向前（相反）的元素 （倒序）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">X = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>] )</span><br><span class="line">print(X.shape)</span><br><span class="line">print(<span class="string">'\n'</span>)</span><br><span class="line">print(X[<span class="number">3</span>:])</span><br><span class="line">print(X[:<span class="number">7</span>])</span><br><span class="line">print(X[::<span class="number">-1</span>][:<span class="number">3</span>]) <span class="comment"># 在进行了[::-1]之后得到倒序数组，再取[：3]</span></span><br></pre></td></tr></table></figure><p>结果如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">8</span>,)</span><br><span class="line">[<span class="number">4</span> <span class="number">5</span> <span class="number">6</span> <span class="number">7</span> <span class="number">8</span>]</span><br><span class="line">[<span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span> <span class="number">7</span>]</span><br><span class="line">[<span class="number">8</span>,<span class="number">7</span>,<span class="number">6</span>]</span><br></pre></td></tr></table></figure><h3 id="二维：-X-0-、X-1-、-X-m-n"><a href="#二维：-X-0-、X-1-、-X-m-n" class="headerlink" title="二维： X[:,0] 、X[:,1] 、 X[:, m:n]"></a>二维： X[:,0] 、X[:,1] 、 X[:, m:n]</h3><p>X[:,0]是numpy中数组的一种写法，表示对一个二维数组，取该二维数组第一维中的所有数据，第二维中取第0个数据，直观来说</p><p>X[:,0]：取第二维（所有行）的第0个数据,就是<code>第0列</code></p><p>X[:,1] ：取第二维（所有行）的第1个数据，就是<code>第1列</code></p><p>X[:, m:n]，即取所有数据的<code>第m到n-1列数据，含左不含右</code></p><p>示例如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">X = np.array([[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>],[<span class="number">9</span>,<span class="number">10</span>,<span class="number">11</span>],[<span class="number">12</span>,<span class="number">13</span>,<span class="number">14</span>],[<span class="number">15</span>,<span class="number">16</span>,<span class="number">17</span>],[<span class="number">18</span>,<span class="number">19</span>,<span class="number">20</span>]])</span><br><span class="line">print(X.shape)</span><br><span class="line">print(<span class="string">'\n'</span>)</span><br><span class="line"><span class="keyword">print</span> (X[:,<span class="number">1</span>:<span class="number">3</span>])</span><br></pre></td></tr></table></figure><p>结果如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">7</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">[[ <span class="number">1</span>  <span class="number">2</span>]</span><br><span class="line"> [ <span class="number">4</span>  <span class="number">5</span>]</span><br><span class="line"> [ <span class="number">7</span>  <span class="number">8</span>]</span><br><span class="line"> [<span class="number">10</span> <span class="number">11</span>]</span><br><span class="line"> [<span class="number">13</span> <span class="number">14</span>]</span><br><span class="line"> [<span class="number">16</span> <span class="number">17</span>]</span><br><span class="line"> [<span class="number">19</span> <span class="number">20</span>]]</span><br></pre></td></tr></table></figure><h3 id="三维-X-0-、X-1-、X-m-n"><a href="#三维-X-0-、X-1-、X-m-n" class="headerlink" title="三维 X[:,:,0]、X[:,:,1]、X[:,:,m:n]"></a>三维 X[:,:,0]、X[:,:,1]、X[:,:,m:n]</h3><p>类比于二维，原理相同</p><p>X[:,:,0]：取第三维矩阵中第0列的所有数据</p><p>X[:,:,1]：取第三维矩阵中第1列的所有数据</p><p>X[:,:,m:n]：取第三维矩阵中第m列到第n-1列的所有数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">注：shape（<span class="number">9</span>,<span class="number">5</span>,<span class="number">2</span>）指的是最外层有<span class="number">9</span>个括号，每个括号里嵌套<span class="number">5</span>个括号，在<span class="number">5</span>个括号里又每个有<span class="number">2</span>个元素</span><br><span class="line"></span><br><span class="line">判断的时候先先出数组的shape，根据shape进行判断</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !usr/bin/env python</span></span><br><span class="line"><span class="comment"># encoding:utf-8</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">simple_test</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    简单的小实验</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    data_list = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>], [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>], [<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>], [<span class="number">6</span>, <span class="number">7</span>, <span class="number">9</span>], [<span class="number">0</span>, <span class="number">4</span>, <span class="number">7</span>], [<span class="number">4</span>, <span class="number">6</span>, <span class="number">0</span>],[<span class="number">2</span>, <span class="number">9</span>, <span class="number">1</span>], [<span class="number">5</span>, <span class="number">8</span>, <span class="number">7</span>], [<span class="number">9</span>, <span class="number">7</span>, <span class="number">8</span>], [<span class="number">3</span>, <span class="number">7</span>, <span class="number">9</span>]]</span><br><span class="line">    <span class="comment"># data_list.toarray()</span></span><br><span class="line">    data_list = np.array(data_list)</span><br><span class="line">    print(<span class="string">'X[:,0]结果输出为：'</span>)</span><br><span class="line">    print( data_list[:, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    print(  <span class="string">'X[:,1]结果输出为：'</span>)</span><br><span class="line">    print( data_list[:, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    print(<span class="string">'X[:,m:n]结果输出为：'</span>)</span><br><span class="line"></span><br><span class="line">    print( data_list[:, <span class="number">0</span>:<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    data_list = [[[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">7</span>, <span class="number">9</span>], [<span class="number">4</span>, <span class="number">0</span>]], [[<span class="number">1</span>, <span class="number">4</span>], [<span class="number">1</span>, <span class="number">5</span>], [<span class="number">3</span>, <span class="number">6</span>], [<span class="number">8</span>, <span class="number">9</span>], [<span class="number">5</span>, <span class="number">0</span>]],</span><br><span class="line">                 [[<span class="number">8</span>, <span class="number">2</span>], [<span class="number">1</span>, <span class="number">8</span>], [<span class="number">3</span>, <span class="number">5</span>], [<span class="number">7</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">6</span>]],</span><br><span class="line">                 [[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">5</span>], [<span class="number">7</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]], [[<span class="number">9</span>, <span class="number">2</span>], [<span class="number">1</span>, <span class="number">3</span>], [<span class="number">3</span>, <span class="number">5</span>], [<span class="number">7</span>, <span class="number">67</span>], [<span class="number">4</span>, <span class="number">4</span>]],</span><br><span class="line">                 [[<span class="number">8</span>, <span class="number">2</span>], [<span class="number">1</span>, <span class="number">9</span>], [<span class="number">3</span>, <span class="number">43</span>], [<span class="number">7</span>, <span class="number">3</span>], [<span class="number">43</span>, <span class="number">0</span>]],</span><br><span class="line">                 [[<span class="number">1</span>, <span class="number">22</span>], [<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">42</span>], [<span class="number">7</span>, <span class="number">29</span>], [<span class="number">4</span>, <span class="number">20</span>]], [[<span class="number">1</span>, <span class="number">5</span>], [<span class="number">1</span>, <span class="number">20</span>], [<span class="number">3</span>, <span class="number">24</span>], [<span class="number">17</span>, <span class="number">9</span>], [<span class="number">4</span>, <span class="number">10</span>]],</span><br><span class="line">                 [[<span class="number">11</span>, <span class="number">2</span>], [<span class="number">1</span>, <span class="number">110</span>], [<span class="number">3</span>, <span class="number">14</span>], [<span class="number">7</span>, <span class="number">4</span>], [<span class="number">4</span>, <span class="number">2</span>]]]</span><br><span class="line">    data_list = np.array(data_list)</span><br><span class="line">    print(data_list.shape)</span><br><span class="line">    print(<span class="string">'X[:,:,0]结果输出为：'</span>)</span><br><span class="line"></span><br><span class="line">    print( data_list[:, :, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    print(<span class="string">'X[:,:,1]结果输出为：'</span>)</span><br><span class="line"></span><br><span class="line">    print(data_list[:, :, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    print(<span class="string">'X[:,:,m:n]结果输出为：'</span>)</span><br><span class="line"></span><br><span class="line">    print( data_list[:, :, <span class="number">0</span>:<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    simple_test()</span><br></pre></td></tr></table></figure><p>部分结如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">X[:,:,<span class="number">0</span>]结果输出为：</span><br><span class="line">[[ <span class="number">1</span>  <span class="number">1</span>  <span class="number">3</span>  <span class="number">7</span>  <span class="number">4</span>]</span><br><span class="line"> [ <span class="number">1</span>  <span class="number">1</span>  <span class="number">3</span>  <span class="number">8</span>  <span class="number">5</span>]</span><br><span class="line"> [ <span class="number">8</span>  <span class="number">1</span>  <span class="number">3</span>  <span class="number">7</span>  <span class="number">4</span>]</span><br><span class="line"> [ <span class="number">1</span>  <span class="number">1</span>  <span class="number">3</span>  <span class="number">7</span>  <span class="number">7</span>]</span><br><span class="line"> [ <span class="number">9</span>  <span class="number">1</span>  <span class="number">3</span>  <span class="number">7</span>  <span class="number">4</span>]</span><br><span class="line"> [ <span class="number">8</span>  <span class="number">1</span>  <span class="number">3</span>  <span class="number">7</span> <span class="number">43</span>]</span><br><span class="line"> [ <span class="number">1</span>  <span class="number">1</span>  <span class="number">3</span>  <span class="number">7</span>  <span class="number">4</span>]</span><br><span class="line"> [ <span class="number">1</span>  <span class="number">1</span>  <span class="number">3</span> <span class="number">17</span>  <span class="number">4</span>]</span><br><span class="line"> [<span class="number">11</span>  <span class="number">1</span>  <span class="number">3</span>  <span class="number">7</span>  <span class="number">4</span>]]</span><br><span class="line">X[:,:,<span class="number">1</span>]结果输出为：</span><br><span class="line">[[  <span class="number">2</span>   <span class="number">0</span>   <span class="number">4</span>   <span class="number">9</span>   <span class="number">0</span>]</span><br><span class="line"> [  <span class="number">4</span>   <span class="number">5</span>   <span class="number">6</span>   <span class="number">9</span>   <span class="number">0</span>]</span><br><span class="line"> [  <span class="number">2</span>   <span class="number">8</span>   <span class="number">5</span>   <span class="number">3</span>   <span class="number">6</span>]</span><br><span class="line"> [  <span class="number">1</span>   <span class="number">2</span>   <span class="number">5</span>   <span class="number">6</span>   <span class="number">8</span>]</span><br><span class="line"> [  <span class="number">2</span>   <span class="number">3</span>   <span class="number">5</span>  <span class="number">67</span>   <span class="number">4</span>]</span><br><span class="line"> [  <span class="number">2</span>   <span class="number">9</span>  <span class="number">43</span>   <span class="number">3</span>   <span class="number">0</span>]</span><br><span class="line"> [ <span class="number">22</span>   <span class="number">2</span>  <span class="number">42</span>  <span class="number">29</span>  <span class="number">20</span>]</span><br><span class="line"> [  <span class="number">5</span>  <span class="number">20</span>  <span class="number">24</span>   <span class="number">9</span>  <span class="number">10</span>]</span><br><span class="line"> [  <span class="number">2</span> <span class="number">110</span>  <span class="number">14</span>   <span class="number">4</span>   <span class="number">2</span>]]</span><br></pre></td></tr></table></figure><h3 id="start：end：step"><a href="#start：end：step" class="headerlink" title="[start：end：step]"></a>[start：end：step]</h3><p>start:开始索引；end:结束索引；step:步长（步长为正时，从左到右索引，正序取值；步长为负时，从右到左索引，倒序取值）</p><p>[::2]  步长为2</p><p>[3:7:2] 第3个元素开始，第6个元素结束，步长为2</p><p>参考</p><p><a href="https://blog.csdn.net/Together_CZ/article/details/79593952" target="_blank" rel="noopener">https://blog.csdn.net/Together_CZ/article/details/79593952</a></p>]]></content>
    
    <summary type="html">
    
      关于切片的几点疑惑记录
    
    </summary>
    
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>2020-07-25-服务器心得</title>
    <link href="http://yoursite.com/2020/07/25/2020-07-25-%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%BF%83%E5%BE%97/"/>
    <id>http://yoursite.com/2020/07/25/2020-07-25-%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%BF%83%E5%BE%97/</id>
    <published>2020-07-25T12:14:32.000Z</published>
    <updated>2020-08-09T10:37:04.256Z</updated>
    
    <content type="html"><![CDATA[<p>目前使用的是Xshell + FileZilla（连接+传输） ， MobaXterm也可用于ssh连接的备用软件。</p><p>MobaXterm（<a href="https://link.zhihu.com/?target=https%3A//mobaxterm.mobatek.net/">https://mobaxterm.mobatek.net/</a>）：功能很全，免费，有免安装版，支持多标签，同时自带文件传输系统，唯一的不足是对Z-moderm支持较差。</p><h3 id="linux后台执行命令：-amp-和nohup"><a href="#linux后台执行命令：-amp-和nohup" class="headerlink" title="linux后台执行命令：&amp;和nohup"></a>linux后台执行命令：&amp;和nohup</h3><p>在用本机Xshell连接服务器跑实验时，如果关闭本机电脑，Xshell不再运行时，那么linux终端会话就会关闭，跑的实验就会终止。有时候更希望它能够在每天的非负荷高峰时间段运行(例如凌晨)。为了使这些进程能够在后台运行，也就是说不在终端屏幕上运行，有几种选择方法可供使用。</p><h4 id="amp-使用"><a href="#amp-使用" class="headerlink" title="&amp;使用"></a>&amp;使用</h4><p>在执行文件的时候，可以在命令后面加上<code>&amp;</code>，这样可以实现将进程挂到后台运行。例如： <code>python main.py &amp;</code></p><p>在使用&amp;之后，系统会返回一个进程号PID，需要记下此进程的PID</p><h4 id="nohup使用"><a href="#nohup使用" class="headerlink" title="nohup使用"></a>nohup使用</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup python main.py &amp;</span><br></pre></td></tr></table></figure><p>这样执行的时候会将代码放在服务器后台执行，你的终端是看不到运行过程的，<strong>期间运行的结果</strong>（代码运行过程中打印出来的）会在一个生成的<code>nohup.out文件</code>中保存。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">nohup python main.py &gt;test.log  <span class="number">2</span>&gt;&amp;<span class="number">1</span> &amp;</span><br><span class="line"><span class="comment"># nohup和&amp; 一起使用。 &amp; 放后台</span></span><br><span class="line"><span class="comment"># &gt;表示将标准输出（STDOUT）重定向到test.log文件</span></span><br><span class="line"><span class="comment">#2&gt;&amp;1 ：把标准输出和标准错误一起重定向到一个文件中。1是标准输出的文件描述符，2是标准错误的文件描述符；</span></span><br></pre></td></tr></table></figure><p>可以实现运行main.py ，并将输出结果打印到<code>test.log文件</code>中（如果这个文件不存在, 那就创建, 否则就覆盖）</p><p>要想使ssh连接断掉也可以继续后台运行，需要用<code>exit命令</code>断开，否则其它关闭行为视为断开异常（如直接关掉xsheel软件），不会后台运行</p><h4 id="nohup与session的关系"><a href="#nohup与session的关系" class="headerlink" title="nohup与session的关系"></a>nohup与session的关系</h4><p><code>如果我们在 session 中执行了 nohup 等类似的命令，当 session 消亡时，相关的进程并不会随着 session 结束，原因是这些进程不再受 SIGHUP 信号的影响。</code>比如我们执行下面的命令：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ nohup sleep <span class="number">1000</span> &gt;/dev/null <span class="number">2</span>&gt;&amp;<span class="number">1</span> &amp;</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2020/07/29/cAzPToGdLKmRZ2e.png" alt="img"></p><p>此时 sleep 进程的 sid 和其它进程是相同的，还可以通过 pstree 命令看到进程间的父子关系：</p><p><img src="https://i.loli.net/2020/07/29/8ZQDMlkmTHy1a2G.png" alt="img"></p><p><code>如果我们退出当前 session 的领头进程(bash)，sleep 进程并不会退出，这样我们就可以放心的等待该进程运行结果了。</code><br>nohup 并不改变进程的 sid，同时也说明在这种情况中，虽然 session 的领头进程退出了，但是 session 依然没有被销毁(至少 sid 还在被引用)。重新建立连接，通过下面的命令查看 sleep 进程的信息，发现进程的 sid 依然是 7837：</p><p><img src="https://i.loli.net/2020/07/29/5O83jJ2Hfsdoyil.png" alt="img"></p><p>但是<code>此时的 sleep 已经被系统的 1 号进程 systemd 收养了</code>：</p><p><img src="https://i.loli.net/2020/07/29/9quLvTHCNnES4F7.png" alt="img"></p><h5 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h5><blockquote><p><a href="https://www.cnblogs.com/sparkdev/p/12146305.html" target="_blank" rel="noopener">https://www.cnblogs.com/sparkdev/p/12146305.html</a></p></blockquote><h4 id="忘记进程"><a href="#忘记进程" class="headerlink" title="忘记进程"></a>忘记进程</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ps -ef | grep main.py  <span class="comment">#其中main.py是要查找的关键字</span></span><br><span class="line">ps -ef | grep main.py  | grep -v grep <span class="comment">#grep -v 排除进程。此时是排除grep自身的进程</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">ps命令将某个进程显示出来</span><br><span class="line"></span><br><span class="line">grep命令是查找</span><br><span class="line"></span><br><span class="line">中间的|是管道命令 是指ps命令与grep同时执行</span><br><span class="line"></span><br><span class="line">字段含义如下：</span><br><span class="line">UID    PID    PPID    C   STIME   TTY    TIME     CMD</span><br><span class="line"></span><br><span class="line">zzw   <span class="number">14124</span>  <span class="number">13991</span>   <span class="number">0</span>   <span class="number">00</span>:<span class="number">38</span>   pts/<span class="number">0</span>   <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span>  grep --color=auto dae</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">UID   ：程序被该 UID 所拥有</span><br><span class="line"></span><br><span class="line">PID   ：就是这个程序的 ID </span><br><span class="line"></span><br><span class="line">PPID  ：则是其上级父程序的ID</span><br><span class="line"></span><br><span class="line">C     ：CPU使用的资源百分比</span><br><span class="line"></span><br><span class="line">STIME ：系统启动时间</span><br><span class="line"></span><br><span class="line">TTY   ：登入者的终端机位置</span><br><span class="line"></span><br><span class="line">TIME  ：使用掉的CPU时间。</span><br><span class="line"></span><br><span class="line">CMD  ：所下达的是什么指令</span><br></pre></td></tr></table></figure><p>这里是两个shell命令通过管道进行了结合，第一个ps能够列出当前系统所有活跃的进程，然后通过grep 关键字查找就能找到带有关键字的进程。<code>找到PID</code>（PID是输出的第二列那个数字）再杀掉。</p><h4 id="关闭进程"><a href="#关闭进程" class="headerlink" title="关闭进程"></a>关闭进程</h4><p><code>kill -9 PID</code>  . 用普通的ctrl+C是关不掉的</p><h4 id="疑问："><a href="#疑问：" class="headerlink" title="疑问："></a>疑问：</h4><p>在根指令行可以进行nohup，但是我用tmux建立会话之后，在tmux中是不可以运用notop，会弹出 <code>exit 1</code> 的错误指令？？？？</p><p>一种方法就是在后台运行之后，再进入tmux操作</p><h3 id="htop使用"><a href="#htop使用" class="headerlink" title="htop使用"></a>htop使用</h3><h4 id="功能介绍"><a href="#功能介绍" class="headerlink" title="功能介绍"></a>功能介绍</h4><p>监视内存，线程，CPU运行状态</p><p>htop是Linux系统下一个基本文本模式的、交互式的进程查看器，主要用于控制台或shell中，可以替代top，或者说是top的高级版。</p><h4 id="安装htop"><a href="#安装htop" class="headerlink" title="安装htop"></a>安装htop</h4><p> Ubuntu    <code>sudo apt-get install htop</code></p><h4 id="使用htop"><a href="#使用htop" class="headerlink" title="使用htop"></a>使用htop</h4><h5 id="界面概述"><a href="#界面概述" class="headerlink" title="界面概述"></a>界面概述</h5><p>安装完成后，命令行中直接敲击 htop 命令，即可进入 htop 的界面</p><p><a href="https://blog.xiewenlong.com/2018/12/htop/htop.png" target="_blank" rel="noopener"><img src="https://blog.xiewenlong.com/2018/12/htop/htop.png" alt="img"></a></p><p>各项从上至下分别说明如下：</p><p><a href="https://blog.xiewenlong.com/2018/12/htop/base.png" target="_blank" rel="noopener"><img src="https://blog.xiewenlong.com/2018/12/htop/base.png" alt="img"></a></p><p>左边部分从上至下，分别为，cpu、内存、交换分区的使用情况，右边部分为：Tasks 为进程总数，当前运行的进程数、Load average 为系统 1 分钟，5 分钟，10 分钟的平均负载情况、Uptime 为系统运行的时间。</p><p><a href="https://blog.xiewenlong.com/2018/12/htop/process.png" target="_blank" rel="noopener"><img src="https://blog.xiewenlong.com/2018/12/htop/process.png" alt="img"></a></p><p>以上各项分别为：</p><ul><li><strong>PID：</strong>进程的标识号</li><li><strong>USER：</strong>运行此进程的用户</li><li><strong>PRI：</strong>进程的优先级</li><li><strong>NI：</strong>进程的优先级别值，默认的为 0，可以进行调整</li><li><strong>VIRT：</strong>进程占用的虚拟内存值</li><li><strong>RES：</strong>进程占用的物理内存值</li><li><strong>SHR：</strong>进程占用的共享内存值</li><li><strong>S：</strong>进程的运行状况，R 表示正在运行、S 表示休眠，等待唤醒、Z 表示僵死状态</li><li><strong>%CPU：</strong>该进程占用的CPU使用率</li><li><strong>%MEM：</strong>该进程占用的物理内存和总内存的百分比</li><li><strong>TIME+：</strong>该进程启动后占用的总的 CPU 时间</li><li><strong>COMMAND：</strong>进程启动的启动命令名称</li></ul><h4 id="操作说明"><a href="#操作说明" class="headerlink" title="操作说明"></a>操作说明</h4><p><code>htop</code> 界面底部给出了 F1 ~ F10 按键的简单说明。</p><p><a href="https://blog.xiewenlong.com/2018/12/htop/bottom.png" target="_blank" rel="noopener"><img src="https://blog.xiewenlong.com/2018/12/htop/bottom.png" alt="img"></a></p><h5 id="标注进程条目"><a href="#标注进程条目" class="headerlink" title="标注进程条目"></a>标注进程条目</h5><p>在系统中运行着的实时进程视图中，要追踪某个进程是个大问题。因为整个列表在不停的刷新着，进程的排列顺序也在变动着。为了这个问题， <code>htop</code> 提供了一个很简单的解决方案：颜色标注。是的，你可以标注一个进程条目，它会以不同的颜色显示，因此要追踪它就变得容易了。</p><p>要标注某个进程条目，需要做的就是选中此条目，然后按下<code>空格</code>键。例如，在下面的截图示例中，我已经颜色标注了两个进程条目（黄色高亮显示的两行）:</p><p><a href="https://blog.xiewenlong.com/2018/12/htop/tag.png" target="_blank" rel="noopener"><img src="https://blog.xiewenlong.com/2018/12/htop/tag.png" alt="img"></a></p><h5 id="命令行选项"><a href="#命令行选项" class="headerlink" title="命令行选项"></a>命令行选项</h5><p>除了上面介绍的一些热键，<code>htop</code> 还提供了很有用的命令行选项。下面是其中一部分:</p><ul><li><code>-s 选项</code> : 按指定的列排序。例如，<code>htop -s PID</code> 命令会按 PID 列的大小排序来显示。</li><li><code>-u 选项</code> : 显示指定的用户的进程信息列表。例如，<code>htop -u vagrant</code> 命令会只显示出用户名为 vagrant 的相关进程。</li><li><code>-d 选项</code> : 设置刷新的延迟时间。例如，<code>htop -d 100</code> 命令会使输出在 1 秒后才会刷新（参数 -d 的单位是 10 微秒）。</li><li><code>-p 选项</code>：只显示给定的PIDs。例如， <code>htop -p PID</code></li></ul><h5 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h5><p> <strong>shift + m</strong> ： 按照内存大小排序。<br> <strong>shift + h</strong> ： 收缩线程。<br>      <strong>q</strong>       ： 退出</p><p><strong>上下键</strong> 或 <strong>PgUP，PgDn</strong> : 选定想要的进程，<br><strong>左右键</strong> 或 <strong>Home，End</strong> : 移动字段，当然也可以直接用鼠标选定进程；<br><strong>Space</strong>  标记/取消标记一个进程（类似 windows 按着 Ctrl 多选一样 ）。命令可以作用于多个进程，例如 “kill”，将应用于所有已标记的进程</p><h4 id="参考-1"><a href="#参考-1" class="headerlink" title="参考"></a>参考</h4><blockquote><p><a href="https://blog.xiewenlong.com/2018/12/htop/" target="_blank" rel="noopener">https://blog.xiewenlong.com/2018/12/htop/</a></p></blockquote><h3 id="nvidia-smi使用"><a href="#nvidia-smi使用" class="headerlink" title="nvidia-smi使用"></a>nvidia-smi使用</h3><p><code>nvidia-smi</code> 显示出当前GPU的所有基础信息，监控GPU状态和使用情况。命令判断哪几块GPU空闲</p><p><img src="https://i.loli.net/2020/07/22/u1CtU8Xgor45inh.png" alt="image-20200722153957282"></p><h4 id="解释相关参数含义"><a href="#解释相关参数含义" class="headerlink" title="解释相关参数含义"></a>解释相关参数含义</h4><p>GPU：本机中的GPU编号</p><p>Name：GPU 类型</p><p>Persistence-M：</p><p>Fan：风扇转速</p><p>Temp：温度，单位摄氏度</p><p>Perf：表征性能状态，从P0到P12，P0表示最大性能，P12表示状态最小性能</p><p>Pwr:Usage/Cap：能耗表示</p><p>Bus-Id：涉及GPU总线的相关信息；</p><p>Disp.A：Display Active，表示GPU的显示是否初始化</p><p>Memory-Usage：显存使用率</p><p>Volatile GPU-Util：浮动的GPU利用率</p><p>Uncorr. ECC：关于ECC的东西</p><p>Compute M.：计算模式</p><p>Processes 显示每块GPU上每个进程所使用的显存情况</p><h3 id="Tmux使用"><a href="#Tmux使用" class="headerlink" title="Tmux使用"></a>Tmux使用</h3><h4 id="安装tmux-linux"><a href="#安装tmux-linux" class="headerlink" title="安装tmux  -linux"></a>安装tmux  -linux</h4><p><code>sudo apt-get install tmux</code></p><h4 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h4><p>tmux采用C/S模型构建，<code>输入tmux命令就相当于开启了一个服务器</code>，此时默认将新建一个会话，然后会话中默认新建一个窗口，窗口中默认新建一个面板。会话、窗口、面板之间的联系如下：</p><p><img src="https://i.loli.net/2020/07/27/l4YyAISG8d1Lcp9.png" alt="image-20200726233552371"></p><p>一个tmux <code>session</code>（会话）可以包含多个<code>window</code>（窗口），窗口默认充满会话界面，因此这些窗口中可以运行相关性不大的任务。</p><p>一个<code>window</code>又可以包含多个<code>pane</code>（面板），窗口下的面板，都处于同一界面下，这些面板适合运行相关性高的任务，以便同时观察到它们的运行情况。</p><p>一般在一个<code>session</code>里进行新建<code>windows</code>和<code>pane</code>操作即可</p><p>一个session显示如图</p><p><img src="https://i.loli.net/2020/07/27/3bWsR57zZTUODF6.png" alt="img"></p><h4 id="会话"><a href="#会话" class="headerlink" title="会话"></a>会话</h4><h5 id="新建会话"><a href="#新建会话" class="headerlink" title="新建会话"></a>新建会话</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tmux  <span class="comment"># 新建一个无名称的会话 </span></span><br><span class="line"></span><br><span class="line">tmux new -s s1 <span class="comment"># 新建一个名称为s1的会话</span></span><br></pre></td></tr></table></figure><h5 id="断开当前会话"><a href="#断开当前会话" class="headerlink" title="断开当前会话"></a>断开当前会话</h5><p>暂时断开会话，可以进入到原始命令行界面进行操作。也可以<code>Ctrl+B  +d</code>进行断开</p><p>操作如 <code>tmux new -s demo</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tmux detach <span class="comment"># 断开当前会话，会话在后台运行</span></span><br></pre></td></tr></table></figure><h5 id="进入之前的会话"><a href="#进入之前的会话" class="headerlink" title="进入之前的会话"></a>进入之前的会话</h5><p>断开会话后，想要接着上次留下的现场继续工作，就要使用到tmux的attach命令了，语法为<code>tmux attach-session -t session-name</code>，可简写为<code>tmux a -t session-name</code> 或 <code>tmux a</code>。通常我们使用如下两种方式之一即可：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tmux a <span class="comment"># 默认进入第一个会话</span></span><br><span class="line">tmux a -t demo <span class="comment"># 进入到名称为demo的会话</span></span><br></pre></td></tr></table></figure><h5 id="关闭会话"><a href="#关闭会话" class="headerlink" title="关闭会话"></a>关闭会话</h5><p>会话的使命完成后，一定是要关闭的。我们可以使用tmux的kill命令</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tmux kill-session -t demo <span class="comment"># 关闭demo会话 </span></span><br><span class="line"></span><br><span class="line">tmux kill-server <span class="comment"># 关闭服务器，即关闭所有会话</span></span><br><span class="line"></span><br><span class="line">tmux kill-session -a -t s1　　<span class="comment">#关闭除s1外的所有会话</span></span><br></pre></td></tr></table></figure><h5 id="查看所有会话"><a href="#查看所有会话" class="headerlink" title="查看所有会话"></a>查看所有会话</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tmux ls <span class="comment"># 查看所有会话，显示会话列表</span></span><br></pre></td></tr></table></figure><h5 id="重命名会话S1为S2"><a href="#重命名会话S1为S2" class="headerlink" title="重命名会话S1为S2"></a>重命名会话S1为S2</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tmux rename -t s1 s2</span><br></pre></td></tr></table></figure><h4 id="Tmux快捷指令"><a href="#Tmux快捷指令" class="headerlink" title="Tmux快捷指令"></a>Tmux快捷指令</h4><p>tmux的所有指令，都包含同一个前缀，默认为<code>Ctrl+b</code>，输入完前缀过后，控制台激活，命令按键才能生效。</p><p>表一：常用指令</p><table><thead><tr><th align="center">前缀</th><th align="center">指令</th><th align="center">描述</th></tr></thead><tbody><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>s</code></td><td align="center">显示会话列表用于选择并切换 （上下键选择+回车）</td></tr><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>d</code></td><td align="center">断开当前会话</td></tr><tr><td align="center">===</td><td align="center"></td><td align="center"></td></tr><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>c</code></td><td align="center">新建窗口（windows）</td></tr><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>&amp;</code></td><td align="center">关闭当前窗口（关闭前需输入<code>y</code> or <code>n</code>确认）</td></tr><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>w</code></td><td align="center">打开窗口列表，用于且切换窗口</td></tr><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>,</code></td><td align="center">重命名当前窗口</td></tr><tr><td align="center">===</td><td align="center"></td><td align="center"></td></tr><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>&quot;</code></td><td align="center">当前面板上下一分为二，下侧新建面板</td></tr><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>%</code></td><td align="center">当前面板左右一分为二，右侧新建面板</td></tr><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>x</code></td><td align="center">关闭当前面板（关闭前需输入<code>y</code> or <code>n</code>确认）</td></tr><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>z</code></td><td align="center">最大化当前面板，再重复一次按键后恢复正常（v1.8版本新增）</td></tr><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>方向键</code></td><td align="center">移动光标切换面板</td></tr><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>o</code></td><td align="center">选择下一面板</td></tr><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>t</code></td><td align="center">显示时钟</td></tr></tbody></table><p>表二：系统指令</p><table><thead><tr><th align="center">前缀</th><th align="center">指令</th><th align="center">描述</th></tr></thead><tbody><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>?</code></td><td align="center">显示快捷键帮助文档</td></tr><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>d</code></td><td align="center">断开当前会话</td></tr><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>D</code></td><td align="center">选择要断开的会话</td></tr><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>Ctrl+z</code></td><td align="center">挂起当前会话</td></tr><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>r</code></td><td align="center">强制重载当前会话</td></tr><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>s</code></td><td align="center">显示会话列表用于选择并切换</td></tr><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>:</code></td><td align="center">进入命令行模式，此时可直接输入<code>ls</code>等命令</td></tr><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>[</code></td><td align="center">进入复制模式，按<code>q</code>退出</td></tr><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>]</code></td><td align="center">粘贴复制模式中复制的文本</td></tr><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>~</code></td><td align="center">列出提示信息缓存</td></tr></tbody></table><p>表三：窗口（window）指令</p><table><thead><tr><th align="center">前缀</th><th align="center">指令</th><th align="center">描述</th></tr></thead><tbody><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>c</code></td><td align="center">新建窗口</td></tr><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>&amp;</code></td><td align="center">关闭当前窗口（关闭前需输入<code>y</code> or <code>n</code>确认）</td></tr><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>0~9</code></td><td align="center">切换到指定窗口</td></tr><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>p</code></td><td align="center">切换到上一窗口</td></tr><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>n</code></td><td align="center">切换到下一窗口</td></tr><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>w</code></td><td align="center">打开窗口列表，用于且切换窗口</td></tr><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>,</code></td><td align="center">重命名当前窗口</td></tr><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>.</code></td><td align="center">修改当前窗口编号（适用于窗口重新排序）</td></tr><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>f</code></td><td align="center">快速定位到窗口（输入关键字匹配窗口名称）</td></tr></tbody></table><p>表四：面板（pane）指令</p><table><thead><tr><th align="center">前缀</th><th align="center">指令</th><th align="left">描述</th></tr></thead><tbody><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>&quot;</code></td><td align="left">当前面板上下一分为二，下侧新建面板</td></tr><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>%</code></td><td align="left">当前面板左右一分为二，右侧新建面板</td></tr><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>x</code></td><td align="left">关闭当前面板（关闭前需输入<code>y</code> or <code>n</code>确认）</td></tr><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>z</code></td><td align="left">最大化当前面板，再重复一次按键后恢复正常（v1.8版本新增）</td></tr><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>!</code></td><td align="left">将当前面板移动到新的窗口打开（原窗口中存在两个及以上面板有效）</td></tr><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>;</code></td><td align="left">切换到最后一次使用的面板</td></tr><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>q</code></td><td align="left">显示面板编号，在编号消失前输入对应的数字可切换到相应的面板</td></tr><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>{</code></td><td align="left">向前置换当前面板</td></tr><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>}</code></td><td align="left">向后置换当前面板</td></tr><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>Ctrl+o</code></td><td align="left">顺时针旋转当前窗口中的所有面板</td></tr><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>方向键</code></td><td align="left">移动光标切换面板</td></tr><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>o</code></td><td align="left">选择下一面板</td></tr><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>空格键</code></td><td align="left">在自带的面板布局中循环切换</td></tr><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>Alt+方向键</code></td><td align="left">以5个单元格为单位调整当前面板边缘</td></tr><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>Ctrl+方向键</code></td><td align="left">以1个单元格为单位调整当前面板边缘（Mac下被系统快捷键覆盖）</td></tr><tr><td align="center"><code>Ctrl+b</code></td><td align="center"><code>t</code></td><td align="left">显示时钟</td></tr></tbody></table><h4 id="tmux用于代码后台运行"><a href="#tmux用于代码后台运行" class="headerlink" title="tmux用于代码后台运行"></a>tmux用于代码后台运行</h4><p>要想使代码后台运行，我用nohup训练模型时重定向到log文件发现日志显示不全，影响实验结果的呈现，而tmux可以解决这个问题。</p><p>在进入tmux，新建一个session的时候，实际上tmux在服务器创建了虚拟终端自己连自己。所以用ctrl+B +D退出tmux，并且断掉ssh连接时后台实验是一直在运行的。下次连接ssh后，再打开tmux的session即可，session不会断掉。并且不需要重定向，结果直接显示在屏幕上。</p><h4 id="tmux的个性化设置-–-待完成"><a href="#tmux的个性化设置-–-待完成" class="headerlink" title="tmux的个性化设置 – 待完成"></a>tmux的个性化设置 – 待完成</h4><p>比如通过写脚本，在新建session的时候就自动建多个pane，并运行命令</p><h4 id="参考-2"><a href="#参考-2" class="headerlink" title="参考"></a>参考</h4><blockquote><p><a href="http://louiszhai.github.io/2017/09/30/tmux/" target="_blank" rel="noopener">http://louiszhai.github.io/2017/09/30/tmux/</a></p><p><a href="https://harttle.land/2015/11/06/tmux-startup.html" target="_blank" rel="noopener">https://harttle.land/2015/11/06/tmux-startup.html</a></p></blockquote><h3 id="zsh"><a href="#zsh" class="headerlink" title="zsh"></a>zsh</h3><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install git-core zsh</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      连接服务器时的总结体会
    
    </summary>
    
    
      <category term="服务器" scheme="http://yoursite.com/categories/%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
    
    
      <category term="服务器" scheme="http://yoursite.com/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
    
  </entry>
  
</feed>
