<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>思建的NLP之旅</title>
  
  <subtitle>沉淀自己</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-10-13T13:03:01.546Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>李思建</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>2020-10-13-小样本学习</title>
    <link href="http://yoursite.com/2020/10/13/2020-10-13-%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yoursite.com/2020/10/13/2020-10-13-%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0/</id>
    <published>2020-10-13T07:44:01.000Z</published>
    <updated>2020-10-13T13:03:01.546Z</updated>
    
    <content type="html"><![CDATA[<p>小（少）样本学习可以看做每个类别样本数目远远小于类别数目。<strong>在标注数据比较少的情况下，深度学习的应用和效果都受到了限制。</strong></p><p>在人类的快速学习能力的启发下，研究人员希望机器学习模型在学习了一定类别的大量数据后，对于新的类别，只需要少量的样本就能快速学习，这就是 Few-shot Learning 要解决的问题。</p><p>区分概念：</p><p>One-shot Learning 、Zero-shot Learning、Few-shot Learning？</p><p>One/zero-shot learning都是用来进行<strong>学习分类</strong>的算法。</p><p>Zero-shot Learing <strong>就是训练样本里没有这个类别的样本，但是如果我们可以学到一个牛逼的映射，这个映射好到我们即使在训练的时候没看到这个类，但是我们在遇到的时候依然能通过这个映射得到这个新类的特征。</strong></p><p><strong>即：训练集</strong>中<strong>没有出现过</strong>的<strong>类别</strong>，能自动创造出相应的映射。</p><p>One-shot Learing 就是类别下<strong>训练样本只有一个或者很少，我们依然可以进行分类</strong>。</p><p>比如我们<strong>可以在一个更大的数据集上或者利用knowledge graph、domain-knowledge 等方法，学到一个一般化的映射，也就是学习一个映射，然后再到小数据集上进行更新升级映射。</strong></p><p>Few-shot Learning：也即<strong>One-shot Learning</strong>。</p><p><strong>关键就在于如何学到一个好的映射，能应用到没有看到的问题上。</strong></p><ul><li><strong>「Older」指的是基于度量学习的方法</strong>，其目标是<strong>学习一个从图像到嵌入空间的映射，在该空间中，同一类图像彼此间的距离较近，而不同类的图像距离则较远</strong>。我们希望这种性质适用于那些没有见过的类。</li><li><strong>元学习方法：</strong>这类模型建立在当前所面对的任务的基础上，因此<strong>使用不同的分类器作为支持集的函数</strong>。其<strong>思路是寻找模型的超参数和参数，这样一来在不对使用的小样本过拟合的条件下可以很容易地适应新的任务。</strong></li><li><strong>数据增强方法：</strong>其思想是学习数据增强的方式，从而可以<strong>通过少量可用的样本生成更多的样本。</strong></li><li><strong>基于语义的方法：</strong>这类方法受到了零样本学习（zero-shot learning）的启发，其中分类任务的完成仅仅基于类别的名称、文本描述或属性。当视觉信息稀缺时，这些额外的语义信息也可能很有用。</li></ul><p>idea总结：</p><p>（1）、为了避免元学习模型对训练任务过拟合，作者在<strong>输出预测时加入了一个正则化项</strong>。正则化要么会使预测具有更高的熵（即预测的概率不会看起来像一个独热矢量），要么使模型在不同任务之间的差异更小（即在不同任务上表现相同）。显然，对于小样本学习来说，有一个强大的正则化机制是十分重要的。【Task Agnostic Meta-Learning for Few-Shot Learning】</p><p>（2）、对一个预训练的模型调优，其中权值是冻结的，在每一层中只学习放缩和偏置（Scaling and Shifting）</p><p>（3）、one-shot learning 或者zero-shot learning，少样本学习有时候也叫few-shot learning，意思是通过有限的几个样本让模型来学习一个新类别。这是一个很前沿的领域。</p><p>问题的引出：</p><p>（a）、例如典型的 MNIST 分类问题，一共有 10 个类，训练集一共有 6000 个样本，平均下来每个类大约 600 个样本，但是我们想一下我们人类自己，我们区分 0 到 9 的数字图片的时候需要看 6000 张图片才知道怎么区分吗？很显然，不需要！这表明当前的深度学习技术和我们人类智能差距还是很大的，要想弥补这一差距，<strong>小样本学习是一个很关键的问题。</strong></p><p>(b)、如果想要构建新的数据集（以分类数据集为例），我们需要标记大量的数据，但是有的时候标记数据集需要某些领域的专家（例如医学图像的标记），这费时又费力，因此<strong>如果我们可以解决少样本学习问题，只需要每个类标记几张图片就可以高准确率的给剩余大量图片自动标记。</strong></p><p><strong>以上两个原因使得小样本学习的研究成为热点。</strong></p><p>首先区分几个概念：</p><p><strong>小样本学习（Few-Shot Learning，以下简称 FSL ）</strong>用于解决当可用的数据量比较少时，如何提升神经网络的性能。</p><p>【例如】:以 MNIST 的 few-shot 为例，我们只要一个类各一张图片，也就是 10*1 = 10 张图片来训练我们的模型，就可以分类剩余的所有图片，如果可以到这一步，深度学习和人类智能的差距会缩小很大一部分。也即研究人员希望机器学习模型在学习了一定类别的大量数据后，对于新的类别，只需要少量的样本就能快速学习，这就是 Few-shot Learning 要解决的问题。这个问题的难点在于训练样本的数量对于深度学习来说太少了，很容易过拟合。</p><p>Few-Shot Learning【解决方案】：</p><ol type="1"><li><strong>数据增强和正则化</strong>（解决过拟合的问题）</li></ol><p>在训练的时候加入一个正则项，但是这个正则项的<strong>构建选择</strong>是关键。</p><ol start="2" type="1"><li><strong>Meta-learning（元学习）</strong>：这是<strong>现在主流的方案</strong>，元学习的<strong>目标是利用已经学到的知识来解决新的问题</strong>。这也是基于人类学习的机制，我们学习都是基于已有知识的，而不像深度学习一样都是从 0 开始学习的。（比如如果你会玩英雄联盟，那你学习玩王者荣耀会快很多。）</li></ol><p>Meta-learning 其实还有一个名字叫做“学会学习” (Learning to learn)。在 meta training 阶段将数据集分解为不同的 meta task，<strong>去学习类别变化的情况下模型的泛化能力，</strong>在 meta testing 阶段，面对全新的类别，<strong>不需要变动已有的模型，就可以完成分类</strong>。</p><p><strong>如果我们已有的先验知识来帮助我们解决新的问题，那么我们对于新的问题就可以不需要那么多的样本，从而解决 few-shot 问题</strong>。</p><p>但是<strong>元学习需要用一些别的数据来学习这个先验知识</strong>，例如分类 MiniImagenet，其中有 100 个类，我们用其中 60 个类来学习先验知识，20个做 validation，剩余 20 个做测试。<strong>注意我们测试的 20 个类和前面 80 各类是完全不同的，也就是新的类、新的概念、新的问题，并且这 20 个类每个类只有很少的几张图片 (few-shot 问题)！</strong>然后前面的 80 个类用来用来训练模型和确定超参数，也就是学习帮助我们解决新问题的先验知识。</p><p><strong>Meta-learning 方法的分类标准</strong>有很多，个人可大致分为以下三类：</p><p>1、学习微调 (Learning to Fine-Tune）</p><p>2、基于 RNN 的记忆 (RNN Memory Based）</p><p>3、度量学习 (Metric Learning)</p><p>Metric Based 方法通过度量 batch 集中的样本和 support 集中样本的距离，借助最近邻的思想完成分类。比如<strong><em>Relation Network，</em></strong>该网络不满足单一且固定的距离度量方式，而是训练一个网络来学习（例如 CNN）距离的度量方式，在 loss 方面也有所改变。</p><p>【元学习存在的问题】：元学习的解决方案：可以在另一个数据集上面训练学习先验知识（前面的 MiniImagenet 中的 100 个类都是一个数据集），例如从 Omniglot 中学习先验知识，用于 MNIST 的少样本分类（因为有种说法 Omniglot 可以看做 MNIST 的一种扩展）。当然<strong>这种方法的效果和两个数据集的相似度有关，两个数据集相似度很好，那么学到的先验知识可以很好地解决新数据集的 few-shot 问题，如果差异很大，可能效果会很一般。</strong></p><p>因此 meta-learning 和 few-shot learning 现在的研究基本都是在一起的。</p><p>少样本学习面临两个重要的问题：（1）已知类别和未知类别之间没有交集，导致它们的数据分布差别很大，不能直接通过训练分类器和微调(finetune)的方式得到很好的性能；（2）未知类别只有极少量数据(每个类别1或者5个训练样本)，导致分类器学习不可靠。</p><p>对于第一个问题，Matching Networks提出了episodic training的策略。一个episode就是一个少样本学习的子任务，包含训练集和测试集。这里的episode类似于深度学习中的mini-batch的概念。</p><p>对于第二个问题，目前解决方法较少。论文【ICLR2019：<strong>Learning to Propagate Labels: Transductive Propagation Network for Few-shot Learning：</strong><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1805.10002.pdf">https://arxiv.org/pdf/1805.10002.pdf</a>】提出利用转导(Transductive)的思想，拿到所有无标注数据，建立权重图，得到全部预测结果。</p><p><img src="E:\myBlog\source_posts\v2-34c21e6e6a6b07fa5d327b05d33fbc47_720w.jpg" alt="img"></p><p><img src="E:\myBlog\source\_posts\640.png" alt="img" style="zoom:50%;"></p><p><a href="https://www.cnblogs.com/jiangxinyang/p/12163215.html" target="_blank" rel="noopener" class="uri">https://www.cnblogs.com/jiangxinyang/p/12163215.html</a></p><p><strong>Few-shot Learning 是</strong> <strong>Meta Learning 在监督学习领域的应用。</strong></p><p>Meta Learning，又称为 learning to learn，在 meta training 阶段将数据集分解为不同的 meta task，去学习类别变化的情况下模型的泛化能力，在 meta testing 阶段，面对全新的类别，不需要变动已有的模型，就可以完成分类。</p><p>形式化来说，few-shot 的训练集中包含了很多的类别，每个类别中有多个样本。在训练阶段，会在训练集中随机抽取 C 个类别，每个类别 K 个样本（总共 CK 个数据），构建一个 meta-task，作为模型的支撑集（support set）输入；再从这 C 个类中剩余的数据中抽取一批（batch）样本作为模型的预测对象（batch set）。即要求模型从 C*K 个数据中学会如何区分这 C 个类别，<strong>这样的任务被称为 C-way K-shot 问题。</strong></p><p>训练过程中，每次训练（episode）都会采样得到不同 meta-task，所以总体来看，训练包含了不同的类别组合，这种机制使得模型学会不同 meta-task 中的共性部分，比如如何提取重要特征及比较样本相似等，忘掉 meta-task 中 task 相关部分。通过这种学习机制学到的模型，在面对新的未见过的 meta-task 时，也能较好地进行分类。</p><p>图 1 展示的是一个 2-way 5-shot 的示例，可以看到 meta training 阶段构建了一系列 meta-task 来让模型学习如何根据 support set 预测 batch set 中的样本的标签；meta testing 阶段的输入数据的形式与训练阶段一致（2-way 5-shot），但是会在全新的类别上构建 support set 和 batch。</p><p><img src="E:\myBlog\source_posts\v2-425a4cceb747a125d92b07add5917b09_720w.jpg" alt="img">▲ 图1：Few-shot Learning示例</p><p><a href="https://zhuanlan.zhihu.com/p/61215293" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/61215293</a></p><h3 id="在图像领域的研究现状">在图像领域的研究现状</h3><p>早期的 Few-shot Learning 算法研究多集中在图像领域，如图 2 所示，<strong>Few-shot Learning</strong> <strong>模型大致可分为三类：</strong>Mode Based，Metric Based 和 Optimization Based。</p><p><img src="E:\myBlog\source_posts\v2-91fcdbfabcb3aa5386e436c27fb52615_720w.jpg" alt="img">▲ 图2：Few-shot Learning模型分类</p><p>其中 Model Based 方法旨在通过模型结构的设计快速在少量样本上更新参数，直接建立输入 x 和预测值 P 的映射函数；Metric Based 方法通过度量 batch 集中的样本和 support 集中样本的距离，借助最近邻的思想完成分类；Optimization Based 方法认为普通的梯度下降方法难以在 few-shot 场景下拟合，因此通过调整优化方法来完成小样本分类的任务。</p><p><strong>Model Based方法</strong></p><p><strong>Santoro 等人</strong> [3] 提出使用记忆增强的方法来解决 Few-shot Learning 任务。基于记忆的神经网络方法早在 2001 年被证明可以用于 meta-learning。他们通过权重更新来调节 bias，并且通过学习将表达快速缓存到记忆中来调节输出。</p><p>然而，利用循环神经网络的内部记忆单元无法扩展到需要对大量新信息进行编码的新任务上。因此，需要让存储在记忆中的表达既要稳定又要是元素粒度访问的，前者是说当需要时就能可靠地访问，后者是说可选择性地访问相关的信息；另外，参数数量不能被内存的大小束缚。神经图灵机（NTMs）和记忆网络就符合这种必要条件。</p><p>文章基于神经网络图灵机（NTMs）的思想，因为 NTMs 能通过外部存储（external memory）进行短时记忆，并能通过缓慢权值更新来进行长时记忆，NTMs 可以学习将表达存入记忆的策略，并如何用这些表达来进行预测。由此，文章方法可以快速准确地预测那些只出现过一次的数据。</p><p>文章基于 LSTM 等 RNN 的模型，将数据看成序列来训练，在测试时输入新的类的样本进行分类。</p><p>具体地，在 t 时刻，模型输入 <img src="E:\myBlog\source_posts\equation" alt="[公式]"> ，也就是在当前时刻预测输入样本的类别，并在下一时刻给出真实的 label，并且添加了 external memory 存储上一次的 x 输入，这使得下一次输入后进行反向传播时，可以让 y (label) 和 x 建立联系，使得之后的 x 能够通过外部记忆获取相关图像进行比对来实现更好的预测。</p><p><img src="E:\myBlog\source_posts\v2-abf2cab4e2fd2f20589c219048bffe09_720w.jpg" alt="img">▲ 图3：Memory Augmented Model</p><p><strong><em>Meta Network</em></strong> [12] 的快速泛化能力源自其“快速权重”的机制，在训练过程中产生的梯度被用来作为快速权重的生成。模型包含一个 meta learner 和一个 base learner，meta learner 用于学习 meta task 之间的泛化信息，并使用 memory 机制保存这种信息，base learner 用于快速适应新的 task，并和 meta learner 交互产生预测输出。</p><p><strong>Metric Based方法</strong></p><p>如果在 Few-shot Learning 的任务中去训练普通的基于 cross-entropy 的神经网络分类器，那么几乎肯定是会过拟合，因为神经网络分类器中有数以万计的参数需要优化。</p><p>相反，很多非参数化的方法（最近邻、K-近邻、Kmeans）是不需要优化参数的，因此可以在 meta-learning 的框架下构造一种可以端到端训练的 few-shot 分类器。该方法是对样本间距离分布进行建模，使得同类样本靠近，异类样本远离。下面介绍相关的方法。</p><p>如图 4 所示，<strong>孪生网络（Siamese Network）</strong>[4] 通过有监督的方式训练孪生网络来学习，然后重用网络所提取的特征进行 one/few-shot 学习。</p><p><img src="E:\myBlog\source_posts\v2-2620b9d172e3e28df69b6a999dd8ba03_720w.jpg" alt="img">▲ 图4：Siamese Network</p><p>具体的网络是一个双路的神经网络，训练时，通过组合的方式构造不同的成对样本，输入网络进行训练，在最上层通过样本对的距离判断他们是否属于同一个类，并产生对应的概率分布。在预测阶段，孪生网络处理测试样本和支撑集之间每一个样本对，最终预测结果为支撑集上概率最高的类别。</p><p>相比孪生网络，<strong>匹配网络（Match Network）</strong>[2] 为支撑集和 Batch 集构建不同的编码器，最终分类器的输出是支撑集样本和 query 之间预测值的加权求和。</p><p>如图 5 所示，该文章也是在不改变网络模型的前提下能对未知类别生成标签，其主要创新体现在建模过程和训练过程上。对于建模过程的创新，文章提出了基于 memory 和 attention 的 matching nets，使得可以快速学习。</p><p>对于训练过程的创新，文章基于传统机器学习的一个原则，即训练和测试是要在同样条件下进行的，提出在训练的时候不断地让网络只看每一类的少量样本，这将和测试的过程是一致的。</p><p>具体地，它显式的定义一个基于支撑集 <img src="E:\myBlog\source_posts\equation" alt="[公式]"> 的分类器，对于一个新的数据 <img src="E:\myBlog\source_posts\equation" alt="[公式]"> ，其分类概率由<img src="https://www.zhihu.com/equation?tex=%5Chat%7Bx%7D" alt="[公式]">与支撑集 S 之间的距离度量得出：</p><p><img src="E:\myBlog\source_posts\v2-0eec568b3cb49b448ab40f34c103d253_720w.jpg" alt="img"></p><p>其中 a 是基于距离度量的 attention score：</p><p><img src="E:\myBlog\source_posts\v2-2ab250d61e2545607a0477a4856426e4_720w.jpg" alt="img"></p><p>进一步，支撑集样本 embedding 模型 g 能继续优化，并且支撑集样本应该可以用来修改测试样本的 embedding 模型 f。</p><p>这个可以通过如下两个方面来解决，即：<strong>1）基于双向 LSTM 学习训练集的 embedding</strong>，使得每个支撑样本的 embedding 是其它训练样本的函数；<strong>2）基于 attention-LSTM 来对测试样本 embedding</strong>，使得每个 Query 样本的 embedding 是支撑集 embedding 的函数。文章称其为 FCE (fully-conditional embedding)。</p><p><img src="E:\myBlog\source_posts\v2-d62b26df5f93b0c4489b467978d43057_720w.jpg" alt="img">▲ 图5：Match Network</p><p><strong>原型网络（Prototype Network）</strong>[5] 基于这样的想法：每个类别都存在一个原型表达，该类的原型是 support set 在 embedding 空间中的均值。然后，分类问题变成在 embedding 空间中的最近邻。</p><p>如图 6 所示，c1、c2、c3 分别是三个类别的均值中心（称 Prototype），将测试样本 x 进行 embedding 后，与这 3 个中心进行距离计算，从而获得 x 的类别。</p><p><img src="E:\myBlog\source_posts\v2-b80a95841c319730ddb4fe3e86b9b445_720w.jpg" alt="img">▲ 图6：Prototype Network</p><p>文章采用在 Bregman 散度下的指数族分布的混合密度估计，文章在训练时采用相对测试时更多的类别数，即训练时每个 episodes 采用 20 个类（20 way），而测试对在 5 个类（5 way）中进行，其效果相对训练时也采用 5 way 的提升了 2.5 个百分点。</p><p>前面介绍的几个网络结构在最终的距离度量上都使用了固定的度量方式，如 cosine，欧式距离等，这种模型结构下所有的学习过程都发生在样本的 embedding 阶段。</p><p>而 <strong><em>Relation Network</em></strong> [6] 认为度量方式也是网络中非常重要的一环，需要对其进行建模，所以该网络不满足单一且固定的距离度量方式，而是训练一个网络来学习（例如 CNN）距离的度量方式，在 loss 方面也有所改变，考虑到 relation network 更多的关注 relation score，更像一种回归，而非 0/1 分类，所以使用了 MSE 取代了 cross-entropy。</p><p><img src="E:\myBlog\source_posts\v2-2f8ed3cf76e138a7be12c6645fd33441_720w.jpg" alt="img">▲ 图7：Relation Networks</p><p><strong>Optimization Based方法</strong></p><p><strong>Ravi 等人</strong> [7] 研究了在少量数据下，基于梯度的优化算法失败的原因，即无法直接用于 meta learning。</p><p>首先，这些梯度优化算法包括 momentum, adagrad, adadelta, ADAM 等，无法在几步内完成优化，特别是在非凸的问题上，多种超参的选取无法保证收敛的速度。</p><p>其次，不同任务分别随机初始化会影响任务收敛到好的解上。虽然 finetune 这种迁移学习能缓解这个问题，但当新数据相对原始数据偏差比较大时，迁移学习的性能会大大下降。我们需要一个系统的学习通用初始化，使得训练从一个好的点开始，它和迁移学习不同的是，它能保证该初始化能让 finetune 从一个好的点开始。</p><p>文章学习的是一个模型参数的更新函数或更新规则。它不是在多轮的 episodes 学习一个单模型，而是在每个 episode 学习特定的模型。</p><p>具体地，学习基于梯度下降的参数更新算法，采用 LSTM 表达 meta learner，用其状态表达目标分类器的参数的更新，最终学会如何在新的分类任务上，对分类器网络（learner）进行初始化和参数更新。这个优化算法同时考虑一个任务的短时知识和跨多个任务的长时知识。</p><p>文章设定目标为通过少量的迭代步骤捕获优化算法的泛化能力，由此 meta learner 可以训练让 learner 在每个任务上收敛到一个好的解。另外，通过捕获所有任务之前共享的基础知识，进而更好地初始化 learner。</p><p>以训练 miniImage 数据集为例，训练过程中，从训练集（64 个类，每类 600 个样本）中随机采样 5 个类，每个类 5 个样本，构成支撑集，去学习 learner；然后从训练集的样本（采出的 5 个类，每类剩下的样本）中采样构成 Batch 集，集合中每类有 15 个样本，用来获得 learner 的 loss，去学习 meta leaner。</p><p>测试时的流程一样，从测试集（16 个类，每类 600 个样本）中随机采样 5 个类，每个类 5 个样本，构成支撑集 Support Set，去学习 learner；然后从测试集剩余的样本（采出的 5 个类，每类剩下的样本）中采样构成 Batch 集，集合中每类有 15 个样本，用来获得 learner 的参数，进而得到预测的类别概率。这两个过程分别如图 8 中虚线左侧和右侧。</p><p><img src="E:\myBlog\source_posts\v2-5b123b7cc752acb08a37ca95d51f5e9b_720w.jpg" alt="img">▲ 图8：Optimization as a model</p><p>meta learner 的目标是在各种不同的学习任务上学出一个模型，使得可以仅用少量的样本就能解决一些新的学习任务。这种任务的挑战是模型需要结合之前的经验和当前新任务的少量样本信息，并避免在新数据上过拟合。</p><p><strong><em>Finn</em></strong> [8] 提出的方法使得可以在小量样本上，用少量的迭代步骤就可以获得较好的泛化性能，而且模型是容易 fine-tine 的。而且这个方法无需关心模型的形式，也不需要为 meta learning 增加新的参数，直接用梯度下降来训练 learner。</p><p>文章的核心思想是学习模型的初始化参数使得在一步或几步迭代后在新任务上的精度最大化。它学的不是模型参数的更新函数或是规则，它不局限于参数的规模和模型架构（比如用 RNN 或 siamese）。它本质上也是学习一个好的特征使得可以适合很多任务（包括分类、回归、增强学习），并通过 fine-tune 来获得好的效果。</p><p>文章提出的方法，可以学习任意标准模型的参数，并让该模型能快速适配。他们认为，一些中间表达更加适合迁移，比如神经网络的内部特征。因此面向泛化性的表达是有益的。因为我们会基于梯度下降策略在新的任务上进行 finetune，所以目标是学习这样一个模型，它能对新的任务从之前任务上快速地进行梯度下降，而不会过拟合。事实上，是要找到一些对任务变化敏感的参数，使得当改变梯度方向，小的参数改动也会产生较大的 loss。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      小样本学习
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-10-12-DETR论文阅读</title>
    <link href="http://yoursite.com/2020/10/12/2020-10-12-DETR%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <id>http://yoursite.com/2020/10/12/2020-10-12-DETR%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</id>
    <published>2020-10-12T14:17:41.000Z</published>
    <updated>2020-10-13T06:23:52.786Z</updated>
    
    <content type="html"><![CDATA[<p>仔细看论文和代码，才发现它的输出是定长的：100个检测框和类别，这种操作可能跟COCO评测的时候取top 100的框有关，我认为他说的有道理。</p><p>当然定长的输出有利于显存对齐，训练的时候会方便一些。</p><p>DETR的整体结构Transformer类似：Backbone得到的特征铺平，加上Position信息之后送到一坨Encoder里，得到一些candidates的特征。这100个candidates是被Decoder<strong>并行解码的</strong>（想想显存就很大，但实现的时候可写成不并行的），以得到最后的检测框。</p><p><strong>2.1 DETR Encoder</strong></p><p>网络一开始是使用Backbone（比如ResNet）提取一些feature，然后降维到d×HW。</p><p>Feature降维之后与Spatial Positional Encoding相加，然后被送到Encoder里。</p><p>为了体现图像在x和y维度上的信息，作者的代码里<strong>分别计算了两个维度的Positional Encoding，然后Cat到一起。</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pos_x = torch.stack((pos_x[:, :, :, <span class="number">0</span>::<span class="number">2</span>].sin(), pos_x[:, :, :, <span class="number">1</span>::<span class="number">2</span>].cos()), dim=<span class="number">4</span>).flatten(<span class="number">3</span>)</span><br><span class="line">pos_y = torch.stack((pos_y[:, :, :, <span class="number">0</span>::<span class="number">2</span>].sin(), pos_y[:, :, :, <span class="number">1</span>::<span class="number">2</span>].cos()), dim=<span class="number">4</span>).flatten(<span class="number">3</span>)</span><br><span class="line">pos = torch.cat((pos_y, pos_x), dim=<span class="number">3</span>).permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br></pre></td></tr></tbody></table></figure><p>FFN、LN等操作也与Transformer类似。<strong>Encoder最后得到的结果是对N个物体编码后的特征。</strong></p><p><strong>2.2 DETR Decoder</strong></p><p>DETR Decoder的结构也与Transformer类似，<strong>区别在于Decoder并行解码N个object。</strong></p><p>每个Decoder有两个输入：一路是Object Query（或者是上一个Decoder的输出），另一路是Encoder的结果。</p><p>我一开始不明白Object Query是怎样得到的。后来从代码看，<strong>Object Query是一组nn.Embedding的weight（就是一组学到的参数）。</strong></p><p><strong>最后一个Decoder后面接了两个FFN，分别预测检测框及其类别。</strong></p><p><strong>2.3 Bipartite Matching</strong></p><p>由于输出物体的顺序不一定与ground truth的序列相同，作者使用二元匹配将GT框与预测框进行匹配。其匹配策略如下：</p><p><img src="E:\myBlog\source_posts\v2-acce6976666e74713f72176499854306_1440w.jpg" alt="img"></p><p>最后的损失函数：</p><p><img src="E:\myBlog\source_posts\v2-cdc64ccbfaa7b299f9baf32ed09b83ad_1440w.jpg" alt="img"></p><p><img src="E:\myBlog\source_posts\image-20201013000354504.png" alt="image-20201013000354504"></p><p><img src="E:\myBlog\source_posts\image-20201013000531053.png" alt="image-20201013000531053"></p><p>=================</p><p>个人觉得最直白的理解方式就是用positional embedding替代了原本的anchor。</p><p>第一步用CNN提feature，然后展开成一维之后加上位置信息进入encoder加工。之后decoder里的object queries，实际上是另一组可学习的positional embedding，其功能类似于anchor。之后每个query进过decoder后算一个bbox和class prob。</p><p>这篇paper怎么说呢，个人觉得有点华而不实。毕竟把原有的二维feature给flatten成一维再加上位置信息实际上在我看来是一道脱裤子放屁的流程。不过在detection上这样操作能work，实际上说明了transformer果然更适合看成GCN的一个特例而不是LSTM的上位替换。</p><p>网络的结构是非常简单的，先是CNN提取特征，这一个常规得不能再常规得做法，然后将CNN提取的特征送入到transformer中，而由于transformer是位置无关，所以为了保持位置信息，需要送入CNN特征的同时，医药送入位置的编码信息，确保整个链路中位置信息不丢失。在transformer中编码之后，送入到解码器，同时送入到解码器的还包括object queries（即文中说的N个查询对象），N个对象以及编码器的输入在在解码器的综合作用下，获取N个输出，这N个输出在FFN的作用下，产生N个位置以及每个位置对应的类别。至此，网络的便具备物体检测的能力。与原始的transformer不同的地方在于decoder每一层都输出结果，计算loss。这种思想还是相对简单并且work的，EV-FlowNet以及龙明盛迁移学习的某一个版本中均有类似的操作。如果仔细探究的话，我想一定会有一种更合计的叠加方式，而不是这种简单的加在一起，毕竟每一层理论上的物理意义都不同，这种叠加loss的方法，限制了decoder只有第一层完成了大部分任务，更多的层只是一个上采样和细化的过程。</p><p>所谓二分的最大匹配，即保证预测值与真值实现最大的匹配，保证预测的N的实例（包括∅）按照位置与真值对应起来。实现一一对应之后，便能够利用分类Loss以及boundingbox Loss进行优化。这种一一对应的关系，同时也另一个好处是，不会多个预测值对应到同一个真实值上，然后再通过NMS进行后处理。</p><p>概括而言，文章的两大核心思想为：</p><p>1、transformer保证了attention，确保对一个实例的识别，是在整幅图的知识下进行的。</p><p>2、二分最大匹配，确保了一一对应的关系。</p><p><img src="E:\myBlog\source_posts\v2-aadd53804ee33600a1d40ad856f146ad_1440w.jpg" alt="img"></p><h4 id="二分图最大匹配问题与匈牙利算法的核心思想">二分图最大匹配问题与匈牙利算法的核心思想</h4><p><a href="https://liam.page/2016/04/03/Hungarian-algorithm-in-the-maximum-matching-problem-of-bigraph/" target="_blank" rel="noopener" class="uri">https://liam.page/2016/04/03/Hungarian-algorithm-in-the-maximum-matching-problem-of-bigraph/</a></p><p>图上的object queries实际上是N个emebding，更具体得说应该是N个实例query的embedding(我理解是这样)，退一步不准确一点可以简单理解成位置。N是固定值但是emebding完之后N个quries都不太一样。所以差不多的意思就是告诉模型要100个实例，然后decoder根据encoder得到特征的位置和显著性decoder出100个抽象点代表instance，其中部分是前景instance，部分是背景instance，前景的就class+box loss，背景的就当背景。这就是训练过程。推理过程也就很简单了，前景的就直接用，背景的就丢掉。</p><p>Transformer encoder： 注意力机制本质是在跑message passing去对提取的特征进行一种滤波，这里面在很大程度上就是<strong>实现了其他分析中的去提取不同位置不同物体之间的相互关系这个功能，通过发掘这个约束提高了对物体识别的可靠性。</strong></p><p><img src="E:\myBlog\source_posts\image-20201013142352470.png" alt="image-20201013142352470"></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      此文是将transformer应用到目标检测，可以作为我研究方向的拓展
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-09-28-windows-terminal探索</title>
    <link href="http://yoursite.com/2020/09/28/2020-09-28-windows-terminal%E6%8E%A2%E7%B4%A2/"/>
    <id>http://yoursite.com/2020/09/28/2020-09-28-windows-terminal%E6%8E%A2%E7%B4%A2/</id>
    <published>2020-09-28T02:38:58.000Z</published>
    <updated>2020-09-30T12:57:58.465Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言">前言</h3><p>优化方法终于考完了，最近几天都是在复习。可以放松两天，折腾一下</p><h3 id="windows-terminal">windows terminal</h3><h4 id="安装windows-terminal">安装Windows Terminal</h4><p>在Windows上，可以安装<code>Windows Terminal</code>。有点类似于MacOS上的<code>iTerm</code>，可以说是Windows下最舒适的终端。</p><p>在<code>windows store</code>中安装即可，比较方便，如下图所示</p><p><img src="https://i.loli.net/2020/09/30/KLziDFhWNBg9qYR.png" alt="image-20200930205359665"></p><h3 id="安装ubuntu子系统">安装Ubuntu子系统</h3><p>此时，我们仅仅安装了一个命令行终端而已，还是需要在<code>Windows</code>上安装<code>Ubuntu</code>。</p><p>只需要两步</p><p>1.在系统中开启子系统功能</p><p>2.在<code>windows store</code>安装linux版本即可。我安装的是<code>ubuntu 18.04 LTS</code>版本的，和实验室服务器一个版本号，利于操作。</p><blockquote><p>关于LTS</p><p>1.LTS= 长期支持版本，你会在较长的时间内获得安全、维护和(有时有)功能的更新。</p><p>2.LTS 版本被认为是最稳定的版本，它经历了广泛的测试，并且大多包含了多年积累的改进。</p><p>3.对于ubuntu，没两年发布一个LST版本，比如ubuntu 16.04 ubuntu 18.04等等，代表的是发布的年份。</p><p>4.最新的 LTS 版本是 Ubuntu 20.04 ，它将被支持到 2025 年 4 月，支持5年的软件更新和修补。换句话说，Ubuntu 20.04 在那之前都会收到软件更新。非 LTS 版本只支持九个月。</p></blockquote><p>如下图，在控制面板，找到程序选项，点击 “启用或关闭Windows功能”。</p><p><img src="https://i.loli.net/2020/09/28/KQ3LocsNrBxiRf5.png" alt="image-20200928105917992"></p><p>从弹出的对话框里，划到最下边，然后给“适用于Linux的Windows子系统“，打勾，完事！</p><p><img src="https://i.loli.net/2020/09/28/HPVhtscoGj8iCRA.png" alt="image-20200928105907117"></p><p>在windows中访问ubuntu系统</p><p>可以认为在windows 文件资源管理器中开辟一个空间用来储存ubuntu系统，但是如何找到位置呢？</p><p>执行如下命令：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /home  </span><br><span class="line">explorer.exe .  <span class="comment">#用文件资源管理器来打开当前home目录所在位置</span></span><br></pre></td></tr></tbody></table></figure><p>可以看到是在<code>网络</code>一栏中， 可以看到ubuntu的文件目录。但是返回到<code>网络</code>根目录，却显示是无文件夹。不知道为什么。</p><p>为了操作方便，我把这个长长的目录，右键映射到了Z盘上。如图，下次在访问Linux的时候，直接访问Z盘就可以了。</p><p><img src="https://i.loli.net/2020/09/28/XqMOBfSKRkwHC93.png" alt="image-20200928111036904"></p><p>这时，就可以看到在我的电脑里就有了Z盘</p><p><img src="https://i.loli.net/2020/09/28/V9MFRujrqgl46me.png" alt="image-20200928111104530"></p><blockquote><p><strong>映射网络驱动器</strong>目的就是为了让远程网络中的资源和本地共享，在本地可以对远程网络中的资源进行访问，并且可以创建文件。</p></blockquote><h3 id="工作区快捷键">工作区快捷键</h3><table><thead><tr class="header"><th style="text-align: left;">Win 快捷键</th><th style="text-align: left;">作用</th><th style="text-align: left;">备注</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;"><strong>Ctrl + Shift + P</strong>，F1</td><td style="text-align: left;">显示命令面板</td><td style="text-align: left;"></td></tr><tr class="even"><td style="text-align: left;"><strong>Ctrl + B</strong></td><td style="text-align: left;">显示/隐藏侧边栏</td><td style="text-align: left;">很实用</td></tr><tr class="odd"><td style="text-align: left;"><code>Ctrl + \</code></td><td style="text-align: left;"><strong>创建多个编辑器</strong></td><td style="text-align: left;">【重要】抄代码利器</td></tr><tr class="even"><td style="text-align: left;"><strong>Ctrl + 1、2</strong></td><td style="text-align: left;">聚焦到第 1、第 2 个编辑器</td><td style="text-align: left;">同上重要</td></tr><tr class="odd"><td style="text-align: left;"><strong>ctrl +/-</strong></td><td style="text-align: left;">将工作区放大/缩小（包括代码字体、左侧导航栏）</td><td style="text-align: left;">在投影仪场景经常用到</td></tr><tr class="even"><td style="text-align: left;">Ctrl + J</td><td style="text-align: left;">显示/隐藏控制台</td><td style="text-align: left;"></td></tr><tr class="odd"><td style="text-align: left;"><strong>Ctrl + Shift + N</strong></td><td style="text-align: left;">重新开一个软件的窗口</td><td style="text-align: left;">很常用</td></tr><tr class="even"><td style="text-align: left;">Ctrl + Shift + W</td><td style="text-align: left;">关闭软件的当前窗口</td><td style="text-align: left;"></td></tr><tr class="odd"><td style="text-align: left;">Ctrl + N</td><td style="text-align: left;">新建文件</td><td style="text-align: left;"></td></tr><tr class="even"><td style="text-align: left;">Ctrl + W</td><td style="text-align: left;">关闭当前文件</td><td style="text-align: left;"></td></tr></tbody></table><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      才考完优化方法，折腾一下微软神器windows terminal
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-09-19-一周论文分享（第2期）</title>
    <link href="http://yoursite.com/2020/09/19/2020-09-19-%E4%B8%80%E5%91%A8%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB%EF%BC%88%E7%AC%AC2%E6%9C%9F%EF%BC%89/"/>
    <id>http://yoursite.com/2020/09/19/2020-09-19-%E4%B8%80%E5%91%A8%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB%EF%BC%88%E7%AC%AC2%E6%9C%9F%EF%BC%89/</id>
    <published>2020-09-19T02:58:48.000Z</published>
    <updated>2020-09-28T02:38:16.191Z</updated>
    
    <content type="html"><![CDATA[<h4 id="联邦学习">联邦学习</h4><p>参考 <a href="https://zhuanlan.zhihu.com/p/79284686" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/79284686</a></p><p>背景</p><p>1.现实生活中，除了少数巨头公司能够满足，绝大多数企业都存在数据量少，数据质量差的问题，不足以支撑人工智能技术的实现；</p><p>2.同时国内外监管环境也在逐步加强数据保护，因此数据在安全合规的前提下自由流动，成了大势所趋，所以不能获取很多涉及用户隐私的信息。</p><p>3.数据的不充分交流，同时也导致即使在同一个公司内，数据也往往以孤岛形式出现。</p><p>基于以上不足以支撑实现、不允许粗暴交换、不愿意贡献价值三点，</p><p>现在大量存在的<strong>数据孤岛</strong>，以及隐私保护问题，联邦学习被提出。</p><p>概念</p><p>本质：联邦学习本质上是一种<strong>分布式</strong>机器学习技术，或机器学习<strong>框架</strong>。</p><p>目标：联邦学习的目标是在保证数据隐私安全及合法合规的基础上，实现共同建模，提升AI模型的效果。</p><p>前身：联邦学习最早在 2016 年由谷歌提出，原本用于解决安卓手机终端用户在本地更新模型的问题；</p><p><img src="E:\myBlog\source_posts\v2-657a9f63512351691e60af9d88a34605_720w.jpg" alt="img"></p><h2 id="横向联邦学习">3.1 横向联邦学习</h2><p><strong>适用场景：</strong></p><p>横向联邦学习的本质是<strong>样本的联合</strong>，适用于参与者间业态相同但触达客户不同，即特征重叠多，用户重叠少时的场景，比如不同地区的银行间，他们的业务相似（特征相似），但用户不同（样本不同）</p><p><strong>学习过程：</strong></p><p><img src="E:\myBlog\source_posts\v2-23616816b92a6d62be206b0aa28ba393_720w.jpg" alt="img"></p><p>step1：参与方各自从服务器A下载最新模型；</p><p>step2：每个参与方利用本地数据训练模型，加密梯度上传给服务器A，服务器A聚合各用户的梯度更新模型参数；</p><p>step3：服务器A返回更新后的模型给各参与方；</p><p>step4：各参与方更新各自模型。</p><p><strong>步骤解读：</strong>在传统的机器学习建模中，通常是把模型训练需要的数据集合到一个数据中心然后再训练模型，之后预测。在横向联邦学习中，可以看作是<strong>基于样本的分布式模型训练</strong>，分发全部数据到不同的机器，每台机器从服务器下载模型，然后利用本地数据训练模型，之后返回给服务器需要更新的参数；服务器聚合各机器上的返回的参数，更新模型，再把最新的模型反馈到每台机器。</p><p>在这个过程中，每台机器下都是<strong>相同且完整的模型</strong>，且机器之间不交流不依赖，在预测时每台机器也可以<strong>独立预测</strong>，可以把这个过程看作成基于样本的分布式模型训练。谷歌最初就是采用横向联邦的方式解决安卓手机终端用户在本地更新模型的问题的。</p><h2 id="简介"><strong>简介</strong></h2><p>NAS</p><p>深度学习可以自动学习出有用的特征，脱离了对特征工程的依赖，在图像、语音等任务上取得了超越其他算法的结果。这种成功很大程度上得益于新神经网络结构的出现，如ResNet、Inception、DenseNet等。但设计出高性能的神经网络需要大量的专业知识与反复试验，成本极高，限制了神经网络在很多问题上的应用。神经结构搜索（Neural Architecture Search，简称NAS）是一种自动设计神经网络的技术，可以通过算法根据样本集自动设计出高性能的网络结构，在某些任务上甚至可以媲美人类专家的水准，甚至发现某些人类之前未曾提出的网络结构，这可以有效的降低神经网络的使用和实现成本。</p><p>NAS的原理是给定一个称为搜索空间的候选神经网络结构集合，用某种策略从中搜索出最优网络结构。神经网络结构的优劣即性能用某些指标如精度、速度来度量，称为性能评估。这一过程如下图所示。</p><p><img src="E:\myBlog\source_posts\v2-261f4e89d5c60e5d336052e7fc6d116d_720w.png" alt="img"></p><p>在搜索过程的每次迭代中，从搜索空间产生“样本”即得到一个神经网络结构，称为“子网络”。在训练样本集上训练子网络，然后在验证集上评估其性能。逐步优化网络结构，直至找到最优的子网络。</p><p>搜索空间，搜索策略，性能评估策略是NAS算法的核心要素。搜索空间定义了可以搜索的神经网络结构的集合，即解的空间。搜索策略定义了如何在搜索空间中寻找最优网络结构。性能评估策略定义了如何评估搜索出的网络结构的性能。对这些要素的不同实现得到了各种不同的NAS算法，本节将选择有代表性的进行介绍。</p><h3 id="fisher-information">Fisher Information</h3><p>反映了我们对参数估计的准确度，它越大，对参数估计的准确度越高，即代表了越多的信息。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      记录每周值得分享的论文，周一发布、
《reformer-the eficient transformer》、
《Transformer-XL-Attentive Language Models Beyond a Fixed-Length Context》

    
    </summary>
    
    
      <category term="论文分享" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"/>
    
    
      <category term="论文" scheme="http://yoursite.com/tags/%E8%AE%BA%E6%96%87/"/>
    
      <category term="论文分享" scheme="http://yoursite.com/tags/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"/>
    
  </entry>
  
  <entry>
    <title>2020-09-14-修改pytorch版transformer代码</title>
    <link href="http://yoursite.com/2020/09/14/2020-09-14-%E4%BF%AE%E6%94%B9pytorch%E7%89%88transformer%E4%BB%A3%E7%A0%81/"/>
    <id>http://yoursite.com/2020/09/14/2020-09-14-%E4%BF%AE%E6%94%B9pytorch%E7%89%88transformer%E4%BB%A3%E7%A0%81/</id>
    <published>2020-09-14T08:26:17.000Z</published>
    <updated>2020-09-14T09:14:07.600Z</updated>
    
    <content type="html"><![CDATA[<p><del>源代码是<code>main3.py</code> ,在此基础上进行修改，修改后文件为<code>main3-2.py</code></del></p><p>740中<code>annotated-transformer</code>中<code>main.py</code>和哈佛的一样</p><p>复制到了本地<code>main.py</code> ,再复制到<code>annotated-transformer1</code>中的main.py</p><p><strong>所以改前的代码是<code>main.py</code> ，改后的代码是<code>main-1.py</code></strong></p><p>注：</p><p><strong><code>python main.py &gt;main.txt 2&gt;&amp;1</code>，在将结果重定向到main.txt中，会覆盖main.txt之前的内容</strong></p><p><strong>每次跑实验的预测都是不一样的，但是都是和输入差不多</strong></p><ol type="1"><li>将<code>attention</code>函数去掉，合并到<code>MultiHeadedAttention</code>中，服务器上测试<strong>可行</strong></li></ol><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      记录修改过程
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-09-11-gpu实验加速</title>
    <link href="http://yoursite.com/2020/09/11/2020-09-11-gpu%E5%AE%9E%E9%AA%8C%E5%8A%A0%E9%80%9F/"/>
    <id>http://yoursite.com/2020/09/11/2020-09-11-gpu%E5%AE%9E%E9%AA%8C%E5%8A%A0%E9%80%9F/</id>
    <published>2020-09-11T00:48:00.000Z</published>
    <updated>2020-09-16T08:00:54.184Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言">前言</h3><p>将深度学习应用到实际问题中，一个非常大的问题在于训练深度学习模型需要的计算量太大。为了加速训练过程，本文将介绍如何如何在TensorFlow中使用单个GPU进行计算加速</p><h3 id="简介">简介</h3><h4 id="cuda">CUDA</h4><p><a href="https://developer.nvidia.com/cuda-toolkit-archive" target="_blank" rel="noopener">CUDA</a>（Compute Unified Device Architecture,点击进入安装网站），是显卡厂商NVIDIA推出的运算平台。 CUDA™是一种由NVIDIA推出的通用并行计算架构，该架构使GPU能够解决复杂的计算问题。安装GPU版tensorflow,必须有这个环境。</p><p>CUDA是NVIDIA推出的用于自家GPU的并行计算框架，也就是说CUDA只能在NVIDIA的GPU上运行，而且只有当要解决的计算问题是可以大量并行计算的时候才能发挥CUDA的作用。</p><h4 id="cudnn">cuDNN</h4><p>NVIDIA <a href="https://developer.nvidia.com/cuda-toolkit-archive" target="_blank" rel="noopener">cuDNN</a>是用于深度神经网络的GPU加速库。它强调性能、易用性和低内存开销。</p><h3 id="安装">安装</h3><p>必须要安装对应版本的CUDA、cuDNN和tensorflow</p><p>我在实验室服务器R740上的安装版本如下，是可以运行的</p><blockquote><p>CUDA： V10.1 # nvcc --version</p><p>cuDNN：V7</p><p>tensorflow-gpu：1.14.0</p></blockquote><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">conda install python==3.6.10  <span class="comment">#这样可以 ，但是为什么 python==3.6.1 和  python==3.6.0 是不可以的呢？？</span></span><br><span class="line">pip install tensorflow-gpu==1.14.0  <span class="comment">#安装成功gpu版本</span></span><br><span class="line"><span class="comment">#用conda装tensorflow时候，会自动下载cuda和cudnn，所以推荐用pip安装</span></span><br><span class="line"></span><br><span class="line">pip install tensorflow-gpu==1.2 <span class="comment">#如果安装错误，可以用pip卸载，没测试过。 或者直接再新建一个虚拟环境也可以</span></span><br></pre></td></tr></tbody></table></figure><h3 id="测试tensorflow-gpu">测试tensorflow-gpu</h3><p>测试安装的tensorflow是否可用GPU，测试如下：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pyhton <span class="comment">#进入python操作环境</span></span><br><span class="line"></span><br><span class="line">import tensorflow as tf </span><br><span class="line">sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))</span><br></pre></td></tr></tbody></table></figure><p>显示如下则表示tensorflow支持的，输出如下：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">2020-09-11 08:30:54.735834: Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA</span><br><span class="line">2020-09-11 08:30:54.821023:  Successfully opened dynamic library libcuda.so.1</span><br><span class="line">2020-09-11 08:30:55.698894:  XLA service 0x5654b4f86600 executing computations on platform CUDA. Devices:</span><br><span class="line">2020-09-11 08:30:55.699000:   StreamExecutor device (0): Tesla M60, Compute Capability 5.2</span><br><span class="line">2020-09-11 08:30:55.699022:   StreamExecutor device (1): Tesla M60, Compute Capability 5.2</span><br><span class="line">2020-09-11 08:30:55.699042:   StreamExecutor device (2): Tesla M60, Compute Capability 5.2</span><br><span class="line">2020-09-11 08:30:55.699062:   StreamExecutor device (3): Tesla M60, Compute Capability 5.2</span><br><span class="line">2020-09-11 08:30:55.732911:   CPU Frequency: 2100000000 Hz</span><br><span class="line">2020-09-11 08:30:55.738953: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5654b54aa810 executing computations on platform Host. Devices:</span><br><span class="line">2020-09-11 08:30:55.739001: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;</span><br><span class="line">2020-09-11 08:30:55.741878: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: </span><br><span class="line">name: Tesla M60 major: 5 minor: 2 memoryClockRate(GHz): 1.1775</span><br><span class="line">pciBusID: 0000:b1:00.0</span><br><span class="line">2020-09-11 08:30:55.742665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: </span><br><span class="line">name: Tesla M60 major: 5 minor: 2 memoryClockRate(GHz): 1.1775</span><br><span class="line">pciBusID: 0000:b2:00.0</span><br><span class="line">2020-09-11 08:30:55.743420: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties: </span><br><span class="line">name: Tesla M60 major: 5 minor: 2 memoryClockRate(GHz): 1.1775</span><br><span class="line">pciBusID: 0000:da:00.0</span><br><span class="line">2020-09-11 08:30:55.744263: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties: </span><br><span class="line">name: Tesla M60 major: 5 minor: 2 memoryClockRate(GHz): 1.1775</span><br><span class="line">pciBusID: 0000:db:00.0</span><br><span class="line">2020-09-11 08:30:55.744692: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library <span class="string">'libcudart.so.10.0'</span>; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/dell2/.mujoco/mjpro150/bin:/usr/<span class="built_in">local</span>/cuda-10.1/lib64:</span><br><span class="line">2020-09-11 08:30:55.744798: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library <span class="string">'libcublas.so.10.0'</span>; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/dell2/.mujoco/mjpro150/bin:/usr/<span class="built_in">local</span>/cuda-10.1/lib64:</span><br><span class="line">2020-09-11 08:30:55.744891: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library <span class="string">'libcufft.so.10.0'</span>; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/dell2/.mujoco/mjpro150/bin:/usr/<span class="built_in">local</span>/cuda-10.1/lib64:</span><br><span class="line">2020-09-11 08:30:55.744980: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library <span class="string">'libcurand.so.10.0'</span>; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/dell2/.mujoco/mjpro150/bin:/usr/<span class="built_in">local</span>/cuda-10.1/lib64:</span><br><span class="line">2020-09-11 08:30:55.745070: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library <span class="string">'libcusolver.so.10.0'</span>; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/dell2/.mujoco/mjpro150/bin:/usr/<span class="built_in">local</span>/cuda-10.1/lib64:</span><br><span class="line">2020-09-11 08:30:55.745166: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library <span class="string">'libcusparse.so.10.0'</span>; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/dell2/.mujoco/mjpro150/bin:/usr/<span class="built_in">local</span>/cuda-10.1/lib64:</span><br><span class="line">2020-09-11 08:30:55.750141: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7</span><br><span class="line">2020-09-11 08:30:55.750170: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1663] Cannot dlopen some GPU libraries. Skipping registering GPU devices...</span><br><span class="line">2020-09-11 08:30:55.750542: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:</span><br><span class="line">2020-09-11 08:30:55.750706:       0 1 2 3 </span><br><span class="line">2020-09-11 08:30:55.750797:  0:   N Y Y Y </span><br><span class="line">2020-09-11 08:30:55.750887:  1:   Y N Y Y </span><br><span class="line">2020-09-11 08:30:55.750974:  2:   Y Y N Y </span><br><span class="line">2020-09-11 08:30:55.751059:  3:   Y Y Y N </span><br><span class="line">Device mapping:</span><br><span class="line">/job:localhost/replica:0/task:0/device:XLA_GPU:0 -&gt; device: XLA_GPU device</span><br><span class="line">/job:localhost/replica:0/task:0/device:XLA_GPU:1 -&gt; device: XLA_GPU device</span><br><span class="line">/job:localhost/replica:0/task:0/device:XLA_GPU:2 -&gt; device: XLA_GPU device</span><br><span class="line">/job:localhost/replica:0/task:0/device:XLA_GPU:3 -&gt; device: XLA_GPU device</span><br><span class="line">/job:localhost/replica:0/task:0/device:XLA_CPU:0 -&gt; device: XLA_CPU device</span><br><span class="line">2020-09-11 08:30:55.757190: I tensorflow/core/common_runtime/direct_session.cc:296] Device mapping:</span><br><span class="line">/job:localhost/replica:0/task:0/device:XLA_GPU:0 -&gt; device: XLA_GPU device</span><br><span class="line">/job:localhost/replica:0/task:0/device:XLA_GPU:1 -&gt; device: XLA_GPU device</span><br><span class="line">/job:localhost/replica:0/task:0/device:XLA_GPU:2 -&gt; device: XLA_GPU device</span><br><span class="line">/job:localhost/replica:0/task:0/device:XLA_GPU:3 -&gt; device: XLA_GPU device</span><br><span class="line">/job:localhost/replica:0/task:0/device:XLA_CPU:0 -&gt; device: XLA_CPU device</span><br></pre></td></tr></tbody></table></figure><p>表示tensorflow支持device：CPU：0 ，支持device：<code>GPU：0,1,2,3</code>，共4块GPU</p><p>比如CPU在TensorFlow中的名称为/cpu:0。<strong>在默认情况下，即使机器有多个CPU，TensorFlow也不会区分它们，所有的CPU都使用/cpu:0作为名称。</strong></p><p>而一台机器上不同GPU的名称是不同的，第n个GPU在TensorFlow中的名称为/gpu:n。比如第一个GPU的名称为/gpu:0，第二个GPU名称为/gpu:1，以此类推。</p><p>作者：博文视点 链接：https://www.jianshu.com/p/26ac409dfb38 来源：简书 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p><h3 id="ubuntu中查看显卡信息">Ubuntu中查看显卡信息</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lspci | grep -i vga <span class="comment">#显卡</span></span><br></pre></td></tr></tbody></table></figure><p>显示结果如下：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">03:00.0 VGA compatible controller: Matrox Electronics Systems Ltd. Integrated Matrox G200eW3 Graphics Controller (rev 04)</span><br><span class="line">b1:00.0 VGA compatible controller: NVIDIA Corporation GM204GL [Tesla M60] (rev a1)</span><br><span class="line">b2:00.0 VGA compatible controller: NVIDIA Corporation GM204GL [Tesla M60] (rev a1)</span><br><span class="line">da:00.0 VGA compatible controller: NVIDIA Corporation GM204GL [Tesla M60] (rev a1)</span><br><span class="line">db:00.0 VGA compatible controller: NVIDIA Corporation GM204GL [Tesla M60] (rev a1)</span><br></pre></td></tr></tbody></table></figure><h3 id="ubuntu中查看nvidia-gpu">Ubuntu中查看nvidia GPU</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lspci | grep -i nvidia <span class="comment">#查看gpu信息</span></span><br></pre></td></tr></tbody></table></figure><p>显示结果如下：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">b1:00.0 VGA compatible controller: NVIDIA Corporation GM204GL [Tesla M60] (rev a1)</span><br><span class="line">b2:00.0 VGA compatible controller: NVIDIA Corporation GM204GL [Tesla M60] (rev a1)</span><br><span class="line">da:00.0 VGA compatible controller: NVIDIA Corporation GM204GL [Tesla M60] (rev a1)</span><br><span class="line">db:00.0 VGA compatible controller: NVIDIA Corporation GM204GL [Tesla M60] (rev a1)</span><br></pre></td></tr></tbody></table></figure><h3 id="查看nvidia的显卡信息和使用情况">查看Nvidia的显卡信息和使用情况</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvidia-smi</span><br></pre></td></tr></tbody></table></figure><p>显示如下：</p><p><img src="https://i.loli.net/2020/09/11/OQZjHpocke5S9FE.png" alt="image-20200910221947824"></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ps aux | grep train.py #我的实验名称为train.py</span><br></pre></td></tr></tbody></table></figure><p><img src="https://i.loli.net/2020/09/11/7lujWDbiqnV6zwG.png" alt="image-20200910222011793"></p><p>可以看到，我的实验进程号是<code>21195</code>，在<code>processes</code>中可以看到使用了<code>GPU1,2</code></p><h3 id="指定gpu实验加速">指定GPU实验加速</h3><p>如果电脑有多个GPU，tensorflow默认全部使用。</p><p>如果想只使用部分GPU，可以设置<code>CUDA_VISIBLE_DEVICES</code>。</p><h4 id="命令行指定">命令行指定</h4><p>在执行python程序时，可以通过：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=1 python train.py <span class="comment">#只使用GPU1</span></span><br></pre></td></tr></tbody></table></figure><p>以下为一些使用指导：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Environment Variable Syntax      Results</span><br><span class="line"></span><br><span class="line">CUDA_VISIBLE_DEVICES=1           Only device 1 will be seen</span><br><span class="line">CUDA_VISIBLE_DEVICES=0,1         Devices 0 and 1 will be visible</span><br><span class="line">CUDA_VISIBLE_DEVICES=<span class="string">"0,1"</span>       Same as above, quotation marks are optional</span><br><span class="line">CUDA_VISIBLE_DEVICES=0,2,3       Devices 0, 2, 3 will be visible; device 1 is masked</span><br><span class="line">CUDA_VISIBLE_DEVICES=<span class="string">""</span>          No GPU will be visible1234567</span><br></pre></td></tr></tbody></table></figure><h4 id="代码中指定">代码中指定</h4><p>在Python代码中添加以下内容：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="string">"1"</span> <span class="comment">#只使用GPU1</span></span><br></pre></td></tr></tbody></table></figure><h3 id="设置tensorflow使用的显存大小">设置tensorflow使用的显存大小</h3><h4 id="定量设置显存">定量设置显存</h4><p>默认tensorflow是使用GPU尽可能多的显存（内存）。</p><p>用Tensorflow创建session的时候要注意设置内存使用情况，特别是内存资源不够而且要和别人共享一块GPU的时候（留一点给别人用）</p><p>可以通过下面的方式，来设置使用的GPU显存：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=<span class="number">0.7</span>)</span><br><span class="line">sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))</span><br></pre></td></tr></tbody></table></figure><p>上面分配给tensorflow的GPU显存大小为：GPU实际显存*0.7。 可以按照需要，设置不同的值，来分配显存。</p><h4 id="按需设置显存">按需设置显存</h4><p>上面的只能设置固定的大小。如果想按需分配，可以使用allow_growth参数</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">gpu_options = tf.GPUOptions(allow_growth=<span class="literal">True</span>)</span><br><span class="line">sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))  </span><br><span class="line"><span class="comment"># 使用allow_growth option，刚一开始分配少量的GPU容量，然后按需慢慢的增加，由于不会释放内存，所以会导致碎片</span></span><br></pre></td></tr></tbody></table></figure><p>如果一个 TensorFlow 的 operation 中兼有 CPU 和 GPU 的实现, 当这个算子被指派设备时, GPU 有优先权. 比如<code>matmul</code>中 CPU 和 GPU kernel 函数都存在. 那么在 <code>cpu:0</code> 和 <code>gpu:0</code> 中, <code>matmul</code> operation 会被指派给 <code>gpu:0</code> .</p><h4 id="记录设备指派情况">记录设备指派情况</h4><p>为了获取你的 operations 和 Tensor 被指派到哪个设备上运行, 用 <code>log_device_placement</code> 新建一个 <code>session</code>, 并设置为 <code>True</code>.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 新建一个 graph.</span></span><br><span class="line">a = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], shape=[<span class="number">2</span>, <span class="number">3</span>], name=<span class="string">'a'</span>)</span><br><span class="line">b = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], shape=[<span class="number">3</span>, <span class="number">2</span>], name=<span class="string">'b'</span>)</span><br><span class="line">c = tf.matmul(a, b)</span><br><span class="line"><span class="comment"># 新建session with log_device_placement并设置为True.</span></span><br><span class="line">sess = tf.Session(config=tf.ConfigProto(log_device_placement=<span class="literal">True</span>))</span><br><span class="line"><span class="comment"># 运行这个 op.</span></span><br><span class="line"><span class="keyword">print</span> sess.run(c)</span><br></pre></td></tr></tbody></table></figure><p>你应该能看见以下输出:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Device mapping:</span><br><span class="line">/job:localhost/replica:0/task:0/gpu:0 -&gt; device: 0, name: Tesla K40c, pci bus</span><br><span class="line">id: <span class="number">0000</span>:<span class="number">05</span>:<span class="number">00.0</span></span><br><span class="line">b: /job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/gpu:<span class="number">0</span></span><br><span class="line">a: /job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/gpu:<span class="number">0</span></span><br><span class="line">MatMul: /job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/gpu:<span class="number">0</span></span><br><span class="line">[[ <span class="number">22.</span>  <span class="number">28.</span>]</span><br><span class="line"> [ <span class="number">49.</span>  <span class="number">64.</span>]]</span><br></pre></td></tr></tbody></table></figure><h4 id="section"></h4><h3 id="gpu和cpu">GPU和CPU</h3><p>一个GPU被多个实验使用，但是如果实验超过显存大小，就会都被挂掉，会显示<code>stopped</code>字样</p><p>一个实验可以用多个GPU，但是需要更改部分代码，让其支持多GPU</p><p>不要tensorflow-gpu和tensorflow(cpu版)一起装，因为这样装有个先后顺序问题，先安装tensorflow-gpu再安装tensorflow，gpu版本直接不能用了。</p><p>如果想测试cpu和gpu版本性能的，最好创建两个python的虚拟环境，一个装tensorflow-gpu，另一个装tensorflow。</p><hr><p>在Tensorflow中使用gpu和cpu是有很大的差别的。在小数据集的情况下，cpu和gpu的性能差别不大。不过在大数据集的情况下，cpu的时间显著增加，而gpu变化并不明显。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> timeit</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cpu_run</span><span class="params">(num)</span>:</span></span><br><span class="line">  <span class="keyword">with</span> tf.device(<span class="string">'/cpu:0'</span>):</span><br><span class="line">    cpu_a=tf.random.normal([<span class="number">1</span>,num])</span><br><span class="line">    cpu_b=tf.random.normal([num,<span class="number">1</span>])</span><br><span class="line">    c=tf.matmul(cpu_a,cpu_b)</span><br><span class="line">  <span class="keyword">return</span> c</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gpu_run</span><span class="params">(num)</span>:</span></span><br><span class="line">  <span class="keyword">with</span> tf.device(<span class="string">'/gpu:0'</span>):</span><br><span class="line">    gpu_a=tf.random.normal([<span class="number">1</span>,num])</span><br><span class="line">    gpu_b=tf.random.normal([num,<span class="number">1</span>])</span><br><span class="line">    c=tf.matmul(gpu_a,gpu_b)</span><br><span class="line">  <span class="keyword">return</span> c</span><br><span class="line">k=<span class="number">10</span></span><br><span class="line">m=<span class="number">7</span></span><br><span class="line">cpu_result=np.arange(m,dtype=np.float32)</span><br><span class="line">gpu_result=np.arange(m,dtype=np.float32)</span><br><span class="line">x_time=np.arange(m)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">  k=k*<span class="number">10</span></span><br><span class="line">  x_time[i]=k</span><br><span class="line">  cpu_str=<span class="string">'cpu_run('</span>+str(k)+<span class="string">')'</span></span><br><span class="line">  gpu_str=<span class="string">'gpu_run('</span>+str(k)+<span class="string">')'</span></span><br><span class="line">  <span class="comment">#print(cpu_str)</span></span><br><span class="line">  cpu_time=timeit.timeit(cpu_str,<span class="string">'from __main__ import cpu_run'</span>,number=<span class="number">10</span>)</span><br><span class="line">  gpu_time=timeit.timeit(gpu_str,<span class="string">'from __main__ import gpu_run'</span>,number=<span class="number">10</span>)</span><br><span class="line">  <span class="comment"># 正式计算10次，取平均时间</span></span><br><span class="line">  cpu_time=timeit.timeit(cpu_str,<span class="string">'from __main__ import cpu_run'</span>,number=<span class="number">10</span>)</span><br><span class="line">  gpu_time=timeit.timeit(gpu_str,<span class="string">'from __main__ import gpu_run'</span>,number=<span class="number">10</span>)</span><br><span class="line">  cpu_result[i]=cpu_time</span><br><span class="line">  gpu_result[i]=gpu_time</span><br><span class="line"></span><br><span class="line">print(cpu_result)</span><br><span class="line">print(gpu_result)</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.set_xscale(<span class="string">"log"</span>)</span><br><span class="line">ax.set_adjustable(<span class="string">"datalim"</span>)</span><br><span class="line">ax.plot(x_time,cpu_result)</span><br><span class="line">ax.plot(x_time,gpu_result)</span><br><span class="line">ax.grid()</span><br><span class="line">plt.draw()</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="https://i.loli.net/2020/09/11/tRxAb2wY5qKGF4D.png" alt="在这里插入图片描述"> 蓝线是cpu的耗时，而红线是gpu的耗时。</p><p>更多gpu内容可参考</p><blockquote><p><a href="https://docs.pythontab.com/tensorflow/how_tos/using_gpu/" target="_blank" rel="noopener">tensorflow官方文档，使用 GPUs</a></p><p><a href="https://www.cnblogs.com/nxf-rabbit75/p/10639833.html" target="_blank" rel="noopener">Tensorflow检验GPU是否安装成功 及 使用GPU训练注意事项</a></p><p><a href="https://www.jianshu.com/p/26ac409dfb38" target="_blank" rel="noopener">TensorFlow：实战Google深度学习框架（第2版）:GPU加速</a></p></blockquote><h3 id="tensorflow匹配的关系">tensorflow匹配的关系</h3><p><img src="https://i.loli.net/2020/09/13/FqJ1cXThMzKHvA5.png" alt="image-20200913144848843"></p><p><img src="E:\myBlog\source_posts\FqJ1cXThMzKHvA5.png"></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      记录加速过程以及知识点
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-09-07-实验日志</title>
    <link href="http://yoursite.com/2020/09/07/2020-09-07-%E5%AE%9E%E9%AA%8C%E6%97%A5%E5%BF%97/"/>
    <id>http://yoursite.com/2020/09/07/2020-09-07-%E5%AE%9E%E9%AA%8C%E6%97%A5%E5%BF%97/</id>
    <published>2020-09-07T12:44:43.000Z</published>
    <updated>2020-10-02T13:11:18.491Z</updated>
    
    <content type="html"><![CDATA[<h3 id="虚拟环境配置">虚拟环境配置</h3><h4 id="笔记本">笔记本</h4><table><thead><tr class="header"><th>名称</th><th>配置</th><th>用处</th></tr></thead><tbody><tr class="odd"><td>sijian36</td><td>tf 1.9.0</td><td>普通跑实验</td></tr><tr class="even"><td>python714</td><td>tf 1.14.0</td><td>RKN</td></tr><tr class="odd"><td>ronghe</td><td>tf 1.14.0</td><td>transformer和RKN</td></tr></tbody></table><h4 id="r740服务器">R740服务器</h4><p>cuda-10.2</p><table><thead><tr class="header"><th>名称</th><th>配置</th><th>用处</th></tr></thead><tbody><tr class="odd"><td>sijian1</td><td>tf 1.13.0</td><td>一般实验</td></tr><tr class="even"><td>pytorth030</td><td>torch 0.3</td><td>哈佛torch版transformer</td></tr><tr class="odd"><td>lsjRKN</td><td>tf 1.14.0</td><td>RKN</td></tr><tr class="even"><td>ronghe</td><td>tf 1.14.0</td><td>transformer和RKN</td></tr><tr class="odd"><td>ronghe6</td><td>tf-gpu1.14.0</td><td>gpu加速融合</td></tr></tbody></table><h3 id="日志">日志</h3><h4 id="section"><strong>2020-09-06</strong></h4><h5 id="主要内容">主要内容</h5><p>笔记本的RKN实验</p><p>跑通是在RKNmaster文件跑</p><hr><p>哈佛torch版transformer实验</p><p>R740中 LSJ/annotated-transformer1/main5.py(pytorch030)</p><p>是之前上传到R740跑的实验</p><p>LSJ/annotated-transformer是前一阵为了将函数改成直通型流程而从笔记本上上传的</p><h5 id="出现问题">出现问题</h5><p>在R740跑RKN实验</p><p><code>attributeerror: module 'tensorflow.keras.initializers'  has no attribute 'normal'</code> 解决</p><p>将RKN.py 77行 normal 改为 <strong>RandomNormal</strong> 还是出错</p><p>再次出错 keep.dim出错</p><p>修改 将keep.dim=True参数去掉 再运行</p><p>​ <img src="https://i.loli.net/2020/09/08/h7Ma3lm82eFSuWX.png" alt="img"></p><ul><li>运行结果是没有tf.matrix_band_part 这个参数，于是百度发现，</li><li>新版本：tf.matrix_band_part变成tf.linalg.band_part 于是修改再运行</li></ul><p>运行结果显示</p><p>​ <img src="https://i.loli.net/2020/09/08/h7Ma3lm82eFSuWX.png" alt="img"></p><p>于是百度，原因是</p><p>The image from your input pipeline is of type 'uint8', you need to type cast it to 'float32', You can do this after the image jpeg decoder:</p><p>以下更改，在RKN.py中插入h = tf.cast(h, tf.float32)</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def _prop_through_layers(inputs, layers):</span><br><span class="line"></span><br><span class="line">​    """propagates inputs through layers"""</span><br><span class="line"></span><br><span class="line">​    h = inputs</span><br><span class="line"></span><br><span class="line">h = tf.cast(h, tf.float32)</span><br><span class="line"></span><br><span class="line">​    for layer in layers:</span><br><span class="line"></span><br><span class="line">​        h = layer(h)</span><br><span class="line"></span><br><span class="line">​    return h</span><br></pre></td></tr></tbody></table></figure><p>还是报错</p><p><strong>放弃使用sijian1 以及刚刚对RKNmaser的修改</strong></p><p>将笔记本中的RKNmaster 复制为rknmas上传到R740 名字为<strong>rknmas</strong></p><p>参考了笔记本中的虚拟环境，在R740新建lsjRKN的虚拟环境，<strong>tf版本为1.14 python：3.6</strong></p><p>可以跑通实验</p><p>实验可以在R740跑起来，但是为什么论文作者的github代码上tensorflow版本是1.13 不好使，但是在tensorflow1.14就可以跑起来？？？？</p><p>在笔记本上跑的 设置epoch=5</p><p>​ <img src="https://i.loli.net/2020/09/08/h7Ma3lm82eFSuWX.png" alt="img"></p><h4 id="section-1"><strong>2020-09-07</strong></h4><h5 id="主要内容-1">主要内容</h5><p>配置transformer和RKN融合的实验虚拟环境 测试代码</p><p>下载的是<a href="https://github.com/Kyubyong/transformer" target="_blank" rel="noopener">Kyubyong/transformer</a> 代码，准备融合RKN</p><p>具体的配置如下：</p><hr><p><strong>Requirements</strong></p><ul><li>python==3.x (Let's move on to python 3 if you still use python 2)</li><li>tensorflow==1.12.0</li><li>numpy&gt;=1.15.4</li><li>sentencepiece==0.1.8</li><li>tqdm&gt;=4.28.1 #显示进度条的包</li></ul><hr><p>github下载代码，放到<code>C:\Users\Administrator\PycharmProjects</code>目录下，文件名为 <code>transformer-master</code></p><p><code>python714</code>是可以运行RKN的，在笔记本上，根据<code>python714</code> clone了<code>ronghe</code> ，并添加所需要的包</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda install tqdm</span><br><span class="line"></span><br><span class="line">pip install  sentencepiece==0.1.8 <span class="comment"># conda 安装出错 ，于是用pip安装</span></span><br></pre></td></tr></tbody></table></figure><h5 id="出现问题-1"><strong>出现问题</strong></h5><p>此代码不是官方代码，虽然可以实现transformer，但是使用的数据集是小型的<code>IWSLT 2016 de-en</code>，而不是transformer论文中使用的大型数据集WMT，但是官方代码又很难读，而且有很多用不到的接口</p><p>在纠结，要用目前的代码进行融合，还是用官方的代码呢？</p><p>问过师兄，现在还是不用官方的transformer代码，就用目前的代码，只是验证，不用管实验数据集，先将现在的代码结合RKN再说</p><h4 id="section-2"><strong>2020-09-08</strong></h4><h5 id="主要内容-2">主要内容</h5><p>阅读整理RKN的代码</p><p>将昨天的transformer数据集无法读取的问题解决</p><p>将RKN在R740上跑，并保存在<code>test1.txt</code>文件中，可以用<code>less</code> 查看</p><h5 id="遇到问题">遇到问题</h5><p>RKN代码读的一脸懵</p><p>transformer代码bug还未修复 <span class="github-emoji" style="display:inline;vertical-align:middle;color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f62d.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f62d.png?v8">😭</span></p><h4 id="section-3"><strong>2020-09-09</strong></h4><h5 id="主要内容-3">主要内容</h5><p>在R740新建环境<code>ronghe</code>，根据虚拟环境<code>lsjRKN</code>来建的</p><p>第三方包也安装成功</p><h5 id="遇到问题-1"><strong>遇到问题</strong></h5><ol type="1"><li></li><li>添加上encoding='ascii',error='ignore'就可以解决</li></ol><p><img src="https://i.loli.net/2020/09/11/vCfMGWJ975VEiRl.png" alt="image-20200909094144959"></p><h6 id="注">注</h6><p>在解决完之后，一定要看报错的位置，可能这个已解决，但是其它相同的问题不同位置也会报错，同样解决就可以了</p><ol start="2" type="1"><li>在笔记本上跑此实验，发现内存不够，超出内容超过10%</li></ol><p><code>Allocation of 1196032000 exceeds 10% of system memory</code></p><p><strong>解决</strong></p><p>减少<code>banch_size</code> ， 但是还是超出，但是应该是在现有环境下实验可以跑通的，于是想着在R740上跑</p><p>在R740跑<code>prepro.py</code>实验，如下输出，并<code>INFO：done</code> （表示完成）</p><p><img src="https://i.loli.net/2020/09/11/BAb9krnJ8dCYKTX.png" alt="image-20200909132740936"></p><p>开始跑<code>train.py</code> 并将输出保存在train99.txt中（9月9日）</p><p><img src="https://i.loli.net/2020/09/11/7UpXhTBcnGNILtH.png" alt="image-20200909134011883"></p><p>这个WARNING是什么意思呢？</p><p>猜想：源代码需要的是<code>tf1.12</code>版本 我配置的是<code>tf 1.14</code>版本，不知道是不是这个原因 。晚上回寝百度一下</p><ol start="3" type="1"><li>在740中跑的太慢了，不知道具体原因。在看源代码进行修改</li></ol><h4 id="section-4">2020-09-10</h4><h5 id="主要内容-4">主要内容</h5><p>更改虚拟环境，可以使用GPU对实验进行加速</p><p>阅读transformer的代码，明天融合</p><p>对跑实验的一些warning都已经修改了，复制项目名字为<code>transformer-mas</code></p><p>上传到R740中，命名<code>transformer-mas</code></p><h5 id="遇到问题-2">遇到问题</h5><p><img src="https://i.loli.net/2020/09/11/HUMOjGrYSiZlPo5.png" alt="image-20200910200948426" style="zoom:200%;"></p><p>新建<code>ronghe3</code>虚拟环境</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda install python==3.6.10  <span class="comment">#这样可以 ，但是为什么 python==3.6.1 和  python==3.6.0 是不可以的呢？？</span></span><br><span class="line">pip install tensorflow-gpu==1.13.1</span><br></pre></td></tr></tbody></table></figure><p>在已经安装了tensorflow-gpu的<code>ronghe3</code>基础上，克隆了<code>ronghe4</code>，进行接下来的操作</p><h6 id="注-1">注</h6><p>如果执行<code>conda install tensorflow==1.13.1</code></p><p>安装错误 ，导入不了tensorflow-gpu，应该是和CUDA版本不匹配</p><p><img src="https://i.loli.net/2020/09/11/CgXZf5vQzdODpyU.png" alt="image-20200910211644840"></p><h5 id="参考">参考</h5><p><a href="https://www.tensorflow.org/install/gpu?hl=zh-cn" target="_blank" rel="noopener">tensorflow官方，GPU 支持</a></p><p><code>ronghe5</code></p><p><code>pip install tensorflow-gpu==1.12.0</code></p><p>还是跑不通</p><p><code>ronghe6</code></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda install python==3.6.10  <span class="comment">#这样可以 ，但是为什么 python==3.6.1 和  python==3.6.0 是不可以的呢？？</span></span><br><span class="line">pip install tensorflow-gpu==1.14.0</span><br></pre></td></tr></tbody></table></figure><p>终于可以跑通了，不会报错了！！！</p><p>测试安装的tensorflow是否可用GPU，可以使用。测试如下：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pyhton <span class="comment">#进入python操作环境</span></span><br><span class="line"></span><br><span class="line">import tensorflow as tf </span><br><span class="line">sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))</span><br></pre></td></tr></tbody></table></figure><p>RKN实验跑完了，保存在test2.txt中</p><blockquote><p>tensorflow-gpu 1.5版本及以上要求CUDA版本为9.0</p><p>tensorflow-gpu 1.3及以上版本要求cudnn版本为V6及以上</p></blockquote><h4 id="section-5">2020-09-11</h4><h5 id="主要内容-5">主要内容</h5><p>解决在linux显示图形的问题</p><p>解决transformer实验报错</p><h5 id="遇到问题-3">遇到问题</h5><ol type="1"><li>用xshell在服务器linux端只能显示控制台输出，如果想要显示图像，比如<code>matplotlib</code>包，则要下载<code>xmanage</code></li></ol><p>由于需要收费，没有下载</p><p><img src="https://i.loli.net/2020/09/11/zkqMvhElw2ubg9U.png" alt="image-20200911212759446"></p><p>解决方法： 可以用<code>plt.savafig</code>保存到服务器，再保存在本地笔记本</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line">matplotlib.use(<span class="string">'Agg'</span>)</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.font_manager <span class="keyword">import</span> FontProperties</span><br><span class="line">X = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</span><br><span class="line">dataY = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>]</span><br><span class="line">plt.xlabel(<span class="string">"x轴"</span>);</span><br><span class="line">plt.ylabel(<span class="string">"y轴"</span>);</span><br><span class="line">plt.savefig(<span class="string">"./lisijian.png"</span>,dpi=<span class="number">100</span>) <span class="comment">#保存在本文件夹下的lisijian.png</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p>报错<code>_tkinter.TclError: couldn't connect to display "localhost:32.0"</code></p><p>原因： 问题在于，您使用的是一个交互式后端，它试图为您创建图形窗口，但由于您断开了启动模拟时可用的x服务器，所以失败了。</p><p>解决方法：使用非交互式后端(请参见<a href="https://matplotlib.org/faq/usage_faq.html#what-is-a-backend" target="_blank" rel="noopener">后端</a>？)比如：Agg(用于Png格式，PDF, SVG或PS。在生成图形的脚本中，只需在import matplotlib.pyplot as plt之前调用matplotlib.use(）即可</p><p>比如<code>matplotlib.use('Agg')</code></p><ol start="2" type="1"><li><p>在transformer实验中，才开始没注意，今天才发现有一个错误，如下:</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">AssertionError: Bad argument number <span class="keyword">for</span> Name: 3, expecting 4</span><br></pre></td></tr></tbody></table></figure></li></ol><p>解决方法：因为对结果的影响不可观,所以就没去在意 ,后面发现用其他docker并没有多少问题,而且每次都出现一堆warning很影响美观性,于是百度准备解决这个问题</p><p><strong>后来发现是有个gast的库版本太高,导致不兼容的问题,降级gast即可解决</strong></p><p>使用pip进行降级</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install --user gast==0.2.2</span><br></pre></td></tr></tbody></table></figure><p><strong>待解决：</strong></p><p><strong>tensorflow的兼容性问题 cuda的兼容性问题 ？？</strong></p><p><strong>一般如果要对服务器上的实验进行更改的话，怎能会简单一些？？</strong></p><h4 id="section-6">2020-09-13</h4><h5 id="主要内容-6">主要内容</h5><p>解决transformer报错的问题</p><p>解决tensorflow目前不支持CUDA10.1的问题</p><p>修改：</p><p>将batch 由 128 改为 32</p><p>将maxlen1 和maxlen2 由100改为101</p><h5 id="遇到问题-4">遇到问题</h5><ol type="1"><li><p>在运行transformer代码的时候，程序报错如下（部分内容，具体参考<code>train911.txt</code>）：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">"train.py"</span>, line 81, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    hypotheses = get_hypotheses(num_eval_batches, num_eval_samples, sess, y_hat, m.idx2token)</span><br><span class="line">  File <span class="string">"/home/dell2/LSJ/transformer-master/utils.py"</span>, line 144, <span class="keyword">in</span> get_hypotheses</span><br><span class="line">    h = sess.run(tensor)</span><br><span class="line">  File <span class="string">"/home/dell2/anaconda3/envs/ronghe6/lib/python3.6/site-packages/tensorflow/python/client/session.py"</span>, line 950, <span class="keyword">in</span> run</span><br><span class="line">    run_metadata_ptr)</span><br><span class="line">  File <span class="string">"/home/dell2/anaconda3/envs/ronghe6/lib/python3.6/site-packages/tensorflow/python/client/session.py"</span>, line 1173, <span class="keyword">in</span> _run</span><br><span class="line">    feed_dict_tensor, options, run_metadata)</span><br><span class="line">  File <span class="string">"/home/dell2/anaconda3/envs/ronghe6/lib/python3.6/site-packages/tensorflow/python/client/session.py"</span>, line 1350, <span class="keyword">in</span> _do_run</span><br><span class="line">    run_metadata)</span><br><span class="line">  File <span class="string">"/home/dell2/anaconda3/envs/ronghe6/lib/python3.6/site-packages/tensorflow/python/client/session.py"</span>, line 1370, <span class="keyword">in</span> _do_call</span><br><span class="line">    raise <span class="built_in">type</span>(e)(node_def, op, message)</span><br><span class="line">tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[111,100] = 100 is not <span class="keyword">in</span> [0, 100)</span><br><span class="line"> [[node encoder_1/positional_encoding/embedding_lookup (defined at /home/dell2/LSJ/transformer-master/modules.py:290) ]]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Original stack trace <span class="keyword">for</span> <span class="string">'encoder_1/positional_encoding/embedding_lookup'</span>:</span><br><span class="line">  File <span class="string">"train.py"</span>, line 48, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    y_hat, eval_summaries = m.eval(xs, ys)</span><br><span class="line">  File <span class="string">"/home/dell2/LSJ/transformer-master/model.py"</span>, line 176, <span class="keyword">in</span> <span class="built_in">eval</span></span><br><span class="line">    memory, sents1, src_masks = self.encode(xs, False)</span><br><span class="line">  File <span class="string">"/home/dell2/LSJ/transformer-master/model.py"</span>, line 53, <span class="keyword">in</span> encode</span><br><span class="line">    enc += positional_encoding(enc, self.hp.maxlen1)</span><br><span class="line"></span><br><span class="line">  File <span class="string">"/home/dell2/LSJ/transformer-master/modules.py"</span>, line 290, <span class="keyword">in</span> positional_encoding</span><br><span class="line">     outputs = tf.nn.embedding_lookup(position_enc, position_ind)</span><br></pre></td></tr></tbody></table></figure><p>可以追溯到位置编码部分，出现了<code>InvalidArgumentError: indices[111,100] = 100 is not in [0, 100)</code>的错误</p><p>于是在我将超参数maxlen由100改为101，可以正常运行</p><h4 id="参考-1">参考</h4></li><li><p>在rognhe6中安装的tensorflow-gpu：1.14是不支持CUDA10.1版本的，只支持到CUDA10.0版本。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">tf.test.is_gpu_available()</span><br></pre></td></tr></tbody></table></figure><p>输出如下：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pciBusID: 0000:db:00.0</span><br><span class="line">2020-09-13 09:32:43.541828: Could not dlopen library <span class="string">'libcudart.so.10.0'</span>; </span><br><span class="line">dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory;</span><br><span class="line"></span><br><span class="line">False</span><br></pre></td></tr></tbody></table></figure></li></ol><p>可见是不支持目前ubuntu中的CUDA环境，参考了博客，修改如下：</p><p>将<code>cudatoolkit=10.0</code>安装到当前环境下</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install cudatoolkit=10.0</span><br></pre></td></tr></tbody></table></figure><p>问题解决</p><h5 id="参考-2">参考</h5><blockquote><p><a href="https://blog.csdn.net/qq_28193019/article/details/103146116" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/qq_28193019/article/details/103146116</a></p><p><a href="https://zhuanlan.zhihu.com/p/115611908" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/115611908</a></p></blockquote><ol start="3" type="1"><li>可以继续跑实验，可以用GPU，但是还是出现了一些问题</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Resource exhausted: OOM when allocating tensor with shape[1024,98,64] and <span class="built_in">type</span> <span class="built_in">float</span> on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc</span><br></pre></td></tr></tbody></table></figure><p>显示内存不够，于是我将batch_size从128改为32 ，可以正常运行了</p><p>或者可以考虑使用多个GPU呢？</p><h5 id="参考-3">参考</h5><blockquote><p><a href="https://blog.csdn.net/Will_Ye/article/details/89878588" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/Will_Ye/article/details/89878588</a></p><p><a href="https://blog.csdn.net/Will_Ye/article/details/89878588" target="_blank" rel="noopener">OOM ResourceExhaustedError 的完美解决方法</a></p></blockquote><h3 id="section-7">2020-09-13</h3><p>transformer-mas训练部分已经跑了8个epoch，只用了一个GPU，跑的有点慢，于是暂停，以后再跑。</p><p>开始跑test.py文件，但是在跑的时候，<code>TypeError: stat: path should be string, bytes, os.PathLike or integer, &gt; not NoneType</code></p><p>路径写的不对，在ckpt中添加路径即可</p><h3 id="section-8">2020-09-29</h3><p>利用最新的ckpt进行测试，显示的是</p><p>想着可能最新的epoch的图和数据没有完全写入文件中，所以我在log/1文件夹中将最新的ckpt删除了，让次新的ckpt来进行测试。</p><p>发现结果还是unk ，不知道为什么？ 是不是因为我一个epoch保存了多个ckpt</p><h3 id="section-9">2020-09-30</h3><p>今天的实验终于解决了，可以有好的结果了。这段时间真的太煎熬了。不过还是学到了不少东西。</p><p>之前修改的其它地方是没有问题的，不需要再变，是在epoch</p><p>计划以及疑问：</p><p>如何锁死进程， 多个的话，会显示显存不足</p><p>为什么必须要跑完才能显示结果呢？ 在哪体现的呢</p><p>平时想要快速测试代码是否好用？ 有什么办法</p><p>哪些人tensorflow用的好，以后经常请教</p><p>模型验证的作用是啥？ 在代码中没有体现出来啊</p><p>总结遇到的困难以及学到的知识</p><p>解决onetab保存的标签</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      记录在跑实验的一些配置以及遇到的问题解决，保持更新
    
    </summary>
    
    
    
      <category term="故障排除" scheme="http://yoursite.com/tags/%E6%95%85%E9%9A%9C%E6%8E%92%E9%99%A4/"/>
    
  </entry>
  
  <entry>
    <title>2020-09-06-学习工作杂记</title>
    <link href="http://yoursite.com/2020/09/06/2020-09-06-%E5%AD%A6%E4%B9%A0%E5%B7%A5%E4%BD%9C%E6%9D%82%E8%AE%B0/"/>
    <id>http://yoursite.com/2020/09/06/2020-09-06-%E5%AD%A6%E4%B9%A0%E5%B7%A5%E4%BD%9C%E6%9D%82%E8%AE%B0/</id>
    <published>2020-09-06T06:38:39.000Z</published>
    <updated>2020-09-19T14:02:42.167Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言">前言</h3><p>多做总结，提高效率，不要拖延</p><h3 id="怎样从熬夜中恢复过来">怎样从熬夜中恢复过来</h3><p>1． 不要打盹，5min不能够得到休息</p><p>2． 吃早餐 一个小时内吃早餐（全谷物和蛋白质） ，可以充满活力，认知能力可以提高，最好不摄入糖，会让人发困</p><p>3． 出去走。自然光可以让身体发热</p><p>4． 开始办公时候，喝一杯咖啡</p><p>5． 工作：首先完成最困难的部分，最开始的几个小时是一天中效率最高的时候</p><p>6． 会议之前可以喝一杯咖啡，有效时间是半小时</p><p>7． 午饭不吃过多的糖，会犯困</p><p>8． 下午可以喝一杯咖啡这时候是最困的时候。三点之后不能摄入咖啡，有效时间7 小时</p><p>9． 下午可以做一下简单的事情</p><h3 id="在家寝室学习">在家&amp;寝室学习</h3><ol type="1"><li><p>行为影响态度。换掉睡衣，接近类似学校的状态。希望自己成为什么样子， 就穿成什么样子</p></li><li></li></ol><p><img src="https://i.loli.net/2020/09/06/QDutkAwlLGgKTBo.png"></p><ol start="3" type="1"><li>先做一道题再说。（计划太多，无从下手。过分犹豫）不要想太多，直接动手</li></ol><p><img src="https://i.loli.net/2020/09/06/CerZI164jAU85qO.png"></p><ol start="4" type="1"><li>拒绝含糖食物</li></ol><p>自控力需要能量的供给，学习前可以吃块糖，可以补充能量。但是减少高gi食物，如酸奶果汁薯片，或者碳水类食物（米饭面条土豆）。多吃瘦肉、蔬菜、水果，能够增强自控力，还能瘦</p><ol start="5" type="1"><li><p>不要用时间做计划，用学习量做计划。（因为会拖延）拒绝整点学习、计划学习时间，采用今天背多少单词等，一个清晰明确的目标会事半功倍</p></li><li><p>保持工作区的整洁，不放无关的物品。使手机飞行模式、黑白模式</p></li></ol><p><img src="https://i.loli.net/2020/09/06/UO5JydTCmLIKa12.png"></p><h3 id="戒掉手机避免用意志力来克制">戒掉手机（避免用意志力来克制）</h3><p>1． 替代法 并不是真正想做，而是习惯了某种行为。可以买一个手机模型，终止大脑的无意识行为，给大脑一个选择的机会</p><p>2． 心理暗示。‘我不玩手机‘ 而不是’我不能玩手机‘</p><p>3． 优化环境。环境的影响很大。搭建一个良好的环境。睡觉前把手机放在客厅，学习时增加获得手机的难度</p><p>4． 负面反馈。人们对于损失和负面事件的敏感度高于正面事件的敏感</p><p>5． 看实时学习视频，看到别人学习 自己也不好意思玩</p><p>休息放空自己，会使得注意力更集中</p><p>把社交软件放在小文件夹里再放到手指不容易碰到的地方，如果一段时间又习惯了点这个位置的社交软件，就再更换桌面排布</p><h3 id="自己习惯">自己习惯</h3><p>对于我自己来说，习惯睡觉前进行一些文字记录的工作，比如写博客做总结，就是不会再去接触一些新知识。把第二天要做的事情列好，或者直接找好第二天最难工作内容的参考资料，对第二天工作内容有一个大概的印象，这样第二天一早就可以直攻克艰难的部分，避免其它琐碎的事情</p><p>起床的时候，提前找好第二天要穿的衣服，同时可以适量补充水分</p><p>在进行学习的时候，先设置5分钟，休息5分钟，再逐渐增加时间，进入状态，多学习时间不休息</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      之前看的一个b站up主关于如何高效学习的视频，觉得受到了启发，于是记录了下来，以此找到更适合自己的学习习惯
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-09-06-不确定性研究</title>
    <link href="http://yoursite.com/2020/09/06/2020-09-06-%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7%E7%A0%94%E7%A9%B6/"/>
    <id>http://yoursite.com/2020/09/06/2020-09-06-%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7%E7%A0%94%E7%A9%B6/</id>
    <published>2020-09-05T16:06:55.000Z</published>
    <updated>2020-10-10T14:04:27.458Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言">前言</h3><p>近日，旷视上海研究院长危夷晨在将门技术社群做了一次题为《Uncertainty Learning for Visual Recognition》(不确定性学习在视觉计算中的应用) Online Talk，共分为4个部分：</p><ol type="1"><li>Preliminary（基础知识）</li><li>Uncertainty in Deep Learning（深度学习中的不确定性问题）</li><li>Uncertainty in Computer Vision（不确定性的计算机视觉应用）</li><li>Summary（总结）</li></ol><p>我主要参考了前三部分的内容</p><h3 id="基础知识">基础知识</h3><h4 id="何为不确定性估计">何为不确定性估计</h4><p>要理解何为不确定性估计，我们可以先从<strong>确定性预测（deterministic prediction）</strong>开始。假设要对两张人脸进行对比，验证是否是同一个人的照片，那么可以使用人脸识别系统分别对这两张人脸图片提取特征，并使用某种度量指标衡量所提取的两个特征的相似程度，根据所预测出的相似程度来判断两张人脸图像是否从属同一个人。如果相似度很高（比如95%），则可以判断这两张人脸属于同一个人。这种通过预测一个确定性的人脸特征用来判断的方式被称为确定性预测（deterministic prediction）。</p><p>然而这个<strong>相似度分数并不总是有效</strong>，以下图中第二个例子为例，可以看到在输入图像中，一张非常清晰，另一张十分模糊，然而这个时候人脸识别系统依然给二者打出很高的相似度分数，那么面对这种情况，我们是否要相信系统给出的答案，我们是否有办法来判断系统给出这个分数的可靠程度？</p><p>为此，人们提出了另一个<strong>辅助判断的指标</strong>，即判断机器给出的答案是否可信，可信程度多少的分数被称为<strong>confidence score（置信度分数）</strong>。如下图第二行中，系统给出相似度95%，然而confidence score却只有10%，表明<strong>系统给出的相似度分数的可信度很低，因此我们在采纳系统给出的这个判断答案的时候需要十分谨慎。</strong></p><p>从这个案例可以知道，<strong>在confidence score分数背后存在一个核心思想，即很多时候机器学习系统给出的判断不一定是靠谱的，即，系统对于给出的判断具有一定程度的“不确定性”。</strong>那么此时人们就需要知道系统给出这个判断到底有几成把握，因此我们需要诸如置信度分数或者“不确定性”分数这样的额外信息来帮助我们做出更好的决策。</p><p><img src="https://i.loli.net/2020/09/06/xBqNOnrsRiM7o85.jpg" alt="img"></p><h4 id="为何不确定性重要">为何不确定性重要</h4><p>上面介绍完之后，我们再来谈谈它为什么重要。简单来讲，不确定性估计在深度学习之中有着广泛的应用场景，为其落地发挥着不可替代的重要作用，下面讲一些比较要代表性的场景：</p><ol type="1"><li><strong>高风险应用场景</strong>。这类场景<strong>需要非常精确的估计</strong>，因为一旦估计错误，可能出现严重的后果，例如医疗图像诊断、自动驾驶。</li><li><strong>大量机器学习场景</strong>。比如，在主动学习（Active Learning）这种技术框架中，模型需要确定哪些样本更值得被打标签。这也涉及到系统对于估计样本“价值程度”不确定性。同时，研究人员往往也会发现单纯使用机器学习系统进行判断时，会存在少量样本系统无法做出很好的判断，因此这时人们会邀请专家来标记这部分困难样本，以训练模型。</li><li><strong>强化学习</strong>。强化学习由于经常要权衡exploration和exploitation操作，因此如何确定每一台机器的概率分布是否被准确估计，就是对这台机器模型参数的不确定性估计。</li><li><strong>对处于训练数据分布之外情况的检测</strong>。由于很多时候测试数据并不在训练数据中，因此如果测试数据超出了训练数据的数据分布，那这样的预测是没有准确度可言的，这时候就需要一个额外的不确定性估计来确认对当前的预测有多大把握。</li></ol><h4 id="两种不确定性">两种不确定性</h4><p>接下来，我们界定一下不确定性的分类问题。一般来讲，不确定性可以分为两类：</p><ol type="1"><li><strong>数据的不确定性</strong>：也被称为偶然（Aleatoric）不确定性，它描述的是<strong>数据中内在的噪声，即无法避免的误差，这个现象不能通过增加采样数据来削弱。</strong>例如有时候拍照的手稍微颤抖画面便会模糊，这种数据是不能通过增加拍照次数来消除的。因此解决这个问题的方法一般是提升数据采集时候的稳定性，或者提升衡量指标的精度以囊括各类客观影响因素。</li><li><strong>模型的不确定性</strong>：也被称为认知（Epistemic）不确定性。它指出，<strong>模型自身对输入数据的估计可能因为训练不佳、训练数据不够等原因而不准确，与某一单独的数据无关</strong>。因此，认知不确定性测量的，是训练过程本身所估计的模型参数的不确定性。这种不确定性是可以通过有针对性的调整（增加训练数据等方式）来缓解甚至解决的。</li></ol><h3 id="深度学习中的不确定性问题">深度学习中的不确定性问题</h3><p><strong>如果单看深度学习网络本身，它是确定性的，例如简单的多层前馈网络，在训练好以后，其结构、权重以及对某一个样本所输出类别的概率都是确定的。因此，在深度神经网络中引入不确定性的一个方法就是引入贝叶斯法则，从而得到贝叶斯神经网络（BNN）。</strong></p><p>简单而言，如下图，贝叶斯神经网络的<strong>权重不像普通神经网络是一个具体数值，而是一个概率分布，表示每一个权重w遵循一个分布，而非之前是一个确定的数值</strong>。因此在训练和推理中，网络的权重会变化，<strong>根据分布来随机采样</strong>。通过这种方法可以建模各个参数本身存在的不确定性。</p><p><img src="https://i.loli.net/2020/09/06/6IRjbGw4cfT8mSB.jpg" alt="img"></p><p>然而，由于在实际应用中参数量十分巨大，要严格根据贝叶斯公式计算后验概率几乎不现实，因此为了将网络应用于大型数据集，就<strong>需要高效的近似计算方法</strong>。早期比较有名的方法是通过马尔科夫链蒙特卡洛采样法（MCMC-sampling）来逼近假定的参数分布，但是由于这种方法很慢，因此发展出了一系列更好的<strong>近似计算后验概率</strong>的方法，如下：</p><h4 id="变分推断">变分推断</h4><p>变分推断的基本方法就是<strong>引入变分分布对BNN优化过程中涉及到的后验概率进行近似估计，这种方法较为高效。</strong></p><p><img src="https://i.loli.net/2020/09/06/nQzKO2i1PYNkvuU.jpg"></p><h4 id="dropoutbnnvi">Dropout=BNN+VI</h4><p><img src="https://i.loli.net/2020/09/06/aQRWjzwDKIJl6eL.jpg" alt="img"></p><p>这种<strong>dropout方法</strong>也称为蒙特卡洛dropout，进一步简化了对后验概率分布的近似计算，它认为常见的dropout技术实际上等于在贝叶斯网络中进行变分推断。通过上图的对比，我们可以直观理解标准神经网络经过dropout之后，在每一层随机取消一些神经元，把连接变稀疏的网络是什么样子。</p><p>可以证明，<strong>在假设每一个神经元都服从一个离散的伯努利分布的情况下，经dropout方法处理的神经网络的优化过程实际上等价于在一个贝叶斯网络中进行变分推断。</strong>由于这种结构中每个节点的权重是被多个子网络共享的，因此它的训练和推理相对高效。这项理论成果近年来得到了较多的应用。</p><p>我们在前向传播的时候，让某个神经元的激活值以<strong>一定的概率p停止工作</strong>（每一个批次都是随机），这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征。</p><p><strong>dropout掉不同的隐藏神经元就类似在训练不同的网络，随机删掉一半隐藏神经元导致网络结构已经不同，整个dropout过程就相当于对很多个不同的神经网络取平均。而不同的网络产生不同的过拟合，一些互为“反向”的拟合相互抵消就可以达到整体上减少过拟合。</strong></p><h5 id="dropout具体工作流程">Dropout具体工作流程</h5><p>假设我们要训练这样一个神经网络，如图所示。</p><p><img src="https://i.loli.net/2020/09/06/SetbpsjYxEX1yQZ.jpg" alt="标准的神经网络"></p><p>输入是x输出是y，正常的流程是：我们首先把x通过网络前向传播，然后把误差反向传播以决定如何更新参数让网络进行学习。使用Dropout之后，过程变成如下：</p><p>（1）首先<strong>随机（临时）</strong>删掉网络中一半（dropout=0.5时）的隐藏神经元，输入输出神经元保持不变（图中虚线为部分临时被删除的神经元）</p><p><img src="https://i.loli.net/2020/09/06/GnirX4u39lSmyUg.jpg" alt="部分临时被删除的神经元"></p><p>（2） 然后把输入x通过修改后的网络前向传播，然后把得到的损失结果通过修改的网络反向传播。一小批训练样本执行完这个过程后，在<strong>没有被删除的神经元上</strong>按照随机梯度下降法更新对应的参数（w，b）。</p><p>（3）然后继续重复这一过程：</p><ul><li><strong>恢复被删掉的神经元</strong>（此时被删除的神经元保持原样，而没有被删除的神经元已经有所更新）</li><li>从隐藏层神经元中<strong>随机选择</strong>一个一半大小的子集临时删除掉（备份被删除神经元的参数）。</li><li>对一小批训练样本，先前向传播然后反向传播损失并根据随机梯度下降法更新参数（w，b） （没有被删除的那一部分参数得到更新，删除的神经元参数保持被删除前的结果）。</li></ul><p>不断重复这一过程。</p><h5 id="参考">参考</h5><blockquote><p><a href="https://zhuanlan.zhihu.com/p/38200980" target="_blank" rel="noopener">深度学习中Dropout原理解析</a></p></blockquote><h4 id="模型融合">模型融合</h4><p>这也是一种进行不确定性估计的基本方法，其大致思路是，<strong>从一个数据集中进行多次随机采样，分别训练模型，然后再将这些模型的推理结果综合，其均值作为预测结果，方差作为预测的不确定性。</strong>另外需要强调的是，蒙特卡洛dropout可以认为是一种特殊的模型融合方法。</p><p><img src="https://i.loli.net/2020/09/06/noXFIR2ACtwpmk1.jpg" alt="img"></p><h3 id="回归问题中的数据不确定性">回归问题中的数据不确定性</h3><p>这是一种数据估计的标准做法。<strong>给定输入x_i，解一个函数f(x_i)，使得它逼近ground truth y_i。假设这个函数f(x_i)遵循一个高斯分布，那么其均值就是y_i，方差就是σ（也依赖于x_i）。</strong></p><p><strong>这时，如果对这个高斯分布取似然度，再取负的log值，那么就可以得到下图中的损失函数L。因此在优化的时候，除了希望优化f(x_i)逼近y_i，同时也需要优化σ(x_i)，它表示这个高斯分布的不确定性，σ越大越不确定。</strong></p><p>因此当f很容易逼近y的时候，那么公式中第一项L2范数就会很小，这时即便σ也小，但结果依然不会很大；当f很难逼近y，即f很难学习的时候，第一项中的L2范数就会很大，这时优化过程就会使得σ也变大，从而使得整个第一项减小，因此学到的σ会随着数据学习的难度做自我调整。</p><p><img src="https://i.loli.net/2020/09/06/nlD62kW1pXygAV4.jpg" alt="img"></p><h4 id="简单例子">简单例子</h4><p>我们借助一个直观例子来理解模型不确定性与数据不确定性。首先这里的<strong>ground truth函数为一个正弦函数</strong>，即图中橙色的点是测试数据，而<strong>训练数据是从[-5，+5]区间采样的蓝色点，研究人员对每一个蓝色点都添加了高斯噪声，因此可以看到这些蓝色点明显偏离ground truth。</strong></p><p>下方左图是用贝叶斯网络加dropout进行的<strong>模型不确定性估计</strong>。<strong>红色曲线为估计出来的预测值，延其上下分布的黄色面积则为每一个点对应的方差</strong>。在进行模型不确定性估计时，系统会对每个输入点估计多次，每次会随机采样模型的权重，以求出对每个输入点多次预测所得到的均值和方差。可以发现，蓝色点区域之外的部分预测的方差很大，这是因为模型没有见过这样的数据。（<strong>因为蓝色是训练数据</strong>，其它是测试数据，没见过的，所以方差就会较大，也就是不确定性较高）</p><p>下方右图中红色曲线为估计出来的预测值，是<strong>数据不确定性估计</strong>，曲线上下的黄色跨度就是每一个点通过数据不确定性估计方法所学出的方差。可以发现，原本输入数据中有噪声的部分，其预测出的方差比较大，反映出模型对这样的输出拥有较大的不确定性。</p><p><img src="https://i.loli.net/2020/09/06/EjFQnPko6pVhYH5.jpg" alt="img"></p><h3 id="不确定性的计算机视觉应用">不确定性的计算机视觉应用</h3><p><img src="https://i.loli.net/2020/09/06/Gw6pNAvuisDe18a.png" alt="img"></p><p>尽管不确定性在机器学习中已经有很长历史，但是直到2017年（就我所知）随着NeurlPS论文<em>What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision</em>的提出，它才开始真正应用在基于深度学习的视觉预测任务中。这篇论文本身没有太多方法创新，通过将已知的方法 用于语义分割与深度回归任务，取得了不错的结果。<strong>通过在模型中引入不确定性估计的已有理论成果，使得原本任务的精度得到了显著提升。</strong></p><p>通过论文给出的定性结果可以较为直观的理解模型不确定性和数据不确定性。如下图，系统估计出来的不确定性是有明确含义即很容易理解的，图中上半部分做语义分割，下半部分做深度估计。</p><p><img src="https://picb.zhimg.com/80/v2-cf9cf0d7715af33b38dc8fa8b71b8aa0_1440w.jpg" alt="img"></p><p><strong>整张图的第4、5列分别是数据不确定性和模型不确定性的结果。红色部分表示不确定程度较大，蓝色部分表示较为确定。从数据不确定性结果（第4列）可以看到，红色部分往往出现在物体边界处，表示这些区域的像素更加具有二义性，系统不太清楚这部分像素究竟属于前景还是背景，另外这部分信息在训练数据中（即ground truth）往往也较模糊。可以发现，系统给出的数据不确定结果符合人类直观理解。</strong></p><p><strong>从模型不确定性结果（第5列）可以看到，模型对出现人的部分给出了很高的不确定性，这是因为模型在训练中很少遇到人的数据，因此模型很难估计出人所处位置的深度，将该区域标记为高度不确定。</strong></p><h4 id="物体检测中的数据不确定性">物体检测中的数据不确定性</h4><p><img src="https://i.loli.net/2020/09/06/RrKDqx7kFG8mJyl.jpg" alt="img"></p><p>在物体检测任务中，很大一部分不确定性来源于标注数据的不确定。上图给出了几个典型例子，可以看到，在标注边界框的时候，由于存在各种物体角度、遮挡，所以往往很难评价一个边界框标注的好坏。由于标注规则不一、数据本身存在的各种不确定性，因此具有二义性的数据标注会导致具有二义性的学习结果，从而将不确定性引入了模型，进一步输出结果也是不确定的。</p><p>针对这个问题，有研究人员在CVPR 2019、ICCV 2019提出了两篇颇有价值的论文，其核心思想类似，将每一个边界框的4个坐标均认为呈高斯分布，然后分别估计其均值和方差。用上述介绍的数据不确定性回归公式来替代传统的L1损失，将原来所需要预测的4个变量扩充为8个变量。</p><p>因此，这种方法除了可以估计边界框每一个坐标之外，还让它们都带有了一个不确定性参数。利用这些不确定性数据，可以进一步做很多事情（比如在NMS中作为权重来对边界框位置进行投票）。</p><h4 id="人脸识别中的模型不确定性">人脸识别中的模型不确定性</h4><p><img src="https://i.loli.net/2020/09/06/nWDBsXI2dNfaHTV.jpg" alt="img"></p><p>对于在人脸识别任务中如何估计模型不确定性，推荐大家上图中的论文工作，其核心思想是，将BNN+dropout用到人脸识别任务中，如图所示，dropout层（红色）被加在每一个卷积block之后，从而构建了一个蒙特卡洛Dropout网络。在训练过程中，每当流程到达这些层的时候，就会随机丢掉一些神经元，从而实现模拟参数分布的效果。在测试过程中，每一个图像都会经过该网络多次，进而可以多这些结果估计均值与方差，将方差作为预测结果的不确定性。</p><h4 id="人脸识别中的数据不确定性pfe方法">人脸识别中的数据不确定性：PFE方法</h4><p><img src="https://i.loli.net/2020/09/06/qPMvAkWN1UHK2e9.jpg" alt="img"></p><p>PFE方法全称为Probabilistic Face Embeddings，其核心思想是用概率分布来替代传统确定的人脸嵌入特征。传统的方法会将输入图像映射到一个高维空间中，得到一个表示特征的向量，然而对于这些方法而言，输出的向量都是确定的，因此被称为deterministic embedding。PFE引入了不确定性，将输出向量认为是一个概率分布，而不再是一个确定的向量，它有一个均值μ、方差σ。</p><p>均值代表对图像的嵌入，方差描述了模型对输入数据的不确定性。该方法希望同时估计μ和σ，并且σ能够根据输入图像的质量、包含噪声的程度进行自适应的学习。在上图右方的示例中可以看到，每一个输出的特征z不再是一个点，而是一个高斯形状的概率分布（椭圆），椭圆的大小就描述了估计的不确定性。</p><p><img src="https://i.loli.net/2020/09/06/O8mvS1VTIHodxZ5.png" alt="img"></p><p>从具体实现方法来看，PFE的创新值得借鉴，它并不直接去估计每一个均值μ，而是通过一个事先已经训练好的人脸识别模型来抽取每个样本的特征μ_i，然后研究人员再在网络中加入一个小分支，来对每个样本输出一个方差（比如假设μ_i是一个512维的向量，那么此时也会输出一个对μ_i的每一维度单独估计方差的512维方差向量）。</p><p>进一步，论文提出了一种新的metric——mutual likelihood score（MLS），来衡量两个分布间的距离。上图公式中x_i和x_j是两个样本在特定空间中的高斯分布，两个分布所得到的MLS数值就代表了其相似度。在训练过程中，针对所有positive 样本，计算负的MLS数值作为损失，并最小化该损失目标函数，进而可以估计新增加分支（估计方差的分支）的参数。</p><p><img src="https://picb.zhimg.com/80/v2-b2a72843a1b2199fb213df0e93e9d570_1440w.jpg" alt="img"></p><p>上图是论文对方差的解释，较为直观。可以发现红框标注出来的（方差超过一定阈值）图片都是姿态有较大变化、模糊、或者有遮挡的图片，系统都认为它们识别起来有较大不确定性；而正面、高清的图片不确定性普遍较小。为了进一步验证学习出来的不确定性是否能够有效解释图像质量，PFE在下方左图中进行了有关在低质量图像之间使用传统cosine相似度计算是否可靠的研究。</p><p>研究人员对清晰图片添加了不同程度的噪声，蓝色线代表原图与模糊图之间的相似度分数，而红色代表两张来自不同ID的图随模糊程度的增加所计算的相似度。可以发现对于同一ID（蓝线），随着模糊程度增加，相似度也逐渐降低；而对于不同ID，随着模糊程度增加相似度却在增加。这说明依据该相似度可能会将两张来自不同ID的模糊图像错认为是同一张图。这一现象在其它很多论文中也同样被观测到。</p><p><img src="https://picb.zhimg.com/80/v2-e38487eb964f4dc56b7ba89689ea5158_1440w.jpg" alt="img"></p><p>然而在经过PFE论文提出的MLS相似度修正之后，情况得到了很大改善。如右图，当图片模糊度增加时，对同样ID的图来说，其相似度没有变得太小，而不同ID图像的相似度也没有变得太大。这个实验证明这种计算图像相似度的新metric在面对低质量图片时更加鲁棒。</p><h4 id="pfe方法的缺陷">PFE方法的缺陷</h4><p>虽然PFE方法取得了重要进展，但是缺点也很明显，因为它并没有学习身份特征（identity-feature），每一个identity的特征嵌入是确定的，PFE只是增加了一个估计方差的小网络分支，这导致必须用一个新的metric（即MLS）来计算所有样本对的距离。而使用MLS这个度量函数带来的缺陷在实际工业应用中是代价较高的：第一，我们需要额外存储方差向量；第二，相比传统的余弦相似度，MLS相似度的计算资源消耗也更大。</p><p>受此启发，我们团队在投递给CVPR 2020的新论文中不仅做到了估计方差，同时也能更新每个样本的特征。下图为传统方法、PFE与我们团队方法的对比。</p><p><img src="https://i.loli.net/2020/09/06/pYSInMZ9R64euEC.jpg" alt="img"></p><p>可以发现，在图（a）中，虚线框出的蓝色椭圆代表一个类别，圈外存在一个正样本和负样本，而对于传统相似度计算方法来说，很难将负样本和正样本区分开来；而（b）中PFE方法对每个样本估计了一个分布，在带有分布的特征表示下，利用MLS就能够有效将正样本和负样本区分开来，但是PFE中正负样本本身是确定的；在（c）中，我们团队方法能够在估计正负样本方差的同时，也让特征本身修正得更好。</p><p><img src="https://i.loli.net/2020/09/06/ayVXfSxm1DYk5dG.png" alt="img"></p><p>上图是三种方法的对比，可以看到在最后计算相似度的时候，由于特征本身经过了调整，只需要使用cosine相似度来计算两个均值向量就可以得出答案。具体而言，我们团队提出了两种实现方法，如下：</p><h3 id="法1从头学习一个分类模型"><strong>法1：从头学习一个分类模型</strong></h3><p><img src="https://i.loli.net/2020/09/06/yZWVciHGd8bwK74.jpg" alt="img"></p><p>这种方法的主要部分与通用识别模型的结构一致，区别在于，在输出特征的位置，我们让模型输出一个有关每个样本特征的均值μ，以及一个方差σ。进一步，对于每个样本的每一次迭代而言，都随机采样一个ε（如上图最下方）。</p><p>通过这种方式得到的新样本特征s_i就是遵从均值μ、方差为σ的高斯分布采出的值，它可以模拟一个服从高斯分布的特征。通过这种简单的重新采样的技巧，就可以很好进行模型训练。在测试的时候不再需要采样，仅需要将已经得到的均值μ作为特征来计算相似度即可。</p><p><img src="https://i.loli.net/2020/09/06/8PcKyqBlErgUQAm.png" alt="img"></p><p>该方法的损失函数除了包含softmax以及其一切合理变种之外，还有一个KL损失，它使得每一个学出来特征的分布尽可能逼近单位高斯分布。这个损失项的引入来自于2016年一篇名为<em>Deep variational information bottleneck</em>的论文。进一步整个损失函数就可以用标准SGD方法来优化。下图解释了整个损失函数中softmax与kl损失是如何起到平衡的作用的。</p><p><img src="https://i.loli.net/2020/09/06/9wxoyQuj8M2AWem.jpg" alt="img"></p><h3 id="法2从现有模型出发学习回归模型"><strong>法2：从现有模型出发学习回归模型</strong></h3><p><img src="https://i.loli.net/2020/09/06/k8rDhcJY5ZaMQnU.jpg" alt="img"></p><p>这种方法假设输出的特征μ遵循高斯分布，目的是让它逼近期望的特征w。与PFE类似，假设输入的模型已经固定，且输出的特征μ属于类别c，则让μ逼近这个类别c的特征中心w_c（w_c来自事先训练好的人脸分类模型）。这种方法适用于当已经有一个训练好的模型，但依然希望做不确定性估计的情况。相对于PFE而言，它多做了样本特征的学习。下图解释了该损失函数中σ起到的平衡作用。</p><p><img src="https://i.loli.net/2020/09/06/3qWYVXC4QIx1vh8.png" alt="img"></p><p><strong>实验结果：</strong>在三种损失函数上的对比测试结果显示，我们团队提出的分类方法（HUM_cls）在最困难的数据集IJB-C（具有大量模糊、噪声图像）上效果最佳；在LFW、CFP-FP、YTF这些较成熟的数据集上我们提出的两种方法同其他方法区别不大；在较困难的MegFace(R1)数据集上我们团队的分类方法效果最佳。</p><p><img src="https://i.loli.net/2020/09/06/9e3k4ImOhET5Wgo.jpg" alt="img"></p><p>下图展示了在三种数据集上学习出来的方差分布情况，展示了位于不同方差位置的图像的样子。</p><p><img src="https://i.loli.net/2020/09/06/81X3Pk4o9IHZTxE.jpg" alt="img"></p><p>进一步，我们团队使用了ResNet-64作为backbone（与PFE的SOTA模型backbone深度一致），来将本文方法同SOTA方法在最困难的数据集IJB-C上进行性能对比，结果显示在每一个指标上我们团队方法均实现了领先。为了测试本文方法对噪声信息干扰的鲁棒性，团队对图片人工施加了高斯噪声（从0到40%），可以发现，当噪声越明显的时候，本文引入的不确定估计方法的优越性也约高。</p><p><img src="https://i.loli.net/2020/09/06/4VLqFkPgIvaYSm2.jpg" alt="img"></p><h3 id="参考-1">🚀参考</h3><blockquote><p><a href="https://zhuanlan.zhihu.com/p/95774787" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/95774787</a></p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      深度学习模型的不确定性估计，摘自几篇不错的博客
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-09-05-知识点杂</title>
    <link href="http://yoursite.com/2020/09/05/2020-09-05-%E7%9F%A5%E8%AF%86%E7%82%B9%E6%9D%82/"/>
    <id>http://yoursite.com/2020/09/05/2020-09-05-%E7%9F%A5%E8%AF%86%E7%82%B9%E6%9D%82/</id>
    <published>2020-09-05T11:56:22.000Z</published>
    <updated>2020-10-14T00:55:14.792Z</updated>
    
    <content type="html"><![CDATA[<h3 id="对数似然">对数似然</h3><p>最大化对数似然，因此值越大越好。例如，对数似然值 -3 比 -7 好。</p><p>对数为负值是完全可能的，如下图log函数</p><p><img src="https://i.loli.net/2020/09/12/UFihCbxJk9gBfuz.png" alt="对数- 维基百科，自由的百科全书" style="zoom: 33%;"></p><h3 id="高斯分布中考虑对数似然而不是似然">高斯分布中考虑对数似然而不是似然</h3><p>通过最大似然函数来确定高斯分布中未知参数的值，实际上，<strong>最大化似然函数的对数更方便</strong>。因为对数是其论证的单调递增函数，函数的对数的最大化等价于函数本身的最大化。logaithm不仅简化了后续的数学分析，而且还有助于数学计算，<strong>因为大量小概率的乘积很容易使计算机的数值精度下降，但是log就可以通过计算总和来解决</strong>。</p><ol type="1"><li>当要计算随机变量的joint likelihood时很有用，他们之间独立，并且分布相同。</li></ol><p><img src="https://i.loli.net/2020/09/12/jAqSrXz7v39mE2p.png" alt="image-20200905213103449"></p><p>联合概率是所有点的概率的乘积：</p><p><img src="https://i.loli.net/2020/09/12/2sf4RyCm7IVxFpt.png" alt="image-20200905213136550"></p><p><strong>如果是log，则只需要求和即可</strong></p><ol start="2" type="1"><li>由于是<strong>高斯分布</strong>，使用log避免了计算指数</li></ol><p><img src="https://i.loli.net/2020/09/12/pbN4OgBAyoEWv2Y.png" alt="image-20200905213239871"></p><p>可以写成：</p><p><img src="https://i.loli.net/2020/09/12/YHpEnC951eJfvkj.png" alt="image-20200905213249229"></p><ol start="3" type="1"><li>ln x是单调递增的函数，因此log-likelihood和likelihood有相同的关系</li></ol><p><img src="https://i.loli.net/2020/09/12/1NHzhmdvcIZEkQl.png" alt="image-20200905213303446"></p><p><strong>负对数似然</strong>是一种用于解决分类问题的 损失函数 ，它是似然函数得一种自然对数形式，可用于测量两种概率分布之间的相似性，其取负号是为了让最大似然值和最小损失相对应，是最大似然估计及相关领域的常见函数形式。</p><p>机器学习中，习惯用优化 算法 求最小值，因此会用到负对数似然，这是分类问题中的常见的损失函数，且能拓展到 多分类 问题。</p><h3 id="负对数似然和似然估计">负对数似然和似然估计</h3><p><strong>负对数似然</strong>是一种用于解决分类问题的 损失函数 ，它是似然函数的一种自然对数形式，可用于测量两种概率分布之间的相似性，其取负号是为了让最大似然值和最小损失相对应，是最大似然估计及相关领域的常见函数形式。</p><p>机器学习中，习惯用优化 算法 求最小值，因此会用到负对数似然，这是分类问题中的常见的损失函数，且能拓展到 多分类 问题。</p><h3 id="最大似然估计">最大似然估计</h3><blockquote><p><a href="https://zhuanlan.zhihu.com/p/32803109" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/32803109</a></p></blockquote><h3 id="epochiterationbatch_size">Epoch、Iteration、Batch_size</h3><p><a href="https://blog.csdn.net/program_developer/article/details/78597738" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/program_developer/article/details/78597738</a></p><h3 id="tf.tile用法">tf.tile()用法</h3><p><a href="https://blog.csdn.net/tsyccnh/article/details/82459859" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/tsyccnh/article/details/82459859</a></p><h3 id="dataset-api-和-iterator">Dataset API 和 Iterator</h3><p>Dataset API 和 Iterator</p><p><a href="https://blog.csdn.net/briblue/article/details/80962728" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/briblue/article/details/80962728</a></p><p>TensorFlow中的Dataset API</p><p><a href="https://blog.csdn.net/dqcfkyqdxym3f8rb0/article/details/79342369" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/dqcfkyqdxym3f8rb0/article/details/79342369</a></p><p>TensorFlow data模块详解</p><p><a href="https://www.weaf.top/posts/cd5ba0c4/" target="_blank" rel="noopener" class="uri">https://www.weaf.top/posts/cd5ba0c4/</a></p><p>使用Tensorflow的DataSet和Iterator读取数据</p><p><a href="https://www.jianshu.com/p/bcff8a99b15b" target="_blank" rel="noopener" class="uri">https://www.jianshu.com/p/bcff8a99b15b</a></p><p>tensorflow数据读取机制（附代码）</p><p><a href="https://zhuanlan.zhihu.com/p/27238630" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/27238630</a></p><p>Dataset API入门教程</p><p><a href="https://zhuanlan.zhihu.com/p/30751039" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/30751039</a></p><p>Dataset.from_generator</p><p><a href="https://blog.csdn.net/foreseerwang/article/details/80572182" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/foreseerwang/article/details/80572182</a></p><p>看个简单的示例：</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">#创建一个Dataset对象</span><br><span class="line">dataset = tf.data.Dataset.from_tensor_slices([1,2,3,4,5,6,7,8,9])</span><br><span class="line"></span><br><span class="line">#创建一个迭代器</span><br><span class="line">iterator = dataset.make_one_shot_iterator()</span><br><span class="line"></span><br><span class="line">#get_next()函数可以帮助我们从迭代器中获取元素</span><br><span class="line">element = iterator.get_next()</span><br><span class="line"></span><br><span class="line">#遍历迭代器，获取所有元素</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">   for i in range(9):</span><br><span class="line">       print(sess.run(element))</span><br></pre></td></tr></tbody></table></figure><p>以上打印结果为：1 2 3 4 5 6 7 8 9</p><p>from_generator</p><p>创建Dataset由其生成元素的元素generator。</p><p>函数形式：from_generator(generator,output_types,output_shapes=None,args=None)</p><p>参数generator:一个可调用对象，它返回支持该iter()协议的对象 。如果args未指定，generator则不得参数; 否则它必须采取与有值一样多的参数args。 参数output_types：tf.DType对应于由元素生成的元素的每个组件的对象的嵌套结构generator。 参数output_shapes:tf.TensorShape 对应于由元素生成的元素的每个组件的对象 的嵌套结构generator 参数args:tf.Tensor将被计算并将generator作为NumPy数组参数传递的对象元组。</p><p>具体例子</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">#定义一个生成器</span><br><span class="line">def data_generator():</span><br><span class="line">    dataset = np.array(range(9))</span><br><span class="line">    for i in dataset:</span><br><span class="line">        yield i</span><br><span class="line"></span><br><span class="line">#接收生成器，并生产dataset数据结构</span><br><span class="line">dataset = tf.data.Dataset.from_generator(data_generator, (tf.int32))</span><br><span class="line"></span><br><span class="line">iterator = concat_dataset.make_one_shot_iterator()</span><br><span class="line"></span><br><span class="line">element = iterator.get_next()</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">   for i in range(3):</span><br><span class="line">       print(sess.run(element))</span><br></pre></td></tr></tbody></table></figure><p>以上代码运行结果：0 1 2</p><h3 id="strip-和-split">strip() 和 split()</h3><p><a href="https://blog.csdn.net/hjxu2016/article/details/78676859" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/hjxu2016/article/details/78676859</a></p><h3 id="summary用法--tensorborad可视化">Summary用法 -tensorborad可视化</h3><p><a href="https://www.cnblogs.com/lyc-seu/p/8647792.html" target="_blank" rel="noopener" class="uri">https://www.cnblogs.com/lyc-seu/p/8647792.html</a></p><h3 id="math.ceil">math.ceil()</h3><p><a href="https://www.runoob.com/python/func-number-ceil.html" target="_blank" rel="noopener" class="uri">https://www.runoob.com/python/func-number-ceil.html</a></p><h3 id="format-格式化函数">.format() 格式化函数</h3><p><a href="https://www.runoob.com/python/att-string-format.html" target="_blank" rel="noopener" class="uri">https://www.runoob.com/python/att-string-format.html</a></p><h3 id="tf.shapea-和-a.get_shape.as_list-和-tf.split">tf.shape(A) 和 A.get_shape().as_list() 和 tf.split()</h3><p><a href="https://www.itread01.com/content/1544436557.html" target="_blank" rel="noopener" class="uri">https://www.itread01.com/content/1544436557.html</a></p><p><a href="https://blog.csdn.net/xc_zhou/article/details/85632109" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/xc_zhou/article/details/85632109</a></p><ul><li>tf.shape(A) # 獲取張量A（陣列，list, tensor張量）的大小，返回的是一個list</li><li>x.get_shape()，只有<strong>tensor</strong>才可以使用這種方法，返回的是一個元組</li><li>tf.split(dimension, num_split, input)：dimension的意思就是輸入張量的哪一個維度，如果是0就表示對第0維度進行切割。num_split就是切割的數量，如果是2就表示輸入張量被切成2份，每一份是一個列表。</li></ul><h3 id="tf.range">tf.range()</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">w=tf.range(<span class="number">3</span>)</span><br><span class="line"><span class="keyword">print</span> (sess.run(w))<span class="comment">#输出[0 1 2]</span></span><br></pre></td></tr></tbody></table></figure><h3 id="os.path">os.path（）</h3><table><thead><tr class="header"><th style="text-align: left;">方法</th><th style="text-align: left;">说明</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;">os.path.abspath(path)</td><td style="text-align: left;">返回绝对路径</td></tr><tr class="even"><td style="text-align: left;">os.path.basename(path)</td><td style="text-align: left;">返回文件名</td></tr><tr class="odd"><td style="text-align: left;">os.path.join(path1[, path2[, ...]])</td><td style="text-align: left;">把目录和文件名合成一个路径</td></tr><tr class="even"><td style="text-align: left;">os.path.dirname(path)</td><td style="text-align: left;">返回文件路径</td></tr><tr class="odd"><td style="text-align: left;">os.path.exists(path)</td><td style="text-align: left;">如果路径 path 存在，返回 True；如果路径 path 不存在，返回 False。</td></tr><tr class="even"><td style="text-align: left;">os.path.split(path)</td><td style="text-align: left;">把路径分割成 dirname 和 basename，返回一个元组</td></tr></tbody></table><blockquote><p><a href="https://www.runoob.com/python/python-os-path.html" target="_blank" rel="noopener" class="uri">https://www.runoob.com/python/python-os-path.html</a></p></blockquote><h3 id="embedding_lookup">embedding_lookup()</h3><p>tf.nn.embedding_lookup()就是根据input_ids中的id，寻找embeddings中的第id行。比如input_ids=[1,3,5]，则找出embeddings中第1，3，5行，组成一个tensor返回。</p><blockquote><p><a href="https://www.jianshu.com/p/7bb87873f89e" target="_blank" rel="noopener" class="uri">https://www.jianshu.com/p/7bb87873f89e</a></p><p><a href="https://www.zhihu.com/question/52250059" target="_blank" rel="noopener" class="uri">https://www.zhihu.com/question/52250059</a></p></blockquote><h3 id="模型保存和加载">模型保存和加载</h3><p>Saver的作用是将我们训练好的模型的参数保存下来，以便下一次继续用于训练或测试；Restore的用法是将训练好的参数提取出来。</p><p>1.Saver类训练完后，是以<strong>checkpoints文件形式</strong>保存。提取的时候也是从checkpoints文件中恢复变量。 Checkpoints文件是一个二进制文件，它把变量名映射到对应的tensor值 。</p><p>2.通过for循环，Saver类可以自动的生成checkpoint文件。这样我们就可以<strong>保存多个训练结果</strong>。例如，我们可以保存每一步训练的结果。但是为了避免填满整个磁盘，<strong>Saver可以自动的管理Checkpoints文件</strong>。例如，我们可以指定保存最近的N个Checkpoints文件。</p><h3 id="tensorflow模型保存和读取tf.train.saver">Tensorflow模型保存和读取tf.train.Saver</h3><p>目标：训练网络后想保存训练好的模型，以及在程序中读取以保存的训练好的模型。</p><p>首先，保存和恢复都需要实例化一个 tf.train.Saver。</p><blockquote><p>saver = tf.train.Saver()</p></blockquote><p>然后，在训练循环中，定期调用 saver.save() 方法，向文件夹中写入包含了当前模型中所有可训练变量的 checkpoint 文件。</p><blockquote><p>saver.save(sess, save_path, global_step=step)</p></blockquote><p>之后，就可以使用 saver.restore() 方法，重载模型的参数，继续训练或用于测试数据。</p><blockquote><p>saver.restore(sess, save_path)</p></blockquote><p>模型的恢复用的是restore()函数，它需要两个参数restore(sess, save_path)，save_path指的是保存的模型路径。我们可以使用<code>tf.train.latest_checkpoint（）</code>来自动获取最后一次保存的模型。如：</p><figure class="highlight javascript"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model_file=tf.train.latest_checkpoint(<span class="string">'ckpt/'</span>)</span><br><span class="line">saver.restore(sess,model_file)</span><br></pre></td></tr></tbody></table></figure><p>一次 saver.save() 后可以在文件夹中看到新增的四个文件，</p><p><img src="https://i.loli.net/2020/09/29/kRYmSZn8BbwJ4NK.png" alt="image-20200929102459806"></p><p>实际上每调用一次保存操作会创建后3个数据文件并创建一个检查点（checkpoint）文件，简单理解就是权重等参数被保存到 .ckpt.data 文件中，以字典的形式；图和元数据被保存到 .ckpt.meta 文件中，可以被 tf.train.import_meta_graph 加载到当前默认的图。</p><p>saver.restore()时填的文件名，因为在saver.save的时候，每个checkpoint会保存三个文件，如 <code>my-model-10000.meta</code>, <code>my-model-10000.index</code>, <code>my-model-10000.data-00000-of-00001</code></p><p>在<code>import_meta_graph</code>时填的就是<code>meta</code>文件名，我们知道权值都保存在my-model-10000.data-00000-of-00001这个文件中，但是如果在restore方法中填这个文件名，就会报错，应该填的是前缀，这个前缀可以使用<code>tf.train.latest_checkpoint(checkpoint_dir)</code>这个方法获取。</p><p>下面代码是简单的保存和读取模型：（不包括加载图数据）</p><figure class="highlight javascript"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"> </span><br><span class="line">#用numpy产生数据</span><br><span class="line">x_data = np.linspace(-1,1,300)[:, np.newaxis] #转置</span><br><span class="line">noise = np.random.normal(<span class="number">0</span>,<span class="number">0.05</span>, x_data.shape)</span><br><span class="line">y_data = np.square(x_data)<span class="number">-0.5</span>+noise</span><br><span class="line"> </span><br><span class="line">#输入层</span><br><span class="line">x_ph = tf.placeholder(tf.float32, [None, <span class="number">1</span>])</span><br><span class="line">y_ph = tf.placeholder(tf.float32, [None, <span class="number">1</span>])</span><br><span class="line"> </span><br><span class="line">#隐藏层</span><br><span class="line">w1 = tf.Variable(tf.random_normal([<span class="number">1</span>,<span class="number">10</span>]))</span><br><span class="line">b1 = tf.Variable(tf.zeros([<span class="number">1</span>,<span class="number">10</span>])+<span class="number">0.1</span>)</span><br><span class="line">wx_plus_b1 = tf.matmul(x_ph, w1) + b1</span><br><span class="line">hidden = tf.nn.relu(wx_plus_b1)</span><br><span class="line"> </span><br><span class="line">#输出层</span><br><span class="line">w2 = tf.Variable(tf.random_normal([<span class="number">10</span>,<span class="number">1</span>]))</span><br><span class="line">b2 = tf.Variable(tf.zeros([<span class="number">1</span>,<span class="number">1</span>])+<span class="number">0.1</span>)</span><br><span class="line">wx_plus_b2 = tf.matmul(hidden, w2) + b2</span><br><span class="line">y = wx_plus_b2</span><br><span class="line"> </span><br><span class="line">#损失</span><br><span class="line">loss = tf.reduce_mean(tf.reduce_sum(tf.square(y_ph-y),reduction_indices=[<span class="number">1</span>]))</span><br><span class="line">train_op = tf.train.GradientDescentOptimizer(<span class="number">0.1</span>).minimize(loss)</span><br><span class="line"> </span><br><span class="line">#保存模型对象saver</span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line"> </span><br><span class="line">#判断模型保存路径是否存在，不存在就创建</span><br><span class="line"><span class="keyword">if</span> not os.path.exists(<span class="string">'tmp/'</span>):</span><br><span class="line">    os.mkdir(<span class="string">'tmp/'</span>)</span><br><span class="line"> </span><br><span class="line">#初始化</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    if os.path.exists('tmp/checkpoint'):         #判断模型是否存在</span><br><span class="line">        saver.restore(sess, 'tmp/model.ckpt')    #存在就从模型中恢复变量</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        init = tf.global_variables_initializer() #不存在就初始化变量</span><br><span class="line">        sess.run(init)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">        _,loss_value = sess.run([train_op,loss], feed_dict={<span class="attr">x_ph</span>:x_data, <span class="attr">y_ph</span>:y_data})</span><br><span class="line">        <span class="keyword">if</span>(i%<span class="number">50</span>==<span class="number">0</span>):</span><br><span class="line">            save_path = saver.save(sess, <span class="string">'tmp/model.ckpt'</span>)</span><br><span class="line">            print(<span class="string">"迭代次数：%d , 训练损失：%s"</span>%(i, loss_value))</span><br></pre></td></tr></tbody></table></figure><p>注：</p><ul><li>saver 的操作必须在 sess 建立后进行。</li><li>model.ckpt 必须存在给定文件夹中，‘tmp/model.ckpt’ 这里至少要有一层文件夹，否则无法保存。</li><li>恢复模型时同保存时一样，是 ‘tmp/model.ckpt’，和那3个文件名都不一样。</li></ul><p>如果不用<code>tf.train.latest_checkpoint（）</code>来自动获取最后一次保存的模型，则怎么做呢？</p><blockquote><p><a href="https://www.jianshu.com/p/7ebee4d10e49" target="_blank" rel="noopener" class="uri">https://www.jianshu.com/p/7ebee4d10e49</a></p><p><a href="https://blog.csdn.net/mylove0414/article/details/55097486" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/mylove0414/article/details/55097486</a></p></blockquote><h3 id="saver中的max_to_keep-参数">Saver中的max_to_keep 参数</h3><h3 id="keras中的timedistributed函数">keras中的TimeDistributed函数</h3><blockquote><p><a href="https://blog.csdn.net/u012193416/article/details/79477220" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/u012193416/article/details/79477220</a></p><p><a href="https://keras.io/zh/layers/wrappers/" target="_blank" rel="noopener" class="uri">https://keras.io/zh/layers/wrappers/</a></p><p><a href="https://blog.csdn.net/zh_JNU/article/details/85160379" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/zh_JNU/article/details/85160379</a></p><p><a href="https://www.cnblogs.com/CheeseZH/p/13408658.html" target="_blank" rel="noopener" class="uri">https://www.cnblogs.com/CheeseZH/p/13408658.html</a></p></blockquote><h3 id="tf.concat详解">tf.concat()详解</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.concat([tensor1, tensor2, tensor3,...], axis)</span><br><span class="line"><span class="comment"># axis=0     代表在第0个维度拼接</span></span><br><span class="line"><span class="comment"># axis=1     代表在第1个维度拼接 </span></span><br><span class="line"><span class="comment">#axis=-1 代表倒数第一个维度</span></span><br></pre></td></tr></tbody></table></figure><blockquote><p><a href="https://blog.csdn.net/leviopku/article/details/82380118" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/leviopku/article/details/82380118</a></p></blockquote><h3 id="归纳偏置inductive-bias">归纳偏置(Inductive Bias)</h3><p>在机器学习中，很多学习算法经常会对学习的问题做一些<strong>假设</strong>，这些假设就称为归纳偏置(Inductive Bias)。</p><p><strong>归纳(Induction)</strong>是自然科学中常用的两大方法之一(归纳与演绎, induction and deduction)，指的是从一些例子中寻找共性、泛化，形成一个比较通用的规则的过程；<strong>偏置(Bias)</strong>是指我们对模型的偏好。</p><p>因此，归纳偏置可以理解为，从现实生活中观察到的现象中归纳出一定的规则(heuristics)，然后对模型做一定的约束，从而可以起到“模型选择”的作用，即从假设空间中选择出更符合现实规则的模型。其实，贝叶斯学习中的“<strong>先验(Prior)</strong>”这个叫法，可能比“归纳偏置”更直观一些。</p><p>在深度学习方面也是一样。以神经网络为例，各式各样的网络结构/组件/机制往往就来源于归纳偏置。</p><p>在卷积神经网络中，我们假设特征具有局部性(Locality)的特性，即当我们把相邻的一些特征放在一起，会更容易得到“解”；在循环神经网络中，我们假设每一时刻的计算依赖于历史计算结果；还有注意力机制，也是基于从人的直觉、生活经验归纳得到的规则。</p><p>CNN的inductive bias应该是locality和spatial invariance，即空间相近的grid elements有联系而远的没有，和空间不变性（kernel权重共享）</p><p>RNN的inductive bias是sequentiality和time invariance，即序列顺序上的timesteps有联系，和时间变换的不变性（rnn权重共享）</p><h3 id="图灵完备turing-complete">图灵完备（turing complete）</h3><p>在<a href="https://link.jianshu.com?t=https%3A%2F%2Fbaike.baidu.com%2Fitem%2F%E5%8F%AF%E8%AE%A1%E7%AE%97%E6%80%A7%E7%90%86%E8%AE%BA" target="_blank" rel="noopener">可计算性理论</a>里，如果一系列操作数据的规则（如指令集、编程语言、细胞自动机）按照一定的顺序可以计算出结果，被称为图灵完备（turing complete）。</p><p>一个有图灵完备指令集的设备被定义为<a href="https://link.jianshu.com?t=http%3A%2F%2Fbaike.baidu.com%2Fitem%2F%E9%80%9A%E7%94%A8%E8%AE%A1%E7%AE%97%E6%9C%BA" target="_blank" rel="noopener">通用计算机</a>。如果是图灵完备的，它（计算机设备）有能力执行条件跳转（if、while、goto语句）以及改变内存数据。 如果某个东西展现出了图灵完备，它就有能力表现出可以模拟原始计算机，而即使最简单的计算机也能模拟出最复杂的计算机。所有的通用编程语言和现代计算机的指令集都是图灵完备的（C++ template就是图灵完备的），都能解决内存有限的问题。图灵完备的机器都被定义有无限内存，但是机器指令集却通常定义为只工作在特定的、有限数量的RAM上。</p><h3 id="shape">shape</h3><p>numpy数据的形状：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.shape()</span><br></pre></td></tr></tbody></table></figure><p>list 数据的形状：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.shape(x)</span><br></pre></td></tr></tbody></table></figure><p><strong>注：</strong>如果写<code>x.shape()</code> , 则会报错<code>ValueError: invalid literal for int() with base 10</code></p><p>torsor形状：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.get_shape()</span><br></pre></td></tr></tbody></table></figure><h3 id="keras-的-fit函数">keras 的 fit函数</h3><p>fit中以call()方法的形式来run session</p><blockquote><p><a href="https://blog.csdn.net/u012526436/article/details/102488164" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/u012526436/article/details/102488164</a></p></blockquote><h3 id="model-类继承">Model 类继承</h3><p><strong>可以通过继承 <code>Model</code> 类并在 <code>call</code> 方法中实现你自己的前向传播，以创建你自己的完全定制化的模型，</strong>（<code>Model</code> 类继承 API 引入于 Keras 2.2.0）。</p><p>这里是一个用 <code>Model</code> 类继承写的简单的多层感知器的例子：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleMLP</span><span class="params">(keras.Model)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, use_bn=False, use_dp=False, num_classes=<span class="number">10</span>)</span>:</span></span><br><span class="line">        super(SimpleMLP, self).__init__(name=<span class="string">'mlp'</span>)</span><br><span class="line">        self.use_bn = use_bn</span><br><span class="line">        self.use_dp = use_dp</span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line"></span><br><span class="line">        self.dense1 = keras.layers.Dense(<span class="number">32</span>, activation=<span class="string">'relu'</span>)</span><br><span class="line">        self.dense2 = keras.layers.Dense(num_classes, activation=<span class="string">'softmax'</span>)</span><br><span class="line">        <span class="keyword">if</span> self.use_dp:</span><br><span class="line">            self.dp = keras.layers.Dropout(<span class="number">0.5</span>)</span><br><span class="line">        <span class="keyword">if</span> self.use_bn:</span><br><span class="line">            self.bn = keras.layers.BatchNormalization(axis=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        x = self.dense1(inputs)</span><br><span class="line">        <span class="keyword">if</span> self.use_dp:</span><br><span class="line">            x = self.dp(x)</span><br><span class="line">        <span class="keyword">if</span> self.use_bn:</span><br><span class="line">            x = self.bn(x)</span><br><span class="line">        <span class="keyword">return</span> self.dense2(x)</span><br><span class="line"></span><br><span class="line">model = SimpleMLP()</span><br><span class="line">model.compile(...)</span><br><span class="line">model.fit(...)</span><br></pre></td></tr></tbody></table></figure><p>网络层定义在 <code>__init__(self, ...)</code> 中，前向传播在 <code>call(self, inputs)</code> 中指定。在 <code>call</code> 中，你可以指定自定义的损失函数，通过调用 <code>self.add_loss(loss_tensor)</code> （就像你在自定义层中一样）。</p><p>在类继承模型中，模型的拓扑结构是由 Python 代码定义的（而不是网络层的静态图）。这意味着该模型的拓扑结构不能被检查或序列化。因此，以下方法和属性<strong>不适用于类继承模型</strong>：</p><ul><li><code>model.inputs</code> 和 <code>model.outputs</code>。</li><li><code>model.to_yaml()</code> 和 <code>model.to_json()</code>。</li><li><code>model.get_config()</code> 和 <code>model.save()</code>。</li></ul><p><strong>关键点</strong>：为每个任务使用正确的 API。<code>Model</code> 类继承 API 可以为实现复杂模型提供更大的灵活性，但它需要付出代价（比如缺失的特性）：它更冗长，更复杂，并且有更多的用户错误机会。如果可能的话，尽可能使用函数式 API，这对用户更友好。</p><blockquote><p><a href="https://blog.csdn.net/qq_27825451/article/details/90517036" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/qq_27825451/article/details/90517036</a></p></blockquote><h3 id="关于tensorflow的sessiontensorshape等基础知识整理">关于tensorflow的session、tensor、shape等基础知识（整理）</h3><p>在tensorflow程序中，tensor只是占位符，在会话层没有run出tensor的值之前，我们是无法获知tensor的值的</p><blockquote><p><a href="https://blog.csdn.net/jiongnima/article/details/78524551" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/jiongnima/article/details/78524551</a></p><p><a href="https://www.tensorflow.org/guide/tensor?hl=zh-cn" target="_blank" rel="noopener" class="uri">https://www.tensorflow.org/guide/tensor?hl=zh-cn</a></p><p><a href="https://www.jianshu.com/p/75a903a44cf2" target="_blank" rel="noopener" class="uri">https://www.jianshu.com/p/75a903a44cf2</a></p></blockquote><h3 id="tf.layers.flatten">tf.layers.flatten</h3><p>在保留第0轴的情况下对输入的张量进行Flatten(扁平化)</p><p>代码示例：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x=tf.placeholder(shape=(<span class="literal">None</span>,<span class="number">4</span>,<span class="number">4</span>),dtype=<span class="string">'float32'</span>)</span><br><span class="line"></span><br><span class="line">y=tf.layers.flatten(x)</span><br><span class="line"></span><br><span class="line">print(y)</span><br></pre></td></tr></tbody></table></figure><p>输出： 将后两维进行合并</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Tensor("flatten/Reshape:0", shape=(?, 16), dtype=float32)</span><br></pre></td></tr></tbody></table></figure><h3 id="tf.layers.dense">tf.layers.dense</h3><p>全连接层 ，相当于添加一个层。只<strong>改变输入的最后一维</strong></p><h3 id="python---tensorflow中-none-1和之间的区别">python - Tensorflow中 None，-1和？之间的区别</h3><p><code>None</code>表示未指定的维度。因此，如果您定义了一个占位符，您可以使用<code>None</code>来表示“这个维度可以有任何大小”。 占位符可以有多个<code>None</code>维度这仅仅意味着多个维度可以是不同的大小甚至整个形状都可以<code>None</code>来指定未知的维数。 <code>-1</code>是TensorFlow的一条指令，用于自行推断维度的大小。在<code>tf.reshape(input, [-1, input_size])</code>中，这意味着“重塑它，使第二个维度<code>input_size</code>，第一个维度是匹配元素总数所需的任何内容”。 这并不一定意味着维数是未知的，因为对于<code>None</code>如果输入张量的已知大小为10个元素，并且将其重塑为<code>[-1, 2]</code>，则张量流能够推断出完整的形状<code>[5, 2]</code>。 <code>-1</code>纯粹是为了方便。你可以把形状写下来，而不是让Tensorflow推断出来<code>None</code>另一方面，对于接受可变大小张量是必要的。 一个形状中只能有一个<code>-1</code>。多个是没有意义的，因为不可能推断出形状。例如，如果一个张量中有12个元素，则未定义将其重塑为<code>[-1, -1, 2]</code>——我们是否应该这样做？<code>[3, 2, 2]</code>？<code>[2, 3, 2]</code>？… 最后，问号正是tensorflow在打印张量和/或其形状时用来标记“未知”维度的内容。您发布的示例实际上会产生语法错误——您不能自己使用问号。未知维度的原因当然可以是具有<code>[6, 1, 2]</code>维度的占位符，并且通常根据占位符定义的张量（即应用于它们的某些运算的结果）也将具有未知维度。此外，有些操作可能没有指定（部分）它们的输出形状，这也可能导致未知。 这里可能还有一些我遗漏的技术细节，但根据经验：使用<code>None</code>作为占位符，使用<code>None</code>进行整形。这应该涵盖大多数用例。</p><blockquote><p><code>？</code>== <code>None</code> ，维度是未知的</p><p><code>-1</code>代表根据推断之后的维度</p><p><code>(3,)</code> 表明张量是一个一维数组，这个数组的长度为3</p></blockquote><blockquote><p><a href="https://www.coder.work/article/2032326" target="_blank" rel="noopener" class="uri">https://www.coder.work/article/2032326</a></p></blockquote><h3 id="keras的-call-函数build-函数">keras的 call 函数、build 函数</h3><p>build() 用来初始化定义weights, 这里可以用父类的self.add_weight() 函数来初始化数据, 该函数必须将 self.built 设置为True, 以保证该 Layer 已经成功 build , 通常如上所示, 使用 super(MyLayer, self).build(input_shape) 来完成。</p><p>call() 用来执行 Layer 的职能, x就是该层的输入，x与权重kernel做点积，生成新的节点层，即当前 Layer 所有的计算过程均在该函数中完成。</p><p><code>__init__()</code>和<code>build()</code>函数都在对Layer进行初始化，都初始化了一些成员函数</p><p><code>__init__()</code>：保存成员变量的设置</p><p><code>build()</code>：在<code>call()</code>函数第一次执行时会被调用一次，这时候可以知道输入数据的<code>shape</code>。返回去看一看，果然是<code>__init__()</code>函数中只初始化了输出数据的<code>shape</code>，而输入数据的<code>shape</code>需要在<code>build()</code>函数中动态获取，这也解释了为什么在有<code>__init__()</code>函数时还需要使用<code>build()</code>函数</p><p><code>call()</code>函数则是在该layer被调用时执行。</p><blockquote><p><a href="https://blog.csdn.net/qq_32623363/article/details/104128497" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/qq_32623363/article/details/104128497</a></p><p><a href="https://blog.csdn.net/qq_27825451/article/details/90517036" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/qq_27825451/article/details/90517036</a></p></blockquote><h3 id="tf.expand_dims">tf.expand_dims（）</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.expand_dims(input, dim, name=<span class="literal">None</span>) <span class="comment">#在指定位置增加维度</span></span><br></pre></td></tr></tbody></table></figure><blockquote><p><a href="https://blog.csdn.net/jasonzzj/article/details/60811035" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/jasonzzj/article/details/60811035</a></p></blockquote><h3 id="tf.boolean_mask">tf.boolean_mask（）</h3><p>选择张量的特定维度的值</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tf.boolean_mask(tensor,mask,name=<span class="string">'boolean_mask'</span>,axis=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#1-D example</span></span><br><span class="line">tensor = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">mask = np.array([<span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">False</span>])</span><br><span class="line">boolean_mask(tensor, mask)  <span class="comment"># [0, 2]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#2-D example</span></span><br><span class="line">tensor = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>]]</span><br><span class="line">mask = np.array([<span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">True</span>])</span><br><span class="line">boolean_mask(tensor, mask)  <span class="comment"># [[1, 2], [5, 6]]</span></span><br></pre></td></tr></tbody></table></figure><blockquote><p><a href="https://blog.csdn.net/wuguangbin1230/article/details/81334544" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/wuguangbin1230/article/details/81334544</a></p></blockquote><h3 id="pytorch里面的torch.nn.parameter">PyTorch里面的torch.nn.Parameter()</h3><p><strong>作用</strong>：对于<code>self.v = torch.nn.Parameter(torch.FloatTensor(hidden_size))</code>，也就是将一个不可训练的类型<code>Tensor</code>转换成可以训练的类型<code>parameter</code>并将这个<code>parameter</code>绑定到这个<code>module</code>里面(<code>net.parameter()</code>中就有这个绑定的<code>parameter</code>，所以在参数优化的时候可以进行优化的)，所以经过类型转换这个<code>self.v</code>变成了模型的一部分，成为了模型中根据训练可以改动的参数了。</p><p>使用这个函数的目的也是想让某些变量在学习的过程中不断的修改其值以达到最优化。</p><blockquote><p><a href="https://www.jianshu.com/p/d8b77cc02410" target="_blank" rel="noopener" class="uri">https://www.jianshu.com/p/d8b77cc02410</a></p></blockquote><h3 id="pytorch的nn.linear">PyTorch的nn.Linear（）</h3><p>用于设置网络中的<strong>全连接层的</strong></p><blockquote><p><a href="https://blog.csdn.net/qq_42079689/article/details/102873766" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/qq_42079689/article/details/102873766</a></p></blockquote><h3 id="pytorch-nn.embedding-词向量">pytorch nn.embedding() 词向量</h3><p>词嵌入在 pytorch 中非常简单，只需要调用 <code>torch.nn.Embedding(m, n)</code> 就可以了，m 表示单词的总数目，n 表示词嵌入的维度，其实词嵌入就相当于是一个大矩阵，矩阵的每一行表示一个单词。</p><p><strong>随机初始化</strong></p><blockquote><p><a href="https://blog.csdn.net/david0611/article/details/81090371" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/david0611/article/details/81090371</a></p></blockquote><h3 id="pytorch中的torch.mean">pytorch中的torch.mean()</h3><p><strong>torch.mean(input, dim, keepdim=False, out=None)</strong></p><p>返回新的张量，其中包含输入张量input指定维度dim中每行的平均值。</p><p>若keepdim值为True，则在输出张量中，除了被操作的dim维度值降为1，其它维度与输入张量input相同。否则，dim维度相当于被执行torch.squeeze()维度压缩操作，导致此维度消失，最终输出张量会比输入张量少一个维度。</p><p><strong>参数：</strong></p><ul><li>input (Tensor) - 输入张量</li><li>dim (int) - 指定进行均值计算的维度</li><li>keepdim (bool, optional) - 输出张量是否保持与输入张量有相同数量的维度</li><li>out (Tensor) - 结果张量</li></ul><p><strong>例子：</strong></p><blockquote><p>a = torch.randn(4, 5) a 0.3168 0.4953 -0.6758 -0.5559 -0.6906 0.2241 2.2450 1.5735 -1.3815 -1.5199 0.0033 0.5236 -0.9070 -0.5961 -2.1281 0.9605 1.5314 -0.6555 -1.2584 -0.4160 [torch.FloatTensor of size 4x5] torch.mean(a, 1, True) -0.2220 0.2283 -0.6209 0.0324 [torch.FloatTensor of size 4x1]</p></blockquote><h3 id="np.triu-np.tril">np.triu() &amp; np.tril()</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">triu</span>（<span class="title">m</span>， <span class="title">k</span>）：</span></span><br><span class="line"><span class="function">#取上三角阵  </span></span><br><span class="line"><span class="function">#<span class="title">m</span>：表示一个矩阵</span></span><br><span class="line"><span class="function">#<span class="title">K</span>：表示对角线的起始位置（<span class="title">k</span>取值默认为0）</span></span><br><span class="line"><span class="function"></span></span><br><span class="line">#k=0表示正常的上三角矩阵</span><br><span class="line"><span class="comment">#k=-1表示对角线的位置下移1个对角线</span></span><br><span class="line"><span class="comment">#k=1表示对角线的位置上移1个对角线</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#同理，np.tril取下三角阵</span></span><br></pre></td></tr></tbody></table></figure><p>参考</p><blockquote><p><a href="https://blog.csdn.net/weixin_37724529/article/details/102881776" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/weixin_37724529/article/details/102881776</a></p></blockquote><h3 id="python的einops-rearrange函数">python的einops rearrange()函数</h3><p>例子：</p><p>假设我有一个3-D数组：</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[[[0,1,2],</span><br><span class="line">  [0,1,2],</span><br><span class="line">  [0,1,2]],</span><br><span class="line"></span><br><span class="line"> [[3,4,5],</span><br><span class="line">  [3,4,5],</span><br><span class="line">  [3,4,5]]]</span><br></pre></td></tr></tbody></table></figure><p>我想按列重新排列：</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[0,1,2,3,4,5],</span><br><span class="line"> [0,1,2,3,4,5],</span><br><span class="line"> [0,1,2,3,4,5]]</span><br></pre></td></tr></tbody></table></figure><p>使用einops：</p><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">einops.rearrange(a, <span class="string">'x y z -&gt; y (x z) '</span>)</span><br></pre></td></tr></tbody></table></figure><p>并且我建议根据上下文（例如时间，高度等）为轴指定有意义的名称（而不是xyz）。 这将使您易于理解代码的作用</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In : einops.rearrange(a, 'x y z -&gt; y (x z) ')</span><br><span class="line">Out:</span><br><span class="line">array([[0, 1, 2, 3, 4, 5],</span><br><span class="line">       [0, 1, 2, 3, 4, 5],</span><br><span class="line">       [0, 1, 2, 3, 4, 5]])</span><br></pre></td></tr></tbody></table></figure><h3 id="目标检测">🚀 目标检测</h3><blockquote><p><a href="https://bbs.cvmart.net/topics/3056" target="_blank" rel="noopener" class="uri">https://bbs.cvmart.net/topics/3056</a></p></blockquote><h3 id="二分图匹配bipartite-matching">🚀 二分图匹配（bipartite matching ）</h3><blockquote><p><a href="https://liam.page/2016/04/03/Hungarian-algorithm-in-the-maximum-matching-problem-of-bigraph/" target="_blank" rel="noopener" class="uri">https://liam.page/2016/04/03/Hungarian-algorithm-in-the-maximum-matching-problem-of-bigraph/</a></p></blockquote><p><a href="https://blog.csdn.net/COCO56/article/details/100058599" target="_blank" rel="noopener">解决vscode乱码问题，VSCode设置自动推导文件编码</a></p><h3 id="pytorch-中forward的使用以及原理---pytorch使用">pytorch 中forward的使用以及原理 --pytorch使用</h3><p>https://blog.csdn.net/u011501388/article/details/84062483</p><h3 id="pytorch里面的torch.nn.parameter详解">PyTorch里面的torch.nn.Parameter()详解</h3><p>https://cloud.tencent.com/developer/article/1608348</p><p>chrome中github插件</p><p>https://www.bilibili.com/video/BV1Kt4y1X7fw/?spm_id_from=trigger_reload</p><h3 id="论文阅读的思维导图">论文阅读的思维导图</h3><p>conda 安装新版本python之后，会覆盖之前的版本</p><h3 id="linux-杀死暂停继续后台运行进程">LINUX 杀死、暂停、继续、后台运行进程</h3><p>ctrl + z</p><p>可以将一个正在前台执行的命令放到后台，并且暂停</p><p>若想恢复到前台，则</p><ol type="1"><li>jobs #查看当前有多少在后台运行的命令 会有序号 job号</li><li>fg 〔<em>job</em>号〕 将后台中的命令调至前台继续运行 如： fg %1</li></ol><p>https://blog.csdn.net/QQ1910084514/article/details/80390671</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      记录杂乱的知识点，持续更新
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-09-05-讨论总结</title>
    <link href="http://yoursite.com/2020/09/05/2020-09-05-%E8%AE%A8%E8%AE%BA%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2020/09/05/2020-09-05-%E8%AE%A8%E8%AE%BA%E6%80%BB%E7%BB%93/</id>
    <published>2020-09-05T07:33:34.000Z</published>
    <updated>2020-09-05T07:40:45.081Z</updated>
    
    <content type="html"><![CDATA[<p>研究生阶段想要重点去研究透彻某个知识点，还是要去系统学习。不要只是依赖博客</p><p>博客缺点</p><ol type="1"><li>一般不成体系，比较片面</li><li>不管是翻译外国博客还是自己的总结，由于博主本身的能力，导致在写的时候，都会出现一定的误差，可信度不抵论文和书籍</li><li>讲的不深入</li></ol><p>和师兄讨论了卡尔曼滤波的内容，师兄针对我的问题也给了很好的建议，让我有方向去继续。发现一直以来我对卡尔曼滤波理解的太浅显，不深刻，理解只是停留结合例子理解卡尔曼滤波那五个公式，知道计算过程，但是没有去深入理解来源以及公式的意义，变量的含义等等，没有真正转化为自己的东西 因为卡尔曼滤波是RKN的核心基础，所以必须要深入理解，这样才能更好地运用卡尔曼滤波，也更好地理解模型。 因为融合到transformer中，也要讲清楚为什么融合之后效果好，或者为什么不好，只有将本质讲清楚，去理论分析的时候才有信服力。避免只是简单的拼接。</p><p>为什么这么做，这么做的好处。公式间的逻辑关系， 买了本卡尔曼滤波的书，意义和含义，背景和理论公式一步一步推导</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      记录讨论后的一些研究方法感想
    
    </summary>
    
    
    
      <category term="研究方法" scheme="http://yoursite.com/tags/%E7%A0%94%E7%A9%B6%E6%96%B9%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>2020-09-04-transformer直观理解</title>
    <link href="http://yoursite.com/2020/09/04/2020-09-04-transformer%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3/"/>
    <id>http://yoursite.com/2020/09/04/2020-09-04-transformer%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3/</id>
    <published>2020-09-04T13:16:46.000Z</published>
    <updated>2020-10-12T02:46:59.336Z</updated>
    
    <content type="html"><![CDATA[<h3 id="直观attention-模型">直观“Attention 模型”</h3><p>本文试着从直观的角度解析“Attention模型”，应用场景：原文--译文，具体选择 中文--英文，即在<strong>将中文翻译为英文</strong>这一场景中，直观解析“Attention模型”。</p><h4 id="概括">概括</h4><p>一句中文A翻译为一句英文B主要是完成以下“<strong>两项任务</strong>”：</p><ol type="1"><li><p>理解中文A的意思为X；</p></li><li><p>将意思X用英文表达出来，即英文B；</p></li></ol><p>在用计算机完成以上任务前，需要以下三点“<strong>准备工作</strong>”：</p><ol type="i"><li><p>需要将中文的字和英文的单词转换为计算机可以理解（计算）的数（即一个字/单词对应转换为一个向量，称为字向量/单词向量），然后计算机才有可能完成以上两项任务，实现翻译； <code>单词-&gt;单词向量</code></p></li><li><p>另外针对一句中文翻译为一句英文，每个字在句子中的位置也对意思的表达会产生很大的影响，所以每个字在句子中的位置也要定义一个向量来表达（即一个位置对应转换为一个向量，称为位置向量）； <code>位置向量</code></p></li><li><p>将字向量/单词向量加上位置向量（定义两种向量的维度相同，如都是512维，便于此处元素相加），能更好更全面的代表这句话，为更好的翻译做好准备；<code>单词向量+位置向量</code></p></li></ol><p>“Attention模型”实现以上内容，具体情况如下图所示：</p><p><img src="https://i.loli.net/2020/09/05/bEpK5wcj9rkAGCU.jpg" alt="img"></p><p>以下针对“准备工作”、任务1和任务2展开讨论；</p><h4 id="准备工作">准备工作</h4><p>包括字向量/单词向量、位置向量；此处也称为<strong>词嵌入，位置编码</strong>；</p><ol type="a"><li>字向量/单词向量分别是<code>随机产生产生的一组512维向量</code>，如字向量，假设选用了3000个常用汉字，每个字对应一个512维的随机向量，则整个字向量就是一个3000 X 512 的二维矩阵，每一行代表一个字；</li></ol><p><strong>之所以用随机且选择较大维度（如512维），是为了让生成的各个向量间尽可能的独立，即没有相关性</strong>，就像“你、我、他、拿、和、中”指代的具体意思在最初定义时是可以随机互换的，之间也无关系，他们之间的相关性/关系是在该语系语境中根据语义、语法、文化等因素形成的，即上述任务1需要完成的。</p><p>（词嵌入，每个词之间没有关系）</p><p><img src="https://i.loli.net/2020/09/05/9tSTwmsdnfuW7Ol.jpg" alt="img"></p><ol start="2" type="a"><li>位置向量是代表一个字在句子中的位置信息，也定义为一个512维的向量，但并<strong>不是随机产生</strong>的，而是根据位置确切计算得来，即<strong>一个位置对应转化为一个512维向量；</strong></li></ol><p><img src="https://i.loli.net/2020/09/05/FAR6vuDZgBoJwTX.jpg" alt="img"></p><ol start="3" type="a"><li>假设翻译时定义一句话最大长度是10个字，则该句话对应的字向量是一个10 X 512的二维矩阵（每一行代表一个字），位置向量也是一个10 X 512的二维矩阵（每一行代表对应字的位置信息）；<strong>两个矩阵相加得新的二维矩阵能更好更全面的表达这句话；</strong></li></ol><h4 id="任务1编码">任务1：编码</h4><p><strong>理解</strong>一句中文A的意思为X；此处也称为“编码”</p><ol type="i"><li><p>翻译时中文中的“你”、“我”大多时候对应着英文的“you”、“me”，如果都是这样的简单一一对应关系，那翻译是很简单的；而实际情况是<strong>绝大多数都是一对多的关系，即同一个中文字在不同的语境中对应的英文是不一样的单词</strong>，如“和”字在不同语境中翻译为英文可能是“and”、“sum”、“peace”等。</p></li><li><p>一个字从多个可能的意思中选择一个是<strong>根据语境</strong>来确定的，即<strong>根据这个字与句子中所有字的相关关系来确定</strong>；<strong>一句话需要计算该句话中每个字与该句子中所有字的相关关系来确定这句话中每个字在该语境中的意思，即确认中文语境</strong>；</p></li><li><p><span class="github-emoji" style="display:inline;vertical-align:middle;color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8">⭐</span> 在计算相关性之前，对每一个<strong>字对应的向量进行相应的线性变换</strong>以便于<strong>更好的计算相关性</strong>确认最终意思；计算完相关性（确认中文语境）并以此更新向量矩阵后（即self-attention，确认每个字在当前这句话的语境下的“确切意思”），再<strong>进行一次线性变换</strong>，对这个“确切意思”进行再次拟合校准；</p></li></ol><p>具体情况，如下图所示；</p><p><img src="https://i.loli.net/2020/09/05/ZT4OvczK3tyCdGX.jpg" alt="img"></p><p>Notes：i~iii是一个处理单元，<strong>输入“向量矩阵”和输出“新向量矩阵X”的维度是一样的</strong>；完成任务1是以上处理单元循环N次（<strong>强化上述效果</strong>），设定义N=3（论文中N=8）；即由3个处理单元依次链接完成任务1，如下图所示：</p><p><img src="https://i.loli.net/2020/09/05/ZS4YMITXOWyr8kf.jpg" alt="img"></p><h4 id="任务2解码">任务2：解码</h4><p>将意思X用英文表达出来，即英文B；此处也称为“解码”</p><p>和任务1类似，<strong>差异</strong>在于：</p><p>I. 任务1仅考虑中文语境即可，任务2<strong>既考虑中文语境（vanilla-attention），也考虑英文语境（self-attention）；</strong></p><ol start="2" type="i"><li>和任务1类似，经过N个处理单元后获得的向量矩阵，经过“最后一次线性变换”转换为对应英文语系中各个单词的值，然后由softmax转换为是各个英文单词的概率，完成翻译；</li></ol><p>图示如下：</p><p><img src="https://i.loli.net/2020/09/05/yKDA2Fs3Xc4HkIo.jpg" alt="img"></p><p>整体简化图示如下：</p><p><img src="https://i.loli.net/2020/09/05/6epU1uboH2AGP7k.jpg" alt="img"></p><h3 id="attention注意力">Attention注意力</h3><p><img src="https://i.loli.net/2020/09/05/R2XINUW16u4anTs.jpg" alt="img"></p><p>上图是attention模型的总体结构，包含了模型所有节点及流程（因为有循环结构，流程不是特别清楚，下文会详细解释）；模型总体分为两个部分：编码部分和解码部分，分别是上图的左边和右边图示；以下选取翻译情景，以<strong>模型训练</strong>为例解释整个过程；</p><p><strong>训练样本：原文译文(一一对应)</strong></p><h4 id="编码部分inputs">编码部分（inputs）</h4><h5 id="input-embedding">Input embedding:</h5><p>1.1 将原文的所有单词汇总统计频率，删除低频词汇（比如出现次数小于20次的统一</p><p>定义为’<unk>’）；</unk></p><p>此时总共选出了假设10000个单词，则用数字编号为0~9999，一一对应，定义该对应表为word2num；</p><p>然后用<code>xaviers方法</code>生成随机矩阵Matrix ：<strong>10000行N列</strong>（10000行是确定的，对应10000个单词，N列自定义）；这样就可以将10000个不同的单词通过word2num映射成10000个不同的数字（int），然后将10000个不同的数字通过Matrix映射成10000个不同的N维向量（如何映射？比如数字0，3，经过 Matrix映射分别变为向量Matrix[0],Matrix[3]，维度为N维）；</p><p>这样，<strong>任何一个单词，都可以被映射成为唯一的一个N维向量</strong>；</p><p><img src="https://i.loli.net/2020/09/05/86ByVmtDIGfd2bx.png" alt="img"></p><p><strong>Note：此处N自定义为512</strong></p><p>1.2 翻译的时候是<strong>一个句子一个句子的翻译</strong>，所以需要定义一个句子的标准长度，比如10个单词；如果一句话不足10个单词则用0填充（<strong>对应的word即word2num表中的<pad></pad></strong>），如果多了，删掉；</p><p>这样一句话就是标准的10个单词；比如句子 “中国人有中国梦。”，这句话共有八个字（最后一个是结束符），<strong>经过word2num变为一列X：<a href="注：100代表的word是结束符">1,7,3,2,1,7,6,100,0,0</a>,X经过Matrix映射为10行N列的矩阵matX</strong>= [Matrix[1], Matrix[7], Matrix[3], Matrix[2] , Matrix[1] , Matrix[7] , Matrix[6], Matrix[100] , Matrix[0] , Matrix[0]]; embedding 到此基本结束，即完成了将一句话变为 一个矩阵，矩阵的每一行代表一个特定的单词；此处还可以scale一下，即<code>matX*N**(1/2)</code>; （<code>**代表次方，即matX中的每一个元素都乘以N的1/2次方，此时N=512，以此来缩放</code>）</p><p><img src="https://i.loli.net/2020/09/05/xOJBLyNruSIX9s2.png" alt="img"></p><h5 id="positional-encoding">Positional encoding:</h5><p>2.1 单词在句子中的不同位置体现了不同信息，所以需要对位置进行编码，体现不同的信息情况，此处是对绝对位置进行编码，即位置数字0，1，2，3，…N等，进行运算编码，具体编码如下：</p><p>2.1.1 对于句子中的每一个字，其位置pos∈<a href="每句话10个字">0,1,2,…,9</a>,每个字是N（512）维向量，维度 i （i∈[ 0,1,2,3,4,..N]）带入<strong>函数计算</strong>，</p><p><img src="https://i.loli.net/2020/09/05/dR4rAa9DkZJKNQC.png" alt="img"></p><blockquote><p>用sin和cos是因为在后面运算过程中会近似出现sin(a)sin(b)+cos(a)cos(b)的形式，根据三角函数公式上式恰好等于cos(a-b)，当a和b差小时（即两个字离得近）值大，反之小。这在一定程度上可以表达两个字的距离。</p></blockquote><p>2.1.2 经过如上函数运行一次后，获得了一个<strong>10行N列的矩阵matP</strong>；每一行代表一个绝对位置信息，此时matP的shape和matX的shape相同；</p><p><img src="https://i.loli.net/2020/09/05/xFKDaYu1pSNZ7lo.png" alt="img"></p><p>2.1.3 <strong>对于矩阵matP的每一行，第0，2，4，6,...等偶数列上的值用sin()函数激 活，第1，3，5，。。。等奇数列的值用cos()函数激活，以此更新matP</strong>；即 matP[:,0::2]=sin(matP[:,0::2]), matP[:,1::2]=cos(matP[:,1::2])；</p><p><img src="https://i.loli.net/2020/09/05/w1FJXfzq5IRrPCS.png" alt="img"></p><p>2.2 至此positional encoding结束，最后通常也会scale一次，即对更新后的matP进行<code>matP*N**(1/2)</code>运算，得到再次更新的matP，此时的matP的shape还是和matX相同；<strong>然后将matP和matX相加即matEnc=matP+matX，矩阵matEnc其shape=[10，512]；</strong></p><h5 id="multi-head-attention循环单元">Multi-head attention循环单元</h5><p>3.1 然后matEnc进入模型编码部分的循环，即Figure1中左边红色框内部分，每个循环单元又分为4个小部分：multi-head attention, add&amp;norm, feedForward, add&amp;norm；</p><p>3.2 Multi-head attention</p><p><img src="https://i.loli.net/2020/09/05/np9H73tSVYogJG4.jpg" alt="img"></p><p>3.2.1 Multi-head attention 由三个输入，分别为V，K，Q，此处<strong>V=K=Q=matEnc</strong>（在解码部分multi-head attention中的VKQ三者不是这种关系）;</p><p>3.2.2 首先分别对V，K，Q三者分别进行线性变换，即将三者分别输入到三个单层神经网络层，激活函数选择relu，输出新的V，K，Q（三者shape都和原来shape相同，即<strong>经过线性变换时输出维度和输入维度相同</strong>）；</p><p>3.2.3 然后将Q在最后一维上进行切分为num_heads(假设为8)段，然后对切分完的矩阵在axis=0维上进行concat链接起来(纵向连接)；对V和K都进行和Q一样的操作；操作后的矩阵记为Q_,K_,V_；</p><p><strong>可以变化其维度， 由[1,10,512]变为[8,10,64]</strong></p><p><img src="https://i.loli.net/2020/09/05/PqJYGOb4fvTmsFw.png" alt="img"></p><p><img src="https://i.loli.net/2020/09/05/2npJkGobFc4RXwu.png" alt="img"></p><p>3.2.4 <strong>Q_矩阵相乘 K_的转置（对最后2维）</strong>，生成结果记为outputs，然后对outputs 进行scale一次更新为outputs；<strong>此次矩阵相乘是计算词与词的相关性，切成多个num_heads进行计算是为了实现对词与词之间深层次相关性进行计算；</strong></p><p><img src="https://i.loli.net/2020/09/05/imn5tKIUxzyfwLW.png" alt="img"></p><p><code>shape（outputs） = （8,10,10）</code></p><p>3.2.5 对outputs进行softmax运算，更新outputs，即outputs=softmax(outputs);</p><p>3.2.6 最新的outputs（即K和Q的相关性） 矩阵相乘 V_， 其值更新为outputs；</p><p><img src="https://i.loli.net/2020/09/05/oZalMTkQeRVqsUy.png" alt="img"></p><p><code>shape（outputs）= (8,10,64)</code></p><p>3.2.7 最后将outputs在axis=0维上切分为num_heads段，然后在axis=2维上合并， <strong>恢复原来Q的维度</strong>；</p><p><img src="https://i.loli.net/2020/09/05/JrFbIdWauXxA4nK.png" alt="img"></p><p>3.3 Add&amp;norm</p><p>3.3.1 类似ResNet，将<strong>最初的输入与其对应的输出叠加一次</strong>，即outputs=outputs+Q， 使网络有效叠加，<strong>避免梯度消失</strong>；</p><p><img src="https://i.loli.net/2020/09/05/falSI79c3xD2Ey8.png" alt="img"></p><p>3.3.2 标准化矫正一次，在outputs对最后一维计算均值和方差，用outputs减去均值除以方差+spsilon得值更新为outputs，然后变量gamma*outputs+变量beta；（Norm操作）</p><p>3.4 feed Forward （就是dense layer 全连接层）</p><p>3.4.1 对outputs进行第一次卷积操作，结果更新为outputs（卷积核为1*1，每一次卷积操作的计算发生在一个词对应的向量元素上，卷积核数目即最后一维向量长度，也就是一个词对应的向量维数）；</p><p>3.4.2 对最新outputs进行第二次卷积操作，卷积核仍然为1*1，卷积核数目为N；</p><p><img src="https://i.loli.net/2020/09/05/esShU1Nx32AYt8Z.png" alt="img"></p><p>3.5 Add&amp;norm : 和3.3相同，经过以上操作后，此时最新的output和matEnc的shape相同；</p><p>3.6 <strong>令matEnc=outputs, 完成一次循环，然后返回到3.2开始第二次循环</strong>；共循环Nx（自定义；每一次循环其结构相同，但对应的参数是不同的，即是独立训练的）；完成Nx次后，模型的编码部分完成，仍然令matEnc=outputs，准备进入解码部分；</p><p>解码部分：</p><p>​ <strong>此时的outputs指的是上一时间点解码器的输出</strong></p><ol type="1"><li><p>Outputs：<strong>shifted right右移一位</strong>？？？？，是为了解码区最初初始化时第一次输入，并将其统一定义为特定值（在word2num中提前定义）；</p></li><li><p>Outputs embedding: 同编码部分；更新outputs；</p></li><li><p>Positional embedding：同编码部分；更新outputs； （前三步是准备工作）</p></li><li><p>进入解码区循环体； （以下是解码器的顺序操作）</p></li></ol><p>4.1 Masked multi-head attention: 和编码部分的multi-head attention类似，但是多了一 次<strong>masked</strong>，因为在解码部分，解码的时候是从左到右依次解码的，当解出第一个字的时候，第一个字只能与第一个字计算相关性，当解出第二个字的时候，只能计算出第二个字与第一个字和第二个字的相关性，...；所以需要进行一次mask；</p><p><img src="https://i.loli.net/2020/09/05/54eVnEgmh7CdqKH.jpg" alt="img"></p><p>为什么是10*10呢？？？</p><p>4.2 Add&amp;norm：同编码部分，更新outputs；</p><p>4.3 Multi-head attention：同编码部分，但是Q和K，V不再相同，Q=outputs，K=V=matEnc；(outputs是上层的输出，k,v是来自编码器的输出)</p><p>4.4 Add&amp;norm:同编码部分，更新outputs；</p><p>4.5 Feed-Forward：同编码部分，更新outputs；</p><p>4.6 Add&amp;norm: 同编码部分，更新outputs；</p><p>4.7 最新outputs和最开始进入该循环时候的outputs的shape相同；回到4.1，开始第 二次循环。。。；直到完成Nx次循环（自定义；<strong>每一次循环layer结构相同，但对应的参数是不同的，即独立训练的</strong>）；</p><ol start="5" type="1"><li><p>Linear: 将最新的outputs，输入到单层神经网络中，输出层维度为“译文”有效单词总数；更新outputs；</p></li><li><p>Softmax: 对outputs进行softmax运算，确定模型译文和原译文比较计算loss，进行网络优化（参数更新）；</p></li></ol><h4 id="注">注</h4><p>1.解码器的<code>outputs embedding</code> ：在训练的时候就是对应原文的译文，其中第一字统一定义为0,作为输入；在预测时第一次输入也是全是0,然后每循环一次，预测一个字直到出现终止符。</p><p>2.对于matEnc=matP+matX，这里为什么要用add,而不是contact ?</p><p>matX是一个10行512列的矩阵，每一行代表一个字；</p><p>matP是一个10行512列的矩阵，每一行代表一个位置；</p><p><strong>对于不一样的句子，matX是不一样的，matP是完全一样的；</strong></p><p>则对于不一样的句子，add后是不一样的，contact后至少一半是一样的，从直观上，add似乎更好；</p><p>对于一个字，其出现的位置不同，可能表达的意思完全不一样，比如“和”，如果其在句首或者句中出现更可能是“and”的意思，如果在句末出现，更可能是“sum”的意思，而这两个意思几乎完全不一样，即他们的向量完全不一样似乎更合理，而非contact的至少一半一样；</p><p>matP矩阵的特点从上到下对应各元素是递增的，matX是随机产生的（比如均值为0的随机数），即大约在0附近波动的数，与matP做add运算后，相当于均值被依次提高，以此代表融入每个字位置信息；因为每次训练的时候均值被提高的量是一定的，所以可以期望模型训练后能“意识”到这一点；</p><p>add产生“信息混淆”，比如两个字在两个不同的位置上分别add后，结果相近，从直观上这可能会造成问题；这个问题可以通过加大向量维度来降低其出现概率，比如选择512维是很长的维度了，出现这种概率的问题还是很小的；</p><p>如果用contact实际就是在每个字向量后面追加一个位置信息以示区别，做这种区别无需太多维，也许一两维即可；</p><p>深度学习算法的可解释性差，分析大多属于理论上的“纸上谈兵”，最可靠的方式，仍是分别以add和contact两种方式建模，大量测试后的结果更为可靠。</p><ol start="3" type="1"><li><p>待探究</p><p>你文章中的逻辑是，对原始Q/K/V做不同线性变换（三个权重矩阵）得到新的Q/K/V→对新的Q/K/V在最后一个维度做切分得到多头（8组Q/K/V）→各组Q/K/V计算attetion值→8组Q/K/V的attetion值concat得到最终的attention值。</p><p>而原论文的逻辑是，对原始的Q/K/V做不同的线性变换（8（组）×3个权重矩阵）得到新的8组Q/K/V值→各组Q/K/V计算attention值→8组Q/K/V的attention值concat→concat结果经过一个线性变换（为了还原到最初的维度）得到最终的attention值。</p><p>论文提到multi-head attention是为了从不同表征子空间提取信息。个人理解实现这种差异化的提取，是通过多组权重矩阵来实现的，而不是通过embedding值不同分段获取。</p></li><li><p>在<strong>预测阶段</strong>，每次预测后底部decoder的输入是可变的，首先是[<bos>]，然后是[<bos>, word1 ]，再输入[<bos>, word1, word2 ]……，那么decoder内部如何保证它送入linear层的输出是(1, N)的向量呢？</bos></bos></bos></p><p><strong>答</strong>：[<bos>]时，经过解码区的循环部分后 是一个[1, 512]的矩阵， 经过linear层是准备预测1个字的；</bos></p><p>[<bos>, word1 ]时，经过解码区的循环部分 是一个[2, 512]的矩阵；经过linear层是准备预测2个字的；以此类推。</bos></p><p>也就是说，输入bos，输出word1；然后将bos word1输入，再输出word1 word2；再输入bos word1 word2......每次都把输出的最后一个字加到下一轮输入。</p></li><li><p>假设target是<bos>我爱中国<eos>，这算6个字，训练时decoder是不是也输出6个vocab-size长度的向量，那么第一个vocab-size长度的向量预测的是<bos>还是"我"呢？</bos></eos></bos></p><p><strong>答</strong>：预测的第一个是“我”,<bos>作为一个起始引导使用。</bos></p></li><li><p>第一个问题是训练和预测时解码端如何运行，我理解训练时使用mask一次性对所有时间步并行进行解码，预测时则需要先预测出上一步的词，再输入预测下一步，所以不能并行。如果我上面说的没错的话，第一个问题是为什么训练时mask没有掩盖自身，也就是对角线不mask，这样的话不就泄露了要预测那个词吗？第二个问题是预测时该如何进行，因为训练时，输入多少个时间步的词就会输出多少个时间步的预测值，但是在预测解码阶段，假设为t，要预测t + 1该如何操作？难道是先将t + 1随便加一个pad上去然后看预测值softmax吗？</p><p><strong>答</strong>：<strong>在训练时，用mask是一次性的解码</strong>，因为训练时所有label是已知的，用mask实现同时并行运算；<strong>预测时label是未知的，需要一个一个词预测</strong>，当预测第一个词时只能知道第一个词和第一个词的相关性，然后再运行模型一遍，预测出第一个词和第二词，依次循环直到出现终止符，这个过程不是并行的。</p><p>个人理解： 在训练时，把一整句话都作为解码器的输入，这样可以实现并行运算，因为每一个label都是已知的。而在预测时，需要一步一步来</p></li></ol><h3 id="参考">🚀参考</h3><blockquote><p><a href="https://zhuanlan.zhihu.com/p/62397974" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/62397974</a></p><p><a href="https://zhuanlan.zhihu.com/p/44731789" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/44731789</a></p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      摘自一篇不错的博客
    
    </summary>
    
    
      <category term="transformer" scheme="http://yoursite.com/categories/transformer/"/>
    
    
      <category term="transformer" scheme="http://yoursite.com/tags/transformer/"/>
    
  </entry>
  
  <entry>
    <title>2020-08-31-一周论文分享（第1期）</title>
    <link href="http://yoursite.com/2020/08/31/2020-08-31-%E4%B8%80%E5%91%A8%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB%EF%BC%88%E7%AC%AC1%E6%9C%9F%EF%BC%89/"/>
    <id>http://yoursite.com/2020/08/31/2020-08-31-%E4%B8%80%E5%91%A8%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB%EF%BC%88%E7%AC%AC1%E6%9C%9F%EF%BC%89/</id>
    <published>2020-08-31T12:43:16.000Z</published>
    <updated>2020-09-04T08:00:41.608Z</updated>
    
    <content type="html"><![CDATA[<h3 id="reformerthe-eficient-transformer">reformer：the eficient transformer</h3><h4 id="论文概况">论文概况</h4><ul><li>来源：ICLR 2020</li><li>arXiv: 1901.02860</li><li>作者：Nikita Kitaev ，Anselm Levskaya</li><li>论文地址：<a href="https://openreview.net/forum?id=rkgNKkHtvB" target="_blank" rel="noopener" class="uri">https://openreview.net/forum?id=rkgNKkHtvB</a></li><li>Code url：<a href="https://github.com/google/trax/tree/master/trax/models/reformer" target="_blank" rel="noopener" class="uri">https://github.com/google/trax/tree/master/trax/models/reformer</a></li><li>论文组会报告于<code>2020.08.30</code></li></ul><h4 id="背景">背景</h4><p>Transformer架构被广泛用于自然语言处理中，并在许多任务上产生了最新的结果</p><h5 id="问题">问题</h5><ol type="1"><li>大型的 Transformer 可以在许多任务上实现 sota，但是面临着参数过多的问题，导致所占内存过大，造成资源紧张.</li></ol><p>在最大的配置中，参数数量已经超过了 5亿/层，层数多达 64。</p><ol start="2" type="1"><li><p>具有 <em>N</em> 层的模型要消耗 <em>N</em> 倍于单层模型的内存，因为每一层中的激活都需要存储以进行反向传播。</p></li><li><p>由于点乘注意力本身的局限性，导致不能处理长序列数据，否则会导致效率不高</p></li></ol><p>也就是说transformer的上下文窗口有限制范围。最多也就几千个单词。</p><blockquote><p>Transformer 的强大来源于注意力机制 ，通过这一机制，Transformer 将上下文窗口内所有可能的单词对纳入考虑，以理解它们之间的联系。因此，如果文本包含 10 万个单词，Transformer 将需要评估 100 亿单词对（10 万 x 10 万），这显然不切实际。</p><p>另一个问题是如何保存每个模型层的输出 。对于使用大型上下文窗口的应用来说，存储多个模型层输出的内存需求会迅速变得过大。这意味着，实际使用大量层的 Transformer 模型只能用于生成几小段落的文本或一小段的音乐。</p></blockquote><h5 id="解决方案">解决方案</h5><ol type="1"><li><p>使用可逆残差（reversible residual layers）代替标准残差（standard residuals），这使得存储在训练过程中仅激活一次，而不是 n 次（此处 n 指层数），更有效地使用可用内存</p></li><li><p>将点乘注意力（dot-product attention）替换为一个使用局部敏感哈希（locality-sensitive hashing）的点乘注意力，将复杂度从 O(L2 ) 变为 O(L log L)，此处 L 指序列的长度，来降低长序列的处理复杂度</p></li></ol><p><strong>Reformer与使用完全Transformer所获得的结果相匹配，但运行速度要快得多，尤其是在文本任务上，并且内存效率要高几个数量级。</strong></p><h4 id="注意力问题">注意力问题</h4><h5 id="原始注意力">原始注意力</h5><p>公式如下：<span class="math inline">\(Attention (Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V\)</span></p><p>self-attention操作的核心 ——<span class="math inline">\(QK^T\)</span> 表示key和query之间的相似度得分</p><p>计算带有所有k的q的点积，并用√dk进行缩放，然后应用softmax函数来获得v的权重。用来消除hidden size这个参数对注意力分布的影响。对于每个query，我们在所有keys上计算一个softmax，以确保矩阵的每一行和为1—— 确保新的隐藏状态的大小不依赖于序列长度。</p><p>最后，我们用我们的注意力矩阵乘以我们的values矩阵，为每个token生成一个新的隐藏表示。</p><hr><blockquote><h5 id="例子">例子</h5><p>Key:(batch, length,d_model)</p><p>Query:(batch, length,d_model)</p><p>-&gt; <span class="math inline">\(QK^T\)</span>　:(batch, length,length)</p><p>-&gt; 复杂度： O(<span class="math inline">\(L^2\)</span> )</p><p>-&gt;原始的transformer结构难以处理过长的序列长度</p></blockquote><p><img src="https://i.loli.net/2020/09/03/9mdzrXcGKZ5FPi3.png" alt="image-20200903163000933"></p><p><img src="https://i.loli.net/2020/09/03/6QvMn1qxGXzwKmC.png" alt="image-20200903161441529"></p><p>其实，在softmax中，对于每个查询 <em>q</em>，我们只需要注意最接近 <em>q</em> 的键 <em>k</em>。<strong>并不一定需要那些注意力权重很小的token。</strong></p><p>例如，如果序列长度是 64K，对于每个 <em>q</em>，我们可以只考虑 32 或 64 个最近的键的一个小子集。因为这些是和<em>q</em>最需要注意的</p><h5 id="局部敏感哈希lsh">局部敏感哈希(LSH)</h5><p><img src="https://i.loli.net/2020/09/03/zQI9n2ubZYDV7SL.png" alt="image-20200903163152560"></p><p>局部敏感哈希使用<code>球形投影点的随机旋转</code>，通过argmax在有符号轴投影上<code>建立桶（bucket）</code>。 在此高度简化的2D描绘中，对于三个不同的角度hash，两个点x和y不太可能共享相同的哈希桶（上方），除非它们的球面投影彼此靠近（下方）。</p><p>该图演示了一个用<code>4个桶进行3轮哈希的设置</code>。下面的图中的向量映射到了同一个bucket，因为它们的输入很接近，而上一张图中的向量映射到第一个和最后一个bucket。</p><p>LSH是一组将高维向量映射到一组离散值(桶/集群)的方法。是解决在高维空间中快速找到最近邻居（最相似）的问题。</p><p><code>基本思想</code>：选择 <em>hash</em> 函数，对于两个点 p 和 q，如果 q 接近 p，那么很有可能我们有 hash(q) == hash(p)</p><h5 id="lsh注意力">LSH注意力</h5><p><img src="https://i.loli.net/2020/09/03/KgnJ7iB2kImcshX.png" alt="image-20200903163641310"></p><p>完全不同的方法来处理序列长度问题，丢弃了<code>query投影</code>（Q=K）（实验结果发现，学习不同的keys和queries的投影并不是严格必要的），<code>并将注意力权重替换为key的函数（hash函数）</code>，以此降低复杂度</p><p>步骤如下：</p><p>1.使用LSH为每个token计算一个桶</p><p>2.根据相同的桶进行归类排序</p><p>3.分块并将标准的点乘注意力应用到桶中的token的块上，从而大大降低计算负载</p><h4 id="内存问题">内存问题</h4><p>单层能够执行长序列的单模型。但是，当使用梯度下降训练多层模型时，由于需要保存每一层的激活（函数），以用于执行逆推。一个传统的 Transformer 模型具有十几个或更多的层，通过缓存这些层的值，内存将会很快用完。</p><p>可逆层：在反向传播时，按需重新计算每个层的输入，而不是将其保存在内存中。其中来自网络最后一层的激活用于还原来自任何中间层的激活。</p><p>原始的残差网络：<span class="math inline">\(Y=F(x)\)</span></p><p>可逆层的残差网络： 注意我们如何从它的输出(Y ₁, Y ₂)计算物体的输入(X ₁, X ₂)。</p><p><span class="math inline">\(\begin{array}{ll}y_{1}=x_{1}+F\left(x_{2}\right) &amp; y_{2}=x_{2}+G\left(y_{1}\right) \\ x_{2}=y_{2}-G\left(y_{1}\right) &amp; x_{1}=y_{1}-F\left(x_{2}\right) \\ Y_{1}=X_{1}+\text { Attention }\left(X_{2}\right) &amp; Y_{2}=X_{2}+\text { FeedForward }\left(Y_{1}\right)\end{array}\)</span></p><p>示意图如下</p><p><img src="https://i.loli.net/2020/09/03/WX3lcQrmB16pI2Y.png" alt="image-20200903164816966"></p><p><img src="https://i.loli.net/2020/09/03/FgHKa4QNrsjlzMh.png" alt="image-20200903164907447"></p><h4 id="实验">实验</h4><p>作者分别对图像生成任务 <em>imagenet64</em>(长度为 12K)和文本任务 <em>enwik8</em>(长度为 64K)进行了实验，评价了可逆 Transformer 和 LSH 哈希对内存、精度和速度的影响。</p><p>🎉可逆 Transformer 匹配基准：他们的实验结果表明，可逆的 Transformer 可以节省内存不牺牲精度：</p><p><img src="https://i.loli.net/2020/09/03/LwSPM5qFuBfQhaZ.png" alt="null"></p><p>在 enwik8 和 imagenet64 训练中，可逆性对性能的影响</p><p>🎉LSH 注意力匹配基准：注意 LSH 注意力是一个近似的全注意力，其准确性随着散列值的增加而提高。当哈希值为 8 时，LSH 的注意力几乎等于完全注意力：</p><p><img src="https://i.loli.net/2020/09/03/t4zNAXOf6iEZ7cH.jpg" alt="null"></p><p>LSH 注意力作为散列循环对 imagenet64 的影响</p><p>🎉他们也证明了传统注意力的速度随着序列长度的增加而变慢，而 LSH 注意力速度保持稳定，它运行在序列长度~ 100k 在 8GB 的 GPU 上的正常速度：</p><p><img src="https://i.loli.net/2020/09/03/aGRcA42EiNSBz8v.jpg" alt="null"></p><p>注意力评估的速度作为全注意力和 LSH 注意力的输入长度的函数</p><blockquote><p>与 Transformer 模型相比，最终的 Reformer 模型具有更高的存储效率和更快的存储速度。</p></blockquote><h4 id="参考">🚀参考</h4><blockquote><p><a href="https://www.6aiq.com/article/1583729200869" target="_blank" rel="noopener" class="uri">https://www.6aiq.com/article/1583729200869</a></p><p><a href="https://thinkwee.top/2020/02/07/reformer/" target="_blank" rel="noopener" class="uri">https://thinkwee.top/2020/02/07/reformer/</a></p><p><a href="https://aijishu.com/a/1060000000100293" target="_blank" rel="noopener" class="uri">https://aijishu.com/a/1060000000100293</a></p></blockquote><h3 id="transformer-xl-attentive-language-models-beyond-a-fixed-length-context">Transformer-XL : Attentive Language Models Beyond a Fixed-Length Context</h3><h4 id="论文概况-1">论文概况</h4><ul><li>来源：ACL 2019</li><li>arXiv: 1901.02860</li><li>作者：ZihangDai , ZhilinYang , YimingYang</li><li>论文地址： <a href="https://arxiv.org/abs/1901.02860" target="_blank" rel="noopener" class="uri">https://arxiv.org/abs/1901.02860</a></li><li>Code url：<a href="https://github.com/kimiyoung/transformer-xl" target="_blank" rel="noopener" class="uri">https://github.com/kimiyoung/transformer-xl</a></li><li>论文组会报告于<code>2020.08.15</code></li></ul><h4 id="背景-1">背景</h4><h5 id="问题-1">问题</h5><p>Transformer存在局限性：</p><p>1.在语言建模时的设置受到固定长度（segment）的限制。对长距离依赖的建模能力仍然不足</p><p>2.因为transformer将文本等分为相同的片段，导致了上下文碎片</p><h5 id="解决方案-1">解决方案</h5><p>使学习不再仅仅依赖于定长，且不破坏时间的相关性。</p><ol type="1"><li>提出<strong>片段级递归机制(segment-level recurrence mechanism)</strong>，引入一个<strong>记忆(memory)</strong>模块（类似于cache或cell） 之前计算过了不需要重复计算，直接为后面片段使用。</li></ol><ul><li>使得<code>长距离依赖的建模</code>成为可能；</li><li>使得片段之间产生交互，解决上下文碎片化问题</li></ul><p>2.提出<strong>相对位置编码机制</strong>，代替绝对位置编码。 Transformer的绝对位置编码指的是一个片段中，为1 为2 。如果是多个片段同时考虑的话，那么这种1，2就会重复，所以使用了相对位置编码的方法。这样可以在多个片段（segment）中使用相对编码。具体内容见论文</p><p>注：两者是一起使用的，共同解决transformer存在的局限性</p><h4 id="模型-transformer-xl">模型 transformer-XL</h4><h5 id="原始transformer">原始transformer</h5><p><img src="https://i.loli.net/2020/09/04/YU3mC2hIOA9ncrj.gif" alt="v2-732805e00feb35e41f1d00f8df516950_b"></p><h5 id="片段注意力机制">片段注意力机制</h5><p>为了解决长距离依赖，文章引入一个memory状态。</p><p>在训练过程中，每个片段的表示为最后的隐层状态，表示片段的序号，表示片段的长度，表示隐层维度。</p><p>在计算片段的表示时，用memory缓存片段层的隐层状态，用来更新，这样就给下一个片段同了上文，长距离依赖也通过memory保存了下来。并且，最大可能的依赖长度线性增长，达到**N*L**</p><p><img src="https://i.loli.net/2020/09/04/IjUWo7DahsNPAkv.gif" alt="v2-a8210cd2f9bfb9307ba81d694dc4e4b4_b"></p><h5 id="评估阶段">评估阶段</h5><h6 id="原始transformer-1">原始transformer</h6><p><img src="https://i.loli.net/2020/09/04/epOcXjYTy835dJv.gif" alt="v2-13a38126e684b838e5ed207fd5cae944_b"></p><h6 id="transformer-xl">Transformer-XL</h6><p><img src="https://i.loli.net/2020/09/04/qdYL5RsQEOFS8nG.gif" alt="v2-502e1e1fec12b326ace579e059b3b3df_b"></p><h4 id="实验-1">实验</h4><p>实验部分是对基于Transformer-XL的语言模型进行评估，分为字符级和词级。评价指标分别是bpc(每字符位数)和PPL(困惑度)，越小越好。enwiki8和text8用的是bpc。Transformer-XL在多个语言模型基准测试中实现了最先进的结果。 Transformer-XL第一个在char级语言模型基准enwiki8上突破1.0。</p><p><strong>去除实验：</strong></p><p><img src="https://i.loli.net/2020/09/04/ZV1lpewdtanoWi9.png" alt="image-20200904153950861"></p><p>重点是本文设计的相对位置编码<strong>优于</strong>其他工作，memory的设计也有很大的提升。</p><p>最后，Transformer-XL在评估阶段的速度也明显快于 vanilla Transformer，特别是对于较长的上下文。例如，对于 800 个字符的上下文长度，Transformer-XL 比Vanilla Transformer 快 363 倍；而对于 3800 字符的上下文，Transformer-XL 快了 1874 倍。</p><h4 id="创新点">创新点</h4><ul><li>提出了片段级递归机制和相对位置编码机制</li><li>依赖关系比原始Transformer长450％，并且在评估过程中，其速度比原始Transformer快1800倍以上</li></ul><h4 id="参考-1">🚀参考</h4><blockquote><p><a href="https://www.cnblogs.com/shona/p/12041055.html" target="_blank" rel="noopener" class="uri">https://www.cnblogs.com/shona/p/12041055.html</a></p><p><a href="https://zhuanlan.zhihu.com/p/83062195" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/83062195</a></p><p><a href="https://www.cnblogs.com/mj-selina/p/12373636.html" target="_blank" rel="noopener" class="uri">https://www.cnblogs.com/mj-selina/p/12373636.html</a></p><p><a href="https://zhuanlan.zhihu.com/p/70745925" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/70745925</a></p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      记录每周值得分享的论文，周一发布、
《reformer-the eficient transformer》、
《Transformer-XL-Attentive Language Models Beyond a Fixed-Length Context》

    
    </summary>
    
    
      <category term="论文分享" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"/>
    
    
      <category term="论文" scheme="http://yoursite.com/tags/%E8%AE%BA%E6%96%87/"/>
    
      <category term="论文分享" scheme="http://yoursite.com/tags/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"/>
    
      <category term="transformer" scheme="http://yoursite.com/tags/transformer/"/>
    
  </entry>
  
  <entry>
    <title>2020-08-31-git进阶</title>
    <link href="http://yoursite.com/2020/08/31/2020-08-31-git%E8%BF%9B%E9%98%B6/"/>
    <id>http://yoursite.com/2020/08/31/2020-08-31-git%E8%BF%9B%E9%98%B6/</id>
    <published>2020-08-31T08:41:49.000Z</published>
    <updated>2020-09-05T08:34:47.989Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言">前言</h3><p>之前总结过git的一些基本命令，后来使用了更多git，写博客用于记录。不断更新 ，在实践中总结git知识点。</p><p>回顾下之前的git基本操作</p><ul><li>将现有的项目添加提交并上传到远程仓库</li></ul><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">git add . #添加当前文件夹下的所有文件</span><br><span class="line"></span><br><span class="line">git commit -m "first commit " # 引号内是本次的提交说明 </span><br><span class="line"></span><br><span class="line">git push -u origin master # 提交本地分支到远程分支</span><br><span class="line">(若出现failed to push som refs to， 则执行git pull origin master，</span><br><span class="line">将远程服务器github上的master拉下来，再重新push)</span><br></pre></td></tr></tbody></table></figure><ul><li>clone代码</li></ul><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git clone   https://github.com/raymond-zhao/cat-mall.git   ../Github/cat-mall </span><br><span class="line">#将cat-mall代码克隆到  ../Github/cat-mall 中</span><br></pre></td></tr></tbody></table></figure><h3 id="git-status-和-git-diff">git status 和 git diff</h3><p>在对文件进行修改之后，可以用 <code>git status</code> 查看结果，可以让我们时刻掌握仓库当前的状态</p><p><img src="https://i.loli.net/2020/08/31/fTmxaAeZG1iSCLd.png" alt="image-20200831180231338" style="zoom:67%;"></p><p>可以看到在<code>modified</code>部分，可以看到有四个文件被修改了，<strong>但是还没有进行提交（<code>commit</code>）修改</strong></p><p>而下半部分的<code>untracked files</code>表示的是<strong>之前从未提交到仓库分支</strong>的文件（一个markd文件，一个照片）</p><p>上述只是看到被修改的文件，但如果能看看具体修改了什么内容就好了，<code>git diff</code> 可以实现这个功能</p><p><img src="https://i.loli.net/2020/08/31/nVd3hGKLJy6f7zH.png" alt="image-20200831194816455"></p><p>可以看到修改的详细细节（红色为修改前的内容，绿色为修改后的内容）。向下箭头可以下拉文本，<code>q</code>退出查看 （quit）</p><p>这样就可以放心的添加（add）到仓库的暂存区，并提交（commit）到仓库分支</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git add .</span><br><span class="line">git commit -m 20/8/31/commit1</span><br></pre></td></tr></tbody></table></figure><h4 id="小结">小结</h4><ul><li>要随时掌握工作区的状态，使用<code>git status</code>命令。</li><li>如果<code>git status</code>告诉你有文件被修改过，用<code>git diff</code>可以查看修改内容。</li></ul><h3 id="版本回退">版本回退</h3><p>每当文件修改到一定程度的时候，就可以“保存一个快照”，这个快照在Git中被称为<code>commit</code>。一旦你把文件改乱了，或者误删了文件，还可以从最近的一个<code>commit</code>恢复，然后继续工作，而不是把几个月的工作成果全部丢失。</p><p>在Git中，我们用<code>git log</code>命令查看：</p><p><img src="https://i.loli.net/2020/08/31/4lC7ufP6ZmbUvWT.png" alt="image-20200831233324006" style="zoom:80%;"></p><p><code>git log</code>命令显示从最近到最远的提交日志，每一次<code>commit</code>很详细</p><p>可以加上<code>--pretty=oneline</code>参数，来简化显示。推荐使用</p><p><img src="https://i.loli.net/2020/08/31/xNXAn7Pt8rT2mcE.png" alt="image-20200831233340202" style="zoom:80%;"></p><p>其中前面编号类似<code>012214236e...</code>的是<code>commit id</code>（版本号），是一个<code>SHA1</code>计算出来的一个非常大的数字，用十六进制表示</p><p>每个人的编号不一样，因为Git是分布式的版本控制系统，多人在同一个版本库里工作，如果大家都用1，2，3……作为版本号，那肯定就冲突了。</p><blockquote><p><a href="https://1024tools.com/hash" target="_blank" rel="noopener">Hash在线计算、md5计算、sha1计算、sha256计算、sha512计算</a></p></blockquote><h4 id="回退到历史版本">回退到历史版本</h4><p>这样我们就可以进行回退操作</p><p>首先，Git必须知道当前版本是哪个版本。</p><p>在Git中，用<code>HEAD</code>表示当前版本，也就是最新的提交<code>012214236e...</code>，上一个版本就是<code>HEAD^</code>，上上一个版本就是<code>HEAD^^</code>，当然往上100个版本写100个<code>^</code>比较容易数不过来，所以写成<code>HEAD~100</code>。</p><p>我们可以使用<code>git reset</code>命令：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git reset --hard HEAD^ <span class="comment">#回退到上一版本</span></span><br></pre></td></tr></tbody></table></figure><p><img src="https://i.loli.net/2020/08/31/k37ptdoMJxGcufR.png" alt="image-20200831233416378"></p><p>结果显示出现在是<code>ca41b0a</code>，也就是上一次<code>commit</code>的版本。我们成功回退版本！</p><p>当我们再查看日志的时候，发现已经没有<code>20/8/31/commit1</code>版本了</p><p><img src="https://i.loli.net/2020/08/31/xWIlbtTwazUJNdO.png" alt="image-20200831233446124"></p><hr><h4 id="还原到最新版本">还原到最新版本</h4><p>如果想要再还原到<code>20/8/31/commit1</code>版本呢？</p><p>也是可以的，只要<strong><code>上面的命令行窗口还没有被关掉</code></strong>，就可以顺着往上找，找到那个<code>20/8/31/commit1</code>版本的<code>commit id</code>是<code>012214236e...</code>，于是就可以指定回到未来的某个版本：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git reset --hard 0221423</span><br></pre></td></tr></tbody></table></figure><p><img src="https://i.loli.net/2020/08/31/9pK7UsnRrv1loSD.png" alt="image-20200831233507305"></p><p>版本号没必要写全，前几位就可以了，Git会自动去找。当然也不能只写前一两位，因为Git可能会找到多个版本号，就无法确定是哪一个了。</p><p>这样就实现了还原到最后<code>commit</code>版本</p><p>Git的版本回退速度非常快，因为Git在内部有个指向当前版本的<code>HEAD</code>指针，当你回退版本的时候，Git仅仅是把HEAD从指向历史版本，再将工作区的文件更新即可</p><p>如果回退到了某个版本，关掉了命令行窗口，后悔想恢复到新版本但是找不到新版本的<code>commit id</code>怎么办？</p><p>在Git中，总是有后悔药可以吃的。Git提供了一个命令<code>git reflog</code>用来记录你的每一次命令：</p><p><img src="https://i.loli.net/2020/09/01/Ly4MDnv6WAEwQlV.png" alt="image-20200901000008019"></p><p>知道<code>commit_id</code>，还原版本就十分滴完美！</p><blockquote><p><strong>注！！！</strong></p><p>如果从历史版本回到最后的版本，也只能还原到最后<code>commit</code>后的版本。</p><p>我才开始<code>commit</code>了版本A，之后又写了一部分内容 B(未<code>commit</code>)。还原到了A-1版本，之后又想还原到A+B版本，操作完之后发现还原后的没有B部分，也就是我只能还原到A。</p><p>原因就是我在最后一次<code>commit</code>就是A，而写完B之后，没有<code>commit</code> ，于是无法还原。 （多多<code>commit</code>，</p><p>，还原需谨慎。我是真的折腾）<span class="github-emoji" style="display:inline;vertical-align:middle;color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f62d.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f62d.png?v8">😭</span></p></blockquote><h4 id="小结-1">小结</h4><ul><li><code>HEAD</code>指向的版本就是当前版本，因此，Git允许我们在版本的历史之间穿梭，使用命令<code>git reset --hard commit_id</code>。 （commit_id也写成HEAD^）</li><li>穿梭前，用<code>git log</code>可以查看提交历史，以便确定要回退到哪个版本。</li><li>要重返未来，用<code>git reflog</code>查看命令历史，以便确定要回到未来的哪个版本。</li></ul><h4 id="参考">参考</h4><blockquote><p><a href="http://www.ruanyifeng.com/blog/2015/12/git-cheat-sheet.html" target="_blank" rel="noopener">常用git命令清单-阮一峰</a></p><p><a href="http://www.ruanyifeng.com/blog/2012/08/how_to_read_diff.html" target="_blank" rel="noopener">读懂diff-阮一峰</a></p><p><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000" target="_blank" rel="noopener">git教程-廖雪峰</a></p><p><a href="http://www.runoob.com/git/git-install-setup.html" target="_blank" rel="noopener">git教程-菜鸟教程</a></p><p><a href="https://git-scm.com/book/zh/v2" target="_blank" rel="noopener">gitbook</a></p><p><a href="http://gitbook.liuhui998.com/index." target="_blank" rel="noopener">Git Community Book</a></p><p><a href="https://juejin.im/post/6844903586023866375" target="_blank" rel="noopener">从只会git add .的菜鸟到掌握git基本功能</a></p></blockquote><h3 id="工作区和暂存区">工作区和暂存区</h3><h4 id="工作区working-directory">工作区（Working Directory）</h4><p>就是在电脑里能看到的目录，比如我的<code>mynlog</code>文件夹就是一个工作区：</p><p><img src="https://i.loli.net/2020/09/01/lp9hvTzLtVuMPG5.png" alt="image-20200901000937480" style="zoom:80%;"></p><h4 id="版本库repository">版本库（Repository）</h4><p>也就是本地仓库</p><p>工作区有一个隐藏目录<code>.git</code>，这个不算工作区，而是Git的版本库。（选择<code>隐藏文件可见</code>就可以看到）</p><p>Git的版本库里存了很多东西，其中最重要的就是称为stage（或者叫<code>index</code>）的暂存区，还有Git为我们自动创建的第一个分支<code>master</code>，以及指向<code>master</code>的一个指针叫<code>HEAD</code>。</p><p><img src="https://i.loli.net/2020/09/01/wBe5iWuajDJKxdV.png" alt="image-20200901001300425" style="zoom:80%;"></p><p><img src="https://i.loli.net/2020/09/01/KywEFn2dtMJeBkq.png" alt="image-20200901001406385" style="zoom:80%;"></p><p>前面讲了我们把文件往Git版本库里添加的时候，是分两步执行的：</p><p>第一步是用<code>git add</code>把文件添加进去，实际上就是把文件修改添加到暂存区(<code>index</code>)；</p><p>第二步是用<code>git commit</code>提交更改，实际上就是把暂存区的所有内容提交到当前分支(<code>master</code>)。</p><p>因为我们创建Git版本库时，Git自动为我们创建了唯一一个<code>master</code>分支，所以，现在，<code>git commit</code>就是往<code>master</code>分支上提交更改。</p><p>你可以简单理解为，<strong>需要提交的文件修改通通放到暂存区，然后，一次性提交暂存区的所有修改。</strong> 也就是可以多次<code>git add .</code> ,之后再一次性<code>git commit</code></p><p>我对文件进行修改之后，<code>git status</code> 显示如下：</p><p><img src="https://i.loli.net/2020/09/01/qZ4JcAzCrBOQng5.png" alt="image-20200901003431025"></p><p>这是对文件进行了修改，但是未添加（add）到暂存区和提交（commit）到仓库分支。 并且出现了之前从未提交的文件（四张png图片）</p><p>然后<code>git add .</code>,再查看目前的状态 <code>git status</code></p><p><img src="https://i.loli.net/2020/09/01/leMQO9F83N5DxUq.png" alt="image-20200901003844459"></p><p>出现了绿色的<code>new file</code>字样和<code>modified</code>，代表已添加到缓存区。</p><p>现在，暂存区的状态就变成这样了（原文是添加的readme和LICENSE文件）：</p><p><img src="https://i.loli.net/2020/09/01/MHQiJkB674jcAE5.png" alt="image-20200901003951252"></p><p>所以，<code>git add</code>命令实际上就是把要提交的所有修改放到暂存区（index），然后，执行<code>git commit</code>就可以一次性把暂存区的所有修改提交到分支。</p><p><img src="https://i.loli.net/2020/09/01/1uzidT9knarwNMU.png" alt="image-20200901004202394"></p><p>这时候再 <code>git status</code>，则是干净的</p><p>现在版本库变成了这样，暂存区就没有任何内容了：</p><p><img src="https://i.loli.net/2020/09/01/kCXlv3FiurZNbIO.jpg" alt="git-stage-after-commit"></p><h4 id="小结-2">小结</h4><p>了解工作区和暂存区的概念，并通过例子加强<code>git status</code> 、<code>git add</code>、<code>git commit</code>的理解</p><p>如果不用<code>git add</code>到暂存区，那就不会加入到<code>commit</code>中。也就是说<code>commit</code>只会提交暂存区里的内容</p><h3 id="撤销修改">撤销修改</h3><h4 id="在工作区撤销修改">在工作区撤销修改</h4><p>在工作区写的内容想要撤销，当然可以手动删除。同时还有另外的一种方法</p><p><code>git status</code> 查看一下状态</p><p><img src="https://i.loli.net/2020/09/01/qT7BN95PkQeSumd.jpg" alt="img"></p><p>根据git提示，可以知道如下信息：</p><ol type="1"><li><code>changes not staged for commit</code>：表示没有更改添加到暂存区，也就是对于当前的修改还没有进行<code>add</code>操作</li></ol><p><img src="https://i.loli.net/2020/09/01/NfZ2vXuB4etLHxF.jpg" alt="img"></p><ol start="2" type="1"><li><p>可以看到修改的部分是<code>2020-08-31-git 进阶.md</code>文件，不能显示中文，所以用编码表示</p></li><li><p>同时<code>next</code>文件也做了修改。这个每次都有提示，猜想应该是因为next是我<code>clone</code>下来的文件，所以存在<code>.git</code>文件，将<code>.git</code>文件删除就ok了</p></li><li><p>提示显示，<code>git checkout -- file</code>可以丢弃工作区（work directory）的修改</p></li></ol><h5 id="git-checkout----file">git checkout -- file</h5><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git checkout -- <span class="built_in">source</span>/_posts/2020-08-31-git进阶.md （注意--不要遗漏，同时后面有一个空格）</span><br><span class="line"><span class="comment"># git checkout -- .  这种写法也是可以的，表示全部撤销</span></span><br></pre></td></tr></tbody></table></figure><p>命令<code>git checkout -- filename</code>意思就是，把<code>filename</code>文件在工作区的修改全部撤销，这里有两种情况：</p><ul><li>一种是文件自修改后还没有被放到暂存区，现在，撤销修改就回到和版本库一模一样的状态；</li><li>一种是文件已经添加到暂存区后，又作了修改，现在，撤销修改就回到添加到暂存区后的状态。</li></ul><p>总之，就是让这个文件回到最近一次<code>git commit</code>或<code>git add</code>时的状态。</p><p><code>git checkout</code>其实是<strong>用版本库里的版本替换工作区的版本</strong>，无论工作区是修改还是删除，都可以“一键还原”。</p><h5 id="注">注</h5><ul><li>文件必须写当前git bash 下的完整路径，可以参考<code>git status</code>下的modified部分路径名称，如上的<code>source/_posts/</code></li><li>文件名必须写中文（就是正常的文件名），不能按照modified部分的编码后的名称</li></ul><p><img src="https://i.loli.net/2020/09/01/2l6Nxsh14bdpAY8.jpg" alt="img"></p><p>这是错误过程，可以看到最后一次没有提示，表示成功撤销修改</p><p>打开git进阶文件可以看到内容已经撤销</p><h4 id="添加到暂存区后的撤销">添加到暂存区后的撤销</h4><p>如果在工作区已经修改，并且添加到暂存区了，在<code>commit</code>之前，发现了这个问题。用<code>git status</code>查看一下，修改只是添加到了暂存区，还没有提交：</p><p><img src="https://i.loli.net/2020/09/01/CIASBwaTMEY3Gh1.png" alt="添加到暂存区前"></p><p><img src="https://i.loli.net/2020/09/01/T8jefu37XvFnZMJ.png" alt="添加到暂存区后"></p><ul><li>在添加到暂存区后，可以看到在<code>changes to be committed</code> 部分，添加的部分已经变成绿色，等待被<code>commit</code>提交</li><li>根据git提示，用命令<code>git reset HEAD &lt;file&gt;</code>可以把暂存区的修改撤销掉（<code>unstage</code>），重新放回工作区：</li></ul><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git reset HEAD .</span><br></pre></td></tr></tbody></table></figure><blockquote><p><code>git reset</code>命令既可以回退版本，也可以把暂存区的修改回退到工作区。当我们用<code>HEAD</code>时，表示最新的版本。</p></blockquote><p>撤销到工作区的内容可以根据上述内容撤销其修改</p><h4 id="提交到版本库后的撤销">提交到版本库后的撤销</h4><p>前提是<strong>还没有把自己的本地版本库推送到远程</strong>。</p><p>可以利用上述的<code>版本回退</code>功能</p><h4 id="小结-3">小结</h4><ul><li>场景1：当你改乱了<code>工作区</code>某个文件的内容，想直接丢弃工作区的修改时，用命令<code>git checkout -- file</code>。</li><li>场景2：当你不但改乱了工作区某个文件的内容，还<code>添加到了暂存区</code>时，想丢弃修改，分两步，第一步用命令<code>git reset HEAD &lt;file&gt;</code>，就回到了场景1，第二步按场景1操作，用命令<code>git checkout -- file</code>。</li><li>场景3：已经提交了不合适的修改到版本库时，想要撤销本次提交，参考<code>版本回退</code>，不过前提是没有推送到远程库。</li></ul><h3 id="远程仓库">远程仓库</h3><h4 id="section"></h4><p>已经在本地创建了一个Git仓库后，又想在GitHub创建一个Git仓库，并且让这两个仓库进行远程同步，这样，GitHub上的仓库既可以作为备份，又可以让其他人通过该仓库来协作。</p><p>首先，登陆GitHub，然后，在右上角找到“Create a new repo”按钮，创建一个新的仓库</p><p>在Repository name填入<code>shijian</code>，其他保持默认设置，点击“Create repository”按钮，就成功地创建了一个新的Git仓库。出现以下界面：</p><p><img src="https://i.loli.net/2020/09/01/vMnFmb9aQU6xuXp.png" alt="image-20200901142716798"></p><p>复制仓库的SSH链接</p><p>根据提示，可以返回到需要上传的文件夹目录下，右键选择<code>git bash</code></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git init  <span class="comment">#创建.git隐藏文件，用于本地仓库</span></span><br><span class="line"></span><br><span class="line">git remote add origin git@github.com:OopsAaron/shijian.git <span class="comment">#关联本地仓库和github远程仓库</span></span><br></pre></td></tr></tbody></table></figure><p>添加后，<strong>远程库的名字就是<code>origin</code></strong>，这是Git默认的叫法，也可以改成别的，但是<code>origin</code>这个名字一看就知道是远程库。</p><p>接下来就是git的基本三样操作，添加提交并推送</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">git add . <span class="comment">#添加当前文件夹下的所有文件</span></span><br><span class="line"></span><br><span class="line">git commit -m <span class="string">"first commit "</span> <span class="comment"># 引号内是本次的提交说明 </span></span><br><span class="line"></span><br><span class="line">git push -u origin master <span class="comment"># 提交本地分支到远程分支</span></span><br><span class="line">(若出现failed to push som refs to， 则执行git pull origin master，</span><br><span class="line">将远程服务器github上的master拉下来，再重新push)</span><br></pre></td></tr></tbody></table></figure><p>把本地库的内容推送到远程，用<code>git push</code>命令，实际上是把当前分支<code>master</code>推送到远程。这时候在github界面就可以看到推送的文件</p><blockquote><p>第一次push的时候可以添加参数 <code>-u</code> ，之后可以不添加</p><p>由于远程库是空的，我们第一次推送<code>master</code>分支时，加上了<code>-u</code>参数，Git不但会把本地的<code>master</code>分支内容推送的远程新的<code>master</code>分支，还会把本地的<code>master</code>分支和远程的<code>master</code>分支关联起来，在以后的推送或者拉取时就可以简化命令。</p></blockquote><h4 id="section-1"></h4><h3 id="分支">分支</h3><p>分支暂时用不到，就没有学习</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      不断更新 ，在实践中总结git知识点。
    
    </summary>
    
    
      <category term="git" scheme="http://yoursite.com/categories/git/"/>
    
    
      <category term="git" scheme="http://yoursite.com/tags/git/"/>
    
  </entry>
  
  <entry>
    <title>2020-08-30-阅读论文</title>
    <link href="http://yoursite.com/2020/08/30/2020-08-30-%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87/"/>
    <id>http://yoursite.com/2020/08/30/2020-08-30-%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87/</id>
    <published>2020-08-30T07:11:28.000Z</published>
    <updated>2020-09-05T06:42:17.124Z</updated>
    
    <content type="html"><![CDATA[<p>一篇论文至少要看三遍。 第一遍，仔细阅读论文中的标题、摘要和关键词。 第二遍，阅读文中的导言、结论以及图表，快速扫描一下论文剩下的内容。 这一步主要是要把握论文中的关键信息，不光是导言和结论，还包括文章中任何小结论的总 结，文中涉及的补充信息都跳过。 第三遍，阅读论文的整个部分，但是要跳过任何可能陌生看不懂的数学公式，技术术语。</p><p>不过，如果你需要对这个专业领域有一个「深入」的理解，那就必须要搞懂那些公式术语 了。</p><blockquote><p>参考</p><p><a href="https://mp.weixin.qq.com/s?__biz=MzA5ODEzMjIyMA==&amp;mid=2247501483&amp;idx=1&amp;sn=9b21f8e62fa2b4b33045900a1e721d30&amp;chksm=9094cf38a7e3462ed5901bd8b0b8ebf99a892b31d75aa6eaf8b3c8cc7698b1d708fd0891ab3e&amp;mpshare=1&amp;scene=1&amp;srcid=08304BRBOlTyXb4qDBM5SGTj&amp;sharer_sharetime=1598771438122&amp;sharer_shareid=2c9f868695c34cf5ff5f7a42eab3d2ed&amp;key=9b9af4fa8e2c96d71311b901f9c755ada338c70880cf596dfe4bc2b5ca69cfb094a0a310cf713fb50591ef0933e5f438e73110d797ab7406eeefa5dd5f4c460076a3e94537447c235df683c3eb24a048c7472ad2cdef063ec759505ebb6902987eb9a08ca55be656525ace69f39c8cdbedf7f2b71fa3d2c7c2c4b0dd2e660589&amp;ascene=1&amp;uin=ODEyNzQwMTM5&amp;devicetype=Windows+10+x64&amp;version=62090529&amp;lang=zh_CN&amp;exportkey=A4y9BBn%2B1GAGlkX0mhK71n0%3D&amp;pass_ticket=tS9Bcx3H%2FQ34yaxv%2F0nHTttU4aZeBoKDlw2k4Zwl5JMpqZkqPjEwcrpqIlAybtka" target="_blank" rel="noopener">沈向阳：读论文的三个层次</a></p><p><a href="https://www.youtube.com/watch?v=Du7qLsToW-o&amp;t=443s" target="_blank" rel="noopener">youtube视频，沈向阳读论文</a></p><p><a href="https://mp.weixin.qq.com/s?subscene=19&amp;__biz=MzIzNjc1NzUzMw==&amp;mid=2247546863&amp;idx=2&amp;sn=275577791d4cee894bd874eedc846f88&amp;chksm=e8d0809ddfa7098b90f2d601d59c1ea11162180387dd9677a96c03e666c1b1e05f8e719d989e&amp;scene=7&amp;ascene=1&amp;devicetype=Windows+10+x64&amp;version=62090529&amp;nettype=cmnet&amp;abtest_cookie=AAACAA%3D%3D&amp;lang=zh_CN&amp;exportkey=Ax%2BhWVV2Xr753%2BtDF%2BAIKRw%3D&amp;pass_ticket=sT%2F05g2Sqp72CoAfTsiZ8TDrxTKg0f%2FTh968brMSrSyOqE%2F1GuTq0PTOveYYBqof&amp;wx_header=0&amp;key=573aef4c1f9b4b5fc4e631a99eb0547e4182bf3ee5dd048cf4487f739b93c9b004f67e751713dea6880a5a922c03ceb30730558ff6be83d973abec53f6fb592491c98a1e205921d9c380c59d6f30c92ea2b1836956318f54b99e962b4d620a7ca074f2e317b259e495570360cc981c43758194fb5e38587a176b8af431cca351&amp;uin=ODEyNzQwMTM5" target="_blank" rel="noopener">吴恩达教你如何读论文：绘制进度表格，论文至少看三遍，还要问自己问题</a></p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      阅读沈向阳教授的《读论文的三个层次》总结
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-08-27-edge还原到旧版本问题</title>
    <link href="http://yoursite.com/2020/08/27/2020-08-27-edge%E8%BF%98%E5%8E%9F%E5%88%B0%E6%97%A7%E7%89%88%E6%9C%AC%E9%97%AE%E9%A2%98/"/>
    <id>http://yoursite.com/2020/08/27/2020-08-27-edge%E8%BF%98%E5%8E%9F%E5%88%B0%E6%97%A7%E7%89%88%E6%9C%AC%E9%97%AE%E9%A2%98/</id>
    <published>2020-08-27T02:06:02.000Z</published>
    <updated>2020-09-04T07:00:20.595Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言">前言</h3><p>win10自动更新edge，但是新版的edge用的是Chromium内核，新功能添加不少，也全部支持chrome的插件，但是对pdf的支持不友好，和chrome一个德行<span class="github-emoji" style="display:inline;vertical-align:middle;color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f611.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f611.png?v8">😑</span>。导致我在旧版本edge阅读论文时做的笔记在新版edge体验感极差，于是想着回退到旧版本 （edge不就是用来阅读论文的 ）<span class="github-emoji" style="display:inline;vertical-align:middle;color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f606.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f606.png?v8">😆</span></p><h3 id="版本回退">版本回退</h3><p>百度后发现将新版本的edge删除，就可以自己回退到旧版本的edge</p><p>geek<code>强制删除</code>新版edge之后，系统回退到旧版edge了。但是同时发现原来的一些设置消失了，系统不太稳定，可能是强制删除了一些系统配置文件。不太清楚具体原因，待解决</p><p>导致如下两个功能消失</p><ul><li>在开始栏中不显示安装的软件</li><li>在文件夹右键不能使用<code>发送到</code>功能</li></ul><p>目前发现这两个不能使用的功能。可能还存在其它故障。平时经常通过<code>开始栏</code>打开软件，不显示之后有点麻烦</p><h3 id="解决">解决</h3><p>用<code>listary</code>软件快速搜索软件名称，（双击ctrl键打开搜索框），然后添加到Rolan中，如图所示</p><p><img src="https://i.loli.net/2020/09/04/Kbk3ERwsMJ9AhlP.png" alt="image-20200904145340602" style="zoom:80%;"></p><h3 id="小结">小结</h3><ul><li>下次还是少用geek强制删除吧，乖乖在<code>程序与功能</code>中卸载删除吧，有可能涉及系统配置文件的就不要轻易删除</li><li>等待新版edge友好支持论文阅读</li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      edge还原到旧版本出现的问题
    
    </summary>
    
    
    
      <category term="故障排除" scheme="http://yoursite.com/tags/%E6%95%85%E9%9A%9C%E6%8E%92%E9%99%A4/"/>
    
  </entry>
  
  <entry>
    <title>2020-08-25-chrome插件</title>
    <link href="http://yoursite.com/2020/08/25/2020-08-25-chrome%E6%8F%92%E4%BB%B6/"/>
    <id>http://yoursite.com/2020/08/25/2020-08-25-chrome%E6%8F%92%E4%BB%B6/</id>
    <published>2020-08-25T12:21:35.000Z</published>
    <updated>2020-08-31T12:16:47.636Z</updated>
    
    <content type="html"><![CDATA[<h3 id="下载提速"><strong>下载提速</strong></h3><ul><li><h4 id="使用场景"><strong>使用场景</strong></h4></li></ul><p>Chrome的下载速度，有时候确实是慢得可以跟某网盘相媲美了，甚至赶不上某些国产浏览器。</p><p>这是因为，Google为了兼容所有的电脑性能和带宽，在Chrome中采取的是保守<strong>单线程下载机制</strong>，这就导致很多时候下载速度非常慢了。</p><p><img src="https://i.loli.net/2020/08/25/5ZMgPIUfc2YndtE.png" alt="img"></p><p>不过，很多人都不知道的是，Chrome其实也是自带多线程下载功能的。所谓多线程下载，就是可以同时对资源建立多个连接，提升下载速度。</p><p>只是这个功能是默认关闭的，需要用户手动去开启。</p><ul><li><h4 id="使用方法"><strong>使用方法</strong></h4></li></ul><p>在浏览器地址栏输入以下网址并回车：</p><p><strong>chrome://flags/#enable-parallel-downloading</strong></p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/D1XlU0QfU3GnNfPmPtO4D3rncDTK3kFcCxQMtjnyMUqI5hTIZydfXEDTnp06YjKEBIbdlnvUoFj3ht3ibXUatiaw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>在Parallel downloading的后面选项里，把「default」改为「Enabled」，并按照提示重启浏览器。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/D1XlU0QfU3GnNfPmPtO4D3rncDTK3kFcYca3x5SEBJpOky2icdUADwP04jUYiaib6WvUQZ9XlSNHdeiach2RMydRGg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><p>这样就可以开启多线程下载了，经过实际测试，下载速度至少提高了三倍左右（也有可能下载速度飙升一段时间又跌回去）。</p><h3 id="link-to-text-fragment">Link to Text Fragment</h3><p>实际上就是带锚点功能的网页分享工具。</p><p>所谓锚文本，简单来说就像是关键词的定位，将关键字指向指向另一个页面的链接就是锚文本。这个工具则可以让你将网页上选中的文本片段生成为一个锚文本。</p><p><strong>当你点击这个锚文本时，就会直接跳转到该网页对应标记的锚点上了。</strong></p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/D1XlU0QfU3FtLAG0sobAP0xrYk6LJk6m3AU0icjVgSjiavYp3msxibjM7D9U6PXFbzm4wUeZ6OkaFibhPXLFeIBLOQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p><h4 id="使用方法-1"><strong>使用方法</strong></h4><h5 id="生成锚文本"><strong>生成锚文本</strong></h5><p>鼠标划词选中文本，在右键菜单中选择【Copy Link to Text Fragment】，然后可以看到该文本被黄色标记。</p><p><img src="https://i.loli.net/2020/08/25/ns29PiDENtvJp3R.gif" alt="img"></p><h5 id="打开锚文本"><strong>打开锚文本</strong></h5><p>此时，锚文本已经自动生成并复制到你的剪贴板上，你可以将它发送给需要分享的好友，或者在浏览器中打开，另存为书签。</p><p>可以看到，在浏览器内打开这个锚文本，网页会自动定位到我们做了锚点的文本部分，再也不需要我们自行阅读查找，非常方便。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_gif/D1XlU0QfU3FtLAG0sobAP0xrYk6LJk6m5F5vhCQoN8IeaxDibdMzqk2jFVjhDDhGMJdY3ZHpibCicN5yWbsRoN9Bg/640?wx_fmt=gif&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt="img"></p><p>需要注意的是，这个锚文本也<strong>仅限在安装了Link to Text Fragment插件的浏览器上打开</strong>，若没有安装则不会跳转对应位置。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      记录用到的chrome插件
    
    </summary>
    
    
      <category term="chrome" scheme="http://yoursite.com/categories/chrome/"/>
    
    
      <category term="chrome" scheme="http://yoursite.com/tags/chrome/"/>
    
  </entry>
  
  <entry>
    <title>2020-08-23-google搜索的高效使用</title>
    <link href="http://yoursite.com/2020/08/23/2020-08-23-google%E6%90%9C%E7%B4%A2%E7%9A%84%E9%AB%98%E6%95%88%E4%BD%BF%E7%94%A8/"/>
    <id>http://yoursite.com/2020/08/23/2020-08-23-google%E6%90%9C%E7%B4%A2%E7%9A%84%E9%AB%98%E6%95%88%E4%BD%BF%E7%94%A8/</id>
    <published>2020-08-23T09:04:44.000Z</published>
    <updated>2020-08-24T11:51:36.784Z</updated>
    
    <content type="html"><![CDATA[<h3 id="section">“”</h3><p>以整个短语作为搜索关键词，而不是拆开成每个词。</p><p>表示完全匹配，结果中必须出现与搜索文本完全相同的内容。</p><h3 id="a--b">A -B</h3><p>搜索包含A但不包含B的结果（请注意A后面的<strong>空格不能省略</strong>）</p><p><img src="https://i.loli.net/2020/08/23/pjDWIreKGtJU7Oa.png" alt="image-20200823173525379" style="zoom: 67%;"></p><p>当加上 <code>-poweredge</code> 时，就可以屏蔽掉机架式服务器关键字中所有含有poweredge的内容</p><p><img src="https://i.loli.net/2020/08/23/k2vbVtdCMBYzJfQ.png" alt="image-20200823173614387" style="zoom:67%;"></p><h3 id="filetype">filetype</h3><p>搜索对应类型的文件。例如：<code>时间简史 filetype:pdf</code>，即为搜索包含关键字时间简史的pdf文件。（请注意<strong>使用英文的冒号</strong>） （一般不加filetype也可以）</p><p><img src="https://i.loli.net/2020/08/23/UBJHv4YmfiGRhuy.png" alt="image-20200823174000514" style="zoom:67%;"></p><h3 id="site">site</h3><p>在某个网站内搜索，比如：site:<a href="https://link.zhihu.com/?target=http%3A//pan.baidu.com">http://pan.baidu.com</a> 特别好用，用来搜百度云里的资源。再如：</p><p>在我们实验室网站查找<code>招生</code>关键字，则 <code>招生 site:http://www.ubinec.org/</code>，十分便捷。</p><p>（直接招生 site:ubinec.org/ 也可以 ，中间不要加空格 ）</p><p><img src="https://i.loli.net/2020/08/24/aypbHTDAnF79CIi.png" alt="image-20200824160653772" style="zoom:67%;"></p><h3 id="section-1">*</h3><p>很多时候想搜一个东西但是不确定具体名字，可以用星号代替忘了的字，可以代替多个字</p><h3 id="define">define</h3><p><strong>当字典或快速查找意思</strong>，如[define:right]，还能看到单词在书籍中出现频率的年代变化，词源等；</p><p><img src="https://i.loli.net/2020/08/24/skUBwAFtVMm6OWJ.png" alt="image-20200824162755266" style="zoom:67%;"></p><h3 id="section-2">~</h3><p>同时搜索近义词。如搜“higher education” 和 “university”</p><h3 id="or-或逻辑">OR (或)逻辑</h3><p>通过<em>OR</em> 搜索, 可以得到和两个关键词分别相关的结果, 而不仅仅是和两个关键词都同时相关的结果.</p><p><img src="E:\myBlog\source\_posts\image-20200824193744947.png" alt="image-20200824193744947" style="zoom:67%;"></p><p><img src="E:\myBlog\source\_posts\image-20200824193731306.png" alt="image-20200824193731306" style="zoom:67%;"></p><h3 id="限定年份">限定年份</h3><ol type="1"><li>在google工具选项中可以选择时间</li></ol><p><img src="E:\myBlog\source\_posts\image-20200824194504266.png" alt="image-20200824194504266" style="zoom:67%;"></p><ol start="2" type="1"><li><code>世界杯 2010..2014</code></li></ol><h3 id="参考">参考</h3><p><img src="https://i.loli.net/2020/08/24/uqRyUOdYnGJbxHN.jpg" alt="preview"></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      高效利用google搜索，记录使用技巧
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-08-19-期望、方差、协方差及相关系数的基本运算</title>
    <link href="http://yoursite.com/2020/08/19/2020-08-19-%E6%9C%9F%E6%9C%9B%E3%80%81%E6%96%B9%E5%B7%AE%E3%80%81%E5%8D%8F%E6%96%B9%E5%B7%AE%E5%8F%8A%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0%E7%9A%84%E5%9F%BA%E6%9C%AC%E8%BF%90%E7%AE%97/"/>
    <id>http://yoursite.com/2020/08/19/2020-08-19-%E6%9C%9F%E6%9C%9B%E3%80%81%E6%96%B9%E5%B7%AE%E3%80%81%E5%8D%8F%E6%96%B9%E5%B7%AE%E5%8F%8A%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0%E7%9A%84%E5%9F%BA%E6%9C%AC%E8%BF%90%E7%AE%97/</id>
    <published>2020-08-19T12:17:59.000Z</published>
    <updated>2020-08-19T13:21:26.236Z</updated>
    
    <content type="html"><![CDATA[<p>链接</p><p>https://blog.csdn.net/touristman5/article/details/56281887</p><p>https://developer.aliyun.com/article/65262</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;链接&lt;/p&gt;
&lt;p&gt;https://blog.csdn.net/touristman5/article/details/56281887&lt;/p&gt;
&lt;p&gt;https://developer.aliyun.com/article/65262&lt;/p&gt;
&lt;script&gt;
     
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-08-19-解读卡尔曼滤波[第二部分]</title>
    <link href="http://yoursite.com/2020/08/19/2020-08-19-%E8%A7%A3%E8%AF%BB%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2-%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86/"/>
    <id>http://yoursite.com/2020/08/19/2020-08-19-%E8%A7%A3%E8%AF%BB%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2-%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86/</id>
    <published>2020-08-19T06:41:38.000Z</published>
    <updated>2020-08-30T07:21:51.102Z</updated>
    
    <content type="html"><![CDATA[<p>在开始之前，我想解释几个基本术语，如方差（variance）、标准差（standard deviation）、估计值（estimate）、准确度（accuracy）、精度（precision）、平均值（mean）和期望值（expected value）。</p><p>我想本教程的许多读者都熟悉基本统计学知识。但是，在本教程的开头，我承诺提供理解卡尔曼滤波器操作所需的必要背景知识。如果您熟悉这个主题，可以跳过它。</p><ul><li><strong>平均值与期望值</strong></li></ul><p>虽然<strong>平均值(mean)</strong>与<strong>期望值（expected value）</strong>是密切相关的术语。但是，它们是不同的。</p><p>例如，假设有五种不同的硬币——两个5美分的硬币和三个10美分的硬币，我们可以通过平均硬币的价值来轻松计算硬币的平均值。</p><p><img src="https://i.loli.net/2020/08/19/JxTbfDa2QOrz4oj.png" alt="image-20200819144207768"></p><p>上述结果不能被定义为期望值，因为系统状态（硬币值）没有被隐藏（想要表达的是确定的，此处没有任何不确定性），我们已经使用了所有的population（所有5枚硬币）来计算平均值。</p><p><strong>译者补充：因为很多同学经常混淆平均值与期望值的概念，因此，我在此特别解释一下。在解释两个概念之前，先说一下“大数法则”。</strong></p><ul><li>先说一下大数法则：</li></ul><p><img src="https://i.loli.net/2020/08/19/cHPovqVb2RFOS3r.jpg" alt="img"></p><ul><li>思考一下为什么会用到期望值？</li></ul><p><img src="https://i.loli.net/2020/08/19/F6K9M4aPrJxgnOS.jpg" alt="img"></p><p><img src="https://i.loli.net/2020/08/19/1QKexZgmNF6STPt.jpg" alt="img"></p><p>期望值也就是每个数*对应的概率值，再求和</p><p><strong>译者补充完毕。</strong></p><p>现在假设同一个人的五个不同的体重测量值：79.8kg，80kg，80.1kg，79.8kg和80.2kg。</p><p>由于秤的随机测量误差，称重测量值不同。 我们不知道准确的重量值是多少，因为它是一个<strong>隐藏变量Hidden Variable</strong>。 但是，我们可以通过平均尺度测量来估计重量。 （准确测量值不可知）</p><p><img src="https://i.loli.net/2020/08/19/PjARubvWoLmEV5U.png" alt="image-20200819145014158"></p><p>估计的结果是体重的期望值。</p><p>平均数经常使用希腊字母：<strong>μ</strong></p><p>期望值使用字母：<strong>E</strong></p><ul><li><strong>方差与标准差</strong></li></ul><p>方差用来度量随机变量与其期望值（即随机变量的期望值）之间的离散程度。</p><p>标准差是方差的平方根。标准差： <img src="https://www.zhihu.com/equation?tex=%5Csigma" alt="[公式]"> ，方差： <img src="https://www.zhihu.com/equation?tex=%5Csigma%5E%7B2%7D" alt="[公式]"></p><p>例如，我们想比较两个高中篮球队的身高。下表提供了两支球队的球员身高及其平均值。</p><p><img src="https://pic4.zhimg.com/80/v2-d6b2fea25722ffe4774167c3ca530177_1440w.png" alt="img"></p><p>如我们所见，两队的平均身高是一样的。现在让我们检查一下高度变化height variance。</p><p>由于方差用来度量随机变量与其期望值（即随机变量的期望值）之间的离散程度，我们想知道数据集偏离其平均值的情况。我们可以通过从每个变量中减去平均值来计算每个变量与平均值之间的距离。</p><p>我们将用x表示高度，用希腊字母μ表示高度的平均值。每个变量与平均值的距离为：</p><p><img src="https://pic1.zhimg.com/80/v2-8aeb370b55ec20c81cd6fb0ea1581a60_1440w.jpg" alt="img"></p><p>下表给出了每个变量与平均值之间的距离。</p><p><img src="https://picb.zhimg.com/80/v2-47a3cbd7c92a22bc2c1b532557d90609_1440w.png" alt="img"></p><p>下表给出了每个变量与平均值的平方距离。</p><p>有些值是负数。为了消除负值影响，让我们将高度与平均值的距离平方：</p><p><img src="https://picb.zhimg.com/80/v2-2bcc387a3d7e0da6267b04936c845c17_1440w.jpg" alt="img"></p><p><img src="https://pic4.zhimg.com/80/v2-759cb19ebbc545066259cfefb22237fb_1440w.png" alt="img"></p><p>为了计算数据集的离散程度，我们需要从中找出所有平方距离的平均值：</p><p><img src="https://pic2.zhimg.com/80/v2-0d382cddfdce473dfc744a382782c5ac_1440w.jpg" alt="img"></p><p>A队的方差是：</p><p><img src="https://pic3.zhimg.com/80/v2-9cc05c08921aab36b395d5b42134c911_1440w.png" alt="img"></p><p>B队的方差是：</p><p><img src="https://pic2.zhimg.com/80/v2-2b935f764b16ebf4b3420fd0f25574a5_1440w.png" alt="img"></p><p>我们可以看出，虽然两队的平均值相同，但A队的身高分布值高于B队的身高分布值，这意味着A队在控球员、中锋和后卫等不同位置有不同的球员，而B队球员则技能相差无几。</p><p>方差的单位是平方的；查看标准差更方便。正如我已经提到的，标准差是方差的平方根。</p><p><img src="https://pic1.zhimg.com/80/v2-4e5b9f3338566969fe0523fa06731489_1440w.jpg" alt="img"></p><p>A队运动员身高的标准差为0.12米。</p><p>B队运动员身高的标准差为0.036米。</p><p>进一步的，现在，假设我们要计算所有高中篮球运动员的平均值和方差。这是一项非常艰巨的任务，我们需要收集所有高中运动员的数据。</p><p>但是，我们可以通过选择一个大的数据集并对这个数据集进行计算来估计参与者的平均值和方差。（样本估计全局）</p><p>随机选取的100名选手的数据集足以进行准确的估计。</p><p>然而，当我们<strong>估计方差</strong>时，方差计算公式略有不同。我们不用N因子归一化，而是用N - 1因子归一化:</p><p><img src="https://pic1.zhimg.com/80/v2-9045e0012dc9592019009cca6c64f97f_1440w.jpg" alt="img"></p><p>你可以在以下资源中看到这个方程的数学证明：<a href="https://link.zhihu.com/?target=http%3A//www.visiondummy.com/2014/03/divide-variance-n-1/">http://www.visiondummy.com/2014/03/divide-variance-n-1/</a></p><hr><ul><li><strong>正态分布</strong></li></ul><p>事实证明，许多自然现象服从正态分布。继续以篮球运动员身高为例，如果随机选取运动员，构建大数据集，绘制身高VS.身高（heights vs. heights）的频率曲线图，得到“钟形”曲线，如下图所示:</p><p><img src="https://pic4.zhimg.com/80/v2-ca75549d80903118ac9a6ac08b58debc_1440w.jpg" alt="img"></p><p>正如你所看到的，这条曲线关于平均值（平均值是1.9米）对称。平均值附近值的频率高于远处值的频率。</p><p>高度的标准差等于0.2米。68.26%的值在平均值的一个标准差内。如下图所示，68.26%的值介于1.7米和2.1米之间（绿色区域占曲线下总面积的68.26%）。</p><p><img src="https://pic2.zhimg.com/80/v2-0f51f3a38d61049e8dc5dd18e363c114_1440w.jpg" alt="img"></p><p>95.44%的值在距离平均值的两个标准差内。</p><p>99.74%的值在距离平均值的三个标准差内。</p><p>正态分布，也称为高斯分布（它以数学家Carl Friedrich Gauss的名字命名），由以下方程描述：</p><p><img src="https://pic4.zhimg.com/80/v2-e4aee9ac01b76d9d688929e3d07ae69e_1440w.jpg" alt="img"></p><p>通常，测量误差是正态分布的，因此<code>卡尔曼滤波器设计基于测量误差是正态分布的假设。</code></p><hr><ul><li><strong>估计、准确度与精度</strong></li></ul><p><strong>— 估计（Estimate）：</strong>评估系统的隐藏状态。飞机的真实位置对观察者来说是隐藏的。我们可以用雷达等传感器来估计飞机的位置。采用多传感器和先进的估计跟踪算法（如卡尔曼滤波），可以显著提高估计精度。每一个测量或计算参数都是一个估计值。</p><p><strong>— 准确度（Accuracy）：</strong>表明测量值与真实值的接近程度。</p><p><strong>— 精度（Precision）：</strong>描述同一参数的许多 度量值中有多少可变性。准确度和精度是估算的基础。</p><p>下图说明了准确度和精度。</p><p><img src="https://pic3.zhimg.com/80/v2-7a1dfe3f5186ade70b8937099fb180a8_1440w.jpg" alt="img"></p><p><strong>高精度系统的测量方差较低</strong>（即不确定度/离散程度/变化程度较低），而低精度系统的测量方差较大（即不确定度/离散程度/变化程度较高）。方差是由随机测量误差产生的。</p><p>低精度系统被称为偏差系统，因为它们的测量具有内置的系统误差（偏差）。</p><p>通过<strong>平均或平滑测量</strong>可以显著降低方差的影响。例如，如果我们使用一个具有随机测量误差的温度计来测量温度，我们可以进行多次测量并对测量的值进行平均。由于误差是随机的，所以有些测量值会高于真实值，而另一些测量值会低于真实值。我们做的测量越多，估计就越接近。</p><p>另一方面，如果温度计有偏差，估计将包括一个恒定的系统误差。</p><p>本教程中的所有示例都假定系统是无偏差的。</p><p><img src="E:\myBlog\source_posts\image-20200819194609812.png" alt="image-20200819194609812"></p><p>极小化性能指标： 最优解</p><p><img src="E:\myBlog\source_posts\image-20200819195511498.png" alt="image-20200819195511498"></p><p>J就是选择能够令方差 <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%7B%28%7BX_%7Bi%7D%7D%28%5Ctheta_%7Bhat%7D%29-%5Cmu%29%5E2%7D" alt="[公式]"> 最小的的参数。 X^就是最优解</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      对卡尔曼滤波知识点的总结
    
    </summary>
    
    
      <category term="研究方向" scheme="http://yoursite.com/categories/%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91/"/>
    
    
      <category term="卡尔曼滤波" scheme="http://yoursite.com/tags/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2/"/>
    
      <category term="RKN" scheme="http://yoursite.com/tags/RKN/"/>
    
  </entry>
  
</feed>
