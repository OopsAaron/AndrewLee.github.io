<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>思建的NLP之旅</title>
  
  <subtitle>沉淀自己</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-10-31T08:27:16.216Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>李思建</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>2020-10-31-注意力机制总结</title>
    <link href="http://yoursite.com/2020/10/31/2020-10-31-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2020/10/31/2020-10-31-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%80%BB%E7%BB%93/</id>
    <published>2020-10-31T02:19:17.000Z</published>
    <updated>2020-10-31T08:27:16.216Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言">前言</h3><p>注意力模型最近几年在深度学习各个领域被广泛使用，无论是图像处理、语音识别还是自然语言处理的各种不同类型的任务中，都很容易遇到注意力模型的身影。所以，了解注意力机制的工作原理对于关注深度学习技术发展的技术人员来说有很大的必要。</p><blockquote><p>RNN做机器翻译有它自身的弱点，Attention正是为了克服这个弱点而出现的。所以，要理解Attention，就要搞明白两件事： - RNN在做机器翻译时有什么弱点 - Attention是如何克服这个弱点的</p></blockquote><h3 id="rnn做机器翻译的经典思路-encoder-decoder">RNN做机器翻译的经典思路 encoder-decoder</h3><p>用RNN做机器翻译时，通常需要两个RNN网络，一个用来将接收待翻译语句，对其进行编码，最后输出一个vector，这个网络叫encoder。然后，该vector会作为输入，传给另一个RNN网络，该网络用来根据vector产生目标语言的翻译语句，这个网络叫做decoder。如下图所示：</p><p><img src="https://i.loli.net/2020/10/31/UOF2qxEoRB7ftsP.png" alt="image-20201031104117636"></p><p>上图中间的Context就是我们这里说的第一个RNN产生的vector</p><h3 id="encoder-decoder的缺点在哪里">encoder-decoder的缺点在哪里？</h3><p>encoder-decoder最大的缺点是，encoder接收了不管多长的语句，最后输出的只是最后一个vector，当语句很长时，这个vector能否有效地表示该语句是很值得怀疑的。 如何解决这个问题呢？我们很自然会想到，第一个RNN其实在中间会产生很多输出，这些输出都被我们抛弃了，我们只用了最后的一个。如果能利用上中间的输出，兴许可以解决问题。Attention正是利用上了这些中间的输出。</p><h3 id="attention是如何利用中间的输出的">Attention是如何利用中间的输出的</h3><p>先上图，再来解释：</p><p><img src="https://i.loli.net/2020/10/31/ixs6baohzDmjfPn.png" alt="image-20201031104152681"></p><p>上图中的A是我们的encoder， B是我们的decoder。 可以想象，A网络接收了一个四个字的句子，对每个字都产生了一个输出（这些输出都是一个vector），我们称其为s1，s2，s3，s4。</p><p>我们看上图的B网络，在第一个B产生的hidden state（称其为h1）除了传给下一个cell外，还传到了A网络，这里就是Attention发挥作用的地方，我们来看看发生了什么。</p><p><strong>第一步</strong>： h1 分别与s1，s2，s3，s4做点积，产生了四个数，称其为m1，m2，m3，m4（这些都是标量，不是向量了！）</p><p><strong>第二步</strong>： m1，m2，m3，m4 传到一个softmax层，产生一个概率分布a1，a2，a3， a4。</p><p><strong>第三步</strong>： 将a1，a2，a3， a4 与s1，s2，s3，s4分别相乘，再相加，得到得到一个vector，称其为Attention vector。</p><p><strong>第四步</strong>：</p><p>Attention vector 将作为输入传到B网络的第二个cell中，参与预测。</p><p>以上就是Attention机制的基本思想了。我们看到，Attention vector 实际上融合了s1，s2，s3，s4的信息，具体的融合是用一个概率分布来达到的，而这个概率分布又是通过B网络上一个cell的hidden state与s1，s2，s3，s4进行点乘得到的。 Attention vector实际上达到了让B网络聚焦于A网络输出的某一部分的作用。</p><h3 id="attention中产生概率分布的两种方法">Attention中产生概率分布的两种方法</h3><p>在第3部分中，我们的概率分布来自于h与s的点积再做softmax，这只是最基本的方式。在实际中，我们可以有不同的方法来产生这个概率分布，每一种方法都代表了一种具体的Attention机制。</p><ul><li><h4 id="加法attention">1 加法Attention</h4><p>在加法Attention中，我们不再让h与s做点积，而是做如下的运算：</p><p><img src="https://i.loli.net/2020/10/31/dOAr8BHlK25vQuU.png" alt="image-20201031104221351"></p></li></ul><p>va和Wa都是可以训练的参数。h与s之间的分号表示将二者接到一起产生一个更长的vector。这样产生的数再送往softmax层，进而产生一个概率分布。</p><p>当然，我们还可以这么做：</p><p><img src="https://i.loli.net/2020/10/31/Hu4eWOfmQLk9DrK.png" alt="image-20201031104229311"></p><p>这里只是不再把h与s接到一起而已，本质上没有什么区别的。</p><ul><li><h4 id="乘法attention">2 乘法Attention</h4><p>乘法Attention将h与s做如下的运算：</p><p><img src="https://i.loli.net/2020/10/31/V87BZWl3sMUeGvb.png" alt="image-20201031104239592"></p></li></ul><p>显然，乘法Attention的参数更少，效率自然也会更高一些。</p><h3 id="attention机制的扩展">Attention机制的扩展</h3><p>Attention机制的核心在于对一个序列数据进行聚焦，这个聚焦是通过一个概率分布来实现的。这种机制其实有很强的普适性，可以用在各个方面。</p><p>比如，根据图片产生描述该图片的文字， 首先，图片会经过CNN进行特征的提取，提取的数据会输入到产生描述文字的RNN中，这里，我们可以引入Attention机制，让我们在产生下一个文字时，聚焦于我们正在描述的图片部位。</p><p>其次，在句子表示中，self Attention机制是成功扩展的Attention的范例。其基本原理如下：</p><p>假如我们用一个RNN读入了一个句子，产生了h1， h2，h3，h4四个hidden state。 为了得到该句子的摘要，我们可以这样做： 对每一个h计算一个分数：</p><p><img src="https://i.loli.net/2020/10/31/7ltIKSuAZFiBzQn.png" alt="image-20201031104249976"></p><p>四个h共产生了4个分数，将这四个分数送入一个softmax层，产生一个概率分布，根据这个概率分布对四个h进行加和，得到句子摘要的第一个vector。如下图所示：</p><p><img src="https://i.loli.net/2020/10/31/x2DGOb9jBoNJweq.png" alt="image-20201031104257776"></p><p>为了得到更多的vector，我们可以把上面图中的小写va换成一个矩阵，然后，我们的a也就变成了多个概率分布组成的矩阵，每个概率分布都可以用来与h进行加和产生一个vector，这样我们就产生了摘要的多个vector，如下图所示：</p><p><img src="https://i.loli.net/2020/10/31/9esynl6cDKfmEiS.png" alt="image-20201031104320465"></p><h3 id="人类的视觉注意力">人类的视觉注意力</h3><p>从注意力模型的命名方式看，很明显其借鉴了人类的注意力机制，因此，我们首先简单介绍人类视觉的选择性注意力机制。</p><p><img src="https://i.loli.net/2020/10/31/516P2JBYrgOToyU.jpg" alt="人类的视觉注意力"></p><p>视觉注意力机制是人类视觉所特有的大脑信号处理机制。人类视觉通过快速扫描全局图像，获得需要重点关注的目标区域，也就是一般所说的注意力焦点，而后对这一区域投入更多注意力资源，以获取更多所需要关注目标的细节信息，而抑制其他无用信息。</p><p>这是人类利用有限的注意力资源从大量信息中快速筛选出高价值信息的手段，是人类在长期进化中形成的一种生存机制，人类视觉注意力机制极大地提高了视觉信息处理的效率与准确性。</p><p>图1形象化展示了人类在看到一副图像时是如何高效分配有限的注意力资源的，其中红色区域表明视觉系统更关注的目标，很明显对于图1所示的场景，人们会把注意力更多投入到人的脸部，文本的标题以及文章首句等位置。</p><p>深度学习中的注意力机制从本质上讲和人类的选择性视觉注意力机制类似，核心目标也是从众多信息中选择出对当前任务目标更关键的信息。</p><h3 id="encoder-decoder框架">Encoder-Decoder框架</h3><p>要了解深度学习中的注意力模型，就不得不先谈Encoder-Decoder框架，因为目前大多数注意力模型附着在Encoder-Decoder框架下，当然，其实注意力模型可以看作一种通用的思想，本身并不依赖于特定框架，这点需要注意。</p><p>Encoder-Decoder框架可以看作是一种深度学习领域的研究模式，应用场景异常广泛。图2是文本处理领域里常用的Encoder-Decoder框架最抽象的一种表示。</p><p><img src="https://i.loli.net/2020/10/31/ohBVdz1KDn4ZcOb.png" alt="抽象的文本处理领域的Encoder-Decoder框架"></p><p>文本处理领域的Encoder-Decoder框架可以这么直观地去理解：可以把它看作适合处理由一个句子（或篇章）生成另外一个句子（或篇章）的通用处理模型。对于句子对&lt;Source,Target&gt;，我们的目标是给定输入句子Source，期待通过Encoder-Decoder框架来生成目标句子Target。Source和Target可以是同一种语言，也可以是两种不同的语言。而Source和Target分别由各自的单词序列构成：</p><p><img src="https://i.loli.net/2020/10/31/qrCwtas6iRVKYbA.png" alt="img"></p><p>Encoder顾名思义就是对输入句子Source进行编码，将输入句子通过非线性变换转化为中间语义表示C：</p><p><img src="https://i.loli.net/2020/10/31/M7EXx2KPgeHFQhL.png" alt="img"></p><p>对于解码器Decoder来说，其任务是根据句子Source的中间语义表示C和之前已经生成的历史信息 y1,y2,y3...yi-1来生成i时刻要生成的单词yi：</p><p><img src="https://i.loli.net/2020/10/31/6uHkShXDNKOlBQ3.png" alt="img"></p><p>每个yi都依次这么产生，那么看起来就是整个系统根据输入句子Source生成了目标句子Target。如果Source是中文句子，Target是英文句子，那么这就是解决机器翻译问题的Encoder-Decoder框架；如果Source是一篇文章，Target是概括性的几句描述语句，那么这是文本摘要的Encoder-Decoder框架；如果Source是一句问句，Target是一句回答，那么这是问答系统或者对话机器人的Encoder-Decoder框架。由此可见，在文本处理领域，Encoder-Decoder的应用领域相当广泛。</p><p>Encoder-Decoder框架不仅仅在文本领域广泛使用，在语音识别、图像处理等领域也经常使用。比如对于语音识别来说，图2所示的框架完全适用，区别无非是Encoder部分的输入是语音流，输出是对应的文本信息；而对于“图像描述”任务来说，Encoder部分的输入是一副图片，Decoder的输出是能够描述图片语义内容的一句描述语。一般而言，文本处理和语音识别的Encoder部分通常采用RNN模型，图像处理的Encoder一般采用CNN模型。</p><h3 id="attention模型">Attention模型</h3><p>本节先以机器翻译作为例子讲解最常见的Soft Attention模型的基本原理，之后抛离Encoder-Decoder框架抽象出了注意力机制的本质思想，然后简单介绍最近广为使用的Self Attention的基本思路。</p><h4 id="soft-attention模型">Soft Attention模型</h4><p>图2中展示的Encoder-Decoder框架是没有体现出“注意力模型”的，所以可以把它看作是注意力不集中的分心模型。为什么说它注意力不集中呢？请观察下目标句子Target中每个单词的生成过程如下：</p><p><img src="https://i.loli.net/2020/10/31/KNWS6Ax1uI285cH.png" alt="img"></p><p>其中f是Decoder的非线性变换函数。从这里可以看出，在生成目标句子的单词时，不论生成哪个单词，它们使用的输入句子Source的语义编码C都是一样的，没有任何区别。</p><p>而语义编码C是由句子Source的每个单词经过Encoder</p><p>编码产生的，这意味着不论是生成哪个单词，y1,y2还是y3，其实句子Source中任意单词对生成某个目标单词yi来说影响力都是相同的，这是为何说这个模型没有体现出注意力的缘由。这类似于人类看到眼前的画面，但是眼中却没有注意焦点一样。</p><p>如果拿机器翻译来解释这个分心模型的Encoder-Decoder框架更好理解，比如输入的是英文句子：Tom chase Jerry，Encoder-Decoder框架逐步生成中文单词：“汤姆”，“追逐”，“杰瑞”。</p><p>在翻译“杰瑞”这个中文单词的时候，分心模型里面的每个英文单词对于翻译目标单词“杰瑞”贡献是相同的，很明显这里不太合理，显然“Jerry”对于翻译成“杰瑞”更重要，但是分心模型是无法体现这一点的，这就是为何说它没有引入注意力的原因。</p><p>没有引入注意力的模型在输入句子比较短的时候问题不大，但是如果输入句子比较长，此时所有语义完全通过一个中间语义向量来表示，单词自身的信息已经消失，可想而知会丢失很多细节信息，这也是为何要引入注意力模型的重要原因。</p><p>上面的例子中，如果引入Attention模型的话，应该在翻译“杰瑞”的时候，体现出英文单词对于翻译当前中文单词不同的影响程度，比如给出类似下面一个概率分布值：</p><p>（Tom,0.3）(Chase,0.2) (Jerry,0.5)</p><p>每个英文单词的概率代表了翻译当前单词“杰瑞”时，注意力分配模型分配给不同英文单词的注意力大小。这对于正确翻译目标语单词肯定是有帮助的，因为引入了新的信息。</p><p>同理，目标句子中的每个单词都应该学会其对应的源语句子中单词的注意力分配概率信息。这意味着在生成每个单词yi的时候，原先都是相同的中间语义表示C会被替换成根据当前生成单词而不断变化的Ci。理解Attention模型的关键就是这里，即由固定的中间语义表示C换成了根据当前输出单词来调整成加入注意力模型的变化的Ci。增加了注意力模型的Encoder-Decoder框架理解起来如图3所示。</p><p><img src="https://i.loli.net/2020/10/31/oe6E7C9sUHrVuNZ.png" alt="引入注意力模型的Encoder-Decoder框架"></p><p>即生成目标句子单词的过程成了下面的形式：</p><p><img src="https://i.loli.net/2020/10/31/q8ZFEu4BSI1lo9z.png" alt="img"></p><p>而每个Ci可能对应着不同的源语句子单词的注意力分配概率分布，比如对于上面的英汉翻译来说，其对应的信息可能如下：</p><p><img src="https://i.loli.net/2020/10/31/TBg8KZhoyiOlUst.png" alt="img"></p><p>其中，f2函数代表Encoder对输入英文单词的某种变换函数，比如如果Encoder是用的RNN模型的话，这个f2函数的结果往往是某个时刻输入xi后隐层节点的状态值；g代表Encoder根据单词的中间表示合成整个句子中间语义表示的变换函数，一般的做法中，g函数就是对构成元素加权求和，即下列公式：</p><p><img src="https://i.loli.net/2020/10/31/W9SnjkNw3BVasdc.png" alt="img"></p><p>其中，Lx代表输入句子Source的长度，aij代表在Target输出第i个单词时Source输入句子中第j个单词的注意力分配系数，而hj则是Source输入句子中第j个单词的语义编码。假设下标i就是上面例子所说的“ 汤姆” ，那么Lx就是3，h1=f(“Tom”)，h2=f(“Chase”),h3=f(“Jerry”)分别是输入句子每个单词的语义编码，对应的注意力模型权值则分别是0.6,0.2,0.2，所以g函数本质上就是个加权求和函数。如果形象表示的话，翻译中文单词“汤姆”的时候，数学公式对应的中间语义表示Ci的形成过程类似图4。</p><p><img src="https://i.loli.net/2020/10/31/LRJZMaUbN9VuxGr.png" alt="Attention的形成过程"></p><p>这里还有一个问题：生成目标句子某个单词，比如“汤姆”的时候，如何知道Attention模型所需要的输入句子单词注意力分配概率分布值呢？就是说“汤姆”对应的输入句子Source中各个单词的概率分布：(Tom,0.6)(Chase,0.2) (Jerry,0.2) 是如何得到的呢？</p><p>为了便于说明，我们假设对图2的非Attention模型的Encoder-Decoder框架进行细化，Encoder采用RNN模型，Decoder也采用RNN模型，这是比较常见的一种模型配置，则图2的框架转换为图5。</p><p><img src="https://i.loli.net/2020/10/31/ebTh6NzIdlVG1L2.png" alt="RNN作为具体模型的Encoder-Decoder框架"></p><p>那么用下图可以较为便捷地说明注意力分配概率分布值的通用计算过程。</p><p><img src="https://i.loli.net/2020/10/31/TEXoxdzgrHOuiMQ.jpg" alt="注意力分配概率计算"></p><p>对于采用RNN的Decoder来说，在时刻i，如果要生成yi单词，我们是可以知道Target在生成Yi之前的时刻i-1时，隐层节点i-1时刻的输出值Hi-1的，而我们的目的是要计算生成Yi时输入句子中的单词“Tom”、“Chase”、“Jerry”对Yi来说的注意力分配概率分布，那么可以用Target输出句子i-1时刻的隐层节点状态Hi-1去一一和输入句子Source中每个单词对应的RNN隐层节点状态hj进行对比，即通过函数F(hj,Hi-1)来获得目标单词yi和每个输入单词对应的对齐可能性，这个F函数在不同论文里可能会采取不同的方法，然后函数F的输出经过Softmax进行归一化就得到了符合概率分布取值区间的注意力分配概率分布数值。</p><p>绝大多数Attention模型都是采取上述的计算框架来计算注意力分配概率分布信息，区别只是在F的定义上可能有所不同。图7可视化地展示了在英语-德语翻译系统中加入Attention机制后，Source和Target两个句子每个单词对应的注意力分配概率分布。</p><p><img src="https://i.loli.net/2020/10/31/T1tNMbc6DE3HkGQ.png" alt="英语-德语翻译的注意力概率分布"></p><p>上述内容就是经典的Soft Attention模型的基本思想，那么怎么理解Attention模型的物理含义呢？一般在自然语言处理应用里会把Attention模型看作是输出Target句子中某个单词和输入Source句子每个单词的对齐模型，这是非常有道理的。</p><p>目标句子生成的每个单词对应输入句子单词的概率分布可以理解为输入句子单词和这个目标生成单词的对齐概率，这在机器翻译语境下是非常直观的：传统的统计机器翻译一般在做的过程中会专门有一个短语对齐的步骤，而注意力模型其实起的是相同的作用。</p><p><img src="https://i.loli.net/2020/10/31/zYmvXW8pHiO2VUo.jpg" alt="Google 神经网络机器翻译系统结构图"></p><p>图8所示即为Google于2016年部署到线上的基于神经网络的机器翻译系统，相对传统模型翻译效果有大幅提升，翻译错误率降低了60%，其架构就是上文所述的加上Attention机制的Encoder-Decoder框架，主要区别无非是其Encoder和Decoder使用了8层叠加的LSTM模型。</p><h4 id="attention机制的本质思想">Attention机制的本质思想</h4><p>如果把Attention机制从上文讲述例子中的Encoder-Decoder框架中剥离，并进一步做抽象，可以更容易看懂Attention机制的本质思想。</p><p><img src="https://i.loli.net/2020/10/31/y3BToK96rsgUZfF.png" alt="Attention机制的本质思想"></p><p>我们可以这样来看待Attention机制（参考图9）：将Source中的构成元素想象成是由一系列的&lt;Key,Value&gt;数据对构成，此时给定Target中的某个元素Query，通过计算Query和各个Key的相似性或者相关性，得到每个Key对应Value的权重系数，然后对Value进行加权求和，即得到了最终的Attention数值。所以本质上Attention机制是对Source中元素的Value值进行加权求和，而Query和Key用来计算对应Value的权重系数。即可以将其本质思想改写为如下公式：</p><p><img src="https://i.loli.net/2020/10/31/ipGlzuFcmS8n2VR.png" alt="img"></p><p>其中，Lx=||Source||代表Source的长度，公式含义即如上所述。上文所举的机器翻译的例子里，因为在计算Attention的过程中，Source中的Key和Value合二为一，指向的是同一个东西，也即输入句子中每个单词对应的语义编码，所以可能不容易看出这种能够体现本质思想的结构。</p><p>当然，从概念上理解，把Attention仍然理解为从大量信息中有选择地筛选出少量重要信息并聚焦到这些重要信息上，忽略大多不重要的信息，这种思路仍然成立。聚焦的过程体现在权重系数的计算上，权重越大越聚焦于其对应的Value值上，即权重代表了信息的重要性，而Value是其对应的信息。</p><p>从上图可以引出另外一种理解，也可以将Attention机制看作一种软寻址（Soft Addressing）:Source可以看作存储器内存储的内容，元素由地址Key和值Value组成，当前有个Key=Query的查询，目的是取出存储器中对应的Value值，即Attention数值。通过Query和存储器内元素Key的地址进行相似性比较来寻址，之所以说是软寻址，指的不像一般寻址只从存储内容里面找出一条内容，而是可能从每个Key地址都会取出内容，取出内容的重要性根据Query和Key的相似性来决定，之后对Value进行加权求和，这样就可以取出最终的Value值，也即Attention值。所以不少研究人员将Attention机制看作软寻址的一种特例，这也是非常有道理的。</p><p>至于Attention机制的具体计算过程，如果对目前大多数方法进行抽象的话，可以将其归纳为两个过程：第一个过程是根据Query和Key计算权重系数，第二个过程根据权重系数对Value进行加权求和。而第一个过程又可以细分为两个阶段：第一个阶段根据Query和Key计算两者的相似性或者相关性；第二个阶段对第一阶段的原始分值进行归一化处理；这样，可以将Attention的计算过程抽象为如图10展示的三个阶段。</p><p><img src="https://i.loli.net/2020/10/31/tzO9fhXUeBblmVG.png" alt="三阶段计算Attention过程"></p><p>在第一个阶段，可以引入不同的函数和计算机制，根据Query和某个Key_i，计算两者的相似性或者相关性，最常见的方法包括：求两者的向量点积、求两者的向量Cosine相似性或者通过再引入额外的神经网络来求值，即如下方式：</p><p><img src="https://i.loli.net/2020/10/31/9xpPOa7ohFf3u1d.png" alt="img"></p><p>第一阶段产生的分值根据具体产生的方法不同其数值取值范围也不一样，第二阶段引入类似SoftMax的计算方式对第一阶段的得分进行数值转换，一方面可以进行归一化，将原始计算分值整理成所有元素权重之和为1的概率分布；另一方面也可以通过SoftMax的内在机制更加突出重要元素的权重。即一般采用如下公式计算：</p><p><img src="https://i.loli.net/2020/10/31/XFW5tcSGjqBnIyN.png" alt="img"></p><p>第二阶段的计算结果a_i即为value_i对应的权重系数，然后进行加权求和即可得到Attention数值：</p><p><img src="https://i.loli.net/2020/10/31/soa1M9LIPGi3krC.png" alt="img"></p><p>通过如上三个阶段的计算，即可求出针对Query的Attention数值，目前绝大多数具体的注意力机制计算方法都符合上述的三阶段抽象计算过程。</p><h4 id="self-attention模型">Self Attention模型</h4><p>通过上述对Attention本质思想的梳理，我们可以更容易理解本节介绍的Self Attention模型。Self Attention也经常被称为intra Attention（内部Attention），最近一年也获得了比较广泛的使用，比如Google最新的机器翻译模型内部大量采用了Self Attention模型。</p><p>在一般任务的Encoder-Decoder框架中，输入Source和输出Target内容是不一样的，比如对于英-中机器翻译来说，Source是英文句子，Target是对应的翻译出的中文句子，Attention机制发生在Target的元素Query和Source中的所有元素之间。而Self</p><p>Attention顾名思义，指的不是Target和Source之间的Attention机制，而是Source内部元素之间或者Target内部元素之间发生的Attention机制，也可以理解为Target=Source这种特殊情况下的注意力计算机制。其具体计算过程是一样的，只是计算对象发生了变化而已，所以此处不再赘述其计算过程细节。</p><p>如果是常规的Target不等于Source情形下的注意力计算，其物理含义正如上文所讲，比如对于机器翻译来说，本质上是目标语单词和源语单词之间的一种单词对齐机制。那么如果是Self Attention机制，一个很自然的问题是：通过Self Attention到底学到了哪些规律或者抽取出了哪些特征呢？或者说引入Self Attention有什么增益或者好处呢？我们仍然以机器翻译中的Self Attention来说明，图11和图12是可视化地表示Self Attention在同一个英语句子内单词间产生的联系。</p><p><img src="https://i.loli.net/2020/10/31/BqbvNSUWyrnTjt4.jpg" alt="可视化Self Attention实例"></p><p><img src="https://i.loli.net/2020/10/31/q2iCXflnh5oH1WE.jpg" alt="可视化Self Attention实例"></p><p>从两张图（图11、图12）可以看出，Self Attention可以捕获同一个句子中单词之间的一些句法特征（比如图11展示的有一定距离的短语结构）或者语义特征（比如图12展示的its的指代对象Law）。</p><p>很明显，引入Self Attention后会更容易捕获句子中长距离的相互依赖的特征，因为如果是RNN或者LSTM，需要依次序序列计算，对于远距离的相互依赖的特征，要经过若干时间步步骤的信息累积才能将两者联系起来，而距离越远，有效捕获的可能性越小。</p><p>但是Self Attention在计算过程中会直接将句子中任意两个单词的联系通过一个计算步骤直接联系起来，所以远距离依赖特征之间的距离被极大缩短，有利于有效地利用这些特征。除此外，Self Attention对于增加计算的并行性也有直接帮助作用。这是为何Self Attention逐渐被广泛使用的主要原因。</p><h4 id="attention机制的应用">Attention机制的应用</h4><p>前文有述，Attention机制在深度学习的各种应用领域都有广泛的使用场景。上文在介绍过程中我们主要以自然语言处理中的机器翻译任务作为例子，下面分别再从图像处理领域和语音识别选择典型应用实例来对其应用做简单说明。</p><p><img src="https://i.loli.net/2020/10/31/Yog7WcVFCXbRv3U.jpg" alt="图片-描述任务的Encoder-Decoder框架"></p><p>图片描述（Image-Caption）是一种典型的图文结合的深度学习应用，输入一张图片，人工智能系统输出一句描述句子，语义等价地描述图片所示内容。很明显这种应用场景也可以使用Encoder-Decoder框架来解决任务目标，此时Encoder输入部分是一张图片，一般会用CNN来对图片进行特征抽取，Decoder部分使用RNN或者LSTM来输出自然语言句子（参考上图）。</p><p>此时如果加入Attention机制能够明显改善系统输出效果，Attention模型在这里起到了类似人类视觉选择性注意的机制，在输出某个实体单词的时候会将注意力焦点聚焦在图片中相应的区域上。图14给出了根据给定图片生成句子“A person is standing on a beach with a surfboard.”过程时每个单词对应图片中的注意力聚焦区域。</p><p><img src="https://i.loli.net/2020/10/31/RGVXYj8W2yQsqtz.jpg" alt="图片生成句子中每个单词时的注意力聚焦区域"></p><p>下图给出了另外四个例子形象地展示了这种过程，每个例子上方左侧是输入的原图，下方句子是人工智能系统自动产生的描述语句，上方右侧图展示了当AI系统产生语句中划横线单词的时候，对应图片中聚焦的位置区域。比如当输出单词dog的时候，AI系统会将注意力更多地分配给图片中小狗对应的位置。</p><p><img src="https://i.loli.net/2020/10/31/v7Eubye6A4dSLZp.jpg" alt="图像描述任务中Attention机制的聚焦作用"></p><p><img src="https://i.loli.net/2020/10/31/Yt5fPBJWRAaUHNv.jpg" alt="语音识别中音频序列和输出字符之间的Attention"></p><p>语音识别的任务目标是将语音流信号转换成文字，所以也是Encoder-Decoder的典型应用场景。Encoder部分的Source输入是语音流信号，Decoder部分输出语音对应的字符串流。</p><p>上图可视化地展示了在Encoder-Decoder框架中加入Attention机制后，当用户用语音说句子 how much would a woodchuck chuck 时，输入部分的声音特征信号和输出字符之间的注意力分配概率分布情况，颜色越深代表分配到的注意力概率越高。从图中可以看出，在这个场景下，Attention机制起到了将输出字符和输入语音信号进行对齐的功能。</p><p>上述内容仅仅选取了不同AI领域的几个典型Attention机制应用实例，Encoder-Decoder加Attention架构由于其卓越的实际效果，目前在深度学习领域里得到了广泛的使用，了解并熟练使用这一架构对于解决实际问题会有极大帮助。</p><h3 id="总结">总结</h3><p>通过以上的内容，我们了解到，Attention机制最初用来克服RNN做机器翻译时的缺点，然后，人们发现，Attention机制具有广泛的适用性，于是它又被扩展到了产生图片描述，做句子摘要等任务上。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      注意力机制模型的总结
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-10-30-从embedding到BERT预训练模型</title>
    <link href="http://yoursite.com/2020/10/30/2020-10-30-%E4%BB%8Eembedding%E5%88%B0BERT%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"/>
    <id>http://yoursite.com/2020/10/30/2020-10-30-%E4%BB%8Eembedding%E5%88%B0BERT%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/</id>
    <published>2020-10-30T11:53:49.000Z</published>
    <updated>2020-10-30T12:37:24.964Z</updated>
    
    <content type="html"><![CDATA[<p>Bert最近很火，应该是最近最火爆的AI进展，网上的评价很高，那么Bert值得这么高的评价吗？我个人判断是值得。那为什么会有这么高的评价呢？是因为它有重大的理论或者模型创新吗？其实并没有，从模型创新角度看一般，创新不算大。但是架不住效果太好了，基本刷新了很多NLP的任务的最好性能，有些任务还被刷爆了，这个才是关键。另外一点是Bert具备广泛的通用性，就是说绝大部分NLP任务都可以采用类似的两阶段模式直接去提升效果，这个第二关键。客观的说，把Bert当做最近两年NLP重大进展的集大成者更符合事实。</p><p>本文的主题是自然语言处理中的预训练过程，会大致说下NLP中的预训练技术是一步一步如何发展到Bert模型的，从中可以很自然地看到Bert的思路是如何逐渐形成的，Bert的历史沿革是什么，继承了什么，创新了什么，为什么效果那么好，主要原因是什么，以及为何说模型创新不算太大，为何说Bert是近年来NLP重大进展的集大成者。我们一步一步来讲，而串起来这个故事的脉络就是自然语言的预训练过程，但是落脚点还是在Bert身上。要讲自然语言的预训练，得先从图像领域的预训练说起。</p><h3 id="图像领域的预训练">图像领域的预训练</h3><p>自从深度学习火起来后，预训练过程就是做图像或者视频领域的一种比较常规的做法，有比较长的历史了，而且这种做法很有效，能明显促进应用的效果。</p><p><img src="https://i.loli.net/2020/10/30/HSZgfVhv5cAO2Qt.jpg" alt="img"></p><p>那么图像领域怎么做预训练呢，上图展示了这个过程，我们设计好网络结构以后，对于图像来说一般是CNN的多层叠加网络结构，可以先用某个训练集合比如训练集合A或者训练集合B对这个网络进行预先训练，在A任务上或者B任务上学会网络参数，然后存起来以备后用。假设我们面临第三个任务C，网络结构采取相同的网络结构，在比较浅的几层CNN结构，网络参数初始化的时候可以加载A任务或者B任务学习好的参数，其它CNN高层参数仍然随机初始化。之后我们用C任务的训练数据来训练网络，此时有两种做法，一种是浅层加载的参数在训练C任务过程中不动，这种方法被称为“Frozen”;另外一种是底层网络参数尽管被初始化了，在C任务训练过程中仍然随着训练的进程不断改变，这种一般叫“Fine-Tuning”，顾名思义，就是更好地把参数进行调整使得更适应当前的C任务。一般图像或者视频领域要做预训练一般都这么做。</p><p>这么做有几个好处，首先，如果手头任务C的训练集合数据量较少的话，现阶段的好用的CNN比如Resnet/Densenet/Inception等网络结构层数很深，几百万上千万参数量算起步价，上亿参数的也很常见，训练数据少很难很好地训练这么复杂的网络，但是如果其中大量参数通过大的训练集合比如ImageNet预先训练好直接拿来初始化大部分网络结构参数，然后再用C任务手头比较可怜的数据量上Fine-tuning过程去调整参数让它们更适合解决C任务，那事情就好办多了。这样原先训练不了的任务就能解决了，即使手头任务训练数据也不少，加个预训练过程也能极大加快任务训练的收敛速度，所以这种预训练方式是老少皆宜的解决方案，另外疗效又好，所以在做图像处理领域很快就流行开来。</p><p>那么新的问题来了，为什么这种预训练的思路是可行的？</p><p><img src="https://i.loli.net/2020/10/30/hnsrCeotdP4jLSq.jpg" alt="img"></p><p>目前我们已经知道，对于层级的CNN结构来说，不同层级的神经元学习到了不同类型的图像特征，由底向上特征形成层级结构，如上图所示，如果我们手头是个人脸识别任务，训练好网络后，把每层神经元学习到的特征可视化肉眼看一看每层学到了啥特征，你会看到最底层的神经元学到的是线段等特征，图示的第二个隐层学到的是人脸五官的轮廓，第三层学到的是人脸的轮廓，通过三步形成了特征的层级结构，越是底层的特征越是所有不论什么领域的图像都会具备的比如边角线弧线等底层基础特征，越往上抽取出的特征越与手头任务相关。正因为此，所以预训练好的网络参数，尤其是底层的网络参数抽取出特征跟具体任务越无关，越具备任务的通用性，所以这是为何一般用底层预训练好的参数初始化新任务网络参数的原因。而高层特征跟任务关联较大，实际可以不用使用，或者采用Fine-tuning用新数据集合清洗掉高层无关的特征抽取器。</p><p><img src="https://i.loli.net/2020/10/30/RrSMsuboYdOzvKl.jpg" alt="img"></p><p>一般我们喜欢用ImageNet来做网络的预训练，主要有两点，一方面ImageNet是图像领域里有超多事先标注好训练数据的数据集合，分量足是个很大的优势，量越大训练出的参数越靠谱；另外一方面因为ImageNet有1000类，类别多，算是通用的图像数据，跟领域没太大关系，所以通用性好，预训练完后哪哪都能用，是个万金油。分量足的万金油当然老少通吃，人人喜爱。</p><p>听完上述话，如果你是具备研究素质的人，也就是说具备好奇心，你一定会问下面这个问题：”既然图像领域预训练这么好用，那干嘛自然语言处理不做这个事情呢？是不是搞NLP的人比搞CV的傻啊？就算你傻，你看见人家这么做，有样学样不就行了吗？这不就是创新吗，也许能成，万一成了，你看，你的成功来得就是这么突然!”</p><p>嗯，好问题，其实搞NLP的人一点都不比你傻，早就有人尝试过了，不过总体而言不太成功而已。听说过word embedding吗？2003年出品，陈年技术，馥郁芳香。word embedding其实就是NLP里的早期预训练技术。当然也不能说word embedding不成功，一般加到下游任务里，都能有1到2个点的性能提升，只是没有那么耀眼的成功而已。</p><p>没听过？那下面就把这段陈年老账讲给你听听。</p><h3 id="word-embedding考古史">Word Embedding考古史</h3><p>这块大致讲讲Word Embedding的故事，很粗略，因为网上关于这个技术讲的文章太多了，汗牛冲动，我不属牛，此刻更没有流汗，所以其实丝毫没有想讲Word Embedding的冲动和激情，但是要说预训练又得从这开始，那就粗略地讲讲，主要是引出后面更精彩的部分。在说Word Embedding之前，先更粗略地说下语言模型，因为一般NLP里面做预训练一般的选择是用语言模型任务来做。</p><p><img src="https://i.loli.net/2020/10/30/4aFBLDCEQmgXp9Z.jpg" alt="img"></p><p>什么是语言模型？其实看上面这张PPT上扣下来的图就明白了，为了能够量化地衡量哪个句子更像一句人话，可以设计如上图所示函数，核心函数P的思想是根据句子里面前面的一系列前导单词预测后面跟哪个单词的概率大小（理论上除了上文之外，也可以引入单词的下文联合起来预测单词出现概率）。句子里面每个单词都有个根据上文预测自己的过程，把所有这些单词的产生概率乘起来，数值越大代表这越像一句人话。语言模型压下暂且不表，我隐约预感到我这么讲你可能还是不太会明白，但是大概这个意思，不懂的可以去网上找，资料多得一样地汗牛冲动。</p><p>假设现在让你设计一个神经网络结构，去做这个语言模型的任务，就是说给你很多语料做这个事情，训练好一个神经网络，训练好之后，以后输入一句话的前面几个单词，要求这个网络输出后面紧跟的单词应该是哪个，你会怎么做？</p><p><img src="https://i.loli.net/2020/10/30/SUI2jF7xEsaz9Z4.jpg" alt="img"></p><p>你可以像上图这么设计这个网络结构，这其实就是大名鼎鼎的中文人称“神经网络语言模型”，英文小名NNLM的网络结构，用来做语言模型。这个工作有年头了，是个陈年老工作，是Bengio 在2003年发表在JMLR上的论文。它生于2003，火于2013，以后是否会不朽暂且不知，但是不幸的是出生后应该没有引起太大反响，沉寂十年终于时来运转沉冤得雪，在2013年又被NLP考古工作者从海底湿淋淋地捞出来了祭入神殿。为什么会发生这种技术奇遇记？你要想想2013年是什么年头，是深度学习开始渗透NLP领域的光辉时刻，万里长征第一步，而NNLM可以算是南昌起义第一枪。在深度学习火起来之前，极少有人用神经网络做NLP问题，如果你10年前坚持用神经网络做NLP，估计别人会认为你这人神经有问题。所谓红尘滚滚，谁也挡不住历史发展趋势的车轮，这就是个很好的例子。</p><p>上面是闲话，闲言碎语不要讲，我们回来讲一讲NNLM的思路。先说训练过程，现在看其实很简单，见过RNN、LSTM、CNN后的你们回头再看这个网络甚至显得有些简陋。学习任务是输入某个句中单词Wt=“Bert”前面句子的t-1个单词，要求网络正确预测单词Bert，即最大化：</p><p><img src="https://i.loli.net/2020/10/30/k8XpuUwT47WOo5Q.png" alt="image-20201030202319906" style="zoom:50%;"></p><p>前面任意单词Wi用Onehot编码（比如：0001000）作为原始单词输入，之后乘以矩阵Q后获得向量C（Wi），每个单词的 C（Wi）拼接，上接隐层，然后接softmax去预测后面应该后续接哪个单词。这个 C（Wi）是什么？这其实就是单词对应的Word Embedding值，那个矩阵Q包含V行，V代表词典大小，每一行内容代表对应单词的Word embedding值。只不过Q的内容也是网络参数，需要学习获得，训练刚开始用随机值初始化矩阵Q，当这个网络训练好之后，矩阵Q的内容被正确赋值，每一行代表一个单词对应的Word embedding值。所以你看，通过这个网络学习语言模型任务，这个网络不仅自己能够根据上文预测后接单词是什么，同时获得一个副产品，就是那个矩阵Q，这就是单词的Word Embedding是被如何学会的。</p><p>2013年最火的用语言模型做Word Embedding的工具是Word2Vec，后来又出了Glove，Word2Vec是怎么工作的呢？看下图。</p><p><img src="https://i.loli.net/2020/10/30/bXq5RvhS3niaTxB.jpg" alt="img"></p><p>Word2Vec的网络结构其实和NNLM是基本类似的，只是这个图长得清晰度差了点，看上去不像，其实它们是亲兄弟。不过这里需要指出：尽管网络结构相近，而且也是做语言模型任务，但是其训练方法不太一样。Word2Vec有两种训练方法，一种叫CBOW，核心思想是从一个句子里面把一个词抠掉，用这个词的上文和下文去预测被抠掉的这个词；第二种叫做Skip-gram，和CBOW正好反过来，输入某个单词，要求网络预测它的上下文单词。而你回头看看，NNLM是怎么训练的？是输入一个单词的上文，去预测这个单词。这是有显著差异的。为什么Word2Vec这么处理？原因很简单，因为Word2Vec和NNLM不一样，NNLM的主要任务是要学习一个解决语言模型任务的网络结构，语言模型就是要看到上文预测下文，而word embedding只是无心插柳的一个副产品。但是Word2Vec目标不一样，它单纯就是要word embedding的，这是主产品，所以它完全可以随性地这么去训练网络。</p><p>为什么要讲Word2Vec呢？这里主要是要引出CBOW的训练方法，BERT其实跟它有关系，后面会讲它们之间是如何的关系，当然它们的关系BERT作者没说，是我猜的，至于我猜的对不对，后面你看后自己判断。</p><p><img src="https://i.loli.net/2020/10/30/aM9lpsNOS7vrmnq.jpg" alt="img"></p><p>使用Word2Vec或者Glove，通过做语言模型任务，就可以获得每个单词的Word Embedding，那么这种方法的效果如何呢？上图给了网上找的几个例子，可以看出有些例子效果还是很不错的，一个单词表达成Word Embedding后，很容易找出语义相近的其它词汇。</p><p>我们的主题是预训练，那么问题是Word Embedding这种做法能算是预训练吗？这其实就是标准的预训练过程。要理解这一点要看看学会Word Embedding后下游任务是怎么用它的。</p><p><img src="https://i.loli.net/2020/10/30/YEf95lnIW28OGRa.jpg" alt="img"></p><p>假设如上图所示，我们有个NLP的下游任务，比如QA，就是问答问题，所谓问答问题，指的是给定一个问题X，给定另外一个句子Y,要判断句子Y是否是问题X的正确答案。问答问题假设设计的网络结构如上图所示，这里不展开讲了，懂得自然懂，不懂的也没关系，因为这点对于本文主旨来说不关键，关键是网络如何使用训练好的Word Embedding的。它的使用方法其实和前面讲的NNLM是一样的，句子中每个单词以Onehot形式作为输入，然后乘以学好的Word Embedding矩阵Q，就直接取出单词对应的Word Embedding了。这乍看上去好像是个查表操作，不像是预训练的做法是吧？其实不然，那个Word Embedding矩阵Q其实就是网络Onehot层到embedding层映射的网络参数矩阵。所以你看到了，使用Word Embedding等价于什么？等价于把Onehot层到embedding层的网络用预训练好的参数矩阵Q初始化了。这跟前面讲的图像领域的低层预训练过程其实是一样的，区别无非Word Embedding只能初始化第一层网络参数，再高层的参数就无能为力了。下游NLP任务在使用Word Embedding的时候也类似图像有两种做法，一种是Frozen，就是Word Embedding那层网络参数固定不动；另外一种是Fine-Tuning，就是Word Embedding这层参数使用新的训练集合训练也需要跟着训练过程更新掉。</p><p>上面这种做法就是18年之前NLP领域里面采用预训练的典型做法，之前说过，Word Embedding其实对于很多下游NLP任务是有帮助的，只是帮助没有大到闪瞎忘记戴墨镜的围观群众的双眼而已。那么新问题来了，为什么这样训练及使用Word Embedding的效果没有期待中那么好呢？答案很简单，因为Word Embedding有问题呗。这貌似是个比较弱智的答案，关键是Word Embedding存在什么问题？这其实是个好问题。</p><p><img src="https://i.loli.net/2020/10/30/U3YwNJ1RmydDukM.jpg" alt="img"></p><p>这片在Word Embedding头上笼罩了好几年的乌云是什么？是多义词问题。我们知道，多义词是自然语言中经常出现的现象，也是语言灵活性和高效性的一种体现。多义词对Word Embedding来说有什么负面影响？如上图所示，比如多义词Bank，有两个常用含义，但是Word Embedding在对bank这个单词进行编码的时候，是区分不开这两个含义的，因为它们尽管上下文环境中出现的单词不同，但是在用语言模型训练的时候，不论什么上下文的句子经过word2vec，都是预测相同的单词bank，而同一个单词占的是同一行的参数空间，这导致两种不同的上下文信息都会编码到相同的word embedding空间里去。所以word embedding无法区分多义词的不同语义，这就是它的一个比较严重的问题。</p><p>你可能觉得自己很聪明，说这可以解决啊，确实也有很多研究人员提出很多方法试图解决这个问题，但是从今天往回看，这些方法看上去都成本太高或者太繁琐了，有没有简单优美的解决方案呢？</p><p>ELMO提供了一种简洁优雅的解决方案。</p><h3 id="从word-embedding到elmo">从Word Embedding到ELMO</h3><p>ELMO是“Embedding from Language Models”的简称，其实这个名字并没有反应它的本质思想，提出ELMO的论文题目：“Deep contextualized word representation”更能体现其精髓，而精髓在哪里？在deep contextualized这个短语，一个是deep，一个是context，其中context更关键。在此之前的Word Embedding本质上是个静态的方式，所谓静态指的是训练好之后每个单词的表达就固定住了，以后使用的时候，不论新句子上下文单词是什么，这个单词的Word Embedding不会跟着上下文场景的变化而改变，所以对于比如Bank这个词，它事先学好的Word Embedding中混合了几种语义 ，在应用中来了个新句子，即使从上下文中（比如句子包含money等词）明显可以看出它代表的是“银行”的含义，但是对应的Word Embedding内容也不会变，它还是混合了多种语义。这是为何说它是静态的，这也是问题所在。ELMO的本质思想是：我事先用语言模型学好一个单词的Word Embedding，此时多义词无法区分，不过这没关系。在我实际使用Word Embedding的时候，单词已经具备了特定的上下文了，这个时候我可以根据上下文单词的语义去调整单词的Word Embedding表示，这样经过调整后的Word Embedding更能表达在这个上下文中的具体含义，自然也就解决了多义词的问题了。所以ELMO本身是个根据当前上下文对Word Embedding动态调整的思路。</p><p><img src="https://i.loli.net/2020/10/30/m6NvFoRGhWbji83.jpg" alt="img"></p><p>ELMO采用了典型的两阶段过程，第一个阶段是利用语言模型进行预训练；第二个阶段是在做下游任务时，从预训练网络中提取对应单词的网络各层的Word Embedding作为新特征补充到下游任务中。上图展示的是其预训练过程，它的网络结构采用了双层双向LSTM，目前语言模型训练的任务目标是根据单词 Wi 的上下文去正确预测单词 Wi ， Wi 之前的单词序列Context-before称为上文，之后的单词序列Context-after称为下文。图中左端的前向双层LSTM代表正方向编码器，输入的是从左到右顺序的除了预测单词外 Wi 的上文Context-before；右端的逆向双层LSTM代表反方向编码器，输入的是从右到左的逆序的句子下文Context-after；每个编码器的深度都是两层LSTM叠加。这个网络结构其实在NLP中是很常用的。使用这个网络结构利用大量语料做语言模型任务就能预先训练好这个网络，如果训练好这个网络后，输入一个新句子 Snew ，句子中每个单词都能得到对应的三个Embedding:最底层是单词的Word Embedding，往上走是第一层双向LSTM中对应单词位置的Embedding，这层编码单词的句法信息更多一些；再往上走是第二层LSTM中对应单词位置的Embedding，这层编码单词的语义信息更多一些。也就是说，ELMO的预训练过程不仅仅学会单词的Word Embedding，还学会了一个双层双向的LSTM网络结构，而这两者后面都有用。</p><p><img src="https://i.loli.net/2020/10/30/ymSXKwFh8WBcEd5.jpg" alt="img"></p><p>上面介绍的是ELMO的第一阶段：预训练阶段。那么预训练好网络结构后，如何给下游任务使用呢？上图展示了下游任务的使用过程，比如我们的下游任务仍然是QA问题，此时对于问句X，我们可以先将句子X作为预训练好的ELMO网络的输入，这样句子X中每个单词在ELMO网络中都能获得对应的三个Embedding，之后给予这三个Embedding中的每一个Embedding一个权重a，这个权重可以学习得来，根据各自权重累加求和，将三个Embedding整合成一个。然后将整合后的这个Embedding作为X句在自己任务的那个网络结构中对应单词的输入，以此作为补充的新特征给下游任务使用。对于上图所示下游任务QA中的回答句子Y来说也是如此处理。因为ELMO给下游提供的是每个单词的特征形式，所以这一类预训练的方法被称为“Feature-based Pre-Training”。至于为何这么做能够达到区分多义词的效果，你可以想一想，其实比较容易想明白原因。</p><p><img src="https://i.loli.net/2020/10/30/b8ToQxv5BPgELI1.jpg" alt="img"></p><p>上面这个图是TagLM采用类似ELMO的思路做命名实体识别任务的过程，其步骤基本如上述ELMO的思路，所以此处不展开说了。TagLM的论文发表在2017年的ACL会议上，作者就是AllenAI里做ELMO的那些人，所以可以将TagLM看做ELMO的一个前导工作。前几天这个PPT发出去后有人质疑说FastAI的在18年4月提出的ULMFiT才是抛弃传统Word Embedding引入新模式的开山之作，我深不以为然。首先TagLM出现的更早而且模式基本就是ELMO的思路；另外ULMFiT使用的是三阶段模式，在通用语言模型训练之后，加入了一个领域语言模型预训练过程，而且论文重点工作在这块，方法还相对比较繁杂，这并不是一个特别好的主意，因为领域语言模型的限制是它的规模往往不可能特别大，精力放在这里不太合适，放在通用语言模型上感觉更合理；再者，尽管ULFMiT实验做了6个任务，但是都集中在分类问题相对比较窄，不如ELMO验证的问题领域广，我觉得这就是因为第二步那个领域语言模型带来的限制。所以综合看，尽管ULFMiT也是个不错的工作，但是重要性跟ELMO比至少还是要差一档，当然这是我个人看法。每个人的学术审美口味不同，我个人一直比较赞赏要么简洁有效体现问题本质要么思想特别游离现有框架脑洞开得异常大的工作，所以ULFMiT我看论文的时候就感觉看着有点难受，觉得这工作没抓住重点而且特别麻烦，但是看ELMO论文感觉就赏心悦目，觉得思路特别清晰顺畅，看完暗暗点赞，心里说这样的文章获得NAACL2018最佳论文当之无愧，比ACL很多最佳论文也好得不是一点半点，这就是好工作带给一个有经验人士的一种在读论文时候就能产生的本能的感觉，也就是所谓的这道菜对上了食客的审美口味。</p><p><img src="https://i.loli.net/2020/10/30/6y7VvCDm9NRpHJx.jpg" alt="img"></p><p>前面我们提到静态Word Embedding无法解决多义词的问题，那么ELMO引入上下文动态调整单词的embedding后多义词问题解决了吗？解决了，而且比我们期待的解决得还要好。上图给了个例子，对于Glove训练出的Word Embedding来说，多义词比如play，根据它的embedding找出的最接近的其它单词大多数集中在体育领域，这很明显是因为训练数据中包含play的句子中体育领域的数量明显占优导致；而使用ELMO，根据上下文动态调整后的embedding不仅能够找出对应的“演出”的相同语义的句子，而且还可以保证找出的句子中的play对应的词性也是相同的，这是超出期待之处。之所以会这样，是因为我们上面提到过，第一层LSTM编码了很多句法信息，这在这里起到了重要作用。</p><p><img src="https://i.loli.net/2020/10/30/YFAVkuIxemlaHPN.jpg" alt="img"></p><p>ELMO经过这般操作，效果如何呢？实验效果见上图，6个NLP任务中性能都有幅度不同的提升，最高的提升达到25%左右，而且这6个任务的覆盖范围比较广，包含句子语义关系判断，分类任务，阅读理解等多个领域，这说明其适用范围是非常广的，普适性强，这是一个非常好的优点。</p><p><img src="https://i.loli.net/2020/10/30/LpMSFe5kX7Qxroh.jpg" alt="img"></p><p>那么站在现在这个时间节点看，ELMO有什么值得改进的缺点呢？首先，一个非常明显的缺点在特征抽取器选择方面，ELMO使用了LSTM而不是新贵Transformer，Transformer是谷歌在17年做机器翻译任务的“Attention is all you need”的论文中提出的，引起了相当大的反响，很多研究已经证明了Transformer提取特征的能力是要远强于LSTM的。如果ELMO采取Transformer作为特征提取器，那么估计Bert的反响远不如现在的这种火爆场面。另外一点，ELMO采取双向拼接这种融合特征的能力可能比Bert一体化的融合特征方式弱，但是，这只是一种从道理推断产生的怀疑，目前并没有具体实验说明这一点。</p><p>我们如果把ELMO这种预训练方法和图像领域的预训练方法对比，发现两者模式看上去还是有很大差异的。除了以ELMO为代表的这种基于特征融合的预训练方法外，NLP里还有一种典型做法，这种做法和图像领域的方式就是看上去一致的了，一般将这种方法称为“基于Fine-tuning的模式”，而GPT就是这一模式的典型开创者。</p><h3 id="从word-embedding到gpt">从Word Embedding到GPT</h3><p><img src="https://i.loli.net/2020/10/30/IDl2hH8j3JVdx6F.jpg" alt="img"></p><p>GPT是“Generative Pre-Training”的简称，从名字看其含义是指的生成式的预训练。GPT也采用两阶段过程，第一个阶段是利用语言模型进行预训练，第二阶段通过Fine-tuning的模式解决下游任务。上图展示了GPT的预训练过程，其实和ELMO是类似的，主要不同在于两点：首先，特征抽取器不是用的RNN，而是用的Transformer，上面提到过它的特征抽取能力要强于RNN，这个选择很明显是很明智的；其次，GPT的预训练虽然仍然是以语言模型作为目标任务，但是采用的是单向的语言模型，所谓“单向”的含义是指：语言模型训练的任务目标是根据 Wi 单词的上下文去正确预测单词 Wi ， Wi 之前的单词序列Context-before称为上文，之后的单词序列Context-after称为下文。ELMO在做语言模型预训练的时候，预测单词 Wi 同时使用了上文和下文，而GPT则只采用Context-before这个单词的上文来进行预测，而抛开了下文。这个选择现在看不是个太好的选择，原因很简单，它没有把单词的下文融合进来，这限制了其在更多应用场景的效果，比如阅读理解这种任务，在做任务的时候是可以允许同时看到上文和下文一起做决策的。如果预训练时候不把单词的下文嵌入到Word Embedding中，是很吃亏的，白白丢掉了很多信息。</p><p>这里强行插入一段简单提下Transformer，尽管上面提到了，但是说的还不完整，补充两句。首先，Transformer是个叠加的“自注意力机制（Self Attention）”构成的深度网络，是目前NLP里最强的特征提取器，注意力这个机制在此被发扬光大，从任务的配角不断抢戏，直到Transformer一跃成为踢开RNN和CNN传统特征提取器，荣升头牌，大红大紫。注意力机制可以参考“<a href="https://zhuanlan.zhihu.com/p/37601161" target="_blank" rel="noopener">深度学习中的注意力模型</a>”，补充下相关基础知识，如果不了解注意力机制你肯定会落后时代的发展。而介绍Transformer比较好的文章可以参考以下两篇文章：一个是Jay Alammar可视化地介绍Transformer的博客文章<a href="https://link.zhihu.com/?target=https%3A//jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a> ，非常容易理解整个机制，建议先从这篇看起；然后可以参考哈佛大学NLP研究组写的“<a href="https://link.zhihu.com/?target=http%3A//nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer.</a> ”，代码原理双管齐下，讲得非常清楚。我相信上面两个文章足以让你了解Transformer了，所以这里不展开介绍。</p><p>其次，我的判断是Transformer在未来会逐渐替代掉RNN成为主流的NLP工具，RNN一直受困于其并行计算能力，这是因为它本身结构的序列性依赖导致的，尽管很多人在试图通过修正RNN结构来修正这一点，但是我不看好这种模式，因为给马车换轮胎不如把它升级到汽车，这个道理很好懂，更何况目前汽车的雏形已经出现了，干嘛还要执着在换轮胎这个事情呢？是吧？再说CNN，CNN在NLP里一直没有形成主流，CNN的最大优点是易于做并行计算，所以速度快，但是在捕获NLP的序列关系尤其是长距离特征方面天然有缺陷，不是做不到而是做不好，目前也有很多改进模型，但是特别成功的不多。综合各方面情况，很明显Transformer同时具备并行性好，又适合捕获长距离特征，没有理由不在赛跑比赛中跑不过RNN和CNN。</p><p>好了，题外话结束，我们再回到主题，接着说GPT。上面讲的是GPT如何进行第一阶段的预训练，那么假设预训练好了网络模型，后面下游任务怎么用？它有自己的个性，和ELMO的方式大有不同。</p><p><img src="https://i.loli.net/2020/10/30/3Gr9vqoPHkSfcAg.jpg" alt="img"></p><p>上图展示了GPT在第二阶段如何使用。首先，对于不同的下游任务来说，本来你可以任意设计自己的网络结构，现在不行了，你要向GPT的网络结构看齐，把任务的网络结构改造成和GPT的网络结构是一样的。然后，在做下游任务的时候，利用第一步预训练好的参数初始化GPT的网络结构，这样通过预训练学到的语言学知识就被引入到你手头的任务里来了，这是个非常好的事情。再次，你可以用手头的任务去训练这个网络，对网络参数进行Fine-tuning，使得这个网络更适合解决手头的问题。就是这样。看到了么？这有没有让你想起最开始提到的图像领域如何做预训练的过程（请参考上图那句非常容易暴露年龄的歌词）？对，这跟那个模式是一模一样的。</p><p>这里引入了一个新问题：对于NLP各种花样的不同任务，怎么改造才能靠近GPT的网络结构呢？</p><p><img src="https://i.loli.net/2020/10/30/iJqb8TYLwCvdSVk.jpg" alt="img"></p><p>GPT论文给了一个改造施工图如上，其实也很简单：对于分类问题，不用怎么动，加上一个起始和终结符号即可；对于句子关系判断问题，比如Entailment，两个句子中间再加个分隔符即可；对文本相似性判断问题，把两个句子顺序颠倒下做出两个输入即可，这是为了告诉模型句子顺序不重要；对于多项选择问题，则多路输入，每一路把文章和答案选项拼接作为输入即可。从上图可看出，这种改造还是很方便的，不同任务只需要在输入部分施工即可。</p><p><img src="https://i.loli.net/2020/10/30/qnLcVGo5IK6riYh.jpg" alt="img"></p><p>GPT的效果是非常令人惊艳的，在12个任务里，9个达到了最好的效果，有些任务性能提升非常明显。</p><p><img src="https://i.loli.net/2020/10/30/96zdAXvcOmJTuP2.jpg" alt="img"></p><p>那么站在现在的时间节点看，GPT有什么值得改进的地方呢？其实最主要的就是那个单向语言模型，如果改造成双向的语言模型任务估计也没有Bert太多事了。当然，即使如此GPT也是非常非常好的一个工作，跟Bert比，其作者炒作能力亟待提升。</p><h3 id="bert的诞生">Bert的诞生</h3><p><img src="https://i.loli.net/2020/10/30/rSJAqOMB4sathDg.jpg" alt="img"></p><p>我们经过跋山涉水，终于到了目的地Bert模型了。</p><p>Bert采用和GPT完全相同的两阶段模型，首先是语言模型预训练；其次是使用Fine-Tuning模式解决下游任务。和GPT的最主要不同在于在预训练阶段采用了类似ELMO的双向语言模型，当然另外一点是语言模型的数据规模要比GPT大。所以这里Bert的预训练过程不必多讲了。</p><p><img src="https://i.loli.net/2020/10/30/61JpWKSZ5fF3tNk.jpg" alt="img"></p><p>第二阶段，Fine-Tuning阶段，这个阶段的做法和GPT是一样的。当然，它也面临着下游任务网络结构改造的问题，在改造任务方面Bert和GPT有些不同，下面简单介绍一下。</p><p><img src="https://i.loli.net/2020/10/30/UTQdhtVA7PzcIlF.jpg" alt="img"></p><p>在介绍Bert如何改造下游任务之前，先大致说下NLP的几类问题，说这个是为了强调Bert的普适性有多强。通常而言，绝大部分NLP问题可以归入上图所示的四类任务中：一类是序列标注，这是最典型的NLP任务，比如中文分词，词性标注，命名实体识别，语义角色标注等都可以归入这一类问题，它的特点是句子中每个单词要求模型根据上下文都要给出一个分类类别。第二类是分类任务，比如我们常见的文本分类，情感计算等都可以归入这一类。它的特点是不管文章有多长，总体给出一个分类类别即可。第三类任务是句子关系判断，比如Entailment，QA，语义改写，自然语言推理等任务都是这个模式，它的特点是给定两个句子，模型判断出两个句子是否具备某种语义关系；第四类是生成式任务，比如机器翻译，文本摘要，写诗造句，看图说话等都属于这一类。它的特点是输入文本内容后，需要自主生成另外一段文字。</p><p><img src="https://i.loli.net/2020/10/30/mxJybVWl2OatfUc.jpg" alt="img"></p><p>对于种类如此繁多而且各具特点的下游NLP任务，Bert如何改造输入输出部分使得大部分NLP任务都可以使用Bert预训练好的模型参数呢？上图给出示例，对于句子关系类任务，很简单，和GPT类似，加上一个起始和终结符号，句子之间加个分隔符即可。对于输出来说，把第一个起始符号对应的Transformer最后一层位置上面串接一个softmax分类层即可。对于分类问题，与GPT一样，只需要增加起始和终结符号，输出部分和句子关系判断任务类似改造；对于序列标注问题，输入部分和单句分类是一样的，只需要输出部分Transformer最后一层每个单词对应位置都进行分类即可。从这里可以看出，上面列出的NLP四大任务里面，除了生成类任务外，Bert其它都覆盖到了，而且改造起来很简单直观。尽管Bert论文没有提，但是稍微动动脑子就可以想到，其实对于机器翻译或者文本摘要，聊天机器人这种生成式任务，同样可以稍作改造即可引入Bert的预训练成果。只需要附着在S2S结构上，encoder部分是个深度Transformer结构，decoder部分也是个深度Transformer结构。根据任务选择不同的预训练数据初始化encoder和decoder即可。这是相当直观的一种改造方法。当然，也可以更简单一点，比如直接在单个Transformer结构上加装隐层产生输出也是可以的。不论如何，从这里可以看出，NLP四大类任务都可以比较方便地改造成Bert能够接受的方式。这其实是Bert的非常大的优点，这意味着它几乎可以做任何NLP的下游任务，具备普适性，这是很强的。</p><p><img src="https://i.loli.net/2020/10/30/e7tSMGZjDHmY1Ck.jpg" alt="img"></p><p>Bert采用这种两阶段方式解决各种NLP任务效果如何？在11个各种类型的NLP任务中达到目前最好的效果，某些任务性能有极大的提升。一个新模型好不好，效果才是王道。</p><p><img src="https://i.loli.net/2020/10/30/RniS8uQhpDmcHN6.jpg" alt="img"></p><p>到这里我们可以再梳理下几个模型之间的演进关系。从上图可见，Bert其实和ELMO及GPT存在千丝万缕的关系，比如如果我们把GPT预训练阶段换成双向语言模型，那么就得到了Bert；而如果我们把ELMO的特征抽取器换成Transformer，那么我们也会得到Bert。所以你可以看出：Bert最关键两点，一点是特征抽取器采用Transformer；第二点是预训练的时候采用双向语言模型。</p><p>那么新问题来了：对于Transformer来说，怎么才能在这个结构上做双向语言模型任务呢？乍一看上去好像不太好搞。我觉得吧，其实有一种很直观的思路，怎么办？看看ELMO的网络结构图，只需要把两个LSTM替换成两个Transformer，一个负责正向，一个负责反向特征提取，其实应该就可以。当然这是我自己的改造，Bert没这么做。那么Bert是怎么做的呢？我们前面不是提过Word2Vec吗？我前面肯定不是漫无目的地提到它，提它是为了在这里引出那个CBOW训练方法，所谓写作时候埋伏笔的“草蛇灰线，伏脉千里”，大概就是这个意思吧？前面提到了CBOW方法，它的核心思想是：在做语言模型任务的时候，我把要预测的单词抠掉，然后根据它的上文Context-Before和下文Context-after去预测单词。其实Bert怎么做的？Bert就是这么做的。从这里可以看到方法间的继承关系。当然Bert作者没提Word2Vec及CBOW方法，这是我的判断，Bert作者说是受到完形填空任务的启发，这也很可能，但是我觉得他们要是没想到过CBOW估计是不太可能的。</p><p>从这里可以看出，在文章开始我说过Bert在模型方面其实没有太大创新，更像一个最近几年NLP重要技术的集大成者，原因在于此，当然我不确定你怎么看，是否认同这种看法，而且我也不关心你怎么看。其实Bert本身的效果好和普适性强才是最大的亮点。</p><p><img src="https://i.loli.net/2020/10/30/LO9j7cIxEJCe2Ay.jpg" alt="img"></p><p>那么Bert本身在模型和方法角度有什么创新呢？就是论文中指出的Masked 语言模型和Next Sentence Prediction。而Masked语言模型上面讲了，本质思想其实是CBOW，但是细节方面有改进。</p><p><img src="https://i.loli.net/2020/10/30/75DVNACdHgtRbS9.jpg" alt="img"></p><p>Masked双向语言模型向上图展示这么做：随机选择语料中15%的单词，把它抠掉，也就是用[Mask]掩码代替原始单词，然后要求模型去正确预测被抠掉的单词。但是这里有个问题：训练过程大量看到[mask]标记，但是真正后面用的时候是不会有这个标记的，这会引导模型认为输出是针对[mask]这个标记的，但是实际使用又见不到这个标记，这自然会有问题。为了避免这个问题，Bert改造了一下，15%的被上天选中要执行[mask]替身这项光荣任务的单词中，只有80%真正被替换成[mask]标记，10%被狸猫换太子随机替换成另外一个单词，10%情况这个单词还待在原地不做改动。这就是Masked双向语音模型的具体做法。</p><p><img src="https://i.loli.net/2020/10/30/MTCajrZPKuF51Ne.jpg" alt="img"></p><p>至于说“Next Sentence Prediction”，指的是做语言模型预训练的时候，分两种情况选择两个句子，一种是选择语料中真正顺序相连的两个句子；另外一种是第二个句子从语料库中抛色子，随机选择一个拼到第一个句子后面。我们要求模型除了做上述的Masked语言模型任务外，附带再做个句子关系预测，判断第二个句子是不是真的是第一个句子的后续句子。之所以这么做，是考虑到很多NLP任务是句子关系判断任务，单词预测粒度的训练到不了句子关系这个层级，增加这个任务有助于下游句子关系判断任务。所以可以看到，它的预训练是个多任务过程。这也是Bert的一个创新。</p><p><img src="https://i.loli.net/2020/10/30/XQ17TqJcYpPoA3i.jpg" alt="img"></p><p>上面这个图给出了一个我们此前利用微博数据和开源的Bert做预训练时随机抽出的一个中文训练实例，从中可以体会下上面讲的masked语言模型和下句预测任务。训练数据就长这种样子。</p><p><img src="https://i.loli.net/2020/10/30/E1vcQhzTsgbLqkF.jpg" alt="img"></p><p>顺带讲解下Bert的输入部分，也算是有些特色。它的输入部分是个线性序列，两个句子通过分隔符分割，最前面和最后增加两个标识符号。每个单词有三个embedding:位置信息embedding，这是因为NLP中单词顺序是很重要的特征，需要在这里对位置信息进行编码；单词embedding,这个就是我们之前一直提到的单词embedding；第三个是句子embedding，因为前面提到训练数据都是由两个句子构成的，那么每个句子有个句子整体的embedding项对应给每个单词。把单词对应的三个embedding叠加，就形成了Bert的输入。</p><p><img src="https://i.loli.net/2020/10/30/wHfCcyhaiXPMx1d.jpg" alt="img"></p><p>至于Bert在预训练的输出部分如何组织，可以参考上图的注释。</p><p><img src="https://i.loli.net/2020/10/30/B5wItXbYpPr619Z.jpg" alt="img"></p><p>我们说过Bert效果特别好，那么到底是什么因素起作用呢？如上图所示，对比试验可以证明，跟GPT相比，双向语言模型起到了最主要的作用，对于那些需要看到下文的任务来说尤其如此。而预测下个句子来说对整体性能来说影响不算太大，跟具体任务关联度比较高。</p><p><img src="https://i.loli.net/2020/10/30/u9mKAEGjp4fa7ys.jpg" alt="img"></p><p>最后，我讲讲我对Bert的评价和看法，我觉得Bert是NLP里里程碑式的工作，对于后面NLP的研究和工业应用会产生长久的影响，这点毫无疑问。但是从上文介绍也可以看出，从模型或者方法角度看，Bert借鉴了ELMO，GPT及CBOW，主要提出了Masked 语言模型及Next Sentence Prediction，但是这里Next Sentence Prediction基本不影响大局，而Masked LM明显借鉴了CBOW的思想。所以说Bert的模型没什么大的创新，更像最近几年NLP重要进展的集大成者，这点如果你看懂了上文估计也没有太大异议，如果你有大的异议，杠精这个大帽子我随时准备戴给你。如果归纳一下这些进展就是：首先是两阶段模型，第一阶段双向语言模型预训练，这里注意要用双向而不是单向，第二阶段采用具体任务Fine-tuning或者做特征集成；第二是特征抽取要用Transformer作为特征提取器而不是RNN或者CNN；第三，双向语言模型可以采取CBOW的方法去做（当然我觉得这个是个细节问题，不算太关键，前两个因素比较关键）。Bert最大的亮点在于效果好及普适性强，几乎所有NLP任务都可以套用Bert这种两阶段解决思路，而且效果应该会有明显提升。可以预见的是，未来一段时间在NLP应用领域，Transformer将占据主导地位，而且这种两阶段预训练方法也会主导各种应用。</p><p>另外，我们应该弄清楚预训练这个过程本质上是在做什么事情，本质上预训练是通过设计好一个网络结构来做语言模型任务，然后把大量甚至是无穷尽的无标注的自然语言文本利用起来，预训练任务把大量语言学知识抽取出来编码到网络结构中，当手头任务带有标注信息的数据有限时，这些先验的语言学特征当然会对手头任务有极大的特征补充作用，因为当数据有限的时候，很多语言学现象是覆盖不到的，泛化能力就弱，集成尽量通用的语言学知识自然会加强模型的泛化能力。如何引入先验的语言学知识其实一直是NLP尤其是深度学习场景下的NLP的主要目标之一，不过一直没有太好的解决办法，而ELMO/GPT/Bert的这种两阶段模式看起来无疑是解决这个问题自然又简洁的方法，这也是这些方法的主要价值所在。</p><p>对于当前NLP的发展方向，我个人觉得有两点非常重要，一个是需要更强的特征抽取器，目前看Transformer会逐渐担当大任，但是肯定还是不够强的，需要发展更强的特征抽取器；第二个就是如何优雅地引入大量无监督数据中包含的语言学知识，注意我这里强调地是优雅，而不是引入，此前相当多的工作试图做各种语言学知识的嫁接或者引入，但是很多方法看着让人牙疼，就是我说的不优雅。目前看预训练这种两阶段方法还是很有效的，也非常简洁，当然后面肯定还会有更好的模型出现。</p><p>完了，这就是自然语言模型预训练的发展史。</p><h3 id="参考">参考</h3><blockquote><p><a href="https://zhuanlan.zhihu.com/p/49271699" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/49271699</a></p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      从embedding到BERT预训练模型
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-10-30-论文阅读总结</title>
    <link href="http://yoursite.com/2020/10/30/2020-10-30-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2020/10/30/2020-10-30-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E6%80%BB%E7%BB%93/</id>
    <published>2020-10-30T11:27:41.000Z</published>
    <updated>2020-11-03T01:42:06.445Z</updated>
    
    <content type="html"><![CDATA[<h3 id="xlnet-generalized-autoregressive-pretraining-for-language-understanding">XLNet: Generalized Autoregressive Pretraining for Language Understanding</h3><p><del>BERT和GPT都是只使用了transformer的encoder部分，原本transformer层也是可以attend to 双向的，但是GPT为了要基于前面的序列预测下一个word，所以只有上文信息，所以像decoder一样mask掩码掉了，只能利用上文的信息；而BERT没有进行掩码，为了更加利用好双向的关系，BERT在transformer的基础上使用了MLM的策略，主要处理的是自然语言理解的任务。</del></p><blockquote><p>NIPS 2019</p><p>authors ： ZhilinYang , ZihangDai （Carnegie Mellon University, Google AI BrainTeam ）</p><p>code url (official tf) : <a href="https://github.com/zihangdai/xlnet" target="_blank" rel="noopener" class="uri">https://github.com/zihangdai/xlnet</a></p><p>code url (unofficial torch): <a href="https://github.com/huggingface/transformers" target="_blank" rel="noopener" class="uri">https://github.com/huggingface/transformers</a></p></blockquote><p><del>论文名字的含义：一般的AR模型更适合处理自然语言生成的任务，比如transformer、transformer-XL； 而AE更适合处理自然语言理解的任务。本文通过XLNet模型，能够是AR预训练（结合了transformer-XL的思想）能够泛化到处理多个自然语言理解的问题上（与BERT功能类似）</del></p><h4 id="背景">背景</h4><p><img src="https://i.loli.net/2020/11/01/MmsCokbayGLrXdW.png" alt="image-20201031210916531"></p><p>AR语言模型（transformer-XL）只是训练编码一个单向的上下文，然而这限制了其在更多应用场景的效果，比如阅读理解这种任务，在做任务的时候是可以允许同时看到上文和下文一起做决策的。 所以AR语言模型适合自然语言生成的任务 （机器翻译等）</p><p><img src="https://i.loli.net/2020/11/01/8WRguEihUAc1P6G.png" alt="image-20201031211538083"></p><p>因为没有将密度估计作为目标函数的一部分，所以AE语言模型（BERT）就可以获取双向信息，利用上下文信息进行重建masked token。适合自然语言理解任务（阅读理解，问答等）</p><p>借助对双向上下文进行建模的功能，像BERT这种的基于denoising autoencoding （AE）比基于autoregressive language modeling（AR）的方法具有更好的性能。</p><h4 id="问题">问题</h4><p><img src="https://i.loli.net/2020/11/01/LzoWnBEO7GcY5IZ.png" alt="image-20201101103324693"></p><p>BERT的MLM策略的缺点</p><ol type="1"><li><p>mask掉的词之间的联系忽略了，即BERT假设被mask掉的词之间是独立无依赖的</p></li><li><p>pretrain （有mask）和fine-tune（无mask）直接有区别 （pretrain-ﬁnetune discrepancy）</p></li></ol><h4 id="解决">解决</h4><p><img src="https://i.loli.net/2020/11/01/XfrVJ79nZQezpKw.png" alt="image-20201031212429943"></p><p>本文结合AR LM和AE LM，在Transformer-XL的基础上提出generalized autoregressive method，也就是XLNet。</p><p>（1）没有使用BERT的MLM，而是用的是PLM策略。即通过最大化因式分解的所有排列的预期似然性来实现双向上下文学习，克服MLM的缺点。</p><p>所有的分解序列作为一个集合，对所有采样序列，XLNet按照AR LM的计算方式求对数似然期望的极大值。</p><p>（2）XLNet将来自最先进的自回归模型Transformer-XL的思想（segment recurrence mechanism和relative encoding scheme）整合到预训练中，能够提升涉及长文本序列时的效果</p><p>（3）引入Masked Two-Stream Self-Attention 策略来解决PLM出现的目标预测歧义（the ambiguity in target prediction）问题</p><p>每一步在随机排列之后的token，进行的都是一个AR语言模型的过程（在排列顺序中，根据前面的token来预测当前的token ），这样进行T次（具体次数是超参），原则上就可以克服AR中只能看到原始序列顺序之前token的缺点，可以关注到双向信息。</p><h4 id="模型">模型</h4><h5 id="背景知识">背景知识</h5><p>给定文本序列x=[x1,…,xT]，语言模型(AR)的目标是调整参数使得训练数据上的似然函数最大：</p><p><img src="https://i.loli.net/2020/11/01/Zcnh7PNyGksS2JV.png" alt="image-20201031221410608"></p><p>记号x&lt;t表示t时刻之前的所有x，也就是x1:xt−1。hθ(x1:t−1)是RNN或者Transformer。e(x)是词x的embedding。</p><p>BERT是去噪(denoising)自编码的方法。对于序列x，BERT会随机挑选15%的Token变成[MASK]得到带噪声版本的x^。假设被Mask部分的原始值为x¯，那么BERT希望尽量根据上下文恢复(猜测)出原始值了：</p><p><img src="https://i.loli.net/2020/11/01/OXGfrqR6PZKTiEC.png" alt="image-20201031221357201"></p><p>上面的公式中，mt=1表示xt被Mask掉，Hθ是一个Transformer，它把长度为T的序列x映射为隐状态的向量序列。</p><p>不同点：</p><ol type="1"><li>BERT是“≈” ，因为BERT假设被mask掉的词之间是独立无依赖的，没考虑之间的关系，而AR是“=”</li><li>BERT的输入是BERT会随机挑选15%的Token变成[MASK]得到带噪声版本的x^，而AR是原始的x序列</li><li>BERT可以获取上下文的双向信息，而AR只能获得上文token信息</li></ol><h5 id="permutation-language-modeling-plm">Permutation Language Modeling （ PLM ）</h5><h6 id="思想">思想</h6><p>提出了一种序列语言建模目标，它不仅可以保留 AR 模型的优点，同时也允许模型捕获双向语境。</p><p>具体来说，一个长度为 T 的序列 x 拥有 T! 种不同的排序方式，可以执行有效的自回归因式分解。在本文中选择了T种</p><p>如果模型参数在所有因式分解顺序中共享，那么预计模型将学习从两边的所有位置上收集信息。</p><p>以下图为例，对于序列[1,2,3,4]有24种排列方式，那么下图中四种排列方式的，该序列的期望函数分别是：</p><p><img src="https://i.loli.net/2020/11/01/zNDVbehHL9SRtYf.png" alt="image-20201101004521295"></p><p><img src="https://i.loli.net/2020/11/01/Hnt4SWwmQPdbloB.png" alt="image-20201031223904029"></p><p>在给定相同输入序列 x（但因式分解顺序不同）时预测 token x3的示例</p><p>相比于普通的语言模型只能学习一种方向的依赖关系，排列语言模型会学习各种顺序的预测方法</p><p>问题：遍历T!种路径，然后学习语言模型的参数。计算量非常大</p><p>解决：随机采样T！中的部分排列</p><p>PLM的目标是调整模型参数使得下面的似然概率最大：</p><p><img src="https://i.loli.net/2020/11/01/cBRaNlDnrYuoF37.png" alt="image-20201031224727513"></p><p>其中ZT表示长度为T的序列的所有排列组成的集合，则z∈ZT是一种排列方法。</p><p>用Xzt表示排列的第t个元素，zt表示第t个位置，而z&lt;t表示z的第1到第t-1个元素。</p><p>这样pretrain和finetune阶段就一样了，输入都是原始序列，通过attention mask实现随机产生的排列。例如排列是2-4-3-1，那么在预测X3的时候就只有2、4作为先验，并且2、4的位置信息是通过Zt来体现的，这样也保留了排列的时序信息。</p><p>注意：上面的模型只会遍历概率的分解顺序，并不会改变原始词的顺序。</p><p>实现：</p><p>通过Attention Mask来对应不同的分解方法。比如p(x1|x3)p(x2|x1x3)p(x3)，我们可以在用Transformer编码x1时候让它可以Attend to x3，而把x2Mask掉；编码x3的时候把x1,x2都Mask掉。</p><p><img src="https://i.loli.net/2020/11/01/qzQkLmKdMcPrUy7.png" alt="image-20201031235156078"></p><p>将上述的策略结合AR语言模型，那么就可以避免BERT的问题</p><h5 id="基于目标感知的双流注意力模型">基于目标感知的双流注意力模型</h5><p>问题：</p><p><img src="https://i.loli.net/2020/11/01/SUGq9x5MBKeTpsZ.png" alt="image-20201101005004082"></p><p><img src="https://i.loli.net/2020/11/01/UothZ8Lk6S5AWqG.png" alt="image-20201101005315581"></p><p>这两个概率不应该相等的，但是对比这两个公式会发现，这两个公式的概率是相等的。为什么会出现这样的情况呢？上面问题的关键是<strong>模型无法知道当前mask掉的文本在原始序列中的位置。在Transformer中输入的embedding会加入position embedding，输入已经带入了位置信息，但是我们重新排列之后模型无法预测当前位置在原始序列中的位置，因此我们需要让模型来预测当前文本的位置。</strong> 那么在模型中当前位置的文本的概率计算方式则如下所示，其中g（θ）不仅需要输入当前位置之前的文本，还需要输入他们在原始文本中的位置。</p><p><img src="https://i.loli.net/2020/11/01/RWPnZ5dwYAIbU2c.png" alt="image-20201101005459040"></p><p>XLNet 打乱了句子的顺序，这时在预测的时候 token 的位置信息会非常重要，同时在预测的时候也必须将 token 的内容信息遮掩起来 (否则输入包含了要预测的内容信息，模型就无法学到知识)。<strong>也就是说 XLNet 需要看到 token 的位置信息，但是又不能看到 token 的内容信息</strong></p><h6 id="双流self-attention">双流self-attention</h6><p><strong>1.Query Stream</strong>，对于每一个 token，其对应的 Query Stream 只包含了该 token 的位置信息，注意是 token 在原始句子的位置信息，不是重新排列的位置信息。</p><p><strong>2.Content Stream</strong>，对于每一个 token，其对应的 Content Stream 包含了该 token 的内容信息。</p><p>查询表征单元(Query Representation)：查询表征单元和我们上述需要注意的点相同，<strong>可以看到上下文的信息和当前位置，不可以看到当前的Token</strong>，例如[1,2,3,4]在第4个位置只能看到[1,2,3]。查询表征单元中矩阵Q由于计算了各个位置的信息，保留了当前位置，但是KV矩阵分别表示各个context的重要性，没有计算当前位置。</p><p><img src="https://i.loli.net/2020/11/01/CU1PnWlaArwDN8E.png" alt="img"></p><p>内容表征单元(Context Representation):内容表征单元和我们上文中说的Transformer一致，<strong>可以看到上下文的信息和当前的Token</strong>，例如文本序列[1,2,3,4]，在第4个位置，内容表征单元可以看到[1,2,3,4]，在第3个位置内容表征单元可以看到[1,2,3]。如下图所示QKV矩阵的计算都包含了当前位置。</p><p><img src="https://i.loli.net/2020/11/01/xD53wJVQsdIoC7i.png" alt="img"></p><p><strong>Query Stream 和 Content Stream 组合</strong></p><p>XLNet 将 Query Stream 和 Content Stream 组合在一起，整体架构如下图所示。</p><p><img src="https://i.loli.net/2020/11/01/wyni1NAtfYsGrV4.png" alt="image-20201101011712221"></p><p>图中最下面的一层是输入层，其中 e(x) 是单词的词向量，表示输入的 Content Stream，而 w 表示输入的位置信息，即 Query Stream。</p><p>图中的掩码矩阵，红色表示不遮掩，白色表示遮掩。第 1 行表示 token 1 的掩码，可以看到，1 是句子的最后一个 token，因此可以看到之前的所有 token (3,2,4)。3 是句子的第一个 token，看不到句子的任何信息，因此第 3 行都是白色的 (表示遮掩)。</p><h6 id="partial-prediction">Partial Prediction</h6><p>XLNet 将句子重新排列，然后根据排列后的顺序使用 AR 方式预测，但是由于句子是随机排列的，会导致优化比较困难且收敛速度慢。因此 XLNet 采用了 Partial Prediction (部分预测) 的方式进行训练，对于排列后的句子，只预测句子末尾的 1/K 个 token。</p><p>例如 K=4，就是只预测最后 1/4 的 token。给定句子 [1,2,3,4,5,6,7,8] 和一种随机排列 [2,8,3,4,5,1,7,6]，则只预测 7 和 6。论文中训练 XLNet-Large 时使用的 K 为 6，大约是预测末尾 <strong>14.3%</strong>的 token。</p><h4 id="实验">实验</h4><p><img src="https://i.loli.net/2020/11/01/i7qlvJjTtxAgw4a.png" alt="image-20201101012324865"></p><p><img src="https://i.loli.net/2020/11/01/pIOMyiU9V2Y6wSz.png" alt="image-20201101012421507"></p><p>消融实验</p><p>排列语言模型和transfomer-xl对效果的影响很大。而且NSP任务对效果的影响倒是几乎没有，这也是上文中我们没有用NSP任务的原因。</p><h4 id="总结">总结</h4><p>XLNet 的核心思想是 PLM，排列原来的句子，然后预测末尾的单词。这样可以学习到单词之间的依赖关系，而且可以利用 token 前后向的信息。</p><blockquote><p><a href="http://aiblog.top/2019/07/12/XLNET%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E5%8F%8A%E7%90%86%E8%A7%A3/" target="_blank" rel="noopener">http://aiblog.top/2019/07/12/XLNET%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E5%8F%8A%E7%90%86%E8%A7%A3/</a></p><p><a href="https://my.oschina.net/u/4373067/blog/4476706" target="_blank" rel="noopener" class="uri">https://my.oschina.net/u/4373067/blog/4476706</a></p><p><a href="https://baijiahao.baidu.com/s?id=1654814515140351919&amp;wfr=spider&amp;for=pc" target="_blank" rel="noopener" class="uri">https://baijiahao.baidu.com/s?id=1654814515140351919&amp;wfr=spider&amp;for=pc</a></p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      论文阅读, XLNet
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-10-30-预训练模型总结</title>
    <link href="http://yoursite.com/2020/10/30/2020-10-30-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93/"/>
    <id>http://yoursite.com/2020/10/30/2020-10-30-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93/</id>
    <published>2020-10-30T11:24:42.000Z</published>
    <updated>2020-10-30T12:33:02.872Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言">前言</h3><p>Bert模型自18年10月推出，到目前为止快两年了。它卜一问世即引起轰动，之后，各种改进版本的预训练模型（Pre-Training Model, PTM）与应用如过江之鲫，层出不穷。Bert及它的继任者们，确实也不负众望，在NLP各个领域攻城略地，所向披靡，多种NLP数据集竞赛榜单，连续多年被各种新出现的预训练模型霸榜，有些榜单，个别模型已经把指标刷到超过人类。</p><p>那么，在近两年的时间里，诸多改进模型中，有哪些令人印象深刻的新模型？在那些表现突出的新模型中，是哪些因素导致它们的良好表现？预训练模型技术本身有重大的改动或创新么？或者，关于预训练模型，目前有哪些相对明确的结论？根据目前的技术发展水准，如何根据现有结论，来打造最强的预训练模型？本文通过梳理现有技术文献，试图来回答上述一系列问题。本文的数据都客观有出处，但是对数据的解读，带有严重的个人色彩，偏颇难免，还请谨慎参考。</p><p>我们知道，在预训练模型框架下，解决NLP问题，会划分为序列进行的两阶段：第一阶段是预训练阶段，然后是Fine-tuning阶段，本文集中在预训练阶段。</p><p><img src="https://i.loli.net/2020/10/30/DL9wbrUlSxFBGNm.jpg" alt="img"></p><p>如果我们一句话宏观地归纳预训练模型要做的事情（参考上图），其实很好理解，就是下面这句话：</p><p>在Transformer作为特征抽取器基础上，选定合适的模型结构，通过某种自监督学习任务，逼迫Transformer从大量无标注的自由文本中学习语言知识。这些语言知识以模型参数的方式，存储在Transformer结构中，以供下游任务使用。</p><p>我们见到的形形色色的预训练模型，无非就是，实现上述思路的具体做法而已。你可以换个模型结构，可以换个学习任务，也可以换个其它的部件，无非就是各种试，当然，有些做法相对有效，有些做法效果差些。一般而言，通常所说的预训练模型，都是从自由文本中学习语言知识，很明显，我们可以引入新型的知识或数据，比如人类已经挖掘好的结构化知识、多模态数据、多语言数据等，引入这些知识来促进模型理解语言，或者解决特殊类型的任务。</p><p>后文会先介绍预训练模型中常见的几种模型结构，并给出目前能得出的结论。然后，我们会找出目前表现比较好的那些预训练模型，并分析它们起作用的主要因素是什么。接下来，会简要介绍几种非自由文本类知识学习的预训练基本方法。</p><p>在谈这些之前，我们先从RoBERTa讲起。如果时光倒退半年多，你会发现，这是一个价值被严重低估的模型，其实，它很重要。</p><h3 id="预训练模型中的强基准roberta">预训练模型中的强基准：RoBERTa</h3><p>严格来说，原始的Bert模型是个未完成的半成品，而RoBERTa才是遵循Bert思路的完成品，或者说，Bert是进行时中的RoBERTa，也就是说下列等式成立Bert=RoBERTing。为什么这么说呢？因为，我们可以把RoBERTa看作是得到充分训练的Bert模型，而原始版本的Bert模型训练不够充分，这种模型是否得到充分训练的微小差异，能够极大提升原始版本Bert模型的效果。</p><p><img src="https://i.loli.net/2020/10/30/wvSEJsGYH1cpFbx.jpg" alt="img"></p><p>在原始Bert模型的基础上，RoBERTa通过实验，证明了如下几点：</p><ol type="1"><li>进一步增加预训练数据数量，能够改善模型效果；</li><li>延长预训练时间或增加预训练步数，能够改善模型效果；</li><li>急剧放大预训练的每个Batch的Batch Size，能够明显改善模型效果；</li><li>拿掉预训练任务中的Next Sentence Prediction子任务，它不必要存在；</li><li>输入文本的动态Masking策略有帮助；</li></ol><p>上面列出的五项改进中，第四项和第五项改动，对最终的模型效果影响不大，暂时可忽略。第一点改进增加预训练数据对模型效果有帮助，这个符合直觉。第二项和第三项则涉及到模型是否得到充分训练，本质上这两项相结合，代表了更充分训练的Bert模型。如上面的性能对比图所示，如果以Bert Large作为对比基准，可以发现：仅仅将Batch Size放大，三个数据集上的效果就获得了明显提升，如果再加入新的数据，以及不断增加训练步数，还能持续获得效果的进一步提升。可以看出，RoBERTa效果明显比Bert large好，在相同数据情况下，甚至超过了知名度很高的XLNet。这主要归功于数据规模的增加，以及更充分的训练过程，其中更充分的训练过程发挥的作用更大些。这是为何说RoBERTa 在某种意义上，其实是一个完成版本或者加强版本的Bert模型。</p><p>纵观目前公开的预训练模型，我们可以发现，RoBERTa是其中一个效果非常好的超强基准模型。这句话有几个意思：</p><p>首先，尽管看上去RoBERTa也没做啥技术或者模型改进，只是把Bert模型训练得更充分了一些，但是，它的效果是非常好的。目前为止，效果能够明显超过它的模型很少，屈指可数，这个“屈指可数”，不是虚指，是它的字面含义。这个事实，其实隐含了很大的信息量，它说明了一个什么问题呢？您可以想一想，我的看法在后面小节内容会提到。</p><p>其次，对于一个改进模型来说，理论上都应该引入RoBERTa作为对比Baseline，而改进模型的效果，如果不能具备说服力地超过RoBERTa的话，那么这种改进的有效性，多少是成问题的，除非你强调改进模型的优势不在效果好，而在其它方面，比如更小更快等。</p><p>再次，后续的改进预训练模型，从策略角度讲，应该在设计之初，就站在RoBERTa的巨人肩膀上，就是说在增加一定数据量的前提下，增大Batch Size，加长预训练时间，让模型得到充分训练。因为，如果你不这么做，大概率你的效果是很难比过RoBERTa的，而目前我们能够见到的效果很突出的模型，你如果细究，会发现其实都已经引入了RoBERTa的关键要素了，关于这一点，在后面小节我们会做分析。</p><p>还有，对于追求落地效果的人来说，比如公司里做业务的同学，建议以RoBERTa为基础模型来做应用。</p><h3 id="预训练的发动机模型结构">预训练的发动机：模型结构</h3><p>对于预训练模型来说，目前的主流模型大都采用Transformer作为特征抽取器，现阶段看，Transformer的潜力仍然没有被充分挖掘，还有很大潜力可挖，意思是，Transformer效果足够好，而且还可以更好，貌似改进Transformer并非当务之急的事情。预训练模型的知识，是通过Transformer在训练迭代中从数据中不断学习，并以模型参数的形式编码到模型中的。虽然，大家都是用的Transformer，但是怎么用它搭建模型结构学习效率更高？这是一个问题。所谓学习效率高，就是给定相同大小规模的训练数据，它能编码更多的知识到模型里，这就意味着它的学习效率更高。不同的Transformer用法，会产生不同的模型结构，就会导致不同结构的差异化的学习效率。本节我们归纳下目前能得到的，关于模型结构的现有研究结论，会介绍常见的五种模型结构。当然，这里用模型结构来表达不足够确切，因为除了模型结构外，一般还包含自监督的学习方法，常见的学习方法包括AutoEncoding(简称AE)和AutoRegressive(简称AR)。AE即我们常说的双向语言模型，而AR则代表从左到右的单向语言模型。</p><ul><li><strong>Encoder-AE结构</strong></li></ul><p><img src="https://i.loli.net/2020/10/30/1klNabKIiu73QJq.jpg" alt="img"></p><p>Encoder-AE结构如上图所示。这其实是包括原始版本Bert在内的，大多数后续改进模型采取的结构。整个结构就是一个标准的Transformer，在语言模型预训练的时候，采用AE方法。也就是说，输入句中的未被Mask的任意单词两两可见，但是被Mask掉的单词之间都相互独立，互不可见。在预测某个被Mask掉的单词的时候，所有其它被Mask的单词都不起作用，但是句内未被Mask掉的所有单词，都可以参与当前单词的预测。可以看出，Encoder-AE是个采用双向语言模型的单Transformer结构。</p><p><img src="https://i.loli.net/2020/10/30/isTaJS6GdQLPBZU.jpg" alt="img"></p><p>模型结构比较（From BART）</p><p><img src="https://i.loli.net/2020/10/30/Opc45h7Z3PF1yq6.jpg" alt="img"></p><p>模型结构比较（From Google T5）</p><p>从目前对比实验看（上面两图），除了下文要讲述的Encoder-Decoder结构外，貌似对于语言理解类的NLP任务，这种结构都是效果最好的，但是对于语言生成类的任务，这种结构效果相对很差。也就是说，这种结构比较适合做语言理解类的任务。</p><ul><li><strong>Decoder-AR结构</strong></li></ul><p><img src="https://i.loli.net/2020/10/30/LWPdHeDOo5XRQK2.jpg" alt="img"></p><p>Decoder-AR结构如上图所示。它和Encoder-AE结构相同，都是采用单个的标准Transformer，主要区别在于：语言模型预训练的时候，采用AR方法，就是从左到右逐个生成单词，第i个单词 Wi 只能看到它之前的第1到第（i-1）个单词W1,...,Wi-1 ，不能看到后面的单词。采用这种结构的典型模型就是GPT1、GPT2、GPT3系列了。GPT3在文本生成任务方面的表现，确实是出乎意料地好。当然，这不能仅仅归功于这个结构本身，更复杂的模型和更大量的数据可能是主因。可以看出，Decoder-AR结构是个单向语言模型的单Transformer结构。</p><p>从目前对比实验看（参考Encoder-AE小节的两张效果对比图），除了Encoder-Decoder结构外，貌似对于语言生成类的任务，这种结构是效果最好的结构之一。但是相应的，语言理解类的任务，采用这种结构，效果比Encoder-AE结构差距非常明显，这也好理解，因为只看到上文看不到下文，对于很多语言理解类任务而言，信息损失很大，所以效果不好也在情理之中。也就是说，这种结构比较适合做语言生成类的任务。</p><ul><li><strong>Encoder-Decoder结构</strong></li></ul><p>既然Encoder-AE比较适合做语言理解类的任务，Encoder-AR比较适合做语言生成类的任务。那么，我们能否结合两者的优势，使得预训练模型既能做好生成类NLP任务，又能做好理解类任务呢？这是个很自然的想法，而Encoder-Decoder结构就是如此将两者结合的。最早明确提出使用Encoder-Decoder结构做通用领域预训练的，应该是微软提出的MASS模型，不过和这里介绍的做法有差异。</p><p><img src="https://i.loli.net/2020/10/30/7NG2hnjXitKTOcZ.jpg" alt="img"></p><p>Encoder-Decoder结构如上图所示。这种结构在Encoder侧，单独使用一个Transformer，采用了Encoder-AE的结构。也就是说，编码阶段采用双向语言模型，任意两个单词两两可见，以更充分地编码输入信息；而在Decoder侧，使用另外一个Transformer，采用了Decoder-AR结构，从左到右逐个生成单词。</p><p>当然，Decoder侧和标准的Decoder-AR不同的地方还是有的：Decoder侧生成的单词 Wi，除了像Decoder-AR结构一样能看到在它之前生成的单词序列 W1,...,Wi-1 外，还能看到Encoder侧的所有输入单词 。而这一般是通过Decoder侧对Encoder侧单词，进行Attention操作方式来实现的，这种Attention一般放在Encoder顶层Transformer Block的输出上。</p><p>在进行预训练的时候，Encoder和Decoder会同时对不同Mask部分进行预测：Encoder侧双向语言模型生成被随机Mask掉的部分单词；Decoder侧单向语言模型从左到右生成被Mask掉的一部分连续片断。两个任务联合训练，这样Encoder和Decoder两侧都可以得到比较充分地训练。</p><p><img src="https://i.loli.net/2020/10/30/SOAQbIj9czkhVK6.jpg" alt="img"></p><p>模型结构比较（From UniLM v2）</p><p>从目前对比实验看，无论是语言理解类的任务（参考Encoder-AE部分Google T5论文中展示的效果对比图），还是语言生成类的任务（参考上面来自于UniLM v2的效果对比），貌似Encoder-Decoder结构相对其它几种结构来说，效果都是最好的之一。而且，它有另外一个优点，就是用这个结构，可以同时做生成类和理解类的NLP任务，基本做到了不同任务在模型结构上的统一，这点还是很好的，一个结构可以到处使用，比较方便。但是，它也有个问题，因为两侧各用了一个Transformer，所以相对其它结构参数量翻倍，计算量也增加了，就是说比其它模型笨重。而且，Encoder-Decoder结构比其它结构效果好，很可能主要原因来自于参数量增加导致的模型容量增大，当然这是个人猜测。目前，采用这个结构的效果很好的模型包括Google T5以及BART等模型。</p><ul><li><strong>Prefix LM</strong></li></ul><p><img src="https://i.loli.net/2020/10/30/2b1usgtZPwJa58D.jpg" alt="img"></p><p>Prefix LM结构是Google T5论文中给出的叫法，这种结构最早由UniLM模型提出，我们沿用Google T5的这种称谓。如果深入分析的话，Prefix LM其实是Encoder-Decoder模型的变体：标准的Encoder-Decoder模型，Encoder和Decoder各自使用一个独立的Transformer；而Prefix LM，相当于Encoder和Decoder通过分割的方式，分享了同一个Transformer结构，Encoder部分占用左部，Decoder部分占用右部，这种分割占用是通过在Transformer内部使用Attention Mask来实现的。与标准Encoder-Decoder类似，Prefix LM在Encoder部分采用AE模式，就是任意两个单词都相互可见，Decoder部分采用AR模式，即待生成的单词可以见到Encoder侧所有单词和Decoder侧已经生成的单词，但是不能看未来尚未产生的单词，就是说是从左到右生成。</p><p>目前的一些对比实验证明，在其它条件相同的情况下，关于语言理解类的任务（参考Encoder-AE部分Google T5论文中的相关实验），Prefix LM结构的效果要弱于标准Encoder-Decoder结构。这里是值得深入思考下的，因为看上去Prefix LM和标准的Encoder-Decoder结构是等价的。那么，为什么它的效果比不过Encoder-Decoder结构呢？我想，一方面的原因估计是两者的参数规模差异导致的；另外一方面，可能与它这种模式的Decoder侧对Encoder侧的Attention机制有关。在Decoder侧，Transformer的每层 Block对Encoder做Attention的时候，标准的Encoder-Decoder模式，Attention是建立在Encoder侧的最后输出上，这样可以获得更全面完整的全局整合信息；而Prefix LM这种结构，Decoder侧的每层Transformer对Encoder侧的Attention，是建立在Encoder的对应层上的，因为这种模式的Encoder和Decoder分割了同一个Transformer结构，Attention只能在对应层内的单词之间进行，很难低层跨高层。这可能是影响这种结构效果的原因之一。当然这只是个人猜测，无证据证明，还请谨慎参考。</p><p>关于语言生成类的任务，Prefix LM效果虽然要弱于Encoder-Decoder结构（参考Encoder-Decoder小节UniLM v2论文效果对比图），但是总体而言，两者相差不大，相对其它模型，Prefix LM结构在生成类任务表现也比较突出。</p><p>Prefix LM因为是Encoder-Decoder的变体，所以可以看出，它的优势也在于可以同时进行语言理解和语言生成类任务，而且相对Encoder-Decoder来说，因为只用了一个Transformer，所以模型比较轻，这是Prefix LM的优势。缺点则是在效果方面，貌似要弱于Encoder-Decoder模型的效果，语言理解类任务相对有明显差距，生成类任务的效果相差不大。</p><ul><li><strong>Permuted Language Model(PLM)</strong></li></ul><p>PLM最早是在XLNet的论文中提出的，目前有些后续模型也在PLM上进行改进，所以我们把PLM也放在这里一起说一下。</p><p>PLM一样采用单个Transformer模型作为主干结构，但是从训练方法上来说，是个很另类也很有创意的做法，是种“形为AR，实为AE”的做法。在语言模型预训练过程中，它看上去遵循AR从左到右的输入过程，这符合一般生成任务的外在表现形式，但是在内部通过Attention Mask，实际做法其实是AE的做法，无非是把AE的做法隐藏在Transformer内部。它和AE从细节来说，主要有两个区别：首先，预训练过程中，输入句子去掉了Mask标记，改为内部Attention Mask，以保持预训练过程和下游任务Fine-tuning的一致性。关于这一点，目前有实验证明这个虽然有积极影响，但是影响不大（ELECTRA针对预训练过程是否带Mask 标记做了效果对比，带Mask标记的Bert模型GLUE得分82.2，去掉Mask标记利用其它单词代替的对比模型GLUE得分82.4）；其次，也是它和AE的最主要区别，PLM认为被Mask掉的单词之间是相互有影响的，先产生的被Mask掉的单词，应该对后生成的被Mask掉的单词，在预测的时候发生作用，而标准的AE则认为被Mask掉的单词是相互独立的，相互之间不产生作用。</p><p><img src="https://i.loli.net/2020/10/30/eqPo6kEBMm1Unaz.jpg" alt="img"></p><p>其实，如果你仔细分析下PLM的预训练过程，会发现本质上PLM是Prefix LM的一种变体。上图给出了个例子来说明这种情况，对于某个输入句子，PLM首先会进行单词顺序随机变换，然后选定变换后句子的末尾一部分单词进行Mask，被Mask的单词预测顺序是有序的，按照变换后在句中先后顺序来预测，上面例子中会先预测 X1 ，然后再预测 X5 。在预测 X1 的时候，未被Mask的上下文 [X2,X3,X4] 会对预测 X1 有帮助；假设已经预测并输出了 X1 ，在预测 X5 的时候，未被Mask掉的上下文 [X2,X3,X4] ，以及刚预测出的 X1 ，会对预测 X5 有帮助。其实你想，这等价于什么？等价于以 X4 作为边界切割开的Prefix LM模型，Encoder端包含 [X2,X3,X4] ，Decoder侧包含 [X1,X5] ，在预测 X5 的时候，不仅能看到Encoder侧的所有输入，也能看到Decoder侧之前的输出 X1 。当然，因为每个输入句子的长度各异，被Mask掉的单词个数也不固定，所以看上去Encoder和Decoder的边界根据输入句子，边界是在动态变化的。所以，PLM其实是一种边界变化的Prefix LM变体结构。当然，上面纯属个人推理过程，不保证正确性，谨慎参考。</p><p>如果不考虑XLNet里的其它因素，单纯看PLM结构的话，目前有些对比实验，貌似PLM在语言理解类任务中，效果不及Encoder-AE（参考UniLM v2论文中的对比实验，未在本文列出，可参考论文）；在语言生成类任务中，效果略微优于Encoder-AE，但是距离Decoder-AR差距较大（参考Encoder-AE描述部分BART的对比实验）。在两类任务中，都有点上不着村，下不着店的感觉，就是都还可以，但都不够好的感觉。XLNet效果确实是很好的，但是，这说明XLNet效果好，真正起作用的貌似不是PLM，而是其它因素。</p><p>上面内容简述了常见的五种预训练模型结构，如果总结一下的话：</p><p>首先，从模型效果来看，Encoder-Decoder结构无论在语言理解类还是语言生成类任务中，都是效果最好的。当然，效果好的原因很可能在于模型参数多，模型容量大，而不一定是自身结构带来的优势。它的优点一个是效果好，一个是能够将理解和生成任务统一在一个框架下；缺点是参数多计算多，所以模型比较重。采用这个结构的代表模型包括Google T5和BART。</p><p>其次，因为Encoder-Decoder模型比较重，所以，如果从相对轻量结构里进行选择的话，对于语言理解类任务，Encoder-AE结构相对而言效果较好，代表模型很多，典型的比如ALBert、RoBERTa；对于语言生成类任务，Decoder-AR结构和Prefix LM结构相对而言效果较好，都可考虑，Decoder-AR的代表模型是GPT系列，Prefix LM的代表模型是UniLM。语言理解类任务应该用AE任务，语言生成类任务应该用AR任务，这点也很明确了。</p><p>谈完了模型结构，下面我们来盘点下表现比较好的预训练模型，并分析下效果好背后的原因。</p><h3 id="强者的狂欢为什么有些模型表现这么好">强者的狂欢：为什么有些模型表现这么好</h3><p>目前Bert的改进模型有很多，有的表现非常突出，有的表现一般。我的主要目的是想找出那些表现好的模型，并分析下，到底是哪些因素导致这些模型效果超群的。</p><p>首先，我们需要先找出那些表现特别好的模型出来，我这里说的表现好，主要是从模型效果角度来说的，就是那些在公开数据集上指标比较高的模型。一种比较简单的方法就是：找GLUE、SuperGLUE、SQuAD 2.0这几个大规模NLP数据上，那些打榜模型中排名前列的。你可以看一下，自从Bert出现后，这几个榜单，都长年被预训练模型霸榜，指标在被各种新的预训练模型快速刷高，直到超过人类的水准。一般而言，能够打榜把指标刷到前列的，都是好模型，说明这些模型真的能打（插句闲话，这点其实特别值得推荐领域借鉴，就是有个大规模高难度数据集，供各种模型长年刷榜，这其实是促进领域技术进步很好的手段）。当然，也有一些新模型，可能未必会去打榜，所以作为补充措施，我又从比较新的文献中，找出一些模型，前提是它在文献中报道的效果要比RoBERTa好。这样，我筛出了一批表现优秀的模型，包括：RoBERTa，Google T5，ALBERT，ELECTRA，XLNet，GPT3，BART，UNILM v2, StructBert，MacBert。这些模型要么在某个榜单前几名，要么论文实验结果显示效果非常好，二者占其一。这里面，GPT3是个纯生成模型，ELECTRA相对而言方法比较特殊，在后面我会单独说下它。需要说明的是，ERNIE和NEZHA模型，效果也是非常好的，能够排在某些榜单前列。但是因为它们对应的论文比较早，我猜测现在打榜的模型，估计和原始论文中的做法，已经做了变动，但是具体怎么变的不清楚，所以没有在上面列表中列出。上述表单，应该基本囊括了目前时间（2020年9月）绝大多数效果最好的预训练模型了。</p><p>上述模型，都能找到对应的文章，可供仔细分析模型的有效因素。如果你仔细分析上述各个模型的共性，会发现，那些真正有效的因素会慢慢浮出水面。我在这里归纳一下：促进模型性能快速提高的因素，主要包含下列几方面。而且，这几方面的因素是可叠加的，就是说，如果一个模型采纳其中越多的因素，那么这个模型的效果表现可能会更好。</p><p>首先，更高质量、更多数量的预训练数据。</p><p><img src="https://i.loli.net/2020/10/30/E8VF4gCRMDjSlbm.jpg" alt="img"></p><p>关于预训练数据对模型效果的影响，Google T5做了大量对比实验，目前的结论，如果归纳一下的话，应该是这样的：在保证预训练数据质量的前提下，数据规模越大模型效果越好。这里需要注意的是，数据规模越大越好，这点其实从Bert一出来，就是一个容易想到的重要因素。因为数据量越多，数据里蕴含的知识也越多，那么模型能学到的东西越多，所以模型效果会更好，这是一个靠简单推理就能得出的结论。但是，它是有前提的，前提是数据质量要高，光数据量大不行，很多乱七八糟的数据，反而会对模型效果带来负面影响。</p><p>第二，增加模型容量及复杂度。</p><p><img src="https://i.loli.net/2020/10/30/YuIyA7FQh3JCxpW.jpg" alt="img"></p><p>所谓增加模型容量及复杂度，指的是增加Transformer模型的参数量，一般而言，模型容量越大，模型的表达能力越强。最直接的增加模型容量的方式就是增加Transformer Block层深，比如可以从Bert base的12层，增加到Bert Large的24层，还可以继续增加到比如36层，这是纵向增加复杂度，Google T5走的这条路（从上图可以看出，模型容量增加到4倍后，有些数据集效果相对Baseline有大幅度的提升）。除此外，还可以横向增加模型复杂度，比如在固定Transformer层深的情况下，可以通过放大Transformer中构件的大小，比如Hidden Size的增大，FFN层对隐层的放大，Multi-Head Self Attention的Attention头的增加，等多种方式来做到这一点。ALBERT走的这条路，它的xxLarge模型效果最好，只用了12层Transformer Block，但是Hidden Size达到了4096。</p><p>这两种模式还可以相互结合，就是同时纵向和横向增加模型复杂度，GPT 3即是如此，将模型复杂度这点推到了极致。单词特征的Embedding不会放的太大，一般采用64或者128大小，ALBERT证明了如果单词特征Embedding跟着Transformer内部的Hidden Size同步放大，效果反而会降低。也就是说，增加模型容量指的是放大Transformer模型本身的参数量，但不包括输入层Embedding的参数。</p><p>第三，更充分地训练模型；</p><p>这里所谓的“更充分”，一般指的是放大Batch Size、增加预训练步数，就是RoBERTa做的那两个事情。这块上文有述，这里不再赘述。</p><p>第四，有难度的预训练任务；</p><p><img src="https://i.loli.net/2020/10/30/sILxebBoQuO7wt4.jpg" alt="img"></p><p>原始的Bert预训练，有两个训练任务：一个是单词级的Mask语言模型MLM，一个是句子级的下一句预测任务NSP。RoBERTa证明了NSP对于模型效果没什么影响，所以拿掉了这个任务。有很多研究集中在这一块，采取了五花八门的预训练任务（如上图所示）。那么哪些预训练任务相对而言更有效呢？目前已经能够得出些比较明确的结论。</p><p>如果归纳一下的话，应该是这样的：对于单词级的Mask语言模型来说，Span类的预训练任务效果最好。所谓Span类的任务，就是Mask掉的不是一个独立的单词，而是一个连续的单词片断，要求模型正确预测片断内的所有单词。Span类任务，只是一个统称，它会有一些衍生的变体，比如N-Gram，就是Span模型的一个变体，再比如Mask掉的不是单词而是短语，本质上也是Span类任务的变体，这里我们统称为Span类任务。</p><p>目前有相当多的研究证明Span类任务是效果最好的，最近有些工作（微软的ProphetNet和百度的ERNIE-GEN）进一步说明，Span内多个单词独立被生成效果会更好。所谓独立生成，举个例子，假设被Mask掉的片断是：X1,X2,X3 ，之前一般Span类的预训练是顺序生成片断内的单词，就是先生成 X1 ，然后根据上下文及 X1 ，生成 X2 ，这么个顺序，就是说序列生成片断内单词。而独立生成，就是根据上下文，同时生成 X1,X2,X3 , 被生成的单词之间无影响。所以目前单词级的Mask语言模型，独立生成的Span类任务，应该是目前效果最好的。</p><p>对于句子级的任务，NSP任务学习两个句子是否连续句：正例由两个连续句子构成，负例则随机选择一句跟在前一句之后，要求模型预测两者是否连续句子。本质上，NSP在预测两个句子是否表达相近主题，而这个任务，相对MLM来说，过于简单了，导致模型学不到什么知识。ALBERT采用了句子顺序预测SOP（Sentence Order Prediction）：跟NSP一样，两个连续出现的句子作为正例，但是在构造负例的时候，则交换句子正确顺序，要求模型预测两个句子出现顺序是否正确，这样增加任务难度，StructBERT也采取了类似的做法。实验证明SOP是有效的句子级预测任务。</p><p>总而言之，目前证明Span类任务是有效的单词级任务，SOP是有效的句子级任务。目前看，预训练任务越有难度，则预训练模型越能高效率地学习知识，所以寻找更新的更有难度的预训练任务是有较大探索空间以及成功可能的。</p><p>上面列了四个主要因素，那么，还有其它因素么？我的猜测是基本没有了，尽管可能还有一些差异化的改进点是有效的，但它对最终效果的贡献，应该不是特别大，起码不像上述四个因素那么大。上面四个因素，如果进一步要划分重要性的话，估计前三个都很重要，第四个相对而言影响稍小一些。当然，同样地，这是我个人的猜测，谨慎参考。</p><p><img src="https://i.loli.net/2020/10/30/jeYAPHi61MaJFNf.jpg" alt="img"></p><p>如果我们根据上述可叠加的有效因素，来分析现有模型，可得出如上图所示列表（具备某因素的模型，对应的格子做了标记）。从上表中，我们可以得出一些结论：</p><p>首先，所有这些效果表现突出的模型，都增加了更多的高质量预训练数据。另外，通过增大Batch Size以及增加预训练步数方式，都使得模型得到更充分地训练。也就是说，所有这些表现突出的模型，都是站在RoBERTa模型的肩膀上的。其实，只要你站在RoBERTa肩膀上，效果都不会太差，剩下的问题是能比它好多少的问题。</p><p>其次，如果我来冒昧地做个判断的话，貌似对于语言理解类任务来说，估计Google T5和ALBERT是效果最好的预训练模型；而对于语言生成类的任务来说，估计GPT3是效果最好的模型。对于Google T5和ALBERT模型来说，两者都采纳了绝大部分有效因素，主要不同在于预训练任务，Google T5采用了Span类单词级任务，而ALBERT采用了SOP类句子级任务。这三个表现最突出的模型，和其它模型最大的区别，大概率在于它们在增加更多高质量数据的同时，走了大规模提升模型容量的路子。也就是说，在增加数据规模基础上大规模增加模型容量，这应该是拉开不同模型效果最主要的因素。</p><p>再次，我们可以据此预测，如果一个模型，采纳了上述所有有效因素，那么可以获得当前技术水准下的最好模型效果，就如上表中最后一行展示的，目前仍未知的Model X那样。就是说，这个模型应该是这样的：在RoBERTa模型基础上，增加更多高质量数据的同时，充分放大模型容量，而预训练任务则是单词类Span任务和句子类SOP任务的结合。当然，估计这里面起到主要作用的还是大量数据+大模型的因素。</p><p><img src="https://i.loli.net/2020/10/30/BPVtOXFLJye1ZEC.jpg" alt="img"></p><p>这里单独说下ELECTRA，这是一个比较独特的预训练方法(参考上图)。 它形式上采取了类似GAN的模式，但是本质上并非GAN，因为缺乏GAN最关键的生成器和判别器的对抗训练过程。ELECTRA联合训练了小的生成器以及大的判别器，它强迫判别器对生成器产生的所有单词，做个是否经过改写的判断，这无疑增加了模型的学习效率，因为原先的MLM只学习15%的被Mask单词，而ELECTRA对所有单词都要进行判断，并从中学习。ELECTRA论文做了分析，模型的绝大多数收益来自于全部单词参与训练这一步。这意味着，ELECTRA这种所有单词全员参与训练过程的模式，能够在其它条件相同的情况下（模型复杂度，数据量等），使得模型获得更高的学习效率，这个结论和做法还是很有价值的。本质上，ELECTRA这种提升模型效率的方法，和上面所述其它模型的各种做法，是相互互补的。就是说，在ELECTRA的训练模式下，增加训练数据、增加模型规模、模型充分训练，有可能获得更好的模型效果。</p><h2 id="暴力美学简单粗暴但有效"><strong>暴力美学：简单粗暴但有效</strong></h2><p>前文有述，RoBERTa是个非常强的Baseline，相对目前表现最强的Google T5和ALBERT模型，其实RoBERTa与这两个天花板模型之间，它们之间的性能Gap并不是特别大。其它表现突出的模型，要我猜，性能应该介于RoBERTa这个Baseline和两个天花板模型之间。而所有这些模型之间的主要差异，极有可能是模型容量的大小差异带来的。</p><p>从某种角度上看，我们可以认为：RoBERTa可以被看作是经过更充分训练的Bert模型，而ALBERT/Google T5可以理解为进一步增加了模型复杂度的RoBERTa增强版本。从Bert到RoBERTa，再到ALBERT/Google T5，这三类模型，很可能代表了自Bert出现来的最主要技术进展。所以，从模型改进的角度看，自从Bert诞生后近两年，并没有出现特别有效的模型改进方法。尽管从解决NLP任务效果的角度看，新的预训练模型相比Bert有了巨大的提升，但是这些提升，大致可以理解为是因为引入更多高质量数据、采用更多模型参数、模型训练更充分以及增加训练任务难度这几点综合导致的。而其中，在RoBERTa这种充分训练的模型基础上，增加数据，并加上更大的模型，可能在其中起到了主导作用。</p><p>由此进一步推理，我们可以得出如下结论：目前预训练模型都采用的Transformer结构，从模型容量或模型复杂度来说是足够复杂的。就是说，Transformer结构本身，目前并非制约预训练模型效果的瓶颈，我们可以仅仅通过增加高质量数据、增加模型复杂度配以更充分地模型训练，就仍然能够极大幅度地提升Bert的性能。</p><p>这说明了什么呢？这说明了大数据+大模型的暴力美学，这条粗暴简洁但有效的路子，还远远没有走到尽头，还有很大的潜力可挖。尽管这带来的副作用是：好的预训练模型，训练成本会非常高，这不是每个研究者都能够承受的。但是，我的意见，这其实是个好事情。如果仅仅通过加数据、扩模型就能获得更好的效果，这么简单的方式就能推动模型效果不断上升，推动更多应用获得更好效果，这不是天大的好事么？ 至于由此带来的大模型落地难的问题，我相信可以通过搭配知识蒸馏等把模型做小的方案来获得解决。就是说，很可能预训练模型发展会走出一个哑铃模式：两头大，中间小。两个大头中，一头是越来越大的预训练模型，一头是追求各种技术来实用化地把模型做小，这两端会越来越重要。</p><p>如果上述假设成立，即预训练领域的暴力美学依然暴力且美丽，那么从今往后的模型改进，我们应该怎么走呢？我的感觉，应该优先探索大数据+大模型的路，先走到暴力美学的尽头，然后再集中精力探索模型本身的改进。就是说，我们应该先把数据红利吃完，而不是优先发展新型模型，当然两者可以并行做，但是原则上，新型模型优先级不如先把数据红利吃完。为什么这么说呢？因为，目前很多研究表明：大多数改进新模型带来的提升，根本比不过提升数据质量数量的同时扩充模型容量带来的收益。而一些新模型的有效性，在数据量小的时候可能是有效的，但很可能发生的一幕是，当数据增大模型容量加大后，很多改进不再有效。也就是说，目前很多新模型的作用，很可能是增加了特殊类型的语言知识的编码和泛化能力，但是，这是完全可以通过增加数据数量和质量，并加大模型来达成的，这种方式又比较简单直观。所以，这是为何我觉得应该先把精力放到“大数据+大模型” 上，然后再集中精力进行模型改进的主要原因。</p><h2 id="知识补习班其它知识的引入"><strong>知识补习班：其它知识的引入</strong></h2><p>本文开头讲过，大多数预训练模型是从自由文本中学习语言知识。但是，很明显，我们能让模型学的，肯定不止自由文本这一种类型。理论上，任何包含知识的数据，都有些先验知识可供预训练模型学习。我的感觉，预训练模型的发展，会越来越像人脑，日益变成一个黑盒子。就是说，我们可以通过一定手段，喂给它数据，它就会学会其中包含的知识。但是，它是怎么学会的，学到了什么，这很可能对我们来说，会越来越难以理解，就是说，随着预训练模型学习领域的拓展，这个黑盒子，可能会越来越黑。下面我们介绍两个典型的其它领域，看看预训练模型是怎么学的。当然，我相信这种预训练方式，会拓展到越来越多的其它类型的数据或领域，这也是预训练模型领域，一个比较明晰的发展趋势。</p><ul><li><strong>显示知识的引入</strong></li></ul><p>原始Bert的语言学知识，是从大量自由文本中自主学习的，那么很自然的一个问题就是：我们过去已经通过一些技术手段，归纳出大量的结构化知识，比如知识图谱；或者已经建立了很多知识分析工具，比如命名实体识别系统等。那么能否利用这些知识识别工具，抑或已有的结构化知识，让预训练模型能够直接学到这些知识？</p><p>目前也有很多工作在做这个事情，就是让预训练模型能够编码更多的结构化知识或者语言知识。至于如何做，有两种典型的思路：一种以百度ERNIE为代表；一种以清华ERNIE为代表。这两个工作是最早做这个事情的，差不多同时出来，但思路不同，正好是两种具备代表性的方案。</p><p><img src="https://i.loli.net/2020/10/30/OjoakV1tr5MJYEN.jpg" alt="img"></p><p>百度ERNIE的思路是：在预训练阶段被Mask掉的对象上做文章，我们可以使用比如命名实体识别工具／短语识别工具，将输入中的命名实体或者部分短语Mask掉（参考上图），这些被Mask掉的片断，代表了某种类型的语言学知识，通过这种方式，强迫预训练模型去强化地学习相关知识。</p><p><img src="https://i.loli.net/2020/10/30/mwGZYnuSbeJj6xV.jpg" alt="img"></p><p>清华ERNIE则是另外一种思路：我们已经有些结构化知识或者实体关系知识等现成的外部知识库，可以在预训练的过程中，通过工具找出句中的命名实体，句中的命名实体可以触发知识库中其它相关实体，然后预训练模型通过特殊的结构，来融合文本和结构化知识，以进一步促进语言的理解（参考上图）。这是另外一种思路。</p><p>关于知识的融入，后续还有很多工作，但是大体走的是上面两条路线之一。关于将显示知识或者结构化知识引入预训练模型，我是这么看的，纯属个人意见：</p><p>我觉得，假设说我们用来预训练的数据量特别特别大，而且特征抽取器的能力特别强。理论上，结构化知识是蕴含在这些文本内的，因为我们的外部知识库也是通过技术手段从自由文本里挖掘出来的。假设上面两个条件同时能够被满足，理论上，不太需要单独再把结构化知识独立补充给Bert这类预训练模型，预训练模型应该能够直接从自由文本中就学会这些知识。但是，以我们目前的技术条件，上面两个条件完全被满足，还是有一定难度的。于是，在这种约束下，感觉独立强化知识，让Bert在编码的时候更重视这些结构化知识，看上去是有一定补充作用的。我猜测，比较高频出现的知识，已经能够通过常规的语言模型预训练能够捕获了，很可能对于那些偏冷门的知识，引入结构化知识，会对预训练模型做下游任务有直接促进作用。而可以预见的是：随着机器资源能力越来越强大，如果在第一个预训练阶段，不断加大数据数量和质量，不断增加Transformer模型容量，那么，单独补充结构化知识给预训练模型，收益可能会越来越小。当然，以目前的技术发展阶段，感觉这个事情还有空间和潜力可挖掘。当然，上面说的是通用知识，如果手上的外部知识库，领域性很强，通用训练数据中包含的相关领域数据很少，那么，直接把知识引入，对于解决问题还是很有必要的。</p><ul><li><strong>多模态预训练</strong></li></ul><p>随着存储容量越来越大、网络传输速度越来越快、计算速度越来越强，除了传统的文字内容外，图片、视频、音频等各种多模态信息在互联网的内容占比中越来越多。如何融合多种模态信息进行内容理解，就变得越来越重要。那么，能否将多模态信息纳入预训练的框架之内呢？这是个非常有现实价值的问题。</p><p>前文有述，自由文本的预训练，本质上是让模型从海量自由文本中，通过语言模型等任务，来学习其中蕴含的的语言学知识。由此自然引发的问题就是：多模态预训练也是要将某种新型的知识塞到模型参数里，那么，这是一种什么样的知识呢？本质上，多模态预训练要学习的知识是两种模态之间，或者多种模态之间，的知识单元映射关系。比如对于文字-图片这两种多模态信息来说，我们可以把图片想像成一种特殊类型的语言，多模态预训练希望让模型学会这两种不同模态之间的语义映射关系，比如能够将单词“苹果”和图片中出现的苹果区域建立起联系。或者说，希望通过将不同模态的信息映射到相同的语义空间，来学会两者之间的语义映射关系。</p><p><img src="https://i.loli.net/2020/10/30/mrwA5gk478FcNHT.jpg" alt="img"></p><p>如果我们能够成功地学会这种不同媒介间的语义映射，那么就可以做很多有意思的事情，比如说句话，搜出与这句话语义相近的图片（参考上图）；或者反过来，输入一个图片，能够找到或者生成对应的文字描述。再比如VQA（参考上图），就是给定一张图片，你可以针对图片提出一些问题，AI系统能够回答你的问题，给出正确答案。这涉及到图片-文字的跨媒体问答以及一些跨媒体的知识推理。而要想实现这种能力，如何通过预训练模型，让模型学会两种模态之间的语义映射关系就是至关重要的。</p><p>我们面临的第一个问题是：从什么样的数据里来学习不同模态之间的语义映射关系呢？自由文本的预训练模型，可以采纳海量无标注数据来做，然而，多模态预训练要学习不同模态信息间的语义映射关系，所以需要有标注好的“模态1-模态2”的对齐数据，比如：标注好的“文本-图片”或者“文本-视频”平行数据。只有具备跨模态对齐数据，模型才有可能从中学习不同媒介类型之间的语义映射关系。从这个角度讲，相对自由文本预训练来说，多模态预训练因为需要模态对齐训练数据，而这种数据往往是需要人工标注的，所以可获得的数据难度及成本就高了很多，明显不如文本预训练那么自由。</p><p>总体而言，目前的多模态预训练任务中，通常都是“双模态”预训练，常见的包括“文本-图片”、“文本-视频”、“视频-音频”等模态类型组合。其中， 相对而言，“文本-图片”类型的任务技术发展比较快，其它类型的多模态类型发展相对缓慢，我猜测这里的主要原因在于可用标注数据的差异。“文本-图片”目前有一些规模达到几十万到上百万规模的标注数据集合，典型的比如MS-COCO、Visual Gnome等，而其它类型的模态组合数据貌似缺乏大规模数据集合，这严重影响了领域技术进展。下面我们从“文本-图片”这种模态组合来宏观介绍下多模态预训练的常规做法，其它模态组合的技术方案差不太多，所缺的可能主要是标注好的模态对齐数据。</p><p>我们从模型结构和训练目标这两个角度来阐述。目前的大多数技术方案大同小异，主要差异在于采用了不同的模型结构及与不同训练目标的差异组合。</p><p>假设我们有“文本-图片”两种模态数据，需要联合学习三种预训练模型：文本模态自身的预训练模型，图片模态自身的预训练模型，以及两个模态之间的语义对齐预训练模型。从模型结构来说，目前主流的结构有两种：双流交互模型以及单流交互模型。</p><p><img src="https://i.loli.net/2020/10/30/tkIUsf5xFBVwuq4.jpg" alt="img"></p><p>典型双流交互模型结构如上图LXMERT模型所示。文本编码器代表一个流，一般采用Transformer模型捕捉文本单词之间的关系；图片编码器代表另外一个流，一般也是采用Transformer模型，对于图片来说，一般用Faster-RCNN模型识别出图片中包含的多个物体及其对应的矩形位置信息，将高置信度的物体及其对应的位置信息作为图片侧Transformer的输入，用来学习图片中物品的相互关系；在两个流之上，再加入额外的Transformer模型，用于融合两个模态的语义映射关系。在这种双流结构上，模型同时学习文本预训练目标、图片预训练目标，以及图片-文本对齐预训练目标。一般文本预训练目标和标准的Bert做法类似，通过随机Mask一部分文本单词的语言模型来做；图片预训练目标类似，可以Mask掉图片中包含的部分物品，要求模型正确预测物品类别或者预测物品Embedding编码；为了能够让两个模态语义对齐，一般还要学习一个跨模态目标，常规做法是将对齐语料中的“文本-图片”作为正例，随机选择部分图片或者文本作为负例，来要求模型正确做二分类问题，通过这种方式逼迫模型学习两种模态间的对齐关系。典型的双流模型包括LXMERT、ViLBERT等。</p><p><img src="https://i.loli.net/2020/10/30/hyXxRlIvs63Tabm.jpg" alt="img"></p><p>典型的单流交互模型结构如上图Unicoder-VL模型所示。单流和双流的区别在于：单流模型只用一个Transformer，而双流模型，如上所述，需要三个Transformer各自分工协作。输入的图片，经过上述的Faster-RCNN物体识别和位置编码后，和文本单词拼接，整体作为Transformer模型的输入。也就是说，单流模型靠单个Transformer，同时学习文本内部单词交互、图片中包含物体之间大的交互，以及文本-图片之间的细粒度语义单元之间的交互信息。单流模型的预训练目标，与双流交互模型是类似的，往往也需要联合学习文本预训练、图片预训练以及对齐预训练三个目标。典型的单流模型包括Unicoder-VL、VisualBERT、VL-VERT、UNITER等。</p><p><img src="https://i.loli.net/2020/10/30/5aqhOLVsnGuNIjf.jpg" alt="img"></p><p>经过多模态预训练之后，是否模型能够建立起不同模态信息之间的语义映射关系呢？答案可以参考上图：经过预训练后，输入一句话以及对应的图片进入模型，对于文本中的某个单词，我们可以观察这个单词与图片中哪块区域联系密切（根据Attention强度信息可以看出）。从上图示例可以看出，预训练模型确实学会了不同模态单词语义之间的映射关系。</p><p>多模态模型经过预训练之后，针对具体的应用任务，可以采取第二阶段Fine-tuning的模式增强应用效果。从上述描述可见，单流模型结构相对简单，模型参数也相对少些，而且能够在模型底层及早对不同模态之间的语义直接建立联系，所以看起来比双流模式更有发展前景，但是从目前的各种研究对比实验结果看，貌似两种方法的效果在伯仲之间。不过，可以得出的结论是，采用预训练模型的多模态方法，比不用预训练的传统方法，在应用效果上是有明显提升的。</p><p>目前来看，如果希望多模态预训练有更快速的技术发展，以下几个方面是需要重点关注的：</p><p>首先，也是最重要的，可能是急需构建不同模态间的大规模对齐数据。目前，“图片-文本”类型的对齐数据规模尚可，但是继续扩大数据规模无疑是有益的；对其它类型的模态组合而言，大规模的标准对齐数据比较缺乏，这会严重制约多模态预训练的发展。所以明显需要数据先行，这是发展技术的前提条件；</p><p>其次，感觉在自由文本预训练研究领域中，目前得到的一些得到验证的经验，推理起来，应该是能够直接迁移到多模态预训练领域的。典型的经验，比如：在扩大数据规模的同时，增加模型复杂度。增加模型复杂度包括图片特征抽取器模型复杂度（已经有实验验证加深ResNet模型对效果提升明显），以及增加对应的Transformer层深，放大Transformer的Hidden Size等，相信这是能够大幅提升多模态预训练的首选手段；再比如文本预训练任务中的Mask对象，采用Span方式而非单词方式（已有工作这么做了），加大Batch Size延长训练时间等训练方法优化手段，想来都应该是有益的；从训练目标来说，目前的模态间对齐任务还是有点类似NSP这种句子分类任务，明显偏简单了一些，这块可以考虑引入更有难度的对齐任务，以及实体级别细粒度的对齐任务，来增强模态对齐模型的效果。</p><p>再次，可以考虑由目前的两模态向真正的多模态扩展，比如三模态动态联合训练，目前常见的是“文本-图片”，或者“文本-视频”，通常是两模态结构，后面可以考虑“文本-图片-音频”，或者“文本-视频-音频”等三模态甚至更多模态的联合预训练。当然，这么做的前提，仍然是得先有多模态的对齐数据。</p><h2 id="多多益善从两阶段模型到四阶段模型"><strong>多多益善：从两阶段模型到四阶段模型</strong></h2><p>经典的预训练模型框架下，一般我们解决NLP问题有两个阶段：第一阶段是模型预训练阶段，预训练模型从文本等信息中学习语言知识；第二阶段是Fine-tuning阶段，根据手上的有监督数据，对模型参数进行微调，以获得更好的任务效果。</p><p>前文有述，预训练阶段的最明显发展趋势是大数据+大模型，在数据质量有保障的前提下，数据量越大，模型容量越大，预训练阶段学到的语言知识效果越好。其实，关于预训练数据，目前还有很多研究，能够得出另外一个结论：从领域、题材、类型等不同角度看，如果预训练数据和手上任务数据越接近，则预训练模型带来的收益就越大。</p><p>很多时候，我们手头上的任务数据有很强的领域性，比如可能是计算机领域的，因为预训练数据一般具备通用性，即使大量预训练文本里包含部分计算机类的文本，整体占比也很小。于是，这种情况下，由于领域差异比较大，预训练模型带给手头任务的收益，就没期望中那么大。一种直观的，也是不少人在用的解决方案是：把领域性文本，也加入到预训练数据中，一同参与预训练过程，这样能够增加预训练文本和手上任务的相似性，就能提升任务效果。事实上，这样做也确实能解决这个问题。但是，有一个问题：预训练阶段往往会兼顾模型的通用性，尽可能兼顾各种下游任务，希望模型能在不同领域都有效。而且，从趋势看，数据规模和模型规模会越来越大，也就是训练成本会越来越高。所以，这种把领域数据添加到预训练数据一起训练的做法，一则影响模型通用性，二则实现成本高，看上去就不是特别好的方法。</p><p>目前看，要解决这个问题，比较好的方法是把两个阶段分离：第一阶段仍然采取大数据、大模型，走通用普适、各种任务都能受益的路子，不特意考虑领域特点，因为兼顾不过来；第二阶段，在第一阶段训练好的通用预训练模型基础上，利用领域数据，再做一次预训练，等于把通用的预训练模型往领域方向拉动一下。这样两个阶段各司其职，有独立的优化目标，也能兼顾通用性和领域适配性。</p><p><img src="https://i.loli.net/2020/10/30/ELKz135biWBOFyr.jpg" alt="img"></p><p>上面这个方法，我猜应该不少人都已经在这么做了，论文“Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks”也通过大量实验验证了领域数据预训练（DAPT）的有效性，再结合它得出的另外一个重要结论：用手上的任务数据，无论大小，如果做一次任务级数据预训练（TAPT），也就是拿着手上任务数据，在通用预训练模型基础上，再做一次预训练，也能够有效提升任务效果。综合这个文章和其它有关文章的结论，我们不难看出，要想更好地提升任务效果，我们应该从传统的两阶段模型，拓展到如下四阶段模型（参考上图）：</p><p>第一个阶段：通用预训练</p><p>这就是传统两阶段模式中的第一阶段。这个阶段不仅仅追求效果好，也追求领域通用性。它的优化目标是：在尽可能多的下游任务场景中，效果都尽可能好，但不单独考虑某个特殊领域的效果如何。这个阶段，目前看总的发展趋势是：在数据质量有保证的前提下，增加数据数量，以及数据的多样性，同时提升模型复杂度，这样可以提供普遍有效的模型增强能力。很明显，这个阶段，一般只有土豪公司才能做得起，而且从趋势看，会越来越如此。将来的发展模式可能是，超级土豪公司不断优化这个模型，然后放出来供大家用，有能力做这个事情的人，应该会越来越少。</p><p>第二个阶段：领域预训练</p><p>在第一阶段训练好的通用预训练模型基础上，利用不同领域的自由文本，构建多个、不同领域的领域预训练模型。比如我们可以分别收集计算机领域、生物领域、电商领域…等等，多个不同领域的无标注自由文本数据。在第一阶段通用模型基础上，分别用各个领域数据，再分别做一次预训练，这样我们就得到了适合解决各个不同领域的预训练模型：计算机领域、生物领域、电商领域…..等等多个不同的预训练模型。下游任务可以根据自己任务的领域，选择适配性好的领域预训练模型来使用。</p><p>这个阶段的预训练模型，在训练的时候，有个独特的问题需要解决：灾难遗忘问题。所谓“灾难遗忘”，就是说，当你用领域数据进行预训练的时候，因为会调整第一阶段预训练模型的参数，这种偏向领域性的参数调整，可能会导致第一阶段模型学好的参数被改写，这意味着：经过第二阶段预训练，第一阶段预训练模型里学会的很多通用语言知识，可能会被冲掉。灾难遗忘就是这个意思。灾难遗忘问题，对于预训练模型，尤其是领域预训练模型来说，是个很关键也很重要的问题，目前也有一些解决方案，限于篇幅，这里就不展开了。</p><p>这个阶段的预训练，因为数据量相比第一阶段会小很多，所以其实中农公司甚至贫农公司也能做得起，不存在土豪门槛，大家应该都能做。当然，一般我们只做跟自己手头任务相关的领域的预训练模型。如果你想做很多领域的预训练模型，那估计也要备足银行卡。估计后续也会有土豪公司做好很多不同领域的预训练模型，供大家个性化适配使用，虽说目前还没有，但是推断起来，这是个大概率会发生的事件。</p><p>第三个阶段：任务预训练</p><p>在前两个预训练模型基础上，比如从第二个阶段里面的多个不同的领域预训练模型中，选择和手头任务适配的那个领域预训练模型，在这个模型基础上，用手头数据，抛掉数据标签，再做一次预训练，无论手上任务数据有多少。比如手上任务是计算机领域的，那么从第二阶段的多个领域模型里面，选择计算机领域适配过的预训练模型，在这个模型基础上进行一次任务级别的预训练。这样应该能明显提升任务效果。</p><p>第四阶段：任务Fine-tuning</p><p>这是传统两阶段的第二阶段，做法一样，没什么好讲的。</p><p>当然，如果你手上的任务没有那么强的领域性，可以跳过第二阶段，也就是那个领域预训练模型阶段，走剩余的三阶段模式即可，无论如何，任务预训练都是值得做的一个事情。</p><h2 id="聚沙成塔如何建造强大的预训练模型"><strong>聚沙成塔：如何建造强大的预训练模型</strong></h2><p>上文从不同角度或维度，总结了预训练模型某个方面的一些结论，我们综合起来看一下。不论出于什么目的，打榜也好，把手头应用做得更出色也好，如果我们综合各个维度的现有信息，那么，在当前技术水准下，如何构造强大的预训练模型，貌似是可以得出相对明晰结论的。因为NLP里面既有语言理解类任务，也有语言生成类任务，两者差异较大，所以我们分头来看。</p><p><img src="https://i.loli.net/2020/10/30/VdANbfwutJ3MyWn.jpg" alt="img"></p><p>对于语言理解类任务，我假设你的任务不是领域性特别强那种类型的，建议采取如下技术方案：</p><p>使用三阶段模型：通用预训练+任务预训练+任务Fine-tuning。在做完第一阶段预训练后，用手头任务数据，抛掉标签，再做一次任务预训练，然后任务Fine-tuning。</p><p>模型结构建议采取Encoder+Decoder结构，或者Encoder-AE结构；预训练任务配置两个：独立生成Span类语言模型及SOP句子任务；在质量优先的前提下，增加预训练数据的数量；比较关键的一点是，一定要增加模型容量：可以纵向增加Transformer Block层深，或者横向调大Transformer相应位置可配置参数大小。当然，如果你不差钱，两个可以一起上。另外，要使得模型得到充分训练，就是说增大训练过程中的Batch Size和训练步长。</p><p>对于语言生成类任务，建议采取如下技术方案：</p><p>使用两阶段模型：通用预训练+任务Fine-tuning。模型结构建议采取Encoder+Decoder结构，或者Decoder-AR结构；预训练任务采用独立生成Span类语言模型；在质量优先的前提下，增加预训练数据的数量；同样，比较关键的一点是，一定要增加模型容量：可以纵向增加Transformer Block层深，或者横向调大Transformer相应位置可配置参数大小。当然，如果你不差钱，两个可以一起上。另外，也要使得模型得到充分训练，就是说增大训练过程中的Batch Size和训练步长。</p><p>相信采取上述技术方案，你能在打榜过程中获得很好的名次，或者在实际工作中能比较快地完成自己的KPI或OKR。当然，如果是走落地应用的路子，关于知识蒸馏等一系列如何将模型做小这方面，记得要多花点功夫。</p><h3 id="参考">参考</h3><blockquote><p><a href="https://zhuanlan.zhihu.com/p/254821426" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/254821426</a></p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      对于BERT以及之后的预训练模型的总结概括
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-10-29-深度学习调参技巧汇总</title>
    <link href="http://yoursite.com/2020/10/29/2020-10-29-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%B0%83%E5%8F%82%E6%8A%80%E5%B7%A7%E6%B1%87%E6%80%BB/"/>
    <id>http://yoursite.com/2020/10/29/2020-10-29-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%B0%83%E5%8F%82%E6%8A%80%E5%B7%A7%E6%B1%87%E6%80%BB/</id>
    <published>2020-10-29T11:11:29.000Z</published>
    <updated>2020-10-29T13:16:10.809Z</updated>
    
    <content type="html"><![CDATA[<script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script&gt;
        document.querySelectorAll(&#39;.github-emoji&#39;)
          .forEach(el =&gt; {
            if (!el.dataset.src) { return; }
        
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-10-28-transformer综述</title>
    <link href="http://yoursite.com/2020/10/28/2020-10-28-transformer%E7%BB%BC%E8%BF%B0/"/>
    <id>http://yoursite.com/2020/10/28/2020-10-28-transformer%E7%BB%BC%E8%BF%B0/</id>
    <published>2020-10-28T12:39:37.000Z</published>
    <updated>2020-10-31T08:29:23.515Z</updated>
    
    <content type="html"><![CDATA[<h3 id="transformer家族1----transformer详解和源码分析">🚀Transformer家族1 -- Transformer详解和源码分析</h3><h4 id="transformer总体结构">1 Transformer总体结构</h4><p>近几年NLP领域有了突飞猛进的发展，预训练模型功不可没。当前利用预训练模型（pretrain models）在下游任务中进行fine-tune，已经成为了大部分NLP任务的固定范式。Transformer摒弃了RNN的序列结构，完全采用attention和全连接，严格来说不属于预训练模型。但它却是当前几乎所有pretrain models的基本结构，为pretrain models打下了坚实的基础，并逐步发展出了transformer-XL，reformer等优化架构。本文结合论文和源码，对transformer基本结构，进行详细分析。</p><p>Transformer是谷歌在2017年6月提出，发表在NIPS2017上。论文地址 <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need</a>。 分析的代码为Harvardnlp的代码，基于PyTorch， 地址 <a href="https://github.com/harvardnlp/annotated-transformer" target="_blank" rel="noopener">annotated-transformer</a></p><p>Transformer主体框架是一个<strong>encoder-decoder</strong>结构，去掉了RNN序列结构，完全基于attention和全连接。在WMT2014英语翻译德语任务上，bleu值达到了28.4，达到当时的SOTA。其总体结构如下所示</p><p><img src="https://i.loli.net/2020/10/28/6z5wSWXlaQnAF2E.png" alt="在这里插入图片描述"></p><p>总体为一个典型的encoder-decoder结构。代码如下</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 整个模型入口</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_model</span><span class="params">(src_vocab, tgt_vocab, N=<span class="number">6</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">               d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, h=<span class="number">8</span>, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">    <span class="string">"Helper: Construct a model from hyperparameters."</span></span><br><span class="line">    c = copy.deepcopy</span><br><span class="line"></span><br><span class="line">    <span class="comment"># multiHead attention</span></span><br><span class="line">    attn = MultiHeadedAttention(h, d_model)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># feed-forward</span></span><br><span class="line">    ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># position-encoding</span></span><br><span class="line">    position = PositionalEncoding(d_model, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 整体为一个encoder-decoder</span></span><br><span class="line">    model = EncoderDecoder(</span><br><span class="line">        <span class="comment"># encoder编码层</span></span><br><span class="line">        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),</span><br><span class="line"></span><br><span class="line">        <span class="comment"># decoder解码层</span></span><br><span class="line">        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 编码层输入，输入语句进行token embedding和position embedding</span></span><br><span class="line">        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 解码层输入，同样需要做token embedding和position embedding</span></span><br><span class="line">        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),</span><br><span class="line"></span><br><span class="line">        <span class="comment"># linear + softmax，查找vocab中概率最大的字</span></span><br><span class="line">        Generator(d_model, tgt_vocab))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># This was important from their code. </span></span><br><span class="line">    <span class="comment"># Initialize parameters with Glorot / fan_avg.</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">            nn.init.xavier_uniform(p)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"><span class="number">1234567891011121314151617181920212223242526272829303132333435363738</span></span><br></pre></td></tr></tbody></table></figure><p>make_model为Transformer模型定义的入口，它先定义了multi-head attention、feed-forward、position-encoding等一系列子模块，然后定义了一个encoder-decoder结构并返回。下面来看encoder-decoder定义。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderDecoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    一个标准的encoder和decoder框架，可以自定义embedding、encoder、decoder等</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, encoder, decoder, src_embed, tgt_embed, generator)</span>:</span></span><br><span class="line">        super(EncoderDecoder, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># encoder和decoder通过构造函数传入，可灵活更改</span></span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line"></span><br><span class="line">        <span class="comment"># src和target的embedding，也是通过构造函数传入，方便灵活更改</span></span><br><span class="line">        self.src_embed = src_embed</span><br><span class="line">        self.tgt_embed = tgt_embed</span><br><span class="line"></span><br><span class="line">        <span class="comment"># linear + softmax</span></span><br><span class="line">        self.generator = generator</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, src, tgt, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="string">"Take in and process masked src and target sequences."</span></span><br><span class="line">        <span class="comment"># 先对输入进行encode，然后再通过decode输出</span></span><br><span class="line">        <span class="keyword">return</span> self.decode(self.encode(src, src_mask), src_mask,</span><br><span class="line">                            tgt, tgt_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, src, src_mask)</span>:</span></span><br><span class="line">        <span class="comment"># 先对输入进行embedding，然后再经过encoder</span></span><br><span class="line">        <span class="keyword">return</span> self.encoder(self.src_embed(src), src_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span><span class="params">(self, memory, src_mask, tgt, tgt_mask)</span>:</span></span><br><span class="line">        <span class="comment"># 先对目标进行embedding，然后经过decoder</span></span><br><span class="line">        <span class="keyword">return</span> self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)</span><br><span class="line"><span class="number">12345678910111213141516171819202122232425262728293031</span></span><br></pre></td></tr></tbody></table></figure><p>encoder-decoder定义了一个标准的编码解码框架，其中编码器、解码器均可以自定义，有很强的泛化能力。模块运行时会调用forward函数，它先对输入进行encode，然后再通过decode输出。我们就不详细展开了。</p><h4 id="encoder">2 encoder</h4><h5 id="encoder定义">2.1 encoder定义</h5><p>encoder分为两部分</p><ol type="1"><li><strong>输入层embedding</strong>。输入层对inputs文本做token embedding，并对每个字做position encoding，然后叠加在一起，作为最终的输入。</li><li><strong>编码层encoding</strong>。编码层是多层结构相同的layer堆叠而成。每个layer又包括两部分，multi-head self-attention和feed-forward全连接，并在每部分加入了残差连接和归一化。</li></ol><p>代码实现上也验证了这一点。我们看EncoderDecoder类中的encode函数，它先利用输入embedding层对原始输入进行embedding，然后再通过编码层进行encoding。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderDecoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, src, src_mask)</span>:</span></span><br><span class="line">        <span class="comment"># 先对输入进行embedding，然后再经过encoder</span></span><br><span class="line">        <span class="keyword">return</span> self.encoder(self.src_embed(src), src_mask)</span><br><span class="line"><span class="number">1234</span></span><br></pre></td></tr></tbody></table></figure><h5 id="输入层embedding">2.2 输入层embedding</h5><p>原始文本经过embedding层进行向量化，它包括token embedding和position embedding两层。</p><h6 id="token-embedding">2.2.1 token embedding</h6><p>token embedding对文本进行向量化，一般来说有两种方式</p><ol type="1"><li>采用<strong>固定词向量</strong>，比如利用Word2vec预先训练好的。这种方式是LSTM时代常用的方式，比较简单省事，无需训练。但由于词向量是固定的，不能解决一词多义的问题，词语本身也不是contextual的，没有结合上下文语境信息，另外对于不在词向量中的词语，比如特定领域词语或者新词，容易出现OOV问题。</li><li>随机初始化，然后<strong>训练</strong>。这种方式比较麻烦，需要大规模训练语料，但能解决固定词向量的一系列问题。Transformer采用了这种方式。</li></ol><p>另外，基于Transformer的BERT模型在中文处理时，直接基于字做embedding，优点有</p><ol type="1"><li>无需分词，故不会引入分词误差。事实上，只要训练语料充分，模型自然就可以学到分词信息了。</li><li>中文字个数固定，不会导致OOV问题</li><li>中文字相对词，数量少很多，embedding层参数大大缩小，减小了模型体积，并加快了训练速度。</li></ol><p>事实上，就算在LSTM时代，很多case中，我们也碰到过基于字的embedding的效果比基于词的要好一些。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Embeddings</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="comment"># token embedding，随机初始化训练，然后查表找到每个字的embedding</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, vocab)</span>:</span></span><br><span class="line">        super(Embeddings, self).__init__()</span><br><span class="line">        <span class="comment"># 构建一个随机初始化的词向量表，[vocab_size, d_model]。 bert中的设置为[21128, 768]</span></span><br><span class="line">        self.lut = nn.Embedding(vocab, d_model)</span><br><span class="line">        self.d_model = d_model</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># 从词向量表中查找字对应的embedding向量</span></span><br><span class="line">        <span class="keyword">return</span> self.lut(x) * math.sqrt(self.d_model)</span><br><span class="line"></span><br><span class="line"><span class="number">123456789101112</span></span><br></pre></td></tr></tbody></table></figure><p>由代码可见，Transformer采用的是随机初始化，然后训练的方式。词向量维度为[vocab_size, d_model]。例如BERT中为[21128, 768]，参数量还是很大的。ALBert针对embedding层进行矩阵分解，大大减小了embedding层体积。</p><h6 id="position-encoding">2.2.2 position encoding</h6><p>首先一个问题，为啥要进行位置编码呢。原因在于self-attention，将任意两个字之间距离缩小为1，丢失了字的位置信息，故我们需要加上这一信息。我们也可以想到两种方法</p><ol type="1"><li><strong>固定编码</strong>。Transformer采用了这一方式，通过奇数列cos函数，偶数列sin函数方式，利用三角函数对位置进行固定编码。</li><li><strong>动态训练</strong>。BERT采用了这种方式。先随机初始化一个embedding table，然后训练得到table 参数值。predict时通过embedding_lookup找到每个位置的embedding。这种方式和token embedding类似。</li></ol><p>哪一种方法好呢？个人以为各有利弊</p><ol type="1"><li>固定编码方式简洁，不需要训练。且不受embedding table维度影响，理论上可以支持任意长度文本。（但要尽量避免预测文本很长，但训练集文本较短的case）</li><li>动态训练方式，在语料比较大时，准确度比较好。但需要训练，且最致命的是，限制了输入文本长度。当文本长度大于position embedding table维度时，超出的position无法查表得到embedding（可以理解为OOV了）。这也是为什么BERT模型文本长度最大512的原因。</li></ol><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="comment"># 位置编码。transformer利用编码方式实现，无需训练。bert则采用训练embedding_lookup方式</span></span><br><span class="line">    <span class="comment"># 编码方式文本语句长度不受限，但准确度不高</span></span><br><span class="line">    <span class="comment"># 训练方式文本长度会受position维度限制（这也是为什么bert只能处理最大512个字原因），但训练数据多时，准确率高</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, dropout, max_len=<span class="number">5000</span>)</span>:</span></span><br><span class="line">        super(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 采用sin和cos进行position encoding</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) *</span><br><span class="line">                             -(math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)        <span class="comment"># 偶数列</span></span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)        <span class="comment"># 奇数列</span></span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        self.register_buffer(<span class="string">'pe'</span>, pe)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># token embedding和position encoding加在一起</span></span><br><span class="line">        x = x + Variable(self.pe[:, :x.size(<span class="number">1</span>)], </span><br><span class="line">                         requires_grad=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br><span class="line"></span><br><span class="line"><span class="number">123456789101112131415161718192021222324</span></span><br></pre></td></tr></tbody></table></figure><p>由代码可见，position encoding直接采用了三角函数。对偶数列采用sin，奇数列采用cos。 <img src="https://i.loli.net/2020/10/28/zPU8Z9scbDNkS5A.png" alt="在这里插入图片描述"></p><h5 id="编码层">2.3 编码层</h5><p>Encoder层是Transformer的核心，它由<strong>N层相同结构的layer</strong>（默认6层）堆叠而成。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Core encoder is a stack of N layers"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layer, N)</span>:</span></span><br><span class="line">        super(Encoder, self).__init__()</span><br><span class="line">        <span class="comment"># N层堆叠而成，每一层结构都是相同的，训练参数不同</span></span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># layer normalization</span></span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        <span class="number">45</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask)</span>:</span></span><br><span class="line">        <span class="comment"># 1 经过N层堆叠的multi-head attention + feed-forward</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2 对encoder最终输出结果进行layer-norm归一化。层间和层内子模块都做过 add + dropout + layer-norm</span></span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br><span class="line"><span class="number">1234567891011121314151617</span></span><br></pre></td></tr></tbody></table></figure><p>encoder的定义很简洁。先经过N层相同结构的layer，然后再进行归一化输出。重点我们来看layer的定义。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Encoder is made up of self-attn and feed forward (defined below)"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, self_attn, feed_forward, dropout)</span>:</span></span><br><span class="line">        super(EncoderLayer, self).__init__()</span><br><span class="line">        <span class="comment"># 1 self_attention</span></span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2 feed_forward</span></span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3 残差连接。encoder和decoder，每层结构，每个子结构，都有残差连接。</span></span><br><span class="line">        <span class="comment"># add + drop-out + layer-norm</span></span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">2</span>)</span><br><span class="line">        self.size = size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask)</span>:</span></span><br><span class="line">        <span class="comment"># 经过self_attention, 然后和输入进行add + layer-norm</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, mask))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 经过feed_forward， 此模块也有add + layer-norm</span></span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">1</span>](x, self.feed_forward)</span><br><span class="line"></span><br><span class="line"><span class="number">12345678910111213141516171819202122</span></span><br></pre></td></tr></tbody></table></figure><p>encoder layer分为两个子模块</p><ol type="1"><li><strong>self attention</strong>, 并对输入attention前的和经过attention输出的，做残差连接。残差连接先经过layer-norm归一化，然后进行dropout，最后再做add。后面我们详细分析</li><li><strong>feed-forward</strong>全连接，也有残差连接的存在，方式和self attention相同。</li></ol><h6 id="multiheadedattention">2.3.1 MultiHeadedAttention</h6><p>MultiHeaded Attention采用多头self-attention。它先将隐向量切分为h个头，然后每个头内部进行self-attention计算，最后再concat再一起。</p><p><img src="https://i.loli.net/2020/10/28/K4lYeqtN7jUR5fx.png" alt="在这里插入图片描述"> 代码如下</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadedAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, h, d_model, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        super(MultiHeadedAttention, self).__init__()</span><br><span class="line">        <span class="keyword">assert</span> d_model % h == <span class="number">0</span></span><br><span class="line">        <span class="comment"># d_model为隐层维度，也是embedding的维度，h为多头个数。</span></span><br><span class="line">        <span class="comment"># d_k为每个头的隐层维度，要除以多头个数。也就是加入了多头，总隐层维度不变。</span></span><br><span class="line">        self.d_k = d_model // h</span><br><span class="line">        self.h = h</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 线性连接</span></span><br><span class="line">        self.linears = clones(nn.Linear(d_model, d_model), <span class="number">4</span>)</span><br><span class="line">        self.attn = <span class="literal">None</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, mask=None)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 输入mask，在decoder的时候有用到。decode时不能看到要生成字之后的字，所以需要mask</span></span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        nbatches = query.size(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 1) q, k, v形状变化，加入多头， [batch, L, d_model] =&gt; [batch, h, L, d_model/h]</span></span><br><span class="line">        query, key, value = [l(x).view(nbatches, <span class="number">-1</span>, self.h, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">             <span class="keyword">for</span> l, x <span class="keyword">in</span> zip(self.linears, (query, key, value))]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2) attention计算</span></span><br><span class="line">        x, self.attn = attention(query, key, value, mask=mask, </span><br><span class="line">                                 dropout=self.dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3) 多头结果concat在一起，还原为初始形状</span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(nbatches, <span class="number">-1</span>, self.h * self.d_k)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 4）最后经过一个线性层</span></span><br><span class="line">        <span class="keyword">return</span> self.linears[<span class="number">-1</span>](x)</span><br><span class="line"><span class="number">123456789101112131415161718192021222324252627282930313233</span></span><br></pre></td></tr></tbody></table></figure><p>下面重点来看单个头的self-attention。也就是论文中的“Scaled Dot-Product Attention”。attention本质上是一个向量的加权求和。它探讨的是每个位置对当前位置的贡献。步骤如下</p><ol type="1"><li>q向量和每个位置的k向量计算点积，然后除以向量长度的根号。计算点积可以认为是进行权重计算。除以向量长度原因是向量越长，q*k值理论上会越大，故需要在向量长度上做归一化。</li><li><strong>attention-mask</strong>。mask和输入矩阵shape相同，mask矩阵中值为0位置对应的输入矩阵的值更改为-1e9，一个非常非常小的数，经过softmax后趋近于0。decoder中使用了mask，后面我们详细分析。</li><li>softmax归一化，使得q向量和每个位置的k向量的score分布到（0, 1）之间</li><li>加权系数乘以每个位置v向量，然后加起来。</li></ol><p>公式如下：<img src="https://i.loli.net/2020/10/28/C8lDMeBadc4yftg.png" alt="在这里插入图片描述"> 代码如下</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention</span><span class="params">(query, key, value, mask=None, dropout=None)</span>:</span></span><br><span class="line">    <span class="comment"># attention计算，self_attention和soft-attention都是使用这个函数</span></span><br><span class="line">    <span class="comment"># self-attention, q k v 均来自同一文本。要么是encoder，要么是decoder</span></span><br><span class="line">    <span class="comment"># soft-attention, q来自decoder，k和v来自encoder，从而按照decoder和encoder相关性，将encoder信息融合进来</span></span><br><span class="line">    d_k = query.size(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 利用q * k计算两向量间相关度，相关度高则权重大。</span></span><br><span class="line">    <span class="comment"># 除以根号dk的原因是，对向量长度进行归一化。q和k的向量长度越长，q*k的值越大</span></span><br><span class="line">    scores = torch.matmul(query, key.transpose(<span class="number">-2</span>, <span class="number">-1</span>)) / math.sqrt(d_k)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># attention-mask，将 mask中为1的 元素所在的索引，在a中相同的的索引处替换为 value</span></span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        scores = scores.masked_fill(mask == <span class="number">0</span>, <span class="number">-1e9</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># softmax归一化</span></span><br><span class="line">    p_attn = F.softmax(scores, dim = <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># dropout</span></span><br><span class="line">    <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        p_attn = dropout(p_attn)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 最后利用归一化后的加权系数，乘以每一个v向量，再加和在一起，作为attention后的向量。每个字对应一个向量</span></span><br><span class="line">    <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br><span class="line"><span class="number">1234567891011121314151617181920212223</span></span><br></pre></td></tr></tbody></table></figure><p>self-attention和soft-attention共用了这个函数，他们之间的唯一区别是<strong>q k v向量的来源不同</strong>。self-attention中q k v 均来自同一文本。而decoder的soft-attention，q来自于decoder，k和v来自于encoder。它体现的是encoder对decoder的加权贡献。</p><h6 id="positionwisefeedforward">2.3.2 PositionwiseFeedForward</h6><p>feed-forward本质是一个两层的全连接，全连接之间加入了relu非线性和dropout。比较简单，代码如下</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionwiseFeedForward</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="comment"># 全连接层</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, d_ff, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        super(PositionwiseFeedForward, self).__init__()</span><br><span class="line">        <span class="comment"># 第一层全连接  [d_model, d_ff]</span></span><br><span class="line">        self.w_1 = nn.Linear(d_model, d_ff)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第二层全连接 [d_ff, d_model]</span></span><br><span class="line">        self.w_2 = nn.Linear(d_ff, d_model)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># dropout</span></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># 全连接1 -&gt; relu -&gt; dropout -&gt; 全连接2</span></span><br><span class="line">        <span class="keyword">return</span> self.w_2(self.dropout(F.relu(self.w_1(x))))</span><br><span class="line"><span class="number">12345678910111213141516</span></span><br></pre></td></tr></tbody></table></figure><p>总体过程是：<strong>全连接1 -&gt; relu -&gt; dropout -&gt; 全连接2</strong>。两层全连接内部没有shortcut，这儿不要搞混了。</p><h6 id="sublayerconnection">2.3.3 SublayerConnection</h6><p>在每层的self-attention和feed-forward模块中，均应用了残差连接。残差连接先对输入进行layerNorm归一化，然后送入attention或feed-forward模块，然后经过dropout，最后再和原始输入相加。这样做的好处是，让每一层attention和feed-forward模块的输入值，均是经过归一化的，保持在一个量级上，从而可以加快收敛速度。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SublayerConnection</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A residual connection followed by a layer norm.</span></span><br><span class="line"><span class="string">    Note for code simplicity the norm is first as opposed to last.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, dropout)</span>:</span></span><br><span class="line">        super(SublayerConnection, self).__init__()</span><br><span class="line">        <span class="comment"># layer-norm 归一化</span></span><br><span class="line">        self.norm = LayerNorm(size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># dropout</span></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, sublayer)</span>:</span></span><br><span class="line">        <span class="comment"># 先对输入进行layer-norm, 然后经过attention等相关模块，再经过dropout，最后再和输入相加</span></span><br><span class="line">        <span class="keyword">return</span> x + self.dropout(sublayer(self.norm(x)))</span><br><span class="line"><span class="number">12345678910111213141516</span></span><br></pre></td></tr></tbody></table></figure><p>从forward函数可见，先对输入进行layer-norm, 然后经过attention等相关模块，再经过dropout，最后再和输入相加。残差连接的作用就不说了，参考ResNet。</p><h4 id="decoder">3 decoder</h4><p>decoder结构和encoder大体相同，也是堆叠了N层相同结构的layer（默认6层）。不同的是，decoder的每个子层包括三层。</p><ol type="1"><li><strong>masked multi-head self-attention</strong>。这一部分和encoder基本相同，区别在于decoder为了保证模型不能看见要预测字的后面位置的字，加入了mask，从而避免未来信息的穿越问题。mask为一个上三角矩阵，上三角全为1，下三角和对角线全为0</li><li><strong>multi-head soft-attention</strong>。soft-attention和self-attention结构基本相同，甚至实现函数都是同一个。唯一的区别在于，self-attention的q k v矩阵来自同一个，所以叫self-attention。而soft-attention的q来自decoder，k和v来自encoder。表征的是encoder的整体输出对于decoder的贡献。</li><li><strong>feed-forward</strong>。这一块基本相同。</li></ol><p>另外三个模块均使用了残差连接，步骤仍然为 layerNorm -&gt; attention等模块 -&gt; dropout -&gt; 和输入进行add</p><p>decoder每个layer代码如下</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Decoder is made of self-attn, src-attn, and feed forward (defined below)"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, self_attn, src_attn, feed_forward, dropout)</span>:</span></span><br><span class="line">        super(DecoderLayer, self).__init__()</span><br><span class="line">        self.size = size</span><br><span class="line"></span><br><span class="line">        <span class="comment"># self-attention 自注意力</span></span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line"></span><br><span class="line">        <span class="comment"># soft-attenton, encoder的输出对decoder的作用</span></span><br><span class="line">        self.src_attn = src_attn</span><br><span class="line"></span><br><span class="line">        <span class="comment"># feed-forward 全连接</span></span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 残差连接</span></span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, memory, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="comment"># memory为encoder最终输出</span></span><br><span class="line">        m = memory</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1 对decoder输入做self-attention, 再和输入做残差连接</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, tgt_mask))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2 对encoder输出和decoder当前进行soft-attention，此处也有残差连接</span></span><br><span class="line">        x = self.sublayer[<span class="number">1</span>](x, <span class="keyword">lambda</span> x: self.src_attn(x, m, m, src_mask))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3 feed-forward全连接，也有残差连接</span></span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">2</span>](x, self.feed_forward)</span><br><span class="line"><span class="number">123456789101112131415161718192021222324252627282930</span></span><br></pre></td></tr></tbody></table></figure><h4 id="输出层">4 输出层</h4><p>decoder的输出作为最终输出层的输入，经过两步</p><ol type="1"><li>linear线性连接，也即是w * x + b</li><li>softmax归一化，向量长度等于vocabulary的长度，得到vocabulary中每个字的概率。利用beam-search等方法，即可得到生成结果。</li></ol><p>这一层比较简单，代码如下</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Define standard linear + softmax generation step."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, vocab)</span>:</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        self.proj = nn.Linear(d_model, vocab)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># 先经过linear线性层，然后经过softmax得到归一化概率分布</span></span><br><span class="line">        <span class="comment"># 输出向量长度等于vocabulary的维度</span></span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(self.proj(x), dim=<span class="number">-1</span>)</span><br><span class="line"><span class="number">12345678910</span></span><br></pre></td></tr></tbody></table></figure><h4 id="总结">5 总结</h4><p>Transformer相比LSTM的优点</p><ol type="1"><li><strong>完全的并行计算</strong>，Transformer的attention和feed-forward，均可以并行计算。而LSTM则依赖上一时刻，必须串行</li><li><strong>减少长程依赖</strong>，利用self-attention将每个字之间距离缩短为1，大大缓解了长距离依赖问题</li><li><strong>提高网络深度</strong>。由于大大缓解了长程依赖梯度衰减问题，Transformer网络可以很深，基于Transformer的BERT甚至可以做到24层。而LSTM一般只有2层或者4层。网络越深，高阶特征捕获能力越好，模型performance也可以越高。</li><li><strong>真正的双向网络</strong>。Transformer可以同时融合前后位置的信息，而双向LSTM只是简单的将两个方向的结果相加，严格来说仍然是单向的。</li><li><strong>可解释性强</strong>。完全基于attention的Transformer，可以表达字与字之间的相关关系，可解释性更强。</li></ol><p>Transformer也不是一定就比LSTM好，它的缺点如下</p><ol type="1"><li>文本长度很长时，比如篇章级别，<strong>计算量爆炸</strong>。self-attention的计算量为O(n^2), n为文本长度。Transformer-xl利用层级方式，将计算速度提升了1800倍</li><li>Transformer位置信息只靠<strong>position encoding</strong>，效果比较一般。当语句较短时，比如小于10个字，Transformer效果不一定比LSTM好</li><li>Transformer参数量较大，在大规模数据集上，效果远好于LSTM。但在<strong>小规模数据集</strong>上，如果不是利用pretrain models，效果不一定有LSTM好。</li></ol><h3 id="transformer家族2----编码长度优化transformer-xllongformer">🚀Transformer家族2 -- 编码长度优化（Transformer-XL、Longformer）</h3><h4 id="背景">1 背景</h4><p>NLP中经常出现长程依赖问题，比如一个词语可能和它距离上千位置的另一个词语有关系。长程关系的建立十分困难。常见序列结构模型都有一些难点，如下。</p><ol type="1"><li>在RNN中，由于反向传播梯度衰减和梯度爆炸问题，使得模型只能捕获较短距离。</li><li>LSTM利用门限机制，将连乘转变了为连加，提升了模型长程捕获能力，但梯度弥散问题没有从根本上得到解决，故其最大程度只能在400左右。</li><li>Transformer利用self-attention机制进行建模，使得任何两个位置token距离都为1。如果没有内存和算力的限制，Transformer理论上可以编码无限长的文本。但由于attention计算量十分大，而且计算复杂度和序列长度为O(n^2)关系，导致序列长度增加，内存和计算量消耗飞快增加。实际中由于内存和算力有限，一般只能编码一定长度，例如512。</li></ol><p>为了提升模型的长程编码能力，从而提升模型在长文本，特别是document-level语料上的效果，我们必须对Transformer编码长度进行优化。本文带来了Transformer-XL、Longformer，详细分析他们如何实现编码长度优化。</p><p>LongFormer通过降低attention计算所需内存和算力，来实现长文本编码。我们也可以把它归入到算力优化中。但鉴于其名字就重点体现了它的长距离能力，故还是放在了编码长度优化中，和Transformer-XL一起来分析</p><h4 id="transformer-xl">2 Transformer-XL</h4><p><img src="https://i.loli.net/2020/10/28/ctFMhR8IEdPVSZ1.png" alt="在这里插入图片描述"> 论文信息：2019年01月，谷歌 &amp; CMU，ACL 2019 论文地址 https://arxiv.org/abs/1901.02860 代码和模型地址 https://github.com/kimiyoung/transformer-xl</p><h5 id="为什么需要transformer-xl">2.1 为什么需要Transformer-XL</h5><p>为了解决长文本编码问题，原版Transformer采用了固定编码长度的方案，例如512个token。将长文本按照固定长度，切分为多个segment。每个segment内部单独编码，segment之间不产生交互信息。这种方式的问题如下</p><ol type="1"><li>模型无法建模超过固定编码长度的文本</li><li>segment之间没有交互信息，导致了文本碎片化。长语句的编码效果有待提升。</li><li>predict阶段，decoder每生成一个字，就往后挪一个，没有重复利用之前信息，导致计算量爆炸</li></ol><p>train和evaluate过程如下<img src="https://i.loli.net/2020/10/28/8YW7R4Q5J6zbmD1.png" alt="在这里插入图片描述"></p><h5 id="实现方法">2.2 实现方法</h5><h6 id="segment-level-recurrence-with-state-reuse-片段级递归和信息复用">2.2.1 Segment-Level Recurrence with State Reuse 片段级递归和信息复用</h6><p>Transformer-XL在编码后一个segment时，将前一个segment的隐层缓存下来。后一个segment的self-attention计算，会使用到前一个segment的隐层。后一个segment的第n+1层，对前一个segment的第n层隐层进行融合。故最大编码长度理论上为O(N × L)。在预测阶段，由于对segment隐层使用了缓存，故每预测一个词，不需要重新对之前的文本进行计算。大大提升了预测速度，最大可达到原始Transformer的1800倍。如下图所示 <img src="https://i.loli.net/2020/10/28/KmazXviordxyZuw.png" alt="在这里插入图片描述"></p><h6 id="relative-positional-encodings-相对位置编码">2.2.2 Relative Positional Encodings 相对位置编码</h6><p>segment递归中有个比较大的问题，就是如何区分不同segment中的相同位置。如果采用原版Transformer中的绝对编码方案，两者是无法区分的。如下 <img src="https://i.loli.net/2020/10/28/38l7XhVjZnqyuBe.png" alt="gh">不同segment中的相同位置，其position encoding会相同。这显然是有问题的。Transformer-XL将绝对位置编码改为了q和k之间的相对位置编码，代表了两个token之间的相对位置。从语义上讲，是make sense的。我们来看看具体实现方式。</p><p>绝对位置编码的attention计算如下 <img src="https://i.loli.net/2020/10/28/IKCVE7riwaQ9Zl1.png" alt="在这里插入图片描述">分为四部分</p><ol type="1"><li>query的token encoding和 key的token encoding，之间的关联信息</li><li>query的token encoding和 key的position encoding，之间的关联信息。Uj为绝对位置j的编码向量</li><li>query的position encoding和 key的token encoding，之间的关联信息。Ui为绝对位置i的编码向量</li><li>query的position encoding和 key的position encoding，之间的关联信息</li></ol><p>而采用相对位置编码后，attention计算如下 <img src="https://i.loli.net/2020/10/28/Id9fV3bxSnmHyu2.png" alt="在这里插入图片描述">同样包含四部分，仍然为二者token encoding和position encoding之间的关联关系。区别在于</p><ol type="1"><li>Ri-j为i和j之间相对位置编码，其中R为相对位置编码矩阵</li><li>u和v为query的位置编码，采用一个固定向量。因为采用相对位置编码后，无法对单个绝对位置进行编码了。文中称为global content bias，和global positional bias</li></ol><p>也有其他文章，采用了不同的相对位置编码方案。比如"Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.2018. Self-attention with relative position representations. arXiv preprint arXiv:1803.02155." 中只有a和b两部分，丢掉了c和d。Transformer-XL对两种方案进行了对比实验，证明前一种好。</p><h5 id="实验结果">2.3 实验结果</h5><h6 id="长文本编码效果">长文本编码效果</h6><p><img src="https://i.loli.net/2020/10/28/IyoR924J3BQWrOu.png" alt="在这里插入图片描述"> 在WikiText-103上的实验结果。WikiText-103包含词级别的超长文本，平均每篇文章长度为3.6K token。利用它可以验证模型的长文本编码能力。实验结果表明Transformer-XL large的PPL最低，效果最好。同时作者在One Billion Word、enwik8、text8上均进行了实验，都表明Transformer-XL效果最好。</p><h6 id="有效编码长度">有效编码长度</h6><p><img src="https://i.loli.net/2020/10/28/oKQv3akDcXWOZ7B.png" alt="在这里插入图片描述"> 模型可编码的有效长度如上，r为top-r难度样本上的表现。Transformer-XL比RNN长80%，比Transformer长450%。证明Transformer-XL可编码长度最长，长程捕获能力最强。</p><h6 id="预测速度">预测速度</h6><p><img src="https://i.loli.net/2020/10/28/g2TyZhWtRSEA7pB.png" alt="在这里插入图片描述"> 在不同segment长度下，模型预测速度的对比。和原始Transformer相比，预测速度最大可以提升1874倍。</p><h6 id="消融分析">消融分析</h6><p>文章对片段级递归和相对位置编码两个Method进行了消融分析，如下 <img src="https://i.loli.net/2020/10/28/4EeY78JcQojSuhD.png" alt="在这里插入图片描述"> 两个改进点均有作用，其中片段级递归作用更大。</p><h4 id="longformer">3 Longformer</h4><p><img src="https://i.loli.net/2020/10/28/KhcS2VIyvNEAwrg.png" alt="在这里插入图片描述"> 论文信息：2020年04月，allenai 论文地址 https://arxiv.org/abs/2004.05150 代码和模型地址 https://github.com/allenai/longformer</p><h5 id="改进方法">3.1 改进方法</h5><h6 id="attention稀疏化">3.1.1 attention稀疏化</h6><p>Transformer不能捕获长距离信息，本质原因还是因为计算量过大导致的。那我们通过降低attention计算量，是不是就可以提升长距离编码能力呢。答案是肯定的，LongFormer提出了三种attention稀疏化的方法，来降低计算量。 <img src="https://i.loli.net/2020/10/28/hDmpknyqTSIUEW7.png" alt="在这里插入图片描述"> a是原始的全连接方式的attention。后面三种为文章使用的稀疏attention。</p><ol type="1"><li>Sliding Window attention。滑窗方式的attention。假设序列长度为n，滑窗大小w，则每个位置的attention只和滑窗范围内的token进行计算，复杂度从O(n^2)变为了O(n * w)。当w &lt;&lt; n时，可以大大降低计算量。</li><li>Dilated Sliding Window attention。受到空洞卷积的启发，提出了空洞滑窗attention。看下面这张图就明白了。</li></ol><p><img src="https://i.loli.net/2020/10/28/ZwAy1H3kB7SjIP5.png" alt="在这里插入图片描述"></p><ol type="1"><li>Global Attention + sliding window。某些关键位置采用全局attention，这些位置的attention和所有token进行计算。而其他相对不关键的位置则采用滑窗attention。那什么叫关键位置呢？作者举例，分类问题中[CLS]为关键位置，需要计算全局attention。QA中question每个位置均为关键位置，同样计算全局attention。</li></ol><h6 id="tensor-virtual-machine-tvm">3.1.2 Tensor Virtual Machine (TVM)</h6><p>作者使用了TVM构建CUDA kernel，加快了longformer的速度，并降低了显存需求。这个又是另一个模型加速方面的话题，我们就先不展开了。</p><h5 id="实验结果-1">3.2 实验结果</h5><h6 id="大小模型效果">大小模型效果</h6><p><img src="https://i.loli.net/2020/10/28/ZThJDNXByQp2Lod.png" alt="在这里插入图片描述">作者在大小模型上均实验了LongFormer的效果。小模型为12 layers，512 hidden。大模型为30 layers，512 hidden。在text8和enwik8数据集上。小模型达到了SOTA。大模型比18层的Transformer-XL好，虽然不如Adaptive-span-Transformer和Compressive，但胜在可以pretrain-finetune</p><h6 id="消融分析-1">消融分析</h6><p><img src="https://i.loli.net/2020/10/28/pMCoW2ZLv6gq3j4.png" alt="在这里插入图片描述">消融分析中，可以发现</p><ol type="1"><li>Dilation空洞，有一定的收益</li><li>top layer滑窗大小比bottom layer大时，效果好一些。这个也是make sense的。因为top layer捕获高维语义，关联信息距离相对较远，窗口应该尽量大一些。</li></ol><h6 id="语料长度">语料长度</h6><p><img src="https://i.loli.net/2020/10/28/IjGfH7Mi1LK2hsC.png" alt="在这里插入图片描述"> 从上表中我们发现，语料都是特别长的长文本。LongFormer真的是document级别的Transformer。</p><h6 id="下游任务finetune效果">下游任务finetune效果</h6><p><img src="https://i.loli.net/2020/10/28/JdZUrh6L3QFBHIn.png" alt="在这里插入图片描述"> <img src="https://i.loli.net/2020/10/28/WlOZR3iaJSA2nIU.png" alt="在这里插入图片描述"> 第一个table为RoBERTa和LongFormer在问答、指代消解、分类任务中的对比。第二个table为这几个任务数据集平均文本长度。每项任务都是超出RoBERTa，当文本长度大于512时，performance提升特别明显。更加说明了长程捕获能力在NLP中的重要性。</p><h3 id="transformer家族3----计算效率优化adaptive-spanreformerlite-transformer">🚀Transformer家族3 -- 计算效率优化（Adaptive-Span、Reformer、Lite-Transformer）</h3><h4 id="背景-1">1 背景</h4><p>上文我们从编码长度优化的角度，分析了如何对Transformer进行优化。Transformer-XL、LongFormer等模型，通过片段递归和attention稀疏化等方法，将长文本编码能力提升到了很高的高度。基本已经克服了Transformer长文本捕获能力偏弱的问题，使得下游任务模型performance得到了较大提升，特别是文本较长（大于512）的任务上。</p><p>但Transformer计算量和内存消耗过大的问题，还亟待解决。事实上，Transformer-XL、LongFormer已经大大降低了内存和算力消耗。毕竟Transformer之所以长距离编码能力偏弱，就是因为其计算量是序列长度的平方关系，对算力需求过大，导致当前GPU/TPU不能满足需求。编码长度优化和计算量优化，二者是相辅相成的。但着眼于论文的出发点，我们还是分为两个不同的章节进行分析。毕竟总不能所有模型都放在一个章节吧（_）。</p><p>本文我们带来Adaptive-Span Transformer、Reformer、Lite-Transformer等几篇文章</p><h4 id="adaptive-span-transformer">2 Adaptive-Span Transformer</h4><p><img src="https://i.loli.net/2020/10/28/SZIDO7KcoGHqvhF.png" alt="在这里插入图片描述"> 论文信息：2019年5月，FaceBook，ACL2019 论文地址 https://arxiv.org/pdf/1905.07799.pdf 代码和模型地址 https://github.com/facebookresearch/adaptive-span</p><h5 id="为什么需要adaptive-span">2.1 为什么需要Adaptive-Span</h5><p>之前Transformer-XL将长文本编码能力提升到了较高高度，但是否每个layer的每个head，都需要这么长的attention呢？尽管使用了多种优化手段，长距离attention毕竟还是需要较大的内存和算力。研究发现，大部分head只需要50左右的attention长度，只有少部分head需要较长的attention。这个是make sense的，大部分token只和它附近的token有关联。如下图 <img src="https://i.loli.net/2020/10/28/nOstVdScKG68CDz.png" alt="在这里插入图片描述">我们是否可以实现attention span长度的自适应呢？让不同的layer的不同的head，自己去学习自己的attention span长度呢？Adaptive-Span Transformer给出了肯定答案。</p><h5 id="实现方案">2.2 实现方案</h5><p>文章设定每个attention head内的token计算，都使用同一个span长度。我们就可以利用attention mask来实现自适应span。对每个head都添加一个attention mask，mask为0的位置不进行attention计算。文章设计的mask函数如下 <img src="https://i.loli.net/2020/10/28/yRVqrjfJPgBKlIk.png" alt="在这里插入图片描述"> R为超参，控制曲线平滑度。其为单调递减函数，如下图。 <img src="https://i.loli.net/2020/10/28/hvoFmDIp1BjgV5f.png" alt="在这里插入图片描述"></p><h5 id="实验结果-2">2.3 实验结果</h5><p><img src="https://i.loli.net/2020/10/28/EDsBxfipL4HcOwF.png" alt="在这里插入图片描述"> 和Transformer家族其他很多模型一样，Adaptive-span也在字符级别的语言模型上进行了验证，数据集为text8。如上，Transformer注意力长度固定为512，结论如下</p><ol type="1"><li>Transformer-XL长程编码能力确实很强，平均span可达3800。</li><li>注意力长度确实不需要总那么长，Adaptive-Span大模型上，平均长度只有245</li><li>Adaptive-Span在算力需求很小（只有XL的1/3）的情况下，效果可以达到SOTA。</li></ol><p><img src="https://i.loli.net/2020/10/28/IDjAR2ucfELsegw.png" alt="在这里插入图片描述">上面是在enwik8上的结果。Adaptive-Span又一次在算力很小的情况下，达到了最优效果。值得注意的是，64层的Transformer居然需要120G的计算量，又一次证明了原版Transformer是多么的吃计算资源。另外Transformer-XL在节省计算资源上，其实也算可圈可点。</p><h4 id="reformer">3 Reformer</h4><p><img src="https://i.loli.net/2020/10/28/G8AVxR7fuCZdPHn.png" alt="在这里插入图片描述"> 论文信息：2020年1月，谷歌，ICLR2020 论文地址 https://arxiv.org/abs/2001.04451 代码和模型地址 https://github.com/google/trax/tree/master/trax/models/reformer</p><h5 id="为什么需要reformer">3.1 为什么需要Reformer</h5><p>Transformer内存和计算量消耗大的问题，一直以来广为诟病，并导致其一直不能在长文本上进行应用。（BERT、RoBERTa均设置最大长度为512）。Reformer认为Transformer有三大问题</p><ol type="1"><li>attention层计算量和序列长度为平方关系，导致无法进行长距离编码</li><li>内存占用和模型层数呈N倍关系，导致加深Transformer层数，消耗的内存特别大</li><li>feed-forward的dff比隐层dmodel一般大很多，导致FF层占用的内存特别大</li></ol><p>针对这几个问题，Reformer创新性的提出了三点改进方案</p><ol type="1"><li>LOCALITY-SENSITIVE HASHING 局部敏感hash，使得计算量从 O(L^2)降低为O(L log L) ,L为序列长度</li><li>Reversible Transformer 可逆Transformer，使得N层layers内存消耗变为只需要一层，从而使得模型加深不会受内存限制。</li><li>Feed-forward Chunking 分块全连接，大大降低了feed-forward层的内存消耗。</li></ol><p>Reformer是Transformer家族中最为关键的几个模型之一（去掉之一貌似都可以，顶多Transformer-XL不答应），其创新新也特别新颖，很多思想值得我们深入思考和借鉴。其效果也是特别明显，大大提高了内存和计算资源效率，编码长度可达64k。下面针对它的三点改进方案进行分析，有点难懂哦。</p><h5 id="实现方案-1">3.2 实现方案</h5><h6 id="locality-sensitive-hashing-局部敏感hash">3.2.1 LOCALITY-SENSITIVE HASHING 局部敏感hash</h6><p>局部敏感hash有点难懂，Reformer针对Transformer结构进行了深度灵魂拷问</p><h6 id="query和key必须用两套吗">Query和Key必须用两套吗</h6><p>Transformer主体结构为attention，原版attention计算方法如下 <img src="https://i.loli.net/2020/10/28/Z1fYDpO6BnLyHj4.png" alt="在这里插入图片描述"> 每个token，利用其query向量，和其他token的key向量进行点乘，从而代表两个token之间的相关性。归一化后，利用得到的相关性权重，对每个token的value向量进行加权求和。首先一个问题就是，query和key向量可以是同一套吗？我们可否利用key向量去和其他token的key计算相关性呢？</p><p>为此文章进行实验分析，证明是可行的。个人认为这一点也是make sense的。 <img src="https://i.loli.net/2020/10/28/YyTZDG1Bohkswit.png" alt="在这里插入图片描述">在文本和图像上，Q=K的attention，和普通attention，效果差别不大。</p><h6 id="必须和每个token计算相关性吗">必须和每个token计算相关性吗</h6><p>原版attention中，一个token必须和序列中其他所有token计算相关性，导致计算量随序列长度呈平方关系增长，大大制约了可编码最大长度。那必须和每个token计算相关性吗？其实之前Adaptive-Span Transformer也深度拷问过这个话题。它得出的结论是，对于大部分layer的multi-head，长度50范围内进行attention就已经足够了。不过Adaptive-Span采取的方法还是简单粗暴了一点，它约定每个head的attention span长度是固定的，并且attention span为当前token附近的其他token。</p><p>Adaptive-Span Transformer的这种方法显然还是没有抓住Attention计算冗余的痛点。Attention本质是加权求和，权重为两个token间的相关性。最终结果取决于较大的topk权重，其他权值较小的基本就是炮灰。并且softmax归一化更是加剧了这一点。小者更小，大者更大。为了减少计算冗余，我们可以只对相关性大的其他token的key向量计算Attention。</p><h6 id="怎么找到相关性大的向量呢">怎么找到相关性大的向量呢</h6><p>我们现在要从序列中找到与本token相关性最大的token，也就是当前key向量与哪些key向量相关性大。极端例子，如果两个向量完全相同，他们的相关性是最高的。确定两个高维向量的相关性确实比较困难，好在我们可以利用向量Hash来计算。</p><p>Reformer采用了局部敏感hash。我们让两个key向量在随机向量上投影，将它们划分到投影区间内。 <img src="https://i.loli.net/2020/10/28/gSHlhWRaoqfk9pN.png" alt="在这里插入图片描述"> 如图所示，划分了四个区间（4个桶bucket），进行了三次Hash。第一次Hash，使得上面两个向量分别归入bucket0和bucket3中，下面两个向量都归入bucket0。第二次Hash，上面两个向量和下面两个，均归入到bucket2中了。我们可以发现</p><ol type="1"><li>相似的向量，也就是相关性大的，容易归入到一个bucket中</li><li>局部敏感Hash还是有一定的错误率的，我们可以利用多轮Hash来缓解。这也是Reformer的做法，它采取了4轮和8轮的Hash。</li></ol><h6 id="整个流程">整个流程</h6><p>经过局部敏感Hash后，我们可以将相关性大的key归入同一个bucket中。这样只用在bucket内进行普通Attention即可，大大降低了计算冗余度。为了实现并行计算，考虑到每个bucket包含的向量数目可能不同，实际处理中需要多看一个bucket。整个流程如下 <img src="https://i.loli.net/2020/10/28/ZS5dCT8nVcaiob4.png" alt="在这里插入图片描述"></p><ol type="1"><li>让query等于key</li><li>局部敏感Hash（LSH）分桶。上图同一颜色的为同一个桶，共4个桶</li><li>桶排序，将相同的桶放在一起</li><li>为了实现并行计算，将所有桶分块（chunk），每个chunk大小相同</li><li>桶内计算Attention，由于之前做了分块操作，所以需要多看一个块。</li></ol><h6 id="多轮lsh">多轮LSH</h6><p>为了减少分桶错误率，文章采用了多次分桶，计算LSH Attention，Multi-round LSH attention。可以提升整体准确率。如下表。<img src="https://i.loli.net/2020/10/28/L4BDoiwTfaRjInv.png" alt="在这里插入图片描述"></p><h6 id="reversible-transformer-可逆transformer">3.2.2 REVERSIBLE TRANSFORMER 可逆Transformer</h6><p>LSH局部敏感Hash确实比较难理解，可逆Transformer相对好懂一些。这个方案是为了解决Transformer内存占用量，随layers层数线性增长的问题。为什么会线性增长呢？原因是反向传播中，梯度会从top layer向bottom layer传播，所以必须保存住每一层的Q K V向量，也就导致N层就需要N套Q K V。</p><p>那有没有办法不保存每一层的Q K V呢？可逆Transformer正是这个思路。它利用时间换空间的思想，只保留一层的向量，反向传播时，实时计算出之前层的向量。所以叫做Reversible。Reformer每一层分为两部分，x1和x2。输出也两部分，y1和y2。计算如下</p><p><img src="https://i.loli.net/2020/10/28/gKZT1mRFhEj48nU.png" alt="image-20201028214552153"></p><p>采用可逆残差连接后，模型效果基本没有下降。这也是make sense的，毕竟可逆是从计算角度来解决问题的，对模型本身没有改变。 <img src="https://i.loli.net/2020/10/28/hElWr4uHtUomVYG.png" alt="在这里插入图片描述"></p><h6 id="feed-forward-chunking-ff层分块">3.2.3 Feed-Forward chunking FF层分块</h6><p>针对fead-forward层内存消耗过大的问题，Reformer也给出了解决方案，就是FF层分块。如下 <img src="https://i.loli.net/2020/10/28/rpzwVXOSUavEs18.png" alt="在这里插入图片描述"></p><h5 id="实验结果-3">3.3 实验结果</h5><h6 id="内存和时间复杂度">内存和时间复杂度</h6><p>Reformer三个创新点，大大降低了内存和时间复杂度，消融分析如下 <img src="https://i.loli.net/2020/10/28/5ixhnpvKeQHrBw1.png" alt="在这里插入图片描述"></p><h6 id="模型效果">模型效果</h6><p>如下为在机器翻译上的效果。Reformer减少了算力消耗，同时也大大增加了长文本编码能力，故模型效果也得到了提升。如下。 <img src="https://i.loli.net/2020/10/28/8EtHVoBsfYxnmhO.png" alt="在这里插入图片描述"></p><h4 id="lite-transformer">4 Lite Transformer</h4><p><img src="https://i.loli.net/2020/10/28/9UfkunTVEpL5XBG.png" alt="在这里插入图片描述"> 论文信息：2020年4月，MIT &amp; 上海交大，ICLR2020 论文地址 https://arxiv.org/abs/2004.11886 代码和模型地址 https://github.com/mit-han-lab/lite-transformer</p><h5 id="为什么要做lite-transformer">4.1 为什么要做Lite Transformer</h5><p>主要出发点仍然是Transformer计算量太大，计算冗余过多的问题。跟Adaptive-Span Transformer和Reformer想法一样，Lite Transformer也觉得没必要做Full Attention，很多Attention连接是冗余的。不一样的是，它通过压缩Attention通道的方式实现，将多头减少了一半。与Base Transformer相比，计算量减少了2.5倍。并且文章使用了量化和剪枝技术，使得模型体积减小了18.2倍。</p><h5 id="实现方案-2">4.2 实现方案</h5><p>实现方案很简单，仍然采用了原版Transformer的seq2seq结构，创新点为</p><ol type="1"><li>multiHead self-attention变为了两路并行，分别为一半的通道数（多头）。如下图a所示。其中左半部分为正常的fully attention，它用来捕获全局信息。右半部分为CNN卷积，用来捕获布局信息。最终二者通过FFN层融合。这个架构称为LONG-SHORT RANGE ATTENTION (LSRA)，长短期Attention。</li><li>为了进一步降低计算量，作者将CNN转变为了一个depth wise卷积和一个线性全连接。dw卷积在mobileNet中有讲过，不清楚可自行谷歌。</li></ol><p><img src="https://i.loli.net/2020/10/28/WQtJvpbP9rUczhs.png" alt="在这里插入图片描述"></p><h5 id="实验结果-4">4.3 实验结果</h5><h6 id="计算复杂度">计算复杂度</h6><p><img src="https://i.loli.net/2020/10/28/w6b1LlqIpZgt8fX.png" alt="在这里插入图片描述"> 如上图，在文本摘要任务上，Lite Transformer计算量相比Base Transformer，减少了2.5倍。同时Rouge指标基本没变。</p><h6 id="模型体积">模型体积</h6><p><img src="https://i.loli.net/2020/10/28/JYzeocGBUr6jy4W.png" alt="在这里插入图片描述"> Lite Transformer模型体积只有Transformer的2.5分之一，通过8bit量化和剪枝，最终模型体积下降了18.2倍。</p><h4 id="其他">5 其他</h4><p>其他几篇文章，也建议拜读下</p><ol type="1"><li><a href="https://arxiv.org/abs/1904.10509" target="_blank" rel="noopener">Generating Long Sequences with Sparse Transformers</a> (OpenAI, 2019.04)</li><li><a href="https://arxiv.org/abs/1909.00015" target="_blank" rel="noopener">Adaptively Sparse Transformers</a> (EMNLP2019, 2019.09)</li><li><a href="https://arxiv.org/abs/1911.05507" target="_blank" rel="noopener">Compressive Transformers for Long-Range Sequence Modelling</a> (2019.11)</li><li><a href="https://arxiv.org/abs/2002.06170" target="_blank" rel="noopener">Transformer on a Diet</a> (2020.02)</li></ol><h3 id="transformer家族4----通用性优化universal-transformer">🚀Transformer家族4 -- 通用性优化（Universal-Transformer）</h3><h4 id="背景-2">1 背景</h4><p>之前讲Transformer的时候，也提到过它的通用性的缺点。相比于RNN，Transformer不是图灵完备的，虽然大多数任务都是吊打RNN，但在某些看起来极为简单的任务上，却表现很差，比如字符串拷贝等。这个问题其实也不算大，但谷歌还是给出了他的解决方案，也就是Universal Transformer。这篇看看就好了，个人感觉实际应用中作用有限。</p><h4 id="universal-transformer">2 Universal-Transformer</h4><p><img src="https://i.loli.net/2020/10/28/Lk3ymRQKZHpwN8e.png" alt="在这里插入图片描述">论文信息：2018年7月，谷歌，ICLR2019 论文地址 https://arxiv.org/abs/1807.03819 代码和模型地址 https://github.com/tensorflow/tensor2tensor</p><h5 id="为什么需要universal-transformer">2.1 为什么需要Universal-Transformer</h5><p>主要的出发点是原版Transformer不是图灵完备的，有些很简单的任务表现很差，比如字符串拷贝。序列任务还是比较偏好于迭代和递归变换，RNN正好满足了这一点，而Transformer不满足。这一点文章称作归纳偏置（Inductive Bias）。<a href="https://www.zhihu.com/question/41404496/answer/627673667" target="_blank" rel="noopener">深度学习的归纳偏置是什么？</a></p><h5 id="实现方案-3">2.2 实现方案</h5><h6 id="模型结构">模型结构</h6><p><img src="https://i.loli.net/2020/10/28/6aOdB5QqIYKSlEb.png" alt="在这里插入图片描述"> 如上所示为Universal-Transformer的结构，仍然为一个基于multiHead self-attention的seq2seq，几点不同</p><ol type="1"><li>引入了时间步step，从而实现了循环递归。除了第一次是原始信息作为输入，之后都是由前一个step的输出作为后一个的输入。</li><li>Feed-forward换成了Transition函数。根据task不同，可选择separable convolution分解卷积和fully-connected neural network全连接神经网络。</li><li>时间和位置编码，TimeStep embedding和Position embedding，新引入了TimeStep embedding，二者的编码公式和Transformer中的位置编码很像，如下</li></ol><p><img src="https://i.loli.net/2020/10/28/tdJkSqcoZ4FGHNQ.png" alt="在这里插入图片描述"></p><h6 id="adaptive-computation-timeact-自适应计算时间">Adaptive Computation Time（ACT） 自适应计算时间</h6><p>前人已经提到过ACT了，作者在模型中引用了。序列问题中，有些词语比其他的更模糊。他们需要进行更多次的计算。Universal-Transformer利用了ACT机制，可以对每个token设置自适应计算时间。模型会动态调整每个位置所需的计算steps。当某个位置停止计算后，直接copy它的隐状态到下一step。当所有位置都停止计算后，整个过程才停止。<img src="https://i.loli.net/2020/10/28/pXaRS5Q7fVDuxyW.png" alt="在这里插入图片描述"> 如上，不同位置token所需的计算steps是不同的。</p><h5 id="实验结果-5">2.3 实验结果</h5><h6 id="字符串任务">字符串任务</h6><p><img src="https://i.loli.net/2020/10/28/iT5HIsel3WjDCxA.png" alt="在这里插入图片描述">字符串复制、翻转、添加操作的效果。可以发现</p><ol type="1"><li>Transformer效果确实比较差，比LSTM差很多。这也验证了Transformer通用性确实有些问题，也就是本文的出发点</li><li>Universal-Transformer效果很好，超过LSTM很多，成功解决了原版Transformer的问题</li></ol><h6 id="机器翻译">机器翻译</h6><p><img src="https://i.loli.net/2020/10/28/LZTwqn9hjpBiAyr.png" alt="在这里插入图片描述">机器翻译上的结果，Universal-Transformer的BLEU比原版Transformer提高了0.9%</p><h3 id="transformer家族5----推理加速faster-transformer-turbotransformers">🚀Transformer家族5 -- 推理加速（Faster-Transformer 、TurboTransformers）</h3><h4 id="背景-3">1 背景</h4><p>之前介绍了从编码长度、计算效率、通用性等角度对Transformer进行优化，并介绍了几个重要模型。本文介绍如何进行Transformer推理加速。相比于离线训练，在线推理加速更加关键。一方面由于在线流量大，加速可带来硬件成本的节省。另一方面在线推理加速，可大大提升AI应用的用户体验。</p><p>事实上，之前的多种方法，特别是计算效率优化，对推理加速很有帮助。这些模型从算法的角度，进行了推理速度优化。本文主要从框架层的角度，讲解如何对推理进行加速。主要带来NVIDIA的Faster-Transformer框架和腾讯的Turbo-Transformer框架。</p><h4 id="faster-transformer">2 Faster-Transformer</h4><p>PPT资料：https://on-demand.gputechconf.com/gtc-cn/2019/pdf/CN9468/presentation.pdf 代码地址：https://github.com/NVIDIA/DeepLearningExamples/tree/master/FasterTransformer</p><h5 id="实现方案-4">实现方案</h5><p>Faster-Transformer算法结构和原版Transformer基本一致，主要是从框架层角度来实现计算加速。主要方法有</p><ol type="1"><li>算子融合。对除矩阵乘法外的所有算子，进行了合并。比如Add、Sub。从而减少了GPU kernel调度和显存读写。</li><li>半精度F16优化。</li><li>GELU激活函数、层正则化、softmax等调用频次很高的操作的优化</li></ol><h5 id="效果">效果</h5><p><img src="https://i.loli.net/2020/10/28/AGKsp9WPLQ2julY.png" alt="在这里插入图片描述"> Encoder效果对比如上。Faster-Transformer基本吊打TF XLA，提升速度一倍多。<img src="https://i.loli.net/2020/10/28/yYDIensgFQmv3HG.png" alt="在这里插入图片描述"> Decoder效果对比如上。对比了32bit和16bit的结果。Decoding FP32和Decoding FP16为Faster-Transformer 的结果，也是吊打原始TensorFlow。</p><h4 id="turbotransformers">3 <strong>TurboTransformers</strong></h4><p><img src="https://i.loli.net/2020/10/28/IKjLfbDRNgMd5rE.png" alt="在这里插入图片描述">代码地址 https://github.com/Tencent/TurboTransformers</p><h5 id="实现方案-5">实现方案</h5><ol type="1"><li>和Faster-Transformer一样，进行了算子融合。从而减少GPU kernel调用和显存占用</li><li>对于LayerNorm和softmax，由于不适合并行计算，重新开发并实现了并行计算版本。</li><li>内存缓存，避免频繁释放和分配内存。</li></ol><h5 id="和其他方案的对比">和其他方案的对比</h5><p><img src="https://i.loli.net/2020/10/28/BzXjLlFERMe34x2.png" alt="在这里插入图片描述"></p><h5 id="效果-1">效果</h5><p><img src="https://i.loli.net/2020/10/28/FnUxJrzqgL6eEks.png" alt="在这里插入图片描述">V100上的QPS，越高代表框架性能越好。对比了PyTorch、TensorFlow、Faster-Transformer、turboTransformers的效果，其中turboTransformers效果最好</p><h3 id="邱锡鹏教授nlp预训练模型综述">🚀邱锡鹏教授：NLP预训练模型综述</h3><h4 id="引言"><strong>1.引言</strong></h4><p>随深度学习的发展，多种神经网络都被应用在 NLP 任务中，比如 CNN、RNN、GNN 和 attention 机制等，但由于现有的数据集对于大部分有监督 NLP 任务来说都很小，因此，早期的模型对 NLP 任务来说都很“浅”，往往只包含 1-3 层。</p><p>而预训练模型（Pre-trained Models, PTMs）的出现将NLP带入一个新的时代，更“深”的模型和训练技巧的增强也使得 PTMs 由“浅”变“深”，在多项任务都达到了 SOTA 性能。</p><p>近日，复旦大学的邱锡鹏老师等人发布了预训练模型综述 *<strong>Pre-trained Models for Natural Language Processing: A Survey*</strong>，从背景、分类到应用与前景对 PTMs 做了详细而全面的调研。</p><p><img src="https://i.loli.net/2020/10/28/lLPuxzv8IVRTAQO.png" alt="img"></p><p><strong>论文标题：</strong>Pre-trained Models for Natural Language Processing: A Survey</p><p><strong>论文链接：</strong> https://arxiv.org/abs/2003.08271</p><h4 id="背景-4"><strong>2.背景</strong></h4><h5 id="语言表示学习"><strong>2.1 语言表示学习</strong></h5><p>对于语言来说，一个好的表示应当描绘语言的内在规则比如词语含义、句法结构、语义角色甚至语用。</p><p><img src="https://i.loli.net/2020/10/28/ZmF3yXaiHPME1LQ.png" alt="img"></p><p>而分布式表示的核心思想就是通过低维实值向量来描述一段文本的意义，而向量的每一个维度都没有对于意义，整体则代表一个具体的概念。图 1 是 NLP 的通用神经体系架构。</p><p>有两种 embedding（词嵌入）方式：上下文嵌入和非上下文嵌入，两者的区别在于词的 embedding 是否根据词出现的上下文动态地改变。</p><p><strong>非上下文嵌入：</strong>表示语言的第一步就是将分离的语言符号映射到分布式嵌入空间中。也就是对于词汇表中的每个单词（词根），通过 lookup table 映射到一个向量。</p><p>这种嵌入方式有两个局限：一是一个词通过这种方法获得的词嵌入总是静态且与上下文无关的，无法处理多义词；二是难以解决不在词汇表中的词（针对这个问题，很多 NLP 任务提出了字符级或词根级的词表示，如 CharCNN、FastText、Byte-Pair Encoding (BPE)）。</p><p><strong>上下文嵌入：</strong>为解决多义性和上下文相关的问题，将词在不同上下文的语义做区分。通过对词（词根）的 token 加一层 Neural Contextual Encoder（神经上下文编码器）得到词的上下文嵌入。</p><h3 id="section"></h3><h5 id="神经上下文编码器"><strong>2.2 神经上下文编码器</strong></h5><p><img src="https://i.loli.net/2020/10/28/DeQYroznLq4cCkt.png" alt="img"></p><p>如图 2 中所示，大部分的神经上下文编码器都可以被分为三类：卷积模型、序列模型、基于图的模型。</p><p><strong>卷积模型 ：</strong>卷积模型通过卷积操作将输入句子中的 embeddings 与其相邻的局部信息集成。</p><p><strong>序列模型 ：</strong>序列模型通常使用 RNN（如 LSTM 和 GRU）来描述词的上下文表示。实践中，双向 RNN 常用于收集词的两边信息，但表现往往会受到长程依赖问题的影响。</p><p><strong>基于图的模型 ：</strong>基于图的模型将词视做节点，通过预先定义的语言结构（如句法结构和语义联系）来学习上下文表示。但如何构造一个好的图结构往往严重依赖于专家知识和外部 NLP 工具，如依存分析器。</p><p>实际操作中往往直接通过一个全连接图来建模并让模型自己学习结构（一般通过自注意力机制）。一个典型的成功运用就是 Transformer。</p><p><strong>分析：</strong>卷积模型和序列模型都很难解决词之间的长程依赖问题，而 Transformer 虽然能更好地描述词之间的深层联系，却往往需要非常大的语料来训练，且容易在中等规模的数据集上过拟合。</p><h5 id="为什么要预训练"><strong>2.3 为什么要预训练？</strong></h5><p>正如上文提到的，模型参数的数量增长迅速，而为了训练这些参数，就需要更大的数据集来避免过拟合，而大规模的标注数据集成本又非常高。而相比之下，大规模未标注的语料却很容易构建。</p><p>为了利用大量的未标注文本数据，我们可以先从其中学习一个好的表示，再将这些表示用在别的任务中。这一通过 PTMs 从未标注大规模数据集中提取表示的预训练过程在很多 NLP 任务中都取得了很好的表现。</p><p>预训练的优点可以总结为以下三点：1 在大规模语料上通过预训练学习通用语言表示对下游任务很有帮助；2) 预训练提供了更好的模型初始化参数，使得在目标任务上有更好的泛化性能和更快的收敛速度；3) 预训练是一种有效的正则化方法，能够避免在小数据集上过拟合。</p><h4 id="ptms概述"><strong>3.PTMs概述</strong></h4><p>PTMs 的主要区别在于上下文编码器的使用、预训练任务和目标。上下文编码器已在 2.2 中做了叙述，接下来对预训练任务进行分析，并提出一种 PTMs 分类方法。</p><p><img src="https://i.loli.net/2020/10/28/LhoxNVG5rF3McDK.png" alt="img"></p><p>如图 3，这一部分内容作者在文中有一张非常详细的分类图可供参考。</p><p>表 1 从多个角度区分了文中提到的一些 PTMs。</p><p><img src="https://i.loli.net/2020/10/28/ZI176zJekFciyGr.png" alt="img"></p><h5 id="预训练任务"><strong>3.1 预训练任务</strong></h5><p>PTMs 按照预训练任务类型可以被分为两类：有监督学习、无监督学习/自监督学习。</p><p>有监督学习的预训练任务主要有机器翻译 (MT)，典型的模型是 CoVe。而下文进一步根据实现思路将自监督/无监督任务分为两类，一是基于上下文的 (LM, DAE, PLM)，二是基于对比的 (CTL)。</p><h4 id="section-1"></h4><h6 id="语言模型-lm"><strong>3.1.1 语言模型 (LM)</strong></h6><p>作为 NLP 中最常见的无监督任务，LM 一般指自回归 LM (auto-regressive LM) 或者单向 LM (unidirectional LM)。具体训练过程是基于一个大的语料，通过最大似然估计 (MLE) 训练计算一个句子出现的概率。</p><p>然而单向 LM 的缺点则是只能编码一个词左侧的文本和其自身，而更好的上下文应该编码左右两侧的文本。针对这一缺点，解决方案是双向 LM (BiLM)，即一个从左到右和一个从右到左的模型的组合。</p><h6 id="去噪声自编码器-denoising-autoencoder-dae"><strong>3.1.2 去噪声自编码器 (Denoising Autoencoder, DAE)</strong></h6><blockquote><p>这里将原文中 Masked Language Modeling (MLM) 与 DAE 合并为一个部分，因为一般将 BERT 中提出的 MLM 看作是基于 DAE 的思路实现的。</p></blockquote><p>DAE 的目的是通过向输入文本中添加噪声，利用含噪声的样本去重构不含噪声的输入。主要有五个实现方式：挡住 (MASK) token、删除 token、填充 token、句子排列、文本轮换。</p><p>MLM 随机选出一些词用 [MASK] 标记，然后去预测被 MASK 的词。但由于被 MASK 的词并不出现在 fine-tuning 的过程中，会导致预训练和微调的过程出现不一致性。针对这种情况，BERT 通过 80% [MASK]，10% 随机 token,10% 原 token 的方式来进行 mask。</p><p>而 MLM 的一种变体，<strong>Seq2SeqMLM</strong>，则是通过将 encoder-decoder (Seq2Seq) 应用到 MLM 上，这种变体有利于 Seq2Seq 类型的下游任务，比如 QA，总结和机器翻译。这一结构主要用在 MASS 和 T5 中。</p><p>而在 BERT 之后的很多论文都对 MLM 做了一些改进以增强性能，作者将其总结为 E-MLM (Enhanced Masked Language Modeling)。</p><p>其中 RoBERTa 使用动态 masking，UniLM 将对 mask 的预测扩展到三种任务：单向、双向和 Seq2Seq。XLM 通过一种串联并行双语句对叫做 TLM (translation language modeling) 的模型实现 MLM。</p><p>而 SpanBERT 和 StructBERT 则是引入了结构化信息。而 ERINE (Baidu) 则是选择 MASK 实体和短语，E-BERT 和 ERINE (THU) 则是利用了实体 embedding 方法，这三者都是借助了外部知识来丰富 MLM。</p><h6 id="排列语言模型plm"><strong>3.1.3 排列语言模型（PLM）</strong></h6><p>针对 MLM 中使用 MASK 导致的预训练与微调过程的不一致，Permuted Language Modeling (PLM) 对于一个给定序列，生成其所有可能排列进行采样作为训练的目标。值得注意的是，PLM 并不改变原始文本的位置，而是重新定义 token 预测的顺序。</p><h6 id="对比学习ctl"><strong>3.1.4 对比学习（CTL）</strong></h6><p>CTL (Contrastive Learning) 基于一种“learning by comparison”的思路，假设某些观测文本对比随机采样文本在语义上更相似，通过构建正样本和负样本并度量距离来实现学习。CTL 通常比 LM 具有更少的计算复杂度，也因此成为一个值得选择的 PTMs 训练标准。</p><h6 id="deep-infomax-dim"><strong>3.1.5 Deep InfoMax (DIM)</strong></h6><p>DIM 最初是在 CV 领域提出的用于最大化图像全局特征与局部特征之间的互信息（Mutual Information）的方法。</p><p>InfoWord 将 DIM 引入到语义表达学习中，提出用 DIM objective 以最大化句子的全局表示和一个 N-gram 的具备表示之间的互信息。</p><p>噪声对比估计（Noise-Contrastive Estimation，NCE）通过训练一个二元分类器来区分真实样本和假样本，训练词嵌入。NCE 的思想也被用在 word2vec 中。</p><h6 id="replaced-token-detection-rtd"><strong>3.1.6 Replaced Token Detection (RTD)</strong></h6><p>RTD 和 NCE 大体相同，根据上下文来预测 token 是否替换。</p><p>CBOW 的 negetive sampling 就可以看作是一个 RTD 的简单版本，其中采样是根据词汇表中的分布进行采样。</p><p>ELECTRA 基于 RTD 提出了一种新的 generator-discriminator 框架。首先用 MLM 任务训练 generator，再用 generator 的权重初始化 discriminator，再用判别任务（判别哪些 token 被 generator 替换过）训练 discriminator。</p><p>最终在下游任务只需要对 discriminator 进行 fine-tuning。TRD 也是一种很好的解决 MLM 导致的不一致问题的方法。</p><p>WKLM 则是通过在实体层面（entity-level）进行词替换，替换为同一个实体类型的实体名。</p><h5 id="section-2"></h5><h6 id="next-sentence-prediction-nsp"><strong>3.1.7 Next Sentence Prediction (NSP)</strong></h6><p>NSP 训练模型区分两个输入语句是否为训练语料中连续的片段，在选择预训练句对时，第二个句子 50% 是第一个句子实际的连续片段，50% 是语料中的随机段落。NSP 能够教会模型理解两个输入句子之间的联系，从而使得如 QA 和 NLI 这种对此类信息敏感的下游任务受益。</p><p>然而，近来 NSP 的必要性也遭到了质疑，XLNet 的作者发现不用 NSP loss 的单句训练优于使用 NSP 的句对训练。RoBERTa 的作者进一步分析表明：在对单个文本中的文本块训练时，去除 NSP 会在下游任务稍微提高性能。</p><h6 id="sentence-order-prediction-sop"><strong>3.1.8 Sentence Order Prediction (SOP)</strong></h6><p>NSP 结合了主题预测相关性预测，而因为主题预测更容易，模型将更依赖于主题预测。为更好建模句子之间的相关性，ALBERT 提出使用 SOP loss 替换 NSP loss，SOP 使用一个文档中的两个连续片段作为正样本，将这两个片段交换顺序作为负样本。</p><p>采用了 SOP 的 ALBERT 在多项下游任务中结果都优于 BERT。StructBERT 和 BERTje 也使用 SOP 作为自监督学习任务。</p><h3 id="section-3"></h3><h5 id="ptms的拓展"><strong>3.2 PTMs的拓展</strong></h5><h6 id="引入知识的ptms"><strong>3.2.1 引入知识的PTMs</strong></h6><p>通常 PTMs 都是用大量语料训练通用的语言表示，而将外部的领域知识引入到 PTMs 被证明式有效的。自 BERT 以来，就有很多预训练任务用以将外部知识纳入 PTMs，如：</p><p><strong>LIBERT：</strong>linguistically-informed BERT ，通过附加语言约束任务纳入了语言知识。</p><p><strong>SentiLR：</strong>通过对每个单词添加情感极性，将 MLM 拓展至 Label-Aware MLM (LA-MLM)，在多个情感分类任务达到 SOTA。</p><p><strong>SenseBERT：</strong>不仅能预测被 mask 的 token，还能预测 WordNet 中的 supersense。</p><p><strong>ERINE (THU)：</strong>将知识图谱中预训练的实体嵌入与文本中相应的实体提及相结合，以增强文本表示。</p><p><strong>KnowBERT：</strong>端到端将带实体连接模型与实体表示集成。</p><p><strong>KEPLER：</strong>将知识嵌入和语言模型对象联合。</p><p><strong>K-BERT：</strong>不同于以上几个模型通过实体嵌入引入知识图谱中的结构化信息，K-BERT 通过直接将知识图谱中相关三元组引入句子，获得一个 BERT 的拓展的树形输入。</p><p><strong>K-Adapter：</strong>针对不同预训练任务独立训练不同的适配器以引入多种知识，以解决上述模型在注入多种知识出现的遗忘问题。</p><h6 id="多模态ptms"><strong>3.2.2 多模态PTMs</strong></h6><p>随 PTMs 在 NLP 领域的广泛应用，一些多模态 PTMs 也被设计出来，在一些语音、视频、图像数据集上进行了预训练，比如：</p><ul><li><strong>视频-语言：</strong>VideoBERT、CBT</li><li><strong>图像-语言：</strong>用于 visual question answering (VQA) and visual commonsense reasoning (VCR)，如 ViLBERT、LXMERT、VisualBERT、B2T2、VLBERT、 Unicoder-VL、UNITER</li><li><strong>音频-文本：</strong>用于端到端 Speech Question Answering (SQA) 任务，如 SpeechBERT</li></ul><h4 id="section-4"></h4><h6 id="领域预训练ptms"><strong>3.2.3 领域预训练PTMs </strong></h6><p>大多数 PTMs 都是在 Wikipedia 这样的通用领域语料库上训练的，这就限制了他们在特定领域内的表现。</p><p>近期有一些用专业领域语料训练的 PTMs，比如：生物医学领域的 BioBERT，科学领域的 SciBERT，临床医学领域的 ClinicalBERT。还有一些工作尝试将预训练模型更好地使用目标应用，比如生物医学实体归一化、专利分类等。</p><h6 id="多语言与特定语言ptms"><strong>3.2.4 多语言与特定语言PTMs </strong></h6><p>学习多语言文本表示对于跨语言 NLP 任务是很重要的。早期工作着力于学习来自同一语义环境下的多语言词嵌入，这一方法往往缺乏语言间的校准。近期有如下几个多语言 PTMs：</p><p><strong>Multilingual-BERT：</strong>M-BERT，在 Wikipedia 上 104 种种语言的文本上进行 MLM 训练，每个训练样本都是单语言的，也没有专门设计跨语言目标，但即便如此，M-BERT 在跨语言任务上表现还是非常好。</p><p><strong>XLM：</strong>通过结合跨语言任务 TLM (translation language modeling)，提升了 M-BERT 的性能。</p><p><strong>Unicoder：</strong>提出三个跨语言预训练任务：1) cross-lingual word recovery; 2) cross-lingual paraphrase classification; 3) cross-lingual masked language model。</p><p>除此之外还有一些单语言的 PTMs：BERT-wwm，ZEN，NEZHA，ERNIE (Baidu)，BERTje，CamemBERT， FlauBERT ，RobBERT 。</p><h5 id="如何压缩ptms"><strong>3.3 如何压缩PTMs</strong></h5><p>预训练模型往往包含至少几千万个参数，这也使得模型难以部署到生活中的线上服务以及资源有限的设备上，这就使得模型压缩成为一条可能能够压缩模型尺寸并提高计算效率的方法。表 2 展示了一些压缩的 PTMs 的对比。</p><p><img src="https://i.loli.net/2020/10/28/6cBzpUQ3J7e1Xd2.png" alt="img"></p><p>压缩 PTMs 一般有四个方法：</p><ul><li><strong>剪枝（pruning）：</strong>去除不那么重要的参数（e.g. 权重、层数、通道数、attention heads）</li><li><strong>量化（weight quantization）：</strong>使用占位更少（低精度）的参数</li><li><strong>参数共享（parameter sharing）：</strong>相似模型单元间共享参数</li><li><strong>知识蒸馏（knowledge diistillation）：</strong>用一些优化目标从大型 teacher 模型学习一个小的 student 模型，一些利用知识蒸馏的 PTMs 见表 3。</li></ul><p><img src="https://i.loli.net/2020/10/28/5tdJqBHve3XYuCo.png" alt="img"></p><h4 id="如何将ptms应用至下游任务"><strong>4.如何将PTMs应用至下游任务</strong></h4><h5 id="迁移学习"><strong>4.1 迁移学习</strong></h5><p>迁移学习就是将源任务中的知识适应到目标任务，将 PTMs 适应到下游任务是一种顺序迁移学习任务。那么，如何迁移呢？我们需要考虑以下几个问题：</p><ul><li><strong>选择合适的预训练任务</strong>：近期，LM 是最流行的预训练任务，也有效解决了很多 NLP 问题。但不同的预训练任务在不同的下游任务上有不同的效果，比如 NSP 任务能帮助 PTM 理解句子之间的关系，因此 PTM 对于 QA 和 NLI 这样的下游任务很有帮助。</li><li><strong>选择合适的模型架构</strong>：比如 BERT 使用的 MLM 和 Transformer 结构使其擅长 NLU 任务，却很难生成语言。</li><li><strong>选择合适的语料</strong>：下游任务的数据应该接近 PTMs 的预训练任务。</li><li><strong>选择合适的layers</strong>：在“深”的预训练模型中，不同的 layer 往往描绘不同种类的信息。有三种选择 layers 的方式：1) 只用 Embedding，如 word2vec 和 Glove；2) Top Layer，如 BERT；3) All Layers，如 ELMo。</li><li><strong>是否进行fine-tune</strong>：模型迁移一般有两种方法：特征提取和 fine-tuning。特征提取的参数是冻结的，且往往需要特定任务的体系结构。fine-tunig 的参数是非冻结的，比特征提取方法更为通用且方便。</li></ul><h5 id="fine-tuning的策略"><strong>4.2 fine-tuning的策略</strong></h5><p>自 ULMFit 和 BERT 起，fine-tuning 已经成为 PTMs 主要的适配方法。这里有一些实用的 fine-tunig 策略：</p><ul><li>两阶段 fine-tuning：两阶段迁移的方法在预训练和 fine-tuning 阶段引入了一个中间阶段。在第一阶段，通过中间任务或语料来微调模型。在第二阶段，通过目标任务微调模型。</li><li>多任务 fine-tuning：liu等人在多任务学习框架下对 BERT 进行了微调，结果显示多任务学习和预训练是互补的方法。</li><li>采用额外的适配器 fine-tuning：fine-tuning 的主要缺点是参数效率低，在每一个下游任务上都有各自的 dine-tuning 参数。对此的解决方案是在固定原始参数时引入一些可以 fine-tuning 的适配器。</li><li>其他：逐层解冻而非连续 fine-tune 所有层；self-ensemble 和 self-distillation</li></ul><h4 id="一些ptms的资源"><strong>5.一些PTMs的资源</strong></h4><h5 id="一些开源的应用"><strong>一些开源的应用：</strong></h5><p><img src="https://i.loli.net/2020/10/28/jTUZBNqcrlm9hR2.png" alt="img"></p><p><strong>word2vec:</strong></p><p>https://github.com/tmikolov/word2vec</p><p><strong>GloVe:</strong></p><p>https://nlp.stanford.edu/projects/glove</p><p><strong>FastText:</strong></p><p>https://github.com/facebookresearch/fastText</p><p><strong>Transformers:</strong></p><p>https://github.com/huggingface/transformers</p><p><strong>Fairseq:</strong></p><p>https://github.com/pytorch/fairseq</p><p><strong>Flair:</strong></p><p>https://github.com/flairNLP/flair</p><p><strong>AllenNLP:</strong></p><p>https://github.com/allenai/allennlp</p><p><strong>FastNLP:</strong></p><p>https://github.com/fastnlp/fastNLP</p><p><strong>Chinese-BERT:</strong></p><p>https://github.com/ymcui/Chinese-BERT-wwm</p><p><strong>BERT:</strong></p><p>https://github.com/google-research/bert</p><p><strong>RoBERTa:</strong></p><p>https://github.com/pytorch/fairseq/tree/master/examples/roberta</p><p><strong>XLNet:</strong></p><p>https://github.com/zihangdai/xlnet/</p><p><strong>ALBERT:</strong></p><p>https://github.com/google-research/ALBERT</p><p><strong>T5:</strong></p><p>https://github.com/google-research/text-to-text-transfer-transformer</p><p><strong>ERNIE (Baidu):</strong></p><p>https://github.com/PaddlePaddle/ERNIE</p><p><strong>相关资源：</strong></p><p><strong>论文列表：</strong></p><p>https://github.com/thunlp/PLMpapers</p><p>https://github.com/tomohideshibata/BERT-related-papers</p><p>https://github.com/cedrickchee/awesome-bert-nlp</p><p><strong>BERT Lang Street（收集 BERT 在不同数据集和任务上的表现）：</strong></p><p>https://bertlang.unibocconi.it/</p><p><strong>BERTViz（应用 transformer 的模型的注意力可视化）：</strong></p><p>https://github.com/jessevig/bertviz</p><h4 id="应用"><strong>6.应用</strong></h4><h5 id="通用评估标准"><strong>6.1 通用评估标准</strong></h5><p>GLUE (The General Language Understanding Evaluation) 标准是一个集合了 9 个自然语言理解任务的标准。</p><p>其中包括：单个句子分类任务（CoLA和SST-2）、文本对分类任务（MNLI, RTE, WNLI, QQP, MRPC）、文本相似度任务（STSB）、相关性排行任务（QNLI）。GLUE 标准能够能够很好地评估模型的鲁棒性和通用性。</p><p>而近期 NLP 的快速发展促使了新的标准 SuperGLUE 的提出，相比 GLUE，SuperGLUE 有更多富有挑战性且多种多样的任务，如指代消解和 QA。</p><h3 id="section-5"></h3><h5 id="机器翻译-1"><strong>6.2 机器翻译</strong></h5><p>机器翻译（Machine Translation, MT）也是 NLP 的一项重要任务。几乎所有 MT 模型都使用了 encoder-decoder 框架。而近期随预训练模型的发展，也有不少尝试将 BERT 之类的预训练模型用于初始化 encoder，取得了一定成效。</p><h3 id="section-6"></h3><h5 id="问答系统"><strong>6.3 问答系统</strong></h5><p>问答系统（Question answering, QA）或是狭义概念的机器阅读理解（machine reading comprehension, MRC）也是 NLP 的重要任务。</p><p>从易到难，有三种类型的 QA 任务：单回合提取 QA (single-round extractive QA, SQuAD)、多回合生成QA (multi-round generative QA, CoQA)、多跳问答 (multi-hop QA, HotpotQA)。</p><p>针对提取 QA，有通过 PTM 初始化 encoder 的回溯阅读架构（retrospective reader architecture）；针对多回合生成 QA，有“PTM+Adversarial Training+Rationale Tagging+Knowledge Distillation”架构；针对多跳 QA，有“Select, Answer, and Explain” (SAE) 系统。</p><h5 id="情感分析"><strong>6.4 情感分析</strong></h5><p>BERT 通过在广泛使用的情感分析数据集 SST-2 上进行微调后，表现超过了先前的 SOTA 模型。而后又有很多将 BERT 进行调整以应用在 aspect 级的情感分析（ABSA）任务上。</p><h5 id="总结-1"><strong>6.5 总结</strong></h5><p>从长文本中总结出短文本也是近期 NLP 的热点。也有很多尝试将 PTM 应用在总结文本任务上，如将 BERT 通过插入 [CLS] token 来学习句子表示的模型 BERTSUM。</p><h5 id="命名实体识别"><strong>6.6 命名实体识别</strong></h5><p>命名实体识别（Named Entity Recognition, NER）也是知识提取的一个基础任务，在很多 NLP 任务上都有重要作用。TagLM 和 ELMo 利用预训练语言模型的最后一层的输入和各层的加权总和作为词嵌入的一部分。</p><h4 id="未来方向"><strong>7.未来方向</strong></h4><h5 id="ptms的上界"><strong>7.1 PTMs的上界</strong></h5><p>随 BERT 的出现，我们可以发现，很多模型都可以通过更长的训练步长不在和更大的语料来提升性能，比如去年的 T5 使用的 C4 数据集。而我们也可以通过加深模型来提升性能，比如 Turing-NLG 使用了 72 个 transformer 层。</p><p>PTMs 的共同目标都是学习语言的本质通用知识(或者说是世界的知识)，然而，随着模型的不断加深，语料的不断增大，训练模型的花销也越来越大。一种更可行的解决方案是设计更有效的模型架构、自监督预训练任务、优化器和软硬件方面的技巧等。ELECTRA 就是这个方向上一个很好的尝试。</p><h5 id="面向任务的预训练与模型压缩"><strong>7.2 面向任务的预训练与模型压缩</strong></h5><p>在实践中，不同的下游任务要求 PTMs 拥有不同的功能。而 PTMs 与下游目标任务间的差异通常表现在两方面：模型架构与数据分布。较大的 PTMs 通常情况下会有更好的性能，但实际问题是如何在低容量设备和低时延应用上使用如此庞大的 PTM。</p><p>除此之外，我们可以通过模型压缩来将通用 PTMs 教给面向对象的 PTM。尽管 CV 中对 CNNs 的压缩已经非常成熟，但 Tansformer 的全连接结构使得模型压缩非常具有挑战性。</p><h5 id="ptms架构"><strong>7.3 PTMs架构</strong></h5><p>Transformer 是 PTMs 的一个高效的框架，但 Transformer 的局限在于计算复杂度。由于 GPU 显存大小的限制，目前大多数 PTM 无法处理序列长度超过 512 个 token 的序列。搭配这一限制需要改进 Transformer 的结构，如 Transformer-XL。因此，寻求更有效的模型架构对于解决长程文本信息也是很重要的。</p><h5 id="fine-tunig中的知识迁移"><strong>7.4 Fine-tunig中的知识迁移 </strong></h5><p>Fine-tuning 是目前将 PTM 的知识迁移至下游任务的主要方法，但参数效率却很低，每个下游任务都有特定的 fine-tuned 参数。</p><p>一个可以改进的解决方案是固定 PTMs 的原始参数，并为特定任务添加小型的可微调的适配器，这样就可以在不同的下游任务使用共享的 PTMs。从 PTM‘s 中挖掘知识也可以更灵活，比如：知识提取、知识蒸馏、数据增加、将 PTMs 作为外部知识等等。</p><h5 id="ptms的可解释性与可靠性"><strong>7.5 PTMs的可解释性与可靠性 </strong></h5><p>PTMs 的深且非线性的架构使得决策制定的过程非常不透明。近期，可解释人工智能（explainable artificial intelligence, XAI）成为热点。通过对模型词嵌入的研究我们可以分析 PTMs 中的语言和世界知识，但更多有关注意力机制的可解释性的问题还值得探讨。</p><p>PTMs 这种深模型很容易受到对抗样本的扰动而产生错误的预测。在 CV 领域，对抗攻击与防御已经被广泛学习，而由于语言的特性，文本的对抗还非常具有挑战性。PTMs 的对抗防御也对于提升 PTMs 的鲁棒性很重要。</p><h4 id="总结-2"><strong>8.总结</strong></h4><p>邱锡鹏老师的这篇综述很全面地概括了预训练模型，也非常适合初学者当作一个 roadmap 来阅读。我们可以看到 NLP 的发展过程是非常令人感动的，从最开始的“要表示语言”的目标，使用词袋模型和 N-gram。</p><p>再想到“词语具有多义性”，所以需要有上下文，使用 LSTM。LSTM 只有单向，那就使用双向 LSTM。“想要更大范围的上下文”，就产生了 transformer。</p><p>“再大一些”，有了 transformer-XL。还是不够好，怎么办？“更多知识”，于是不断加大语料库，不断堆 GPU，直到 T5 探索了“Limits of Transfer Learning with a Uniﬁed Text-to-Text Transformer”。</p><p>模型太大，成本太高，那就压缩模型，改进框架，于是有了 ELECTRA。预训练模型缺乏尝试推理能力，那就知识提取，于是有了 COMET。每一步尝试都是在靠近语言的本质与世界的知识。</p><p><em>“The whole of science is nothing more than a refinement of everyday thinking.”</em></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      transformer综述
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-10-26-BERT论文阅读及详解</title>
    <link href="http://yoursite.com/2020/10/26/2020-10-26-BERT%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%8F%8A%E8%AF%A6%E8%A7%A3/"/>
    <id>http://yoursite.com/2020/10/26/2020-10-26-BERT%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%8F%8A%E8%AF%A6%E8%A7%A3/</id>
    <published>2020-10-26T12:05:46.000Z</published>
    <updated>2020-10-28T12:39:23.136Z</updated>
    
    <content type="html"><![CDATA[<p>BERT是谷歌发布的基于双向 Transformer的大规模预训练语言模型，该预训练模型能高效抽取文本信息并应用于各种NLP任务，并刷新了 11 项 NLP 任务的当前最优性能记录。</p><p>BERT的全称是基于Transformer的双向编码器表征，<strong>其中“双向”表示模型在处理某一个词时，它能同时利用前面的词和后面的词两部分信息</strong>。</p><p>BERT的本质上是通过在海量的语料的基础上运行自监督学习方法<strong>为单词学习一个好的特征表示</strong>，所谓自监督学习是指在没有人工标注的数据上运行的监督学习。</p><p>在以后特定的NLP任务中，<strong>我们可以直接使用BERT的特征表示作为该任务的词嵌入特征。</strong>所以BERT提供的是一个供其它任务迁移学习的模型，该模型可以根据任务微调或者固定之后作为特征提取器。</p><hr><p>BERT仍然使用的是Transformer模型，它pretraining的不是普通的语言模型，而是Mask语言模型。在介绍Mask语言模型之前我们先介绍BERT的输入表示。</p><h3 id="输入表示">输入表示</h3><p>BERT的输入表示如图下图所示。比如输入的是两个句子”my dog is cute”，”he likes playing”。后面会解释为什么需要两个句子。这里采用类似GPT的两个句子的表示方法，首先会在第一个句子的开头增加一个特殊的Token [CLS]，在cute的后面增加一个[SEP]表示第一个句子结束，在##ing后面也会增加一个[SEP]。</p><p>注意这里的分词会把”playing”分成”play”和”##ing”两个Token，这种把词分成更细粒度的Word Piece的方法在前面的机器翻译部分介绍过了，这是一种解决未登录词的常见办法，后面的代码部分也会简单介绍。</p><p>接着对每个Token进行3个Embedding：词的Embedding；位置的Embedding和Segment的Embedding。词的Embedding大家都很熟悉了，而位置的Embedding和词类似，把一个位置(比如2)映射成一个低维稠密的向量。而Segment只有两个，要么是属于第一个句子(segment)要么属于第二个句子，不管那个句子，它都对应一个Embedding向量。同一个句子的Segment Embedding是共享的，这样它能够学习到属于不同Segment的信息。</p><blockquote><p>对于情感分类这样的任务，只有一个句子，因此Segment id总是0；而对于Entailment任务，输入是两个句子，因此Segment是0或者1。</p></blockquote><p>BERT模型要求有一个固定的Sequence的长度，比如128。如果不够就在后面padding，否则就截取掉多余的Token，从而保证输入是一个固定长度的Token序列，后面的代码会详细的介绍。第一个Token总是特殊的[CLS]，它本身没有任何语义，因此它会(必须)编码整个句子(其它词)的语义。</p><p><img src="E:\myBlog\source_posts\bert-2.png" alt="img"> <em>图：BERT的输入表示</em></p><h3 id="mask-lm">Mask LM</h3><p>为了解决只能利用单向信息的问题，<strong>BERT使用的是Mask语言模型而不是普通的语言模型</strong>。Mask语言模型有点类似与完形填空——给定一个句子，<strong>把其中某个词遮挡起来，让人猜测可能的词</strong>。这里会随机的Mask掉15%的词，然后让BERT来预测这些Mask的词，通过调整模型的参数使得模型预测正确的概率尽可能大，这等价于交叉熵的损失函数。这样的Transformer在编码一个词的时候会(必须)参考上下文的信息。</p><p>但是这有一个问题：<strong>在Pretraining Mask LM时会出现特殊的Token [MASK]，但是在后面的fine-tuning时却不会出现，这会出现Mismatch的问题。</strong></p><p>因此BERT中，如果某个Token在被选中的15%个Token里，则按照下面的方式随机的执行：</p><ul><li><p>80%的概率替换成[MASK]，比如my dog is hairy → my dog is [MASK]</p></li><li><p>10%的概率替换成随机的一个词，比如my dog is hairy → my dog is apple</p></li><li><p>10%的概率替换成它本身，比如my dog is hairy → my dog is hairy</p></li></ul><blockquote><p>这样做的好处是，BERT并不知道[MASK]替换的是哪一个词，而且任何一个词都有可能是被替换掉的，比如它看到的apple可能是被替换的词。这样<strong>强迫模型在编码当前时刻的时候不能太依赖于当前的词，而要考虑它的上下文，甚至更加上下文进行”纠错”。</strong>比如上面的例子模型在编码apple是根据上下文my dog is应该把apple(部分)编码成hairy的语义而不是apple的语义。</p></blockquote><h3 id="预测句子关系">预测句子关系</h3><p>在有些任务中，比如问答，<strong>前后两个句子有一定的关联关系，我们希望BERT Pretraining的模型能够学习到这种关系。因此BERT还增加了一个新的任务——预测两个句子是否有关联关系</strong>。这是一种Multi-Task Learing。BERT要求的Pretraining的数据是一个一个的”文章”，比如它使用了BookCorpus和维基百科的数据，BookCorpus是很多本书，每本书的前后句子是有关联关系的；而维基百科的文章的前后句子也是有关系的。对于这个任务，<strong>BERT会以50%的概率抽取有关联的句子(注意这里的句子实际只是联系的Token序列，不是语言学意义上的句子)，另外以50%的概率随机抽取两个无关的句子，然后让BERT模型来判断这两个句子是否相关</strong>。比如下面的两个相关的句子：</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP]</span><br></pre></td></tr></tbody></table></figure><p>下面是两个不相关的句子：</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[CLS] the man [MASK] to the store [SEP] penguin [MASK] are flight ##less birds [SEP]</span><br></pre></td></tr></tbody></table></figure><h3 id="fine-tuning">Fine-Tuning</h3><p>BERT的Fine-Tuning如下图所示，共分为4类任务。</p><p><img src="E:\myBlog\source_posts\bert-3.png" alt="img"> <em>图：BERT的Fine-Tuning</em></p><p>对于普通的分类任务，输入是一个序列，如图中右上所示，<strong>所有的Token都是属于同一个Segment(Id=0)，我们用第一个特殊Token [CLS]的最后一层输出接上softmax进行分类，用分类的数据来进行Fine-Tuning</strong>。</p><p>对于相似度计算等输入为两个序列的任务，过程如图左上所示。两个序列的Token对应不同的Segment(Id=0/1)。我们也是用第一个特殊Token [CLS]的最后一层输出接上softmax进行分类，然后用分类数据进行Fine-Tuning。</p><p>第三类任务是<strong>序列标注，比如命名实体识别</strong>，输入是一个句子(Token序列)，<strong>除了[CLS]和[SEP]的每个时刻都会有输出的Tag</strong>，比如B-PER表示人名的开始，本章的序列标注部分已经介绍过怎么把NER变成序列标注的问题了，这里不再赘述。然后用输出的Tag来进行Fine-Tuning，过程如图右下所示。</p><p>第四类是问答类问题，比如SQuAD v1.1数据集，<strong>输入是一个问题和一段很长的包含答案的文字(Paragraph)，输出在这段文字里找到问题的答案。</strong></p><p>比如输入的问题是：</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Where do water droplets collide with ice crystals to form precipitation?</span><br></pre></td></tr></tbody></table></figure><p>包含答案的文字是：</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">... Precipitation forms as smaller droplets coalesce via collision with other rain drops or ice crystals within a cloud. ...</span><br></pre></td></tr></tbody></table></figure><p>正确答案是”within a cloud”。</p><p>我们怎么用BERT处理这样的问题呢？我们首先把问题和Paragraph表示成一个长的序列，中间用[SEP]分开，问题对应一个Segment(id=0)，包含答案的文字对于另一个Segment(id=1)。这里有一个假设，那就是答案是Paragraph里的一段<strong>连续的文字(Span)</strong>。BERT把寻找答案的问题转化成寻找这个Span的开始下标和结束下标的问题。</p><p>如<a href="http://fancyerii.github.io/2019/03/09/bert-theory/#bert-3" target="_blank" rel="noopener">上图</a>的左下所示。对于Paragraph的第i个Token，BERT的最后一层把它编码成TiTi，然后我们用一个向量S(这是模型的参数，需要根据训练数据调整)和它相乘(内积)计算它是开始位置的得分，因为Paragraph的每一个Token(当然WordPiece的中间，比如##ing是不可能是开始的)都有可能是开始可能，我们用softmax把它变成概率，然后选择概率最大的作为答案的开始：</p><p><span class="math inline">\(P_{i}=\frac{e^{S \cdot T_{i}}}{\sum_{j} e^{S \cdot T_{j}}}\)</span></p><p>类似的有一个向量T，用于计算答案结束的位置。</p><p>论文中作者提到了另外的两个模型，分别是OpenAI GPT和ELMo。</p><p>图3展示了这3个模型架构的对比：</p><p><img src="E:\myBlog\source_posts\image-20201026200944962.png" alt="image-20201026200944962"></p><ul><li>BERT使用了双向的Transformer架构，预训练阶段使用了MLM和NSP。</li><li>OpenAI GPT使用了left-to-right的Transformer。</li><li>ELMo分别使用了left-to-right和right-to-left进行独立训练，然后将输出拼接起来，为下游任务提供序列特征。</li></ul><p>上面的三个模型架构中，只有BERT模型的表征在每一层都联合考虑到了左边和右边的上下文信息。另外，除了架构不同，还要说明的一点是：BERT和OpenAI GPT是基于fine-tuning的方法，而ELMo是基于feature-based的方法。</p><hr><h4 id="bert是一套完整的nlp解决方案">2.2. BERT是一套完整的NLP解决方案</h4><p><strong>BERT实际上是一套完整的NLP解决方案，是从训练数据集构建到针对具体任务微调的方法、模型综合体</strong>。它的主要内容包括：（1）基于Transformer的模型结构；（2）预训练；（3）微调；（4）推断。其中推断阶段和朴素贝叶斯之类的模型区别不大，这里不做介绍。</p><h4 id="bert的总体结构">2.3. BERT的总体结构</h4><p>如图2-1，是Devlin等人在论文中给出的BERT结构示意图。BERT的输入是token序列对应的嵌入向量序列。在生命周期的不同阶段，输出是不同的：</p><p>在<strong>预训练阶段</strong>，BERT采用<strong>多任务策略</strong>，输出包括“下一个词语”和“是否为下一句”。</p><p>在<strong>微调和推断阶段</strong>，BERT(针对<strong>具体的任务</strong>)输出NER标签、答案位置等等。</p><p>这个示意图非常概括，BERT内部细节比较模糊。后面进行更详细的介绍。</p><p><img src="E:\myBlog\source_posts\v2-0a6e600043f5fac827034bbd97c17e3b_720w.jpg" alt="img"></p><p>图 2-1 《BERT》中提供的BERT结构原图</p><h4 id="bert的基石transformer">2.4. BERT的基石——Transformer</h4><p>图2-1中隐去了BERT内部的结构，即Transformers。</p><blockquote><p>BERT与GPT在使用Transformer时最大的不同在于，前者使用了Transformer的encoder部分，而后者使用了Transformer的decoder部分。</p></blockquote><p>GPT认为时间循环结构有助于模型更好地刻画类似文本这样的序列数据；而BERT认为，GPT是一个“单向模型”，没有充分发挥Transformer的特征提取能力。</p><p>BERT使用Transformer的encoder部分，即放弃了时间循环结构，完全依赖位置编码来辅助模型刻画序列数据中的时空关联信息。</p><p>失之东隅收之桑榆，BERT放弃了时间循环结构，也因此获得了一个机会：<strong>它可以同时处理整个输入序列，而不需要想GPT那样“扫描”。这样的结果就是，相对于GPT，BERT的并行化程度更高、计算速度更快。</strong></p><p><img src="E:\myBlog\source_posts\v2-baccec0f601c4f84d5b133070cd23925_720w.jpg" alt="img">图 2-2 Transformer的结构</p><h4 id="bert的预训练">2.5. BERT的预训练</h4><p>在预训练中，BERT需要同时完成两个任务：（1）随机遮蔽词语预测（MLM masked language model）；（2）“是否下一句”二分类。（NSP）</p><h4 id="随机遮蔽词语预测bert是双向语言模型">2.5.1. 随机遮蔽词语预测——BERT是双向语言模型</h4><p>在RNN时代，我们会用Bi-RNN或Bi-LSTM来“同时从左到右、从右到左扫描序列数据”。Bi-RNN是一种双向语言模型，刻画了正反两个方向上，序列数据中的时空依赖信息。<strong>双向语言模型，相比RNN等单向模型，可以提取更多的信息，模型潜力也更大。</strong></p><p><strong>Transformer也可以用来构建双向语言模型。最粗暴的方式，就是Bi-Transformer，即让2个Transformer分别从左到右和从右到左扫描输入序列。当然，这样做的话，模型参数太多，训练和推断阶段耗时会比较大。</strong></p><p>BERT没有在Transformer的结构上费工夫，而是<strong>采用特别的训练策略，迫使模型像双向模型一样思考</strong>。这种训练策略就是<strong>随机遮蔽词语预测</strong>，其流程如图2-3。</p><p>BERT会对一个句子的token序列的一部分(15%)进行处理：（1）以80%的概率遮蔽掉(MASK)；（2）以10%的概率替换为其他任意一个token；（3）以10%的概率保持不变。</p><p>“遮蔽”是怎么操作的呢？设定一个<strong>专门的token叫做”[MASK]”</strong>，将目标token替换为“[MASK]”即可。</p><p><img src="E:\myBlog\source_posts\v2-ee790ee9c9843cf405c5a86f874095d0_720w.jpg" alt="img">图 2-3 BERT的随机遮蔽预处理流程</p><p>然后，<strong>BERT会基于未被遮蔽的一部分(剩下85%)预测被遮蔽的tokens</strong>。</p><p>假设输入的token序列是“[start]我是中国人。[end]”，经过随机遮蔽处理成为” [start]我[MASK]中[MASK]人。[end]”，那么<strong>BERT就需要输出”是国”这个token序列</strong>。这就决定了，BERT使用了被遮蔽token左右两边的输入序列片段，即所谓“双向”。这个建模方式，有点类似word2vec模型。</p><blockquote><p>注意，输出是被遮蔽的token序列。</p></blockquote><h4 id="是否下一句二分类">2.5.2. “是否下一句”二分类</h4><p>为了让模型具有理解句子关系，以更好地支持文本匹配这样的下游任务，<strong>BERT在与训练时，还要完成一个二分类任务，即判断句子B是否为句子A的下一句。</strong></p><p>这个<strong>任务可以逼迫BERT学习如何输出更好地句子表示。</strong></p><h4 id="微调">2.6. 微调</h4><p>微调比较简单：添加符合任务要求的输出层，然后基于标注数据训练整个BERT。</p><h4 id="结语">3. 结语</h4><p>从GPT和BERT出现开始，NLP领域进入了“大力出奇迹”时代。在这个时代里，大数据、大规模参数是构建一个好模型的基础——这还需要大量的计算资源。</p><p>土豪们在NLP领域的装备优势越来越大了。不过也不用气馁，我们无需投入几百个GPU去跑一个不知道好不好使的结构，因为机器学习领域已经逐渐形成一种生态：<strong>高水平机构负责创造和训练基准模型，广大普通从业者基于基准模型做进一步的工作。</strong> <strong>预训练 + 微调</strong></p><p>疑问： BERT内部详细结构 ： 是如何进行的？</p><hr><h2 id="前言">前言</h2><p>BERT（<strong>B</strong>idirectional <strong>E</strong>ncoder <strong>R</strong>epresentations from <strong>T</strong>ransformers）近期提出之后，作为一个Word2Vec的替代者，其在NLP领域的11个方向大幅刷新了精度，可以说是近年来自残差网络最优突破性的一项技术了。论文的主要特点以下几点：</p><ol type="1"><li>使用了Transformer [2]作为算法的主要框架，Transformer能更彻底的捕捉语句中的双向关系；</li><li>使用了Mask Language Model(MLM) [3] 和 Next Sentence Prediction(NSP) 的多任务训练目标；</li><li>使用更强大的机器训练更大规模的数据，使BERT的结果达到了全新的高度，并且Google开源了BERT模型，用户可以直接使用BERT作为Word2Vec的转换矩阵并高效的将其应用到自己的任务中。</li></ol><p>BERT的本质上是通过在海量的语料的基础上运行自监督学习方法为单词学习一个好的特征表示，所谓自监督学习是指在没有人工标注的数据上运行的监督学习。在以后特定的NLP任务中，我们可以直接使用BERT的特征表示作为该任务的词嵌入特征。所以BERT提供的是一个供其它任务迁移学习的模型，该模型可以根据任务微调或者固定之后作为特征提取器。BERT的源码和模型10月31号已经在Github上<a href="https://link.zhihu.com/?target=https%3A//github.com/google-research/bert">开源</a>，简体中文和多语言模型也于11月3号开源。</p><h4 id="网络架构">1.1 网络架构</h4><p>BERT的网络架构使用的是《Attention is all you need》中提出的多层Transformer结构，其最大的特点是抛弃了传统的RNN和CNN，通过Attention机制将任意位置的两个单词的距离转换成1，有效的解决了NLP中棘手的长期依赖问题。Transformer的结构在NLP领域中已经得到了广泛应用，并且作者已经发布在TensorFlow的<a href="https://link.zhihu.com/?target=https%3A//github.com/tensorflow/tensor2tensor">tensor2tensor</a>库中。</p><p>Transformer的网络架构如图1所示，Transformer是一个encoder-decoder的结构，由若干个编码器和解码器堆叠形成。图1的左侧部分为编码器，由Multi-Head Attention和一个全连接组成，用于将输入语料转化成特征向量。右侧部分是解码器，其输入为编码器的输出以及已经预测的结果，由Masked Multi-Head Attention, Multi-Head Attention以及一个全连接组成，用于输出最后结果的条件概率。关于Transformer的详细解析参考我之前总结的<a href="https://zhuanlan.zhihu.com/p/48508221" target="_blank" rel="noopener">文档</a>。</p><p><img src="E:\myBlog\source_posts\v2-583765858249a8ebf5c0ececea2cd63c_720w.jpg" alt="img">图1：BERT 中采用的Transformer网络</p><p>图1中的左侧部分是一个Transformer Block，对应到图2中的一个“Trm”。</p><p><img src="E:\myBlog\source_posts\v2-9979c95d66a71a720207a48311702430_720w.jpg" alt="img">图2：BERT的网络结构</p><p>BERT提供了简单和复杂两个模型，对应的超参数分别如下：</p><ul><li><img src="E:\myBlog\source_posts\equation" alt="[公式]"> : L=12，H=768，A=12，参数总量110M；</li><li><img src="E:\myBlog\source_posts\equation" alt="[公式]"> : L=24，H=1024，A=16，参数总量340M；</li></ul><p>在上面的超参数中，L表示网络的层数（即Transformer blocks的数量），A表示Multi-Head Attention中self-Attention的数量，filter的尺寸是4H。</p><p>论文中还对比了BERT和GPT[4]和ELMo[5]，它们两个的结构图如图3所示。</p><p><img src="E:\myBlog\source_posts\v2-d36db30f3c68a097f32ed0b49f9845cc_720w.jpg" alt="img">图3：OpenAI GPT和ELMo</p><p>BERT对比这两个算法的优点是只有BERT表征会<strong>基于所有层中的左右两侧语境</strong>。BERT能做到这一点得益于Transformer中Attention机制将任意位置的两个单词的距离转换成了1。</p><h2 id="输入表示-1">1.2 输入表示</h2><p>BERT的输入的编码向量（长度是512）是3个嵌入特征的单位和，如图4，这三个词嵌入特征是：</p><ol type="1"><li>WordPiece 嵌入[6]：WordPiece是指将单词划分成一组有限的公共子词单元，能在单词的有效性和字符的灵活性之间取得一个折中的平衡。例如图4的示例中‘playing’被拆分成了‘play’和‘ing’；</li><li>位置嵌入（Position Embedding）：位置嵌入是指将单词的位置信息编码成特征向量，位置嵌入是向模型中引入单词位置关系的至关重要的一环。位置嵌入的具体内容参考我之前的<a href="https://link.zhihu.com/?target=https%3A//senliuy.gitbooks.io/advanced-deep-learning/content/di-er-zhang-ff1a-xu-lie-mo-xing/attention-is-all-you-need.html">分析</a>；</li><li>分割嵌入（Segment Embedding）：用于区分两个句子，例如B是否是A的下文（对话场景，问答场景等）。对于句子对，第一个句子的特征值是0，第二个句子的特征值是1。</li></ol><p>最后，说明一下图4中的两个特殊符号<code>[CLS]</code>和<code>[SEP]</code>，其中<code>[CLS]</code>表示该特征用于分类模型，对非分类模型，该符合可以省去。<code>[SEP]</code>表示分句符号，用于断开输入语料中的两个句子。</p><p><img src="E:\myBlog\source_posts\v2-4f9f62a7776afcdd1e1c99dfa57b965f_720w.jpg" alt="img">图4：BERT的输入特征。特征是token嵌入，位置嵌入和分割嵌入的单位和</p><h2 id="预训练任务">1.3 预训练任务</h2><p>BERT是一个多任务模型，它的任务是由两个自监督任务组成，即MLM和NSP。</p><h2 id="task-1-masked-language-model">1.3.1 Task #1： Masked Language Model</h2><p>Masked Language Model（MLM）和核心思想取自Wilson Taylor在1953年发表的一篇论文[7]。所谓MLM是指在训练的时候随即从输入预料上mask掉一些单词，然后通过的上下文预测该单词，该任务非常像我们在中学时期经常做的完形填空。正如传统的语言模型算法和RNN匹配那样，MLM的这个性质和Transformer的结构是非常匹配的。</p><p>在BERT的实验中，15%的WordPiece Token会被随机Mask掉。在训练模型时，一个句子会被多次喂到模型中用于参数学习，但是Google并没有在每次都mask掉这些单词，而是在确定要Mask掉的单词之后，80%的时候会直接替换为[Mask]，10%的时候将其替换为其它任意单词，10%的时候会保留原始Token。</p><ul><li>80%：<code>my dog is hairy -&gt; my dog is [mask]</code></li><li>10%：<code>my dog is hairy -&gt; my dog is apple</code></li><li>10%：<code>my dog is hairy -&gt; my dog is hairy</code></li></ul><p>这么做的原因是如果句子中的某个Token100%都会被mask掉，那么在fine-tuning的时候模型就会有一些没有见过的单词。加入随机Token的原因是因为Transformer要保持对每个输入token的分布式表征，否则模型就会记住这个[mask]是token ’hairy‘。至于单词带来的负面影响，因为一个单词被随机替换掉的概率只有15%*10% =1.5%，这个负面影响其实是可以忽略不计的。</p><p>另外文章指出每次只预测15%的单词，因此模型收敛的比较慢。</p><h2 id="task-2-next-sentence-prediction">1.3.2 Task #2: Next Sentence Prediction</h2><p>Next Sentence Prediction（NSP）的任务是判断句子B是否是句子A的下文。如果是的话输出’IsNext‘，否则输出’NotNext‘。训练数据的生成方式是从平行语料中随机抽取的连续两句话，其中50%保留抽取的两句话，它们符合IsNext关系，另外50%的第二句话是随机从预料中提取的，它们的关系是NotNext的。这个关系保存在图4中的<code>[CLS]</code>符号中。</p><h2 id="微调-1">1.4 微调</h2><p>在海量单预料上训练完BERT之后，便可以将其应用到NLP的各个任务中了。对于NSP任务来说，其条件概率表示为 <img src="E:\myBlog\source_posts\equation" alt="[公式]"> ，其中 <img src="E:\myBlog\source_posts\equation" alt="[公式]"> 是BERT输出中的<code>[CLS]</code>符号， <img src="E:\myBlog\source_posts\equation" alt="[公式]"> 是可学习的权值矩阵。</p><p>对于其它任务来说，我们也可以根据BERT的输出信息作出对应的预测。图5展示了BERT在11个不同任务中的模型，它们只需要在BERT的基础上再添加一个输出层便可以完成对特定任务的微调。这些任务类似于我们做过的文科试卷，其中有选择题，简答题等等。图5中其中Tok表示不同的Token， <img src="E:\myBlog\source_posts\equation" alt="[公式]"> 表示嵌入向量， <img src="E:\myBlog\source_posts\equation" alt="[公式]"> 表示第 <img src="E:\myBlog\source_posts\equation" alt="[公式]"> 个Token在经过BERT处理之后得到的特征向量。</p><p><img src="E:\myBlog\source_posts\v2-f576d9d19c9dcac1c6ee6ea28ea7a2d9_720w.jpg" alt="img">图5：BERT用于模型微调</p><p>微调的任务包括（a）基于句子对的分类任务：</p><ul><li>MNLI：给定一个前提 (Premise) ，根据这个前提去推断假设 (Hypothesis) 与前提的关系。该任务的关系分为三种，蕴含关系 (Entailment)、矛盾关系 (Contradiction) 以及中立关系 (Neutral)。所以这个问题本质上是一个分类问题，我们需要做的是去发掘前提和假设这两个句子对之间的交互信息。</li><li>QQP：基于Quora，判断 Quora 上的两个问题句是否表示的是一样的意思。</li><li>QNLI：用于判断文本是否包含问题的答案，类似于我们做阅读理解定位问题所在的段落。</li><li>STS-B：预测两个句子的相似性，包括5个级别。</li><li>MRPC：也是判断两个句子是否是等价的。</li><li>RTE：类似于MNLI，但是只是对蕴含关系的二分类判断，而且数据集更小。</li><li>SWAG：从四个句子中选择为可能为前句下文的那个。</li></ul><p>（b）基于单个句子的分类任务</p><ul><li>SST-2：电影评价的情感分析。</li><li>CoLA：句子语义判断，是否是可接受的（Acceptable）。</li></ul><p>对于GLUE数据集的分类任务（MNLI，QQP，QNLI，SST-B，MRPC，RTE，SST-2，CoLA），BERT的微调方法是根据<code>[CLS]</code>标志生成一组特征向量 <img src="E:\myBlog\source_posts\equation" alt="[公式]"> ，并通过一层全连接进行微调。损失函数根据任务类型自行设计，例如多分类的softmax或者二分类的sigmoid。</p><p>SWAG的微调方法与GLUE数据集类似，只不过其输出是四个<strong>可能选项</strong>的softmax：</p><p><img src="E:\myBlog\source_posts\equation" alt="[公式]"></p><p>（c）问答任务</p><ul><li>SQuAD v1.1：给定一个句子（通常是一个问题）和一段描述文本，输出这个问题的答案，类似于做阅读理解的简答题。如图5.(c)表示的，SQuAD的输入是问题和描述文本的句子对。输出是特征向量，通过在<strong>描述文本</strong>上接一层激活函数为softmax的全连接来获得输出文本的条件概率，全连接的输出节点个数是语料中Token的个数。</li></ul><p><img src="E:\myBlog\source_posts\equation" alt="[公式]"></p><p>（d）命名实体识别</p><ul><li>CoNLL-2003 NER：判断一个句子中的单词是不是Person，Organization，Location，Miscellaneous或者other（无命名实体）。微调CoNLL-2003 NER时将整个句子作为输入，在每个时间片输出一个概率，并通过softmax得到这个Token的实体类别。</li></ul><h4 id="总结">2. 总结</h4><p>BERT近期火得一塌糊涂不是没有原因的：</p><ol type="1"><li>使用Transformer的结构将已经走向瓶颈期的Word2Vec带向了一个新的方向，并再一次炒火了《Attention is All you Need》这篇论文；</li><li>11个NLP任务的精度大幅提升足以震惊整个深度学习领域；</li><li>无私的开源了多种语言的源码和模型，具有非常高的商业价值。</li><li>迁移学习又一次胜利，而且这次是在NLP领域的大胜，狂胜。</li></ol><p>BERT算法还有很大的优化空间，例如我们在Transformer中讲的如何让模型有捕捉Token序列关系的能力，而不是简单依靠位置嵌入。BERT的训练在目前的计算资源下很难完成，论文中说 <img src="E:\myBlog\source_posts\equation" alt="[]"> 的训练需要在64块TPU芯片上训练4天完成，而一块TPU的速度约是目前主流GPU的7-8倍。非常幸运的是谷歌开源了各种语言的模型，免去了我们自己训练的工作。</p><hr><p>[CLS]就是classification的意思，可以理解为用于下游的分类任务。</p><p>主要用于以下两种任务：</p><ul><li>单文本分类任务：对于文本分类任务，BERT模型在文本前插入一个[CLS]符号，并将该符号对应的输出向量作为整篇文本的语义表示，用于文本分类，如下图所示。可以理解为：<strong>与文本中已有的其它字/词相比，这个无明显语义信息的符号会更“公平”地融合文本中各个字/词的语义信息。</strong></li></ul><p><img src="E:\myBlog\source_posts\20191019174056699.png" alt="img"></p><ul><li>语句对分类任务：该任务的实际应用场景包括：问答（判断一个问题与一个答案是否匹配）、语句匹配（两句话是否表达同一个意思）等。<strong>对于该任务，BERT模型除了添加[CLS]符号并将对应的输出作为文本的语义表示，还对输入的两句话用一个[SEP]符号作分割，并分别对两句话附加两个不同的文本向量以作区分</strong>，如下图所示。</li></ul><p><img src="E:\myBlog\source_posts\20191019174247812.png" alt="img"></p><hr><h3 id="模型架构">模型架构</h3><h4 id="输入表示-2">输入表示</h4><ul><li><p>我们的输入表示能够在一个标记序列中明确地表示单个文本句子或一对文本句子（例如，[问题，答案]）。一个词的输入=词的embeding+段embeding+位置embeding</p><p><img src="E:\myBlog\source_posts\20190418105905367.png" alt="img"></p></li><li><p>对于词embeding论文使用<a href="https://arxiv.org/pdf/1609.08144.pdf" target="_blank" rel="noopener">WordPiece embeddings</a></p></li><li><p>每个序列的第一个字符始终是特殊分类embedding([CLS])。对应于该字符的最终隐藏状态（即，Transformer的输出）被视为<strong>整个序列表示</strong>常用于聚合用作分类任务。<strong>对于非分类任务，将忽略此向量。</strong></p></li></ul><h4 id="预训练">预训练</h4><ul><li>任务一：Masked LM<ul><li>标准条件语言模型只能从左到右或从右到左进行训练，因为双向条件语言模型将允许每个单词在多层self-attention中间接看到自己。<strong>为了避免当前要预测的词在self-attention中看到要预测的答案我们采样的方法是：随机屏蔽掉(mask)输入序列中一定比例的输入词，然后仅预测那些被屏蔽的词,称这个方法叫masked LM(MLM)，最后我们将这个被mask的词的最后隐藏层输出，输入到softmax层中预测这个被mask的词</strong></li><li>在论文的实验中我们<strong>每次mask掉一个序列的15%词</strong></li><li>该任务的两个缺点：<ul><li>第一个：这种操作使得预训练和微调之间不匹配，因为在微调期间可能没有[MASK]字符。为了缓解这种情况我们不总是用[MASK]词来替换被mask掉的词，而是80%的用[MASK]词来替换被mask掉的词，10%用一个随机词来替换被mask掉的词，再，10%保存源词不变。例子：<ul><li>原句：my dog is hairy 我们要mask掉hairy</li><li>80%：my dog is [MASK]</li><li>10%：my dog is apple</li><li>10%：my dog is hairy</li></ul></li><li>第二个：<strong>每个batch中只预测了15％的词，这表明模型可能需要更多的预训练步骤才能收敛。实验证明该任务的训练略微慢一点比起预测每一个词的语言模型。</strong></li></ul></li></ul></li><li>任务二：Next Sentence Prediction<ul><li>为了训练理解句子关系的模型，我们预先训练下一句话预测任务，该任务可以从任何单语言语料库中生成。具体地，在构建每个预训练样本时，选择句子A和B，50％B是A的实际下一句子， 50％B是来自语料库的一个随机句子，例子如下：<ul><li><strong>Input</strong> = [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP]</li><li><strong>Label</strong> = IsNext</li><li><strong>Input</strong> = [CLS] the man [MASK] to the store [SEP] penguin [MASK] are flight ##less birds [SEP]</li><li><strong>Label</strong> = NotNext</li></ul></li></ul></li><li>预训练过程设置：<ul><li>输入序列长度为512，batch_size=256,训练1000000步近似在33亿词的预料库上40 epochs</li><li>使用Adam优化器，learning_rate=1e-4, β_1= 0.9, β_2= 0.999,权重的L2正则项系数为0.01，学习率是预热步数：10000，学习率线性衰退，在每一层使用概率为0.1的dropout，激活函数使用<a href="https://arxiv.org/abs/1606.08415" target="_blank" rel="noopener">gelu</a></li></ul></li></ul><h3 id="微调-2">微调</h3><ul><li><p>对于序列水平的分类任务，我们<strong>获取第一个词[CLS]的最后隐藏层状态,再将C经过一个全连接层得到最后的预测分布，其中K是类别数。</strong>W也是这种特殊任务唯一添加的模型参数。</p></li><li><p>在微调的过程中BERT和W被同时微调。</p></li><li><p><strong>在微调中，大多数模型超参数与预训练相同，一般修改的超参数是：batch_size, learning_rate, epochs。 Dropout的概率始终保持在0.1</strong>。理论上说最佳超参数值随特定于任务不同而不同，但我们发现以下范围的可能值可以在所有任务中很好地工作：</p><ul><li>Batch size: 16, 32</li><li>Learning rate (Adam): 5e-5, 3e-5, 2e-5</li><li>Number of epochs: 3, 4</li></ul></li><li><p>我们还观察到，大数据集对超参数选择的敏感性远小于小数据集</p></li><li><p>微调总结图：</p><p><img src="E:\myBlog\source_posts\20190418111747424.png" alt="img"></p><p>图一：预测两个句子的关系，图二：是对当个句子分类。</p></li></ul><h2 id="模型对比">模型对比</h2><p><img src="E:\myBlog\source_posts\2019041811190075.png" alt="img"></p><h1 id="总结-1">总结</h1><h2 id="词嵌入语言模型的方法">词嵌入语言模型的方法</h2><ul><li><p>NLP词嵌入语言模型的方法：</p><ul><li><p>Feature-based方法</p><ul><li><p>Feature-based指利用预先训练好的语言模型的结果,作为当前特定任务模型（task-specific）的一个额外的特征引入到当前特定任务模型中，例如下图的语言模型</p><p><img src="E:\myBlog\source_posts\20190418112208513.png" alt="img"></p><p>上图中，左边部分为序列标注模型，也就是task-specific model，每个任务可能不同，右边是两个预训练好的前向LM(Left-to-right)和后向LM(Right-To-Left), 将两个LM的结果进行了合并，并将LM embedding与词向量、第一层RNN输出、第二层RNN输出进行了concat操作</p></li><li><p>通常feature-based方法包括两步：</p><ul><li>首先在大的语料A上无监督地训练语言模型，训练完毕得到语言模型。</li><li>然后构造task-specific model例如序列标注模型，采用有label的语料B来有监地训练task-sepcific model，将语言模型的参数固定，语料B的训练数据经过语言模型得到LM embedding，作为task-specific model的额外特征</li></ul></li><li><p>ELMo是这方面的典型代表</p></li></ul></li></ul></li><li><p>Fine-tuning方法</p><ul><li><p>Fine-tuning方式是指在已经训练好的语言模型的基础上，加入少量的task-specific parameters, 例如对于分类问题在语言模型基础上加一层softmax网络，然后在新的语料上重新训练来进行fine-tune。</p></li><li><p>OpenAI GPT 是这一方法的典型代表，其模型如下所示:</p><p><img src="E:\myBlog\source_posts\20190418112554368.png" alt="img"></p><p>GPT首先语言模型采用了Transformer Decoder的方法来进行训练，采用文本预测作为语言模型训练任务，训练完毕之后，加一层Linear Project来完成分类/相似度计算等NLP任务。</p></li><li><p>Fine-Tuning的方法工作包括两步：</p><ul><li>构造语言模型，采用大的语料A来训练语言模型</li><li>在语言模型基础上增加少量神经网络层来完成specific task model例如序列标注、分类等，然后采用有label的语料B来有监督地训练模型，这个过程中语言模型的参数并不固定.</li></ul></li><li><p>而BERT采用了fine-tuning的方法，并且在许多task-specific model中取得了最好的效果</p></li></ul></li></ul><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      BERT论文阅读
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-10-22-MAML论文</title>
    <link href="http://yoursite.com/2020/10/22/2020-10-22-MAML%E8%AE%BA%E6%96%87/"/>
    <id>http://yoursite.com/2020/10/22/2020-10-22-MAML%E8%AE%BA%E6%96%87/</id>
    <published>2020-10-22T07:32:48.000Z</published>
    <updated>2020-11-03T07:39:58.276Z</updated>
    
    <content type="html"><![CDATA[<h3 id="model-agnostic-meta-learning-for-fast-adaptation-of-deep-networks">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</h3><blockquote><p>ICML 2017</p><p>authors ： Chelsea Finn，Pieter Abbeel， Sergey Levine （University of California ,Berkeley； Open AI）</p><p>code url (official tf) : <a href="https://github.com/cbfinn/maml" target="_blank" rel="noopener" class="uri">https://github.com/cbfinn/maml</a></p><p>code url (unofficial torch): <a href="https://github.com/dragen1860/MAML-Pytorch" target="_blank" rel="noopener" class="uri">https://github.com/dragen1860/MAML-Pytorch</a></p><p>被引用次数：2427</p></blockquote><h4 id="背景">背景</h4><p>解决小样本学习问题很有挑战 - &gt; 利用元学习的方法框架</p><p>元学习学习到一个模型，这个模型可以在少量新数据中快速学习。</p><h4 id="问题">问题</h4><p>前人通过学习update function或learning rule的训练方法，需要通过扩充模型的参数量或是限制模型结构（如限定RNN网络）等手段来提高准确率。</p><h4 id="解决">解决</h4><p>model-agnostic：模型无关。</p><p>MAML可以认为是一个框架，提供一个meta-learner用于训练base-learner。这里的meta-learner即MAML的精髓所在，用于 learning to learn；而base-learner则是在目标数据集上被训练，并实际用于预测任务的真正的数学模型。</p><p>绝大多数深度学习模型都可以作为base-learner无缝嵌入MAML中，而MAML甚至可以用于强化学习中，这就是MAML中model-agnostic的含义</p><p>本文的想法是训练一组初始化参数，<strong>通过在初始参数的基础上进行一或多步的梯度调整，来达到仅用少量数据并且一次或几次的梯度更新就能快速适应新task的目的（能够有好的表现，小loss）</strong>。</p><p>为了达到这一目的，训练模型需要最大化新task的loss function的参数敏感度（<em>maximizing the sensitivity of the loss functions of new tasks with respect to the parameters</em>），当敏感度提高时，极小的参数（参数量）变化也可以对模型带来较大的改进。</p><h4 id="贡献">贡献</h4><p>本文提出的算法可以被轻松地使用在全连接网络、卷积网络以及递归神经网络中，并且可以使用多种loss函数，可以适用于多个领域，包括少样本的回归、图像分类，以及强化学习，并且使用更少的参数量达到了当时（2017年）最先进的专注于少样本分类领域的网络的准确率。</p><h4 id="模型">模型</h4><h5 id="算法图解">算法图解</h5><p><img src="E:\myBlog\source_posts\image-20201103104804241.png" alt="image-20201103104804241"></p><p>既然希望使用训练好的meta-learner仅通过几步梯度迭代便可适用于新的task，作者便将目标设定为，通过梯度迭代，找到对于task敏感的参数 θ 。</p><p>训练完成后的模型具有对新task的学习域分布最敏感的参数，因此可以在仅一或多次的梯度迭代中获得最符合新任务的 θ* ，达到较高的准确率。</p><h5 id="算法流程">算法流程</h5><p><img src="E:\myBlog\source\_posts\image-20201103105129835.png" alt="image-20201103105129835" style="zoom:67%;"></p><p>表示的是MAML预训练阶段的算法</p><p>gradient through a gradient</p><h5 id="损失函数">损失函数</h5><p>梯度的计算是需要确定loss function的，MAML中loss根据不同的问题处理有不同的选择：</p><p>对于可监督回归问题，采用MSE</p><p>对于可监督分类问题，采用交叉熵</p><p>总的损失函数计算</p><p><img src="E:\myBlog\source_posts\image-20201103135150758.png" alt="image-20201103135150758"></p><p><img src="E:\myBlog\source\_posts\image-20201103105239355.png" alt="image-20201103105239355" style="zoom: 67%;"></p><p><del>为什么在support set 中梯度只更新一次？</del></p><ol type="1"><li>增加速度</li><li>因为在实际的应用中</li><li>小样本学习中</li></ol><h4 id="实验">实验</h4><p><img src="E:\myBlog\source_posts\image-20201103105805219.png" alt="image-20201103105805219"></p><p>first order approximation (一阶近似）：泰勒展开式对函数展开后，取前两项 ，和原始的不进行一阶近似的差别不大</p><h4 id="参考">参考</h4><blockquote><p><a href="https://zhuanlan.zhihu.com/p/57864886" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/57864886</a></p><p><a href="https://zhuanlan.zhihu.com/p/72920138" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/72920138</a></p></blockquote><p>Learning to Compare: Relation Network for Few-Shot Learning</p><p>应用领域： 图像分类</p><p>使用方法：两个模块： embedding module (CNNEncoder 普通的四层卷积) 和 relation module (RelationNetwork)</p><p><img src="E:\myBlog\source_posts\image-20201023150506263.png" alt="image-20201023150506263"></p><p>思路2： transformer可以嵌入到function函数中，作为特征提取器</p><p>思路3： 结合使用</p><p>Adaptive Subspaces for Few-Shot Learning</p><p><img src="E:\myBlog\source_posts\image-20201023150053311.png" alt="image-20201023150053311"></p><p><img src="E:\myBlog\source_posts\image-20201023150105936.png" alt="image-20201023150105936"></p><p><img src="E:\myBlog\source_posts\image-20201023144851908.png" alt="image-20201023144851908"></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      MAML论文
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-10-20-transformer和GNN的关系理解</title>
    <link href="http://yoursite.com/2020/10/20/2020-10-20-transformer%E5%92%8CGNN%E7%9A%84%E5%85%B3%E7%B3%BB%E7%90%86%E8%A7%A3/"/>
    <id>http://yoursite.com/2020/10/20/2020-10-20-transformer%E5%92%8CGNN%E7%9A%84%E5%85%B3%E7%B3%BB%E7%90%86%E8%A7%A3/</id>
    <published>2020-10-20T07:04:37.000Z</published>
    <updated>2020-10-20T07:09:48.737Z</updated>
    
    <content type="html"><![CDATA[<h3 id="transformer和gnn的关系">transformer和GNN的关系</h3><blockquote><p>Transformer和GNN有什么关系？一开始可能并不明显。但是通过这篇文章，你会从GNN的角度看待Transformer的架构，对于原理有更清楚的认知。</p></blockquote><p>通过这篇博文，现为南洋理工大学助理研究员Chaitanya Joshi 将为读者介绍图神经网络和 Transformer 之间的内在联系。具体而言，作者首先介绍 NLP 和 GNN 中模型架构的基本原理，使用公式和图片来加以联系，然后讨论怎样能够推动这方面的进步。</p><p><strong>transformer是GNN的一种特例</strong></p><h4 id="nlp-中的表示学习">NLP 中的表示学习</h4><p>从一个很高的角度来看，所有的神经网络架构都是<strong>对输入数据构建表示</strong>(<em>representations</em>)——以向量或嵌入矩阵的形式。这种方法将有用的统计或语义信息进行编码。</p><p>这些隐表示(<em>latent</em> or <em>hidden</em> representations)可以被用来进行一些有用的任务，如图像分类或句子翻译。<strong>神经网络通过反馈（即损失函数）来构建更好的表示。</strong></p><p>对于 NLP 来说，传统上，RNN 对每个词都会建立一个表示——<strong>使用序列的方式</strong>。例如，每个时间步一个词。从直观上来说，我们可以想象，一个 RNN 层是一个传送带。词汇以自回归(<em>autoregressively</em>)的方式从左到右被处理。在结束的时候，我们可以得到每个词在句子中的隐藏特征，然后将这些特征输入到下一个 RNN 层中，或者用到任务中去。</p><p><img src="https://i.loli.net/2020/10/20/Do7nd4r83wpJIuQ.jpg" alt="img"></p><p>从机器翻译开始，Transformer 就逐渐开始取代 RNN。<strong>这一模型有着新的表示学习策略</strong>。它不再使用递归，而是使用注意力机制对每个词构建表示——即每个词语在句子中的重要程度。知道了这一点，词的特征更新则是所有词的线性变换之和——通过其重要性进行加权。</p><h4 id="解析transformer">解析Transformer</h4><p>通过将前一段翻译成数学符号以及向量的方式去创建对整个体系结构的认知。将长句 S 中的第 i 个单词的隐藏特征 h 从 ℓ 层更新至ℓ+1 层：</p><p><img src="https://i.loli.net/2020/10/20/ZPO4o7kHUsy9GT2.png" alt="img"></p><p>其中 j∈S 为句子中单词的集合，Q<sup>ℓ、K</sup>ℓ、V^ℓ为可学习的线性权重（分别表示注意力计算的 Query、Key 以及 Value）。针对句子中每个单词的并行执行注意力机制，从而在 one shot 中（在 RNNs 转换器上的另外一点，逐字地更新特征）获取它们的更新特征。</p><p>我们可通过以下途径更好地理解注意力机制：</p><p><img src="https://i.loli.net/2020/10/19/M7EcF9nmQRfLOBA.jpg" alt="img" style="zoom:67%;"></p><p>考虑到 h_j^l； ∀j∈S 句中 h_i^l 和其他词的特征，通过点积计算每对（i，j）的注意力权重，然后在所有 j 上计算出 softmax。最后通过所有 h_j^l 的权重进行相应的加权，得到更新后的单词特征 h_i^l+1。</p><h4 id="多头注意力机制">多头注意力机制</h4><p>让点积注意力机制发挥作用是被证明较为棘手：糟糕的随机初始化可能会破坏学习过程的稳定性，此情况可以通过并行执行多头注意力将结果连接起来，从而克服这个问题（<strong>而每个「head」都有单独的可学习权重</strong>）：</p><p><img src="https://i.loli.net/2020/10/20/8R6hMowJm1zYlPX.png" alt="img"></p><p>其中 Q<sup>k,ℓ、K</sup>k,ℓ、V^k,ℓ是第 K 个注意力 head 的可学习权重，O^ℓ 是向下的投影，用以匹配 h_i^l+1 和 h_i^l 跨层的维度。</p><p>通过观察上一层中隐藏特征的不同的变换过程以及方面，多头机制允许注意力机制从本质上“规避风险”。关于这点，我们将在后面详细讨论。</p><h4 id="尺度问题和前向传播子层">尺度问题和前向传播子层</h4><p>促使形成最终形态的Transformer结构的<strong>关键问题点</strong>是，注意机制之后的<strong>词的特征</strong>可能在不同的尺度或重要性上：1）这可能是由于某些词在将其他词的特征累加时具有非常集中或非常分散的注意力权重 w_ij；（2）在单个特征/向量输入级别，跨多个注意力头（每个可能会以不同的比例输出值）进行级联可以导致最终向量 h_i^ℓ+1的输入具有一个大范围的值。遵循传统的机器学习思路，在上述流程中增加一个归一化层似乎是一个合理的选择。</p><p><img src="https://i.loli.net/2020/10/20/LpmV5fOXb3NMQAR.png" alt="image-20201019200900240"></p><p><strong>Transformers使用LayerNorm克服了问题（2）</strong>，LayerNorm在特征层级上进行归一化并学习一种仿射变换。此外，通过求特征维度的平方根来缩放点积注意力有助于抵消问题（1）。</p><p>最后，作者提出了控制尺度问题的另一个“技巧”：<strong>具有特殊结构的考虑位置的双层MLP</strong>。在多头注意力之后，他们通过一个可学习的权重 h_i^ℓ+1 将投影到一个更高的维度，在该维度中， h_i^ℓ+1 经过ReLU非线性变换，然后投影回其原始维度，然后再进行另一个归一化操作：</p><p><img src="https://i.loli.net/2020/10/20/8mlU4tj9YgzvGKX.png" alt="image-20201019201217429"></p><p>说实话，我不确定超参数化前馈子层背后的确切理由是什么，似乎也没有人对此提出疑问！我认为LayerNorm和缩放的点积不能完全解决突出的问题，因此大型MLP是一种可以相互独立地重新缩放特征向量的手段。</p><p>Transformer层的最终形态如下所示：</p><p><img src="https://i.loli.net/2020/10/19/2YgkMdw7l5JyPGT.png" alt="img" style="zoom:50%;"></p><p>Transformer架构也非常适合非常深的网络，使NLP界能够在模型参数和扩展数据这两方面进行延伸。</p><p>每个多头注意力子层和前馈子层的输入和输出之间的残差连接是堆叠Transformer层的关键（但为了清楚起见，在上图中省略了）。</p><h4 id="gnns构建图的表示">GNNs构建图的表示</h4><p>图卷积网络是图神经网络的一个分类</p><p>图神经网络（GNNs）或图卷积网络（GCNs）在图数据中建立节点和边的表示（representations）。它们是通过<strong>邻域聚合</strong>（或消息传递）（<strong>neighbourhood aggregation</strong> or message passing）来实现的，<strong>在邻域聚合中，每个节点从其邻域收集特征，以更新其周围的局部图结构表示</strong>。通过堆叠多个GNN层使得该模型可以将每个节点的特征传播到整个图中，从其邻居传播到邻居的邻居，依此类推。</p><p><img src="https://i.loli.net/2020/10/20/WD2hXpu18kmSqKU.png" alt="image-20201019201748102"></p><p>以这个表情符号社交网络为例：由GNN产生的节点特征可用于预测性任务，例如识别最有影响力的成员或提出潜在的联系。</p><p>在他们最基本的形式中，GNNs通过以下方法来更新节点i在ℓ层的隐藏层特征h（例如，😆），也就是先将节点自身的特征 h_i^l 和每个邻居节点 j∈N(i) 特征 h_j^l 的聚合相累加，然后再整体做一个<strong>非线性变换</strong>，如下：</p><p><img src="https://i.loli.net/2020/10/20/Meg8lO7IcuADTd3.png" alt="image-20201019202233150"></p><p>其中U<sup>l,V</sup>l是GNN层的可学习的权重矩阵，而<img src="https://i.loli.net/2020/10/20/rtfhOwx4lTWRjXz.png" alt="image-20201019202245048">是一个非线性变换，例如ReLU。</p><p>在上述例子中，N (😆) ={ 😘, 😎, 😜, 🤩 }。</p><p>邻域节点 j∈N(i) 上的求和可以被其他输入大小不变的聚合函数代替，例如简单的均值/最大值函数或其他更强大的函数（如通过注意机制的加权和）。</p><p>这听起来熟悉吗？</p><p>也许这样一条流程可以帮助建立连接：</p><p><img src="https://i.loli.net/2020/10/20/Askv9i74DwX3zIP.png" alt="image-20201019202551797"></p><blockquote><p><strong>如果我们要执行多个并行的邻域聚合头，并且用注意力机制（即加权和）替换领域上的求和 ，我们将获得图注意力网络（GAT）。加上归一化和前馈MLP，瞧，我们就有了Graph Transformer！</strong></p></blockquote><h3 id="句子就是由词全连接而成的图">句子就是由词全连接而成的图</h3><p>Sentences are fully-connected word graphs</p><p>为了使连接更加清晰，可以将一个句子看作一个完全连接的图，其中每个单词都连接到其他每个单词。现在，我们可以<strong>使用GNN来为图（句子）中的每个节点（单词）构建特征</strong>，然后我们可以使用它来执行NLP任务。</p><p><img src="https://i.loli.net/2020/10/20/YbeLK31vHIjUNRp.png" alt="image-20201019203155390"></p><p>广义上来讲，这就是Transformers正在做的事情：Transformers是以多头注意力作为邻居聚合函数的GNNs（<strong>GNNs with multi-head attention</strong> as the neighbourhood aggregation function ）。<strong>标准GNNs从其局部邻域节点j∈N(i) 聚合特征，而NLP的<code>Transformers将整个句子视为局部邻域</code>，在每个层聚合来自每个单词j∈S的特征。</strong></p><p>重要的是，各种特定于问题的技巧（如位置编码、因果/掩码聚合、学习率表和大量的预训练）对于Transformers的成功至关重要，但在GNN界中却很少出现。同时，从GNN的角度看Transformers可以启发我们摆脱模型结构中的许多花哨的玩意。</p><h3 id="可以从transformers和gnn中学到什么">可以从Transformers和GNN中学到什么？</h3><p>现在我们已经在Transformers和GNN之间建立了联系，接着让我们来探讨一些新的问题...</p><h4 id="全连接图是nlp的最佳输入格式吗">全连接图是NLP的最佳输入格式吗？</h4><p>在统计NLP和ML之前，Noam Chomsky等语言学家致力于发展语言结构的最新理论，如语法树/图。Tree LSTMs已经尝试过这一点，但是也许Transformers/GNNs是可以让语言理论和统计NLP的领域结合得更加紧密的更好的架构？</p><p><img src="https://i.loli.net/2020/10/20/XuSVtxfpclmaCjs.png" alt="image-20201019203855101"></p><h4 id="如何学习到长期依赖">如何学习到长期依赖？</h4><p><strong>完全连通图使得学习词与词之间的非常长期的依赖关系变得非常困难，这是完全连通图的另一个问题。</strong>这仅仅是因为图中的边数与节点数成二次平方关系，即在n个单词的句子中，Transformer/GNN将在n^2对单词上进行计算。如果n很大，那将会是一个非常棘手的问题。</p><p>NLP界对长序列和依赖性问题的看法很有意思：例如，使注意力机制在输入大小方面稀疏或自适应，在每一层中添加递归或压缩，以及使用对局部性敏感的哈希法进行有效的注意，这些都是优化Transformers有希望的新想法。</p><p>有趣的是，还可以看到一些GNN界的想法被混入其中，例如，用于句子图稀疏化的二进制分区似乎是另一种令人兴奋的方法。</p><p><img src="https://i.loli.net/2020/10/20/7JndG8ZPg3zUWE1.png" alt="image-20201019210515688"></p><h4 id="transformers在学习神经网络的句法吗">Transformers在学习神经网络的句法吗？</h4><p>NLP界有几篇关于Transformers可能学到什么的有趣论文。其基本前提是，对句子中的所有词对使用注意力机制（目的是确定哪些词对最有趣），可以让Transformers学习特定任务句法之类的东西。</p><p>多头注意力中的不同头也可能“关注”不同的句法属性。</p><p>从图的角度来看，通过在完全图上使用GNN，我们能否从GNN在每一层执行邻域聚合的方法中恢复最重要的边线及其可能带来的影响？我还不太相信这种观点。</p><p><img src="https://i.loli.net/2020/10/20/cNW2ZmO7yExKo4s.png" alt="image-20201019211524983"></p><h4 id="为什么要用多头注意力为什么要用注意力机制">为什么要用多头注意力？为什么要用注意力机制？</h4><p>我更赞同多头机制的优化观点——拥有多个注意力可以改进学习，克服不好的随机初始化。例如，这些论文表明，Transformers头可以在训练后“修剪”或“删除”，并且不会产生重大的性能影响。</p><p>多头邻聚合机制在GNNs中也被证明是有效的，例如在GAT使用相同的多头注意力，MoNet使用多个高斯核来聚合特征。虽然多头技巧是为了稳定注意力机制而发明的，但它能否成为提炼出额外模型性能的标准？</p><p>相反，具有简单聚合函数（如sum或max）的GNNs不需要多个聚合头来维持稳定的训练。如果我们不需要计算句子中每个词对之间的成对兼容性，对Transformers来说不是很好吗？</p><p>Transformers能从抛弃注意力中获益吗？Yann Dauphin和合作者最近的工作提出了另一种ConvNet架构。Transformers也可能最终会做一些类似于ConvNets的事情。</p><p><img src="https://i.loli.net/2020/10/20/x8a9EmGXlARLJpw.png" alt="img"></p><h4 id="为什么transformers这么难训练">为什么Transformers这么难训练？</h4><p>阅读新的Transformer论文让我觉得，在确定最佳学习率表、预热策略和衰减设置时，训练这些模型需要一些类似于黑魔法的东西。这可能仅仅是因为模型太大，而且所研究的NLP任务非常具有挑战性。</p><p>但是最近的结果表明，这也可能是由于结构中归一化和残差连接的特定组合导致的。</p><p>在这一点上我很在意，但是也让我感到怀疑：我们真的需要代价昂贵的成对的多头注意力结构，超参数化的MLP子层以及复杂的学习计划吗？</p><p>我们真的需要具有大量碳足迹的（译者注：有人提出现在训练一个模型相当于5辆汽车一天的排碳量）大规模模型吗？</p><p>具有良好归纳偏差的架构难道不容易训练吗？</p><p>参考</p><blockquote><p><a href="https://graphdeeplearning.github.io/post/transformers-are-gnns/" target="_blank" rel="noopener" class="uri">https://graphdeeplearning.github.io/post/transformers-are-gnns/</a></p><p><a href="https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/104666156" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/104666156</a></p></blockquote><h3 id="总结如下">总结如下</h3><p>句子就是由词全连接而成的图</p><p>如果我们要执行多个并行的邻域聚合头，并且用注意力机制（即加权和）替换领域上的求和 ，我们将获得图注意力网络（GAT）。加上归一化和前馈MLP，瞧，我们就有了Graph Transformer！</p><p>Transformers是以多头注意力作为邻居聚合函数的GNNs（<strong>GNNs with multi-head attention</strong> as the neighbourhood aggregation function ）。<strong>标准GNNs从其局部邻域节点j∈N(i) 聚合特征，而NLP的<code>Transformers将整个句子视为局部邻域</code>，在每个层聚合来自每个单词j∈S的特征。</strong></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      transformer和GNN的关系理解,翻译自南洋理工大学助理研究员Chaitanya Joshi的博客
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-10-17-论文分享</title>
    <link href="http://yoursite.com/2020/10/17/2020-10-17-%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"/>
    <id>http://yoursite.com/2020/10/17/2020-10-17-%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/</id>
    <published>2020-10-17T13:36:45.000Z</published>
    <updated>2020-10-20T11:17:14.332Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言">前言</h3><p>本次阅读的论文是《Greedy Layerwise Learning Can Scale to ImageNet》，本周日论文分享组会中汇报</p><h3 id="论文">论文</h3><blockquote><p>《Greedy Layerwise Learning Can Scale to ImageNet》</p><p>ICML 2019</p><p>code url （pytorch） ：<a href="https://github.com/eugenium/layerCNN" target="_blank" rel="noopener" class="uri">https://github.com/eugenium/layerCNN</a></p></blockquote><p>两年前的code了 ，最近一次提交也是一年前，更新了readme说明部分。但是为什么在论文中没有说明呢？</p><p>看了代码，发现好像没有什么特别之处，就是这样一个逻辑</p><h4 id="背景">背景</h4><p>浅层监督的1隐藏层（ 1-hidden layer ）神经网络具有许多有利的属性，这些属性使它们比深层次的同类神经网络更易于解释，分析和优化，但缺乏表示能力。</p><p>深度神经网络（CNN）并不一定需要共同学习各个CNN层以获得高性能。</p><h4 id="问题">问题</h4><p>同时多层网络较为复杂，尚不清楚各层如何协同工作以实现高精度的预测。</p><p>就计算和内存资源而言，端到端的反向传播效率可能较低。</p><h4 id="解决">解决</h4><p>本文主要工作就是使用1隐藏层学习问题来逐层顺序构建深度网络，从而可以从浅层网络继承有利属性。</p><p>将上述这种greedy layerwise learning 拓展到了imagenet和CIFAR-10数据集等大型数据集上，应用场景是图像分类任务。</p><p>greedy approach 将更少地依赖于获得完整的梯度。不需要存储大多数中间激活，也不需要计算大多数中间梯度。 这样不需要很多的计算资源。本文就是解决将分层训练策略（ layer-wise training strategies）应用到大规模数据集的问题</p><h4 id="结果">结果</h4><p>使用一组简单的架构和培训构想，我们发现解决顺序出现的1隐藏层辅助问题会导致CNN超过ImageNet上的AlexNet性能。</p><h4 id="背景知识">背景知识</h4><h5 id="深层网络的贪婪逐层预训练方法greedy-layer-wise-train">深层网络的贪婪逐层预训练方法（greedy layer-wise train）</h5><p>每次只训练网络中的一层，即我们首先训练一个只含一个隐藏层的网络，仅当这层网络训练结束之后才开始训练一个有两个隐藏层的网络，以此类推。</p><p>在每一步中，我们把已经训练好的前k-1层固定，然后增加第k层（也就是将我们已经训练好的前k-1的输出作为输入）。每一层的训练可以是有监督的（例如，将每一步的分类误差作为目标函数），但更通常使用无监督方法（例如自动编码器）。</p><p>这些各层单独训练所得到的权重被用来初始化最终（或者说全部）的深度网络的权重，然后对整个网络进行“微调”（即把所有层放在一起来优化有标签训练集上的训练误差）。</p><p>考虑一个神经网络，如下图所示。它的输入是6维向量，输出是3维向量，代表输入样本属于三个类别的概率。</p><p><img src="https://i.loli.net/2020/10/18/OUnJaYGuvNDTzq1.png" alt="img"></p><p>最开始我们通过高斯分布随机初始化网络参数，然后逐层地优化网络参数。首先第一层。如下图，我们只保留输入层Input和第一个隐藏层Features I，其余层去掉。</p><p>之后，加入一个输出层，该<strong>输出层的输出向量维度和输入层一样</strong>，从而构成一个自编码器。我们训练这个自编码器，便可以得到第一层的网络参数，即绿线部分。</p><p><img src="https://i.loli.net/2020/10/18/SeutKCU7da1ImxF.png" alt="img"></p><p>然后是第二层的网络参数。如下图，我们只保留原始神经网络中的第一个隐藏层和第二个隐藏层，其余层去掉。</p><p>之后添加一个输出层，其输出向量维度和第一个隐藏层维度一样，从而构成一个自编码器，自编码器的输入是第一个隐藏层。</p><p>优化这个自编码器，我们就可以得到第二层网络参数，即红线部分。</p><p><img src="https://i.loli.net/2020/10/18/HVRc9A8xG5y2Mvj.png" alt="img"></p><p>优化这两个自编码器的过程就是逐层贪婪预训练。由于每个自编码器都只是优化了一层隐藏层，所以每个隐藏层的参数都只是局部最优的。</p><p>优化完这两个自编码器之后，我们把优化后的网络参数作为神经网络的初始值，之后微调（fine tune）整个网络，直到网络收敛。</p><h4 id="模型">模型</h4><h5 id="模型架构">模型架构</h5><p><img src="https://i.loli.net/2020/10/18/3wXjSyEbUKnsP7A.png" alt="image-20201018104055941"></p><p><img src="https://i.loli.net/2020/10/18/jR9JAHOolUP6Bmv.png" alt="image-20201018104658134"></p><p><img src="https://i.loli.net/2020/10/18/IXaq52BcrGbgyDk.png" alt="image-20201018104744620"></p><p>Xj+1 ：先对上一次训练的结果进行下采样 （ invertible downsampling 可逆下采样操作），然后进行CNN ， 再进行RELU函数</p><p>Ｚj+1 : 对Xj+1 进行辅助分类器操作得到，预测zj计算中间分类输出。</p><p><img src="https://i.loli.net/2020/10/18/ZlFPWJGkA5mHBtv.png" alt="image-20201018105215075"></p><p>根据1式，xj已经经过了一层卷积，又W是从0开始的，所以下标只有k-2</p><p><img src="https://i.loli.net/2020/10/18/dPVRxjHkbNmGvws.png" alt="image-20201018105539176"></p><h5 id="损失函数">损失函数</h5><p><img src="https://i.loli.net/2020/10/18/8vO62gaeNxE9Bnl.png" alt="image-20201018104427451"></p><h5 id="算法流程">算法流程</h5><p><img src="https://i.loli.net/2020/10/18/2PJMlYuLHEW4XQN.png" alt="image-20201018104104058"></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      记录阅读的论文
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-10-13-小样本学习</title>
    <link href="http://yoursite.com/2020/10/13/2020-10-13-%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yoursite.com/2020/10/13/2020-10-13-%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0/</id>
    <published>2020-10-13T07:44:01.000Z</published>
    <updated>2020-10-20T11:21:02.117Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言">前言</h3><p>最近了解了一下小样本学习的相关知识，这几天看了几篇论文介绍如下</p><h3 id="综述">综述</h3><p>Generalizing from a Few Examples : A Survey on Few-Shot Learning</p><h4 id="简介">简介</h4><p>问题</p><p>（a）例如典型的 MNIST 分类问题，一共有 10 个类，训练集一共有 6000 个样本，平均下来每个类大约 600 个样本，但是我们想一下我们人类不需要这么多样本，这表明当前的深度学习技术和我们人类智能差距还是很大的，要想弥补这一差距，<strong>小样本学习是一个很关键的问题。</strong></p><ol start="2" type="a"><li>如果想要构建新的数据集（以分类数据集为例），我们需要<strong>标记大量的数据</strong>，但是有的时候标记数据集需要某些领域的专家（例如医学图像的标记），这费时又费力，因此<strong>如果我们可以解决小样本学习问题，只需要每个类标记几张图片就可以高准确率的给剩余大量图片自动标记。</strong></li></ol><p><strong>以上两个原因使得小样本学习的研究成为热点。</strong></p><p><strong>小样本学习（Few-Shot Learning，以下简称 FSL ）</strong>用于解决当可用的数据量比较少时，如何提升神经网络的性能。</p><h4 id="定义">定义</h4><p>以下是FSL的一些符号术语</p><p><img src="https://i.loli.net/2020/10/15/Wy3Yzfv8OibZsgS.png" alt="image-20201015093502514"></p><p><img src="https://i.loli.net/2020/10/15/3Dy7SHqXgEIPwUB.png" alt="image-20201015164223838"></p><p>FSL是机器学习的一个子领域，所以先来介绍一下机器学习定义</p><p>机器学习：</p><p><img src="https://i.loli.net/2020/10/15/kNUTzD2WLYIgpZf.png" alt="image-20201015093951256"></p><p>给定一个任务T，任务的性能P，给定一些经验E，比如通过训练学习得到的标注数据，可以提升任务T的性能P</p><p>以下是简单例子</p><p><img src="https://i.loli.net/2020/10/15/4pRq6PcLUYgbKC5.png" alt="image-20201015094231448"></p><p>在传统机器学习中需要很多样本信息，但是在实际中是很困难的，所以FSL就是解决这类问题。在训练集Dtrain中提供的监督信息有限的情况下，包括（输入xi和对应的输出yi），来获得好的学习性能</p><p>FSL 小样本学习</p><p><img src="https://i.loli.net/2020/10/15/KT7aSbA6hg3iXVq.png" alt="image-20201015094815702"></p><p>和机器学习定义类似，只是E包含了有限的监督信息的样例（example）。也即，每个类class中包含了很少的有标签样例。</p><p><strong>小样本分类</strong>主要就是学习一个分类器h，能够预测每一个输入数据xi的标签yi，通常使用的是N-way-K-shot分类方法，N个类，每个类有K个例子。其中 Dtrain包括I=KN个例子</p><p><img src="https://i.loli.net/2020/10/20/h2EcFBrTmZvMGL4.png" alt="image-20201015095811521"></p><p>只要关注的是image classification .可以看到相比于机器学习，FSL在经验E部分多了一个prior knowledge，也就是如果只是一些少的监督信息的样例不足以去解决tesk T中的问题，如图像分类。所以还是<strong>需要结合一些先验知识。</strong></p><blockquote><p>处理目标T（target T）时，在E中的监督信息如果只有一个例子的话，那么就是one-shot learning。</p><p>处理目标T（target T）时，当E中没有任何监督信息例子的话，那么就是zero-shot learning。</p><p>Zero-shot Learing <strong>就是训练样本里没有这个类别的样本，但是如果我们可以学到一个好的映射，这个映射好到我们即使在训练的时候没看到这个类，但是我们在遇到的时候依然能通过这个映射得到这个新类的特征。</strong></p><p><strong>即：训练集</strong>中<strong>没有出现过</strong>的<strong>类别</strong>，能自动创造出相应的映射。</p></blockquote><p>主要问题</p><p>在机器学习中寻找最适合的假设时通常都是通过找到一组最优的参数来确定这个假设，并通过给定的训练集，最小化损失函数这一目标来指示最优参数的搜索，<strong>最小化损失函数</strong>如下所示：</p><p>　　　　<img src="https://i.loli.net/2020/10/15/kHsi7YgaSIVQeGr.png" alt="img"></p><p>　　在训练模型中，我们是通过训练集来拟合真实分布，我们训练出来的分布和真实分布往往不一样，这<strong>中间的差值称为期望风险（期望损失）</strong>，表达式如下：</p><p>　　　　<img src="https://i.loli.net/2020/10/15/sXLuTmn7SzlaFUV.png" alt="img"></p><p>　理论上说，<strong>让期望风险最下化才能逼近真实分布</strong>，但因为你并不知道真实分布，所有最小化期望风险是无法实现的</p><p>在机器学习中通常用经验风险来替换期望风险，经验风险就是在训练集上预测的结果和真实结果的差异，也是我们常说的<strong>损失函数</strong>，表达式如下：</p><p>　　　　<img src="https://i.loli.net/2020/10/15/UO37Pd9GIqjxBDL.png" alt="img"></p><p>　　我们给出下面一个符号描述：</p><p>　　　　<img src="https://i.loli.net/2020/10/15/OK6DQwaZ34Bbct2.png" alt="img"></p><p>h^是真实分布的假设</p><p>h∗是假设空间H中最接近h^的假设</p><p>而hI是通过最小化经验损失得到的假设。根据机器学习中的误差分解可得：</p><p>　　　　<img src="https://i.loli.net/2020/10/15/h9Loiz4wqbFX2Qj.png" alt="img"></p><p>　　等式右边第一项表示的是<strong>假设空间H中最优的假设和真实假设的误差</strong>，这一项其实<strong>由所选择的模型和参数的初始化分布决定的</strong>，这也就是为什么有的时候，模型选择的简单了，给再多的数据也训练不好，欠拟合。</p><p>第二项就是<strong>训练得到的假设和H中最优假设的误差</strong>，我们训练得到的假设也是从假设空间H中选择的，但有时候会陷入局部最优，或者提供的训练数据分布有偏差，导致无法到全局最优。</p><p>　　但理论上对于第二项，当样本数量II足够大时，有：</p><p>　　　　<img src="https://i.loli.net/2020/10/15/wnJhz4qAXgjUrQR.png" alt="img"></p><p><img src="https://i.loli.net/2020/10/15/diHcTjyW1hPGMNQ.png" alt="image-20201015102743694"></p><p>　<strong>传统的机器学习都是建立在大规模的训练数据上的，因此εest(H,I)是很小的，但是在FSL任务上，训练数据很小，因此εest(H,I)是很大的，</strong>此时采用传统的训练模式，如softmax+交叉熵，是极容易陷入过拟合的。</p><p><strong>所以需要更多的先验知识</strong></p><p>具体的图如下：</p><p><img src="https://i.loli.net/2020/10/15/mp3Bjruof89JskS.png" alt="image-20201015103318798"></p><p>解决方法-分类：</p><p><img src="https://i.loli.net/2020/10/15/XS5ZnfOL8mMHow2.png" alt="image-20201015103338811"></p><p>假设空间的确定就是模型函数的可行性范围</p><p><strong>Data</strong></p><p>　　Data就是通过先验知识来做<strong>数据增强</strong>， 数据量增大可以获得可靠的hI，自然能解决上述问题。</p><p>通常进行手动操作对FSL进行数据预处理。例如在图像上，比如图片的旋转剪切放缩等，句子中的同义词替换等，以及复杂的生成模型生成和真实数据相近的数据。数据增强的方式有很多种，大量的合适的增强一定程度上可以缓解FSL问题，但需要耗费大量的精力，以及很强的域知识，只是针对特定的数据集，很难应用到其它数据集中，因此<strong>不能很好的解决FSL问题</strong></p><p>分类</p><p><img src="https://i.loli.net/2020/10/15/umBnUeEhCTFfrbo.png" alt="image-20201015110602917"></p><p><img src="https://i.loli.net/2020/10/15/VgKI5RENmQXtipo.png" alt="image-20201015110557216"></p><p><strong>Model</strong></p><p>　　通过先验知识来<strong>限制模型复杂度，降低假设空间H的大小</strong>，使得当前的数据集可以满足</p><p>如果我们想使用机器学习模型来解决FSL问题，我们需要使用假设空间H很小的模型，这样样本复杂度也就小了，对于一些简单的任务，这样是可行的，但是对于复杂的任务，小的模型会导致εapp(H)很大，而现实中大多数任务都很复杂，它们的特征很多，且特征维度也很高。</p><p>因此我们<strong>只能一开始给一个假设空间H很大的模型，然后通过一些先验知识将这个空间中无效的hypothesis去掉，缩小假设空间H</strong>，但实际上和<strong>模型剪枝</strong>中的理念类似，你一开始给一个小的模型，这个模型空间离真实假设太远了，而你给一个大的模型空间，它离真实假设近的概率比较大，然后通过先验知识去掉哪些离真实假设远的假设。</p><p><img src="https://i.loli.net/2020/10/15/1R2cuKxSPkaFwUb.png" alt="image-20201015110729588"></p><p><strong>Algorithm</strong></p><p>　　通过先验知识来提供一个好的搜索策略，可以是一个好的搜索起始点，也可以是一个明确的搜索策略，来寻找最优点。</p><p>在机器学习中我们<strong>通常使用SGD以及它的变体，如ADAM，RMSProp等来更新参数，寻找最优的参数</strong>，对应到假设空间H中最优的假设h∗。这种方式在有大量的数据集的情况下可以用来慢慢迭代更新寻找最优参数，但是在FSL任务中，样本数量很少，这种方法就失效了。在这一节，我们不再限制假设空间。</p><p>根据使用不同的先验知识，可以将ALGORITHM分为下面3类：</p><p><img src="https://i.loli.net/2020/10/15/lr86fNGOuwLtPZn.png" alt="image-20201015110816197"></p><p>　　接下来的工作都是围绕这几个方向展开来求解FSL问题。</p><p><img src="https://i.loli.net/2020/10/15/a5x4EvrUSj3WAl1.png" alt="image-20201015103435972"></p><h3 id="应用">应用</h3><p><strong>Few-shot Learning Meta Learning 在监督学习领域的应用。</strong></p><p><strong>Meta Learning研究Task！</strong></p><p>Meta Learning，又称为 learning to learn，在 meta training 阶段将数据集分解为不同的 meta task，去学习类别变化的情况下模型的泛化能力，在 meta testing 阶段，面对全新的类别，不需要变动已有的模型，就可以完成分类。<strong>如果我们构建的深度学习系统能够学到先验知识，并且能够利用这些知识，我们就可以在新的问题上学的更快更好</strong>！那么，这个就是Meta Learning要做的事情了</p><p><strong>不是要学一个具体的模型，我们要学的是一个先验知识</strong>。<strong>如果我们已有的先验知识来帮助我们解决新的问题，那么我们对于新的问题就可以不需要那么多的样本，从而解决 few-shot 问题</strong>。</p><p>形式化来说，few-shot 的训练集中包含了很多的类别，每个类别中有多个样本。在训练阶段，会在训练集中<strong>随机抽取</strong> C 个类别，每个类别 K 个样本（总共 CK 个数据），构建一个 meta-task，作为模型的支撑集（support set）输入；再从这 C 个类中剩余的数据中抽取一批（batch）样本作为模型的预测对象（batch set）。即要求模型从 C*K 个数据中学会如何区分这 C 个类别，<strong>这样的任务被称为 C-way K-shot 问题。</strong></p><p>图 1 展示的是一个 2-way 5-shot 的示例，可以看到 meta training 阶段构建了一系列 meta-task 来让模型学习如何根据 support set 预测 batch set 中的样本的标签；meta testing 阶段的输入数据的形式与训练阶段一致（2-way 5-shot），但是会在全新的类别上构建 support set 和 batch。每一行都是一个task，包含了task的train set和test set。</p><p>我们可以把<strong>每一个task当做一个meta learning的训练样本</strong>。我们要通过多种task的训练，从而在Meta-test的时候也就是在新的task上取得好效果。</p><p><img src="https://i.loli.net/2020/10/15/n9C1hakXsYGuQAf.jpg" alt="img">▲ 图1：Few-shot Learning示例</p><h4 id="通常解决办法">通常解决办法</h4><h4 id="hypernetwork-生成参数">HyperNetwork 生成参数</h4><p><img src="https://i.loli.net/2020/10/15/oEJPShp9M2k5Nzi.png" alt="image-20201015163737823"></p><p>HyperNetwork 简单说就是<strong>用一个网络来生成另外一个网络的参数</strong>。</p><p>那么我们这里非常直接，我们的设想就是<strong>希望用一个hypernetwork输入训练集数据，然后给我输出我的对应模型也就是上图f的参数，我们希望输出的这个参数能够使得在测试图片上取得好的识别效果</strong>。</p><p>有了这样设计，这个hypernetwork其实就是一个meta network。大家可以看到，<strong>本来基本的做法是用训练集直接训练这个模型f，但是现在我们用这个hypernetwork不训练了，直接给你输出参数，这等价于hypernetwork学会了如何学习图像识别，这也是为什么meta learning也同时叫做learning to learn的原因。</strong></p><h4 id="训练">训练</h4><p><strong>训练过程</strong>中，每次训练<strong>（episode）</strong>（Dtrain | Dtest）都会采样得到不同 meta-task，所以总体来看，训练包含了不同的类别组合，这种机制使得模型学会不同 meta-task 中的共性部分，比如如何提取重要特征及比较样本相似等，忘掉 meta-task 中 task 相关部分。通过这种学习机制学到的模型，在面对新的未见过的 meta-task 时，也能较好地进行分类。</p><p>这里有个所谓的<strong>episodic training</strong>！一个<strong>episode就是包含了一个task，有训练集有测试集</strong>。我们<strong>使用训练集输入到hypernetwork，得到f的参数，然后使用测试集输入到f 得到预测的标签，最后用测试集的样本标签得到模型的loss，之后就用梯度下降进行训练。</strong>所以我们可以看到，整个模型是端到端的。通过大量的episodic training，也就是大量的task进行训练，我们就可以训练出一个模型出来。</p><p>在 meta training 阶段将数据集分解为不同的 meta task，<strong>去学习类别变化的情况下模型的泛化能力，</strong>在 meta testing 阶段，面对全新的类别，<strong>不需要变动已有的模型，就可以完成分类</strong>。</p><blockquote><p><a href="https://zhuanlan.zhihu.com/p/61215293" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/61215293</a></p></blockquote><h3 id="其它">其它</h3><h4 id="inductive-learning-与-transductive-learning">inductive learning 与 transductive learning</h4><p>在训练过程中，已知testing data（unlabelled data）是transductive learing</p><p>在训练过程中，并不知道testing data ，训练好模型后去解决未知的testing data 是inductive learing</p><p>通俗地来说inductive learning是特殊到一般的学习，测试数据只是用来测试这个通用模型的好坏；transductive learning是特殊到特殊的学习，目的就是解决target domain的问题。</p><p><img src="https://i.loli.net/2020/10/15/2HOTN5fb3Lvz7pD.png" alt="img"></p><p>现在有这个问题，已知ABC的类别，求问号的类别，</p><p>inductive learning就是只根据现有的ABC，用比如kNN距离算法来预测，在来一个新的数据的时候，还是只根据5个ABC来预测。</p><p>transductive learning直接以某种算法观察出数据的分布，这里呈现三个cluster，就根据cluster判定，不会建立一个预测的模型，如果一个新的数据加进来 就必须重新算一遍整个算法，新加的数据也会导致旧的已预测问号的结果改变</p><h4 id="representation-learning">representation learning</h4><p>在机器学习领域，表征学习（或<strong>特征学习</strong>）是一种将原始数据转换成为能够被机器学习有效开发的一种技术的集合。在特征学习算法出现之前，机器学习研究人员需要利用手动特征工程（manual feature learning）等技术从原始数据的领域知识（domain knowledge）建立特征，然后再部署相关的机器学习算法。</p><p>特征学习弥补了这一点，它使得机器不仅能学习到数据的特征，并能利用这些特征来完成一个具体的任务。</p><p>表征学习的目标不是通过学习原始数据预测某个观察结果，而是学习数据的底层结构（underlying structure），从而可以分析出原始数据的其它特性。</p><p>特征学习可以被分为两类：监督式特征学习（Supervised Representation Learning）和无监督式特征学习（Unsupervised Representation Learning）。</p><p>在监督特征学习中，被标记过的数据被当做特征用来学习。例如神经网络（Neural Networks），多层感知器（Multi-Layer Perception），监督字典学习（Supervised Dictionary Learning）。</p><p>在无监督特征学习中，未被标记过的数据被当做特征用来学习。例如无监督字典学习（Unsupervised Dictionary Learning），主成分分析（Principal Component Analysis），独立成分分析（Independent Component Analysis），自动编码（Auto-encoders），矩阵分解（Matrix Factorization） ，各种聚类分析（Clustering）及其变形。</p><blockquote><p><a href="https://www.jiqizhixin.com/graph/technologies/64d4c374-6061-46cc-8d29-d0a582934876" target="_blank" rel="noopener" class="uri">https://www.jiqizhixin.com/graph/technologies/64d4c374-6061-46cc-8d29-d0a582934876</a></p></blockquote><h3 id="论文-edge-labeling-graph-neural-network-for-few-shot-learning">论文-Edge-Labeling Graph Neural Network for Few-shot Learning</h3><p>CVPR 2019</p><p>code url： <a href="https://github.com/khy0809/fewshot-egnn" target="_blank" rel="noopener" class="uri">https://github.com/khy0809/fewshot-egnn</a></p><h4 id="abstract">Abstract</h4><p>在本文中，提出了一种新颖的边标记图神经网络（edge-labeling graph neural network ）（EGNN），该网络将边标记图上的深层神经网络应用于小样本学习。</p><p>以前在小样本学习中使用的图神经网络（GNN）方法是基于节点标记框架（ node-labeling framework）的，该框架对聚类内的相似度和聚类间不相似度进行隐式地建模（implicitly modeling）。 相反，提出的EGNN学会<strong>预测图上的边标签</strong>，而不是节点标签，从而通过直接利用聚类内的相似性和聚类间不相似性来<strong>迭代更新边标签</strong>，从而实现显式聚类的进化。 <strong>它也非常适合在各种类别上执行而无需重新训练，并且可以轻松扩展以执行直推推理（transductive inference）。</strong></p><p>EGNN的参数是通过带有边标记损失（edge-labeling loss）的episodic training来学习的，从而获得了针对未见的低数据问题的可普遍推广的模型。</p><p>在带有两个基准数据集的有监督和半监督的小样本图像分类任务上，提出的EGNN大大提高了现有GNN的性能。</p><p>对GNN的改进，提高了GNN的性能</p><p>预测图上的边标签，而不是节点标签，直接利用聚类内的相似性和聚类间不相似性来<strong>迭代更新边标签</strong>，从而实现显式聚类的进化。</p><h4 id="背景">背景</h4><p>GNN可以很好的处理数据之间丰富的关系结构，是迭代地通过消息传递（message passing）从邻居节点进行特征聚合。而小样本学习算法已显示要求充分利用support集和query集之间的关系，因此使用GNN可以自然地解决小样本学习问题。</p><p>GNN解决小样本学习问题的思路：</p><p>先建立一个从support到query的全连接的图，节点使用嵌入向量和独热编码label表示，通过邻居聚合迭代的更新节点feature，完成对query的分类。</p><h4 id="问题">问题</h4><p>然而以前在小样本学习中使用的图神经网络（GNN）方法是基于节点标记框架（ node-labeling framework）的，该框架对聚类内的相似度和聚类间不相似度进行隐式建模（implicitly modeling）。</p><h4 id="解决">解决</h4><p>提出的EGNN学会<strong>预测图上的边标签</strong>，而不是节点标签，从而通过直接利用聚类内的相似性和聚类间不相似性来<strong>迭代更新边标签</strong>，从而实现显式聚类的进化。</p><p><img src="https://i.loli.net/2020/10/15/ZewrSL3qdRy68iD.png" alt="image-20201015191818980"></p><h4 id="贡献">贡献</h4><p>EGNN首次使用边标注的方式构建，利用的是 episodic training framework</p><p>在小样本分类的有监督和半监督任务中，表现超过了所有的GNN。同时，证明显式聚类和分别利用类内相似、类间不同都是有效的。</p><h4 id="模型">模型</h4><p>术语介绍</p><p><img src="https://i.loli.net/2020/10/15/63wSKIRlfJGjEXB.png" alt="image-20201015193021049"></p><p>过程</p><p>每个episode中的支持集S都用作标记的训练集，在该训练集上训练模型以最小化其在查询集Q上的预测损失。此训练过程逐个episode反复进行，直到收敛为止。</p><p><img src="https://i.loli.net/2020/10/15/SaoD8MkpsU5uPnA.png" alt="image-20201015193456782"></p><p><img src="https://i.loli.net/2020/10/15/VIE751M3FlTBjKw.png" alt="image-20201015200148142"></p><p>给出tesk所有样本的特征表示，那么就可以构建一个全连接图，其中每个节点代表一个样本，每个边代表两个连接点之间的关系。</p><h4 id="伪代码">伪代码</h4><p><img src="https://i.loli.net/2020/10/15/vIdrJxt1TLEwNRU.jpg" alt="在这里插入图片描述"></p><p>我们可以看到，这个整体更新的方式像极了EM算法。 第一步是获取特征，<strong>这个embedding的网络在文章的图3。</strong></p><p>点特征先通过图(a)的卷积嵌入网络进行初始化， 边特征也被初始化如下面公式 第二步是初始化边</p><p><img src="https://i.loli.net/2020/10/15/LU549hpFxOYynQR.png" alt="image-20201015194613945"></p><p>如果两个点是同一个标签，那么就是1， 否则就是0</p><p><img src="https://i.loli.net/2020/10/15/xACgHdYarn32fpD.jpg" alt="在这里插入图片描述"></p><p>如果其是同一类，或者其不是同一类，或者其相邻节点不属于支持集</p><p>第三步是进入一个更新循环。</p><p>第四步是更新节点 <img src="https://i.loli.net/2020/10/15/eV9Zj1zlRCDBXTw.jpg" alt="在这里插入图片描述"></p><p>是把边的不同维数的特征（分别代表类内，和类间（相似性和不相似性））和节点特征相乘，然后2个结果做连接操作，作为参数传入神经网络，得到更新之后的节点特征，这个时候的节点特征就是包含了相应的边的语意信息了。信息更加饱满。</p><p>可以看到图中就是先进行算法，然后连接操作，然后进入一个多层感知机，最后得到更新之后的节点信息。 其中：eijd是做了归一化操作，f是点转移网络（transformation network）</p><p>第五步是更新边 <img src="https://i.loli.net/2020/10/15/kcoI5NE1KxQgACq.jpg" alt="在这里插入图片描述"></p><p>然后不断进行第四五步的循环L次，结束后计算出我们要测试的数据所属于的类的概率。</p><p>最终的边标签预测结果就是最后的边特征</p><p><img src="https://i.loli.net/2020/10/15/I36BJMaP2WlZy8g.png" alt="image-20201015200438652"></p><h4 id="损失函数">损失函数</h4><p><img src="https://i.loli.net/2020/10/15/sXD6vTOF1p7weiC.png" alt="image-20201015200559689"></p><p><img src="https://i.loli.net/2020/10/15/XwjzkxMNTQ1hGv9.png" alt="image-20201015200725295"></p><p>L代表第L层，M代表M个任务，这个是episodic training可以理解为多任务。</p><p>λ是学习率，L是二元交叉滴损失，Y是真实的标签，Yˆ是预测的标签。就是说损失函数是所有M个任务L层的所有损失的和。</p><h3 id="小样本的预训练">小样本的预训练</h3><p><strong>预训练</strong>是（Pre-training）大家都熟悉且非常有效的获取先验知识的方法。具体就是在大型数据集上，学习一个强大的神经网络作为特征提取器，例如CV里面常见的在ImageNet上预训练的ResNet网络，或是NLP里面在Wikipedia上训练的BERT，都代表一种特征表达的先验知识。</p><p><strong>在预训练基础上，我们只需在样本数量少的目标任务中，微调部分（例如只训练最后一层fc分类器）或者全部网络的参数，便得到了一个可以解决小样本学习问题的模型。</strong></p><p>预训练相当于给了小样本学习一个好的起点，就像一个人在上课前预习了大量的知识点。不过想要更上一层楼，还需要<strong>有效的学习方法</strong>。<strong>元学习</strong>（meta learning）的目的就是找到这种方法。具体来说，我们可以从<strong>预训练集</strong>中，每次采样出来一个“沙盒”版小样本任务，例如选5个类，每个类选5张图片作为训练集（support set），再选15张作为测试集（query set），然后我们要求模型在support set训练的结果，能够在query set上面取得好的表现。其实这种学习策略在我们身边随处可见，例如准备考试的时候，我们会提前做一些模拟测试，了解题型，规划答题节奏等等，这就是一种元学习。在小样本学习的实际操作中，我们可以使用元学习训练一个模型的初始化参数（MAML），或是一个分类器参数的生成网络（LEO）等等。通过元学习得到的知识，就构成了一种学习方法的先验知识，在预训练的网络之上，进一步提升小样本学习的表现。</p><p><strong>预训练是小样本学习中一个核心的环节</strong>，无论是基于微调的，还是基于元学习的方法，都以预训练为开始。那么从常理来说，更强的预训练，应该会带来更好的小样本学习的表现，例如在现有文献中，使用更深层的神经网络架构<strong>WRN-28-10</strong>的微调结果，往往会比相对较浅的<strong>ResNet-10</strong>表现好很多。</p><blockquote><p>利用其它的网络进行预训练，然后再进行元学习+微调，或者直接微调操作</p></blockquote><p><strong>小样本学习的解决思路</strong>，可以用下面这张图来概括：我们先在一个大的数据集 D 上面预训练一个特征提取网络Ω ，之后我们既可以直接使用 Ω在每一个小样本任务中微调(红色方块的Fine-Tuning);</p><p>也可以进一步使用元学习(Meta-Learning)，将D 拆成一个个由support set S和query set Q 组成的沙盒任务（Si，Qi） ，训练高效的学习方法；元学习结束以后，我们就可以用这种高效的学习方法，在小样本学习的任务中进行微调(绿色方块的Fine-Tuning)。</p><p><img src="https://i.loli.net/2020/10/20/SGetivHsnoBFrIk.jpg" alt="img">小样本学习的两种解决思路。</p><hr><h3 id="小样本的演变">小样本的演变</h3><p>小样本学习一般会简化为N-way K-shot问题，如图[1]。其中N代表类别数量，K代表每一类中(支持集)的样本量；</p><p><img src="https://i.loli.net/2020/10/20/9lRfqxs1U8uP6Fo.jpg" alt="图[1] N-way K-shot"></p><p>解决分类问题，人们最先想到的是采用传统监督学习的方式，直接在训练集上进行训练，在测试集上进行测试，如图[2]，但神经网络需要优化的参数量是巨大的，<strong>在少样本条件下，几乎都会发生过拟合</strong>；</p><p><img src="https://i.loli.net/2020/10/20/xrJlIDiOtwfLoj8.jpg" alt="图[2] 传统监督学习"></p><p>为了解决上述问题，人们首先想到的是通过<strong>使用迁移学习+Fine-tune的方式</strong>，<strong>利用Base-classes中的大量数据进行网络训练，得到的Pre-trained模型迁移到Novel-classes进行Fine-tune</strong>，如图[3]。虽然是Pre-trained网络+Fine-tune微调可以避免部分情况的过拟合问题，但是当数据量很少的时候，<strong>仍然存在较大过拟合的风险</strong>。</p><p><img src="https://i.loli.net/2020/10/20/H84WS5KRIuoQ2bs.jpg" alt="图[3] Pre-trained网络+Fine-tune微调"></p><p>接下来讲的就是小样本学习中极具分量的<strong>Meta-learning</strong>方法，现阶段绝大部分的小样本学习都使用的是Meta-learning方法。Meta-learning，即learn to learn，翻译成中文是元学习。Meta-learning共分为Training和Testing两个阶段，Training阶段的思路如图[4]。简单描述下流程：</p><p>1：将训练集采样成Support set和Query set两部分；</p><p>2：基于Support set生成一个分类模型；</p><p>3：利用模型对Query set进行分类预测生成predict labels；</p><p>4：通过query labels和predict labels进行Loss(e.g., cross entropy loss )计算，从而对分类模型中的参数θ进行优化。</p><p><img src="https://i.loli.net/2020/10/20/yGt6YfLuJADW5rx.jpg" alt="图[4] Meta-learning Training阶段思路"></p><p>Testing阶段的思路如图[5]，利用Training阶段学来的分类模型在Novel class的Support set上进行进一步学习，学到的模型对Novel class的Query set进行预测。</p><p><img src="https://i.loli.net/2020/10/20/oE2RtPAfVxOrI9L.jpg" alt="图[5] Meta-learning Testing阶段思路"></p><p>介绍到这里，Meta-learning的整体流程的流程就介绍完了，如图[6];</p><p>现在反过来看，Meta-learning核心点之一是如何通过少量样本来学习这个分类模型，即图[6]中的keyu部分。</p><p><img src="https://i.loli.net/2020/10/20/FNi5PM4fruDKWaH.jpg" alt="图[6] Meta-learning整体流程以及key point"></p><h3 id="star-预训练与微调"><span class="github-emoji" style="display:inline;vertical-align:middle;color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8">⭐</span> 预训练与微调</h3><p>假设我们想从图像中识别出不同种类的椅子，然后将购买链接推荐给用户。一种可能的方法是先找出100种常见的椅子，为每种椅子拍摄1,000张不同角度的图像，然后在收集到的图像数据集上训练一个分类模型。这个椅子数据集虽然可能比Fashion-MNIST数据集要庞大，但样本数仍然不及ImageNet数据集中样本数的十分之一。这可能会导致适用于ImageNet数据集的复杂模型在这个椅子数据集上过拟合。同时，因为数据量有限，最终训练得到的模型的精度也可能达不到实用的要求。</p><p>为了应对上述问题，一个显而易见的解决办法是收集更多的数据。然而，收集和标注数据会花费大量的时间和资金。例如，为了收集ImageNet数据集，研究人员花费了数百万美元的研究经费。虽然目前的数据采集成本已降低了不少，但其成本仍然不可忽略。</p><p>另外一种解决办法是应用迁移学习（transfer learning），将从源数据集学到的知识迁移到目标数据集上。例如，虽然ImageNet数据集的图像大多跟椅子无关，但在该数据集上训练的模型可以抽取较通用的图像特征，从而能够帮助识别边缘、纹理、形状和物体组成等。这些类似的特征对于识别椅子也可能同样有效。</p><p>本节我们介绍迁移学习中的一种常用技术：<strong>微调（fine tuning）</strong>。如图所示，微调由以下4步构成。</p><ol type="1"><li><p>在源数据集（如ImageNet数据集）上预训练一个神经网络模型，即源模型。</p></li><li><p>创建一个新的神经网络模型，即目标模型。它复制了源模型上除了输出层外的所有模型设计及其参数。我们假设这些模型参数包含了源数据集上学习到的知识，且这些知识同样适用于目标数据集。我们还假设源模型的输出层与源数据集的标签紧密相关，因此在目标模型中不予采用。</p></li><li><p>为目标模型添加一个输出大小为目标数据集类别个数的输出层，并随机初始化该层的模型参数。</p></li><li><p>在目标数据集（如椅子数据集）上训练目标模型。我们将从头训练输出层，而其余层的参数都是基于源模型的参数微调得到的。</p><p><img src="https://i.loli.net/2020/10/20/xMtWkTzv9hufrni.png" alt="image-20201019213025514"></p></li></ol><p><strong>当目标数据集远小于源数据集时，微调有助于提升模型的泛化能力。</strong></p><h4 id="小结">小结</h4><ul><li>迁移学习将从源数据集学到的知识迁移到目标数据集上。微调是迁移学习的一种常用技术。</li><li>目标模型复制了源模型上除了输出层外的所有模型设计及其参数，并基于目标数据集微调这些参数。而目标模型的输出层需要从头训练。</li><li>一般来说，微调参数会使用较小的学习率，而从头训练输出层可以使用较大的学习率。</li></ul><p><span class="github-emoji" style="display:inline;vertical-align:middle;color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8">⭐</span> 具体实例可以参考下面的链接</p><blockquote><p><a href="http://zh.gluon.ai/chapter_computer-vision/fine-tuning.html" target="_blank" rel="noopener" class="uri">http://zh.gluon.ai/chapter_computer-vision/fine-tuning.html</a></p><p><a href="https://zhuanlan.zhihu.com/p/35890660" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/35890660</a></p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      小样本学习
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-10-12-transformer图像处理论文</title>
    <link href="http://yoursite.com/2020/10/12/2020-10-12-transformer%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E8%AE%BA%E6%96%87/"/>
    <id>http://yoursite.com/2020/10/12/2020-10-12-transformer%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E8%AE%BA%E6%96%87/</id>
    <published>2020-10-12T14:17:41.000Z</published>
    <updated>2020-10-31T08:31:34.953Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言">前言</h3><h3 id="end-to-end-object-detection-with-transformers">End-to-End Object Detection with Transformers</h3><p>CVPR 2020</p><p>code url ： <a href="https://github.com/facebookresearch/detr" target="_blank" rel="noopener" class="uri">https://github.com/facebookresearch/detr</a></p><p>简易 code：</p><p><a href="https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/detr_demo.ipynb" target="_blank" rel="noopener" class="uri">https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/detr_demo.ipynb</a></p><h4 id="abstract">Abstract</h4><p>我们把目标检测看做是一种set prediction的问题，我们的方法也直接移除了一些人工设计的组件，例如NMS和anchor的生成。</p><p>我们的框架DETR，由两个部分构成，一是set-based的全局loss，使用bipartite matching (二分匹配)生成唯一的预测，二是transformer的encoder-decoder 结构。</p><p>只需提供固定大小学习到的目标查询集合，DETR推理出目标与全局图像上下文，直接并行地预测出结果。新的模型非常简单，不需要特定的库来支持。DETR在coco数据集上有着可以和faster-rcnn媲美的准确率与效率。而且它也能完成全景分割的任务。</p><h4 id="问题">问题</h4><p>目标检测的目标是预测一个bbox的集合和各个bbox的标签。目前的检测器不是直接预测一个目标的集合，而是使用替代的回归和分类去处理大量的propoasls、anchors或者window centers。</p><p>模型的效果会受到一系列问题的影响：后处理去消除大量重叠的预测、anchors的设计、怎么把target box与anchor关联起来。为了简化流程，我们提出一种直接set prediction的方式来消除这些替代的方法。</p><h4 id="解决">解决</h4><p>将目标检测看做是一种set prediction（序列预测）的问题，我们的方法也直接移除了一些人工设计的组件，例如NMS和anchor的生成。</p><p><img src="https://i.loli.net/2020/10/16/36UT8nHdBIKNQ4m.png" alt="DETR模型的大体结构"></p><p>DETR模型的大体结构</p><p>DETR可一次预测所有对象，并通过设置损失函数进行端到端训练，该函数执行预测对象与真实对象之间的<strong>二分匹配</strong>。</p><p>与大多数现有的检测方法不同，DETR不需要任何自定义层，因此可以在任何包含标准CNN和转换器类的框架中轻松重现。</p><h4 id="两大核心思想">两大核心思想</h4><p>1、transformer保证了attention，确保对一个实例的识别，是在整幅图的知识下进行的。</p><p>2、二分最大匹配，确保了一一对应的关系。</p><h4 id="局限性">局限性</h4><p>DETR在大型物体上表现出明显更好的性能，这可能是由于transformer的非局部计算所致。然而在小型物体上就表现出一般的性能</p><h4 id="模型">模型</h4><p><img src="https://i.loli.net/2020/10/16/cS3QMqNTJGykKZd.png" alt="image-20201016094016507"></p><p>DETR的整体结构Transformer类似：Backbone得到的特征铺平，加上Position信息之后送到一Encoder里，得到上下文信息。这100个candidates是被Decoder<strong>并行解码的</strong>（显存就很大，但实现的时候可写成不并行的），以得到最后的检测框。</p><p><img src="https://i.loli.net/2020/10/16/ZiGXu5nE3OF8syv.png" alt="image-20201013142352470"></p><h5 id="detr-encoder"><strong>DETR Encoder</strong></h5><p>网络一开始是使用Backbone（比如ResNet）提取一些feature，然后降维到d×HW。</p><p><img src="https://i.loli.net/2020/10/16/3C4qzOXKLsJco5H.png" alt="image-20201016094956398"></p><p>Feature降维之后与<strong>Spatial Positional Encoding相加</strong>，然后被送到Encoder里。</p><p>为了体现图像在x和y维度上的信息，作者的代码里<strong>分别计算了两个维度的Positional Encoding，然后Cat到一起。</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pos_x = torch.stack((pos_x[:, :, :, <span class="number">0</span>::<span class="number">2</span>].sin(), pos_x[:, :, :, <span class="number">1</span>::<span class="number">2</span>].cos()), dim=<span class="number">4</span>).flatten(<span class="number">3</span>)</span><br><span class="line">pos_y = torch.stack((pos_y[:, :, :, <span class="number">0</span>::<span class="number">2</span>].sin(), pos_y[:, :, :, <span class="number">1</span>::<span class="number">2</span>].cos()), dim=<span class="number">4</span>).flatten(<span class="number">3</span>)</span><br><span class="line">pos = torch.cat((pos_y, pos_x), dim=<span class="number">3</span>).permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br></pre></td></tr></tbody></table></figure><p>FFN、LN等操作也与Transformer类似。<strong>Encoder最后得到的结果是对N个物体编码后的特征。</strong></p><h5 id="detr-decoder"><strong>DETR Decoder</strong></h5><p>DETR Decoder的结构也与Transformer类似，<strong>区别在于Decoder并行解码N个object。</strong></p><p>每个Decoder有两个输入：一路是Object Query（或者是上一个Decoder的输出），另一路是Encoder的结果。</p><p>We add prediction FFNs and Hungarian loss after each decoder layer. All predictions FFNs share their parameters。</p><p>在每一个decode层都会添加FFN和Hungarian loss，并行计算出N个object ， FFN是共享参数的</p><p>object queries会输入到每一层中，我一开始不明白Object Query是怎样得到的。后来从代码看，<strong>Object Query是一组nn.Embedding的weight（就是一组学到的参数）。</strong></p><p><strong>最后一个Decoder后面接了两个FFN，分别预测检测框及其类别。</strong></p><p>仔细看论文和代码，才发现它的输出是定长的（N）：100个检测框和类别，这种操作可能跟COCO评测的时候取top 100的框有关。100比一个图像中普遍的目标数量都要多。</p><h4 id="损失函数---bipartite-matching">损失函数 - Bipartite Matching</h4><p>一个难点就是如何去评价预测目标和真实目标（class 、边框大小位置）</p><p><strong>由于输出物体的顺序不一定与ground truth的序列相同</strong>，作者使用二元匹配将GT框与预测框进行匹配。其匹配策略如下：</p><p>（y和y^都是N大小，用no object填充）</p><p><img src="https://i.loli.net/2020/10/16/7HnIS2eMsPoRqpO.jpg" alt="img"></p><p>但是Lmatch中yi和y^的最佳分配需要用到<strong>匈牙利算法</strong>（Hungarian algorithm），参考的是前人的做法</p><h5 id="匈牙利算法"><strong>匈牙利算法</strong></h5><p>寻找二分图的最大匹配</p><p>最后的损失函数：</p><p><img src="https://i.loli.net/2020/10/16/wsH8nTdmuaekDl5.jpg" alt="img"></p><p>所谓二分的最大匹配，即保证预测值与真值实现最大的匹配，保证预测的N的实例（包括∅）按照位置与真值对应起来。实现一一对应之后，便能够利用分类Loss以及boundingbox Loss进行优化。这种一一对应的关系，同时也另一个好处是，不会多个预测值对应到同一个真实值上，然后再通过NMS进行后处理。</p><hr><p><img src="https://i.loli.net/2020/10/16/VaPDHrkwgthvSTb.png" alt="image-20201013000531053"></p><p>个人觉得最直白的理解方式就是用positional embedding替代了原本的anchor。</p><p>第一步用CNN提feature，然后展开成一维之后加上位置信息进入encoder加工。之后decoder里的object queries，实际上是另一组可学习的positional embedding，其功能类似于anchor。之后每个query进过decoder后算一个bbox和class prob。</p><p>网络的结构是非常简单的，先是CNN提取特征，然后将CNN提取的特征送入到transformer中，而由于transformer是位置无关，所以为了保持位置信息，需要送入CNN特征的同时，送入位置的编码信息，确保整个链路中位置信息不丢失。在transformer中编码之后，送入到解码器，同时送入到解码器的还包括object queries（即文中说的N个查询对象），N个对象以及编码器的输入在解码器的综合作用下，获取N个输出，这N个输出在FFN的作用下，产生N个位置以及每个位置对应的类别。</p><p>至此，网络的便具备物体检测的能力。与原始的transformer不同的地方在于decoder每一层都输出结果，计算loss。这种思想还是相对简单并且work的，EV-FlowNet以及龙明盛迁移学习的某一个版本中均有类似的操作。如果仔细探究的话，我想一定会有一种更合计的叠加方式，而不是这种简单的加在一起，毕竟每一层理论上的物理意义都不同，这种叠加loss的方法，限制了decoder只有第一层完成了大部分任务，更多的层只是一个上采样和细化的过程。</p><p>概括而言，文章的两大核心思想为：</p><p>1、transformer保证了attention，确保对一个实例的识别，是在整幅图的知识下进行的。注意力机制本质是在跑message passing去对提取的特征进行一种滤波，这里面在很大程度上就是<strong>实现了其他分析中的去提取不同位置不同物体之间的相互关系这个功能，通过发掘这个约束提高了对物体识别的可靠性。</strong></p><p>2、二分最大匹配，确保了一一对应的关系。</p><p><img src="https://i.loli.net/2020/10/16/E7CjvLOxHBSMR25.jpg" alt="img"></p><h4 id="二分图最大匹配问题与匈牙利算法的核心思想">二分图最大匹配问题与匈牙利算法的核心思想</h4><p><a href="https://liam.page/2016/04/03/Hungarian-algorithm-in-the-maximum-matching-problem-of-bigraph/" target="_blank" rel="noopener" class="uri">https://liam.page/2016/04/03/Hungarian-algorithm-in-the-maximum-matching-problem-of-bigraph/</a></p><p>图上的object queries实际上是N个emebding，更具体得说应该是N个实例query的embedding(我理解是这样)，退一步不准确一点可以简单理解成位置。N是固定值但是emebding完之后N个quries都不太一样。所以差不多的意思就是告诉模型要100个实例，然后decoder根据encoder得到特征的位置和显著性decoder出100个抽象点代表instance，其中部分是前景instance，部分是背景instance，前景的就class+box loss，背景的就当背景。这就是训练过程。推理过程也就很简单了，前景的就直接用，背景的就丢掉。</p><p>Transformer encoder： 注意力机制本质是在跑message passing去对提取的特征进行一种滤波，这里面在很大程度上就是<strong>实现了其他分析中的去提取不同位置不同物体之间的相互关系这个功能，通过发掘这个约束提高了对物体识别的可靠性。</strong></p><hr><h3 id="an-image-is-worth-16x16-words-transformers-for-image-recognition-at-scale">AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE</h3><p>ICLR 2021 under review</p><p>code url (非官方) : <a href="https://github.com/lucidrains/vit-pytorch" target="_blank" rel="noopener" class="uri">https://github.com/lucidrains/vit-pytorch</a></p><h4 id="abstract-1">Abstract</h4><p>patch： 图像块</p><p>尽管Transformer体系结构已成为自然语言处理任务的实际标准，但其在计算机视觉中的应用仍然受到限制。 在视觉领域，注意力要么与卷积网络一起应用，要么用于替换卷积网络的某些组件，同时将其整体结构保持在适当的位置。 我们表明，这种对CNN的依赖不是必需的，并且当直接应用于图像块序列时，纯transformer可以很好地执行图像分类任务。 当对大量数据进行预训练并转移到多个识别基准（ImageNet，CIFAR-100，VTAB等）时，与最先进的卷积网络相比，Vision Transformer可获得出色的结果，而在训练中所需更少的计算资源。</p><h4 id="问题-1">问题</h4><p>在视觉领域中，transformer模型存在着对CNN的依赖，无法做到纯transformer模型。</p><h4 id="解决-1">解决</h4><p>当直接应用于图像块（patch）序列时，纯transformer可以很好地执行图像分类任务。 对transformer进行尽可能少的修改，这样有利于以后模型的拓展。为此，我们将图像拆分为小块，并提供这些小块的线性嵌入序列作为transformer的输入。图像块与NLP中的token（单词）的处理方式相同，应用在图像分类中。</p><h4 id="性能">性能</h4><p><img src="https://i.loli.net/2020/10/16/t9nKLb241IBUXxM.png" alt="image-20201016142258890"></p><p>在一些 mid-sized datasets 数据集（中型数据集）上表现一般，可能是transformer缺少CNN的归纳偏置特性。</p><p>然而在大型数据集中表现超过了CNN的归纳偏置。</p><h4 id="模型-1">模型</h4><h5 id="整体架构">整体架构</h5><p><img src="https://i.loli.net/2020/10/16/3UnGydo5C8aIOhm.png" alt="image-20201016143430430"></p><p><img src="https://i.loli.net/2020/10/16/o3VuAQ5IYZtmMej.png" alt="image-20201016143922712"></p><p>图像transformer遵循为NLP设计的体系结构。</p><p>标准的Transformer接收一维token嵌入的序列作为输入。 为了处理2D图像，我们将图像x∈R H×W×C reshape为一系列flatten的2D的patch，xp∈R N×（P 2·C）。</p><p>（H，W）是原始图像的分辨率，（P，P）是每个图像块的分辨率。 那么N = HW / P2是transformer的有效序列长度。</p><p>transformer在所有图层上使用恒定的宽度(d_model)，因此可训练的线性投影将每个向量图形块映射到模型尺寸D，我们将其输出称为图像块嵌入（patch embedding）。</p><h5 id="总体流程">总体流程</h5><p><img src="https://i.loli.net/2020/10/16/8KpGO94DXLlIZCS.png" alt="image-20201016145353457"></p><p>位置嵌入： 没有使用行和列的位置嵌入（DETR），和token的位置嵌入一样</p><p>为什么位置向量是随机生成的，并进行优化的呢？ 原transformer是计算得到的？？？</p><p><img src="https://i.loli.net/2020/10/16/XCLARSYyO3tewTg.png" alt="image-20201016150139040"></p><p><img src="https://i.loli.net/2020/10/16/pRZHo6XKFYltTfg.png" alt="image-20201016150253070"></p><p>class embeding: 可学习的标签嵌入</p><h5 id="预训练与微调">预训练与微调</h5><p>在实验阶段，在大型数据集上进行预训练，然后在较小的数据集上进行微调，作为tesk。</p><p>在微调时，去掉了pre-trained prediction head （Xclass），并将最后的num_classes改为数据集需要的类数目</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      此文是将transformer应用到目标检测，可以作为我研究方向的拓展
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-09-28-windows-terminal探索</title>
    <link href="http://yoursite.com/2020/09/28/2020-09-28-windows-terminal%E6%8E%A2%E7%B4%A2/"/>
    <id>http://yoursite.com/2020/09/28/2020-09-28-windows-terminal%E6%8E%A2%E7%B4%A2/</id>
    <published>2020-09-28T02:38:58.000Z</published>
    <updated>2020-09-30T12:57:58.465Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言">前言</h3><p>优化方法终于考完了，最近几天都是在复习。可以放松两天，折腾一下</p><h3 id="windows-terminal">windows terminal</h3><h4 id="安装windows-terminal">安装Windows Terminal</h4><p>在Windows上，可以安装<code>Windows Terminal</code>。有点类似于MacOS上的<code>iTerm</code>，可以说是Windows下最舒适的终端。</p><p>在<code>windows store</code>中安装即可，比较方便，如下图所示</p><p><img src="https://i.loli.net/2020/09/30/KLziDFhWNBg9qYR.png" alt="image-20200930205359665"></p><h3 id="安装ubuntu子系统">安装Ubuntu子系统</h3><p>此时，我们仅仅安装了一个命令行终端而已，还是需要在<code>Windows</code>上安装<code>Ubuntu</code>。</p><p>只需要两步</p><p>1.在系统中开启子系统功能</p><p>2.在<code>windows store</code>安装linux版本即可。我安装的是<code>ubuntu 18.04 LTS</code>版本的，和实验室服务器一个版本号，利于操作。</p><blockquote><p>关于LTS</p><p>1.LTS= 长期支持版本，你会在较长的时间内获得安全、维护和(有时有)功能的更新。</p><p>2.LTS 版本被认为是最稳定的版本，它经历了广泛的测试，并且大多包含了多年积累的改进。</p><p>3.对于ubuntu，没两年发布一个LST版本，比如ubuntu 16.04 ubuntu 18.04等等，代表的是发布的年份。</p><p>4.最新的 LTS 版本是 Ubuntu 20.04 ，它将被支持到 2025 年 4 月，支持5年的软件更新和修补。换句话说，Ubuntu 20.04 在那之前都会收到软件更新。非 LTS 版本只支持九个月。</p></blockquote><p>如下图，在控制面板，找到程序选项，点击 “启用或关闭Windows功能”。</p><p><img src="https://i.loli.net/2020/09/28/KQ3LocsNrBxiRf5.png" alt="image-20200928105917992"></p><p>从弹出的对话框里，划到最下边，然后给“适用于Linux的Windows子系统“，打勾，完事！</p><p><img src="https://i.loli.net/2020/09/28/HPVhtscoGj8iCRA.png" alt="image-20200928105907117"></p><p>在windows中访问ubuntu系统</p><p>可以认为在windows 文件资源管理器中开辟一个空间用来储存ubuntu系统，但是如何找到位置呢？</p><p>执行如下命令：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /home  </span><br><span class="line">explorer.exe .  <span class="comment">#用文件资源管理器来打开当前home目录所在位置</span></span><br></pre></td></tr></tbody></table></figure><p>可以看到是在<code>网络</code>一栏中， 可以看到ubuntu的文件目录。但是返回到<code>网络</code>根目录，却显示是无文件夹。不知道为什么。</p><p>为了操作方便，我把这个长长的目录，右键映射到了Z盘上。如图，下次在访问Linux的时候，直接访问Z盘就可以了。</p><p><img src="https://i.loli.net/2020/09/28/XqMOBfSKRkwHC93.png" alt="image-20200928111036904"></p><p>这时，就可以看到在我的电脑里就有了Z盘</p><p><img src="https://i.loli.net/2020/09/28/V9MFRujrqgl46me.png" alt="image-20200928111104530"></p><blockquote><p><strong>映射网络驱动器</strong>目的就是为了让远程网络中的资源和本地共享，在本地可以对远程网络中的资源进行访问，并且可以创建文件。</p></blockquote><h3 id="工作区快捷键">工作区快捷键</h3><table><thead><tr class="header"><th style="text-align: left;">Win 快捷键</th><th style="text-align: left;">作用</th><th style="text-align: left;">备注</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;"><strong>Ctrl + Shift + P</strong>，F1</td><td style="text-align: left;">显示命令面板</td><td style="text-align: left;"></td></tr><tr class="even"><td style="text-align: left;"><strong>Ctrl + B</strong></td><td style="text-align: left;">显示/隐藏侧边栏</td><td style="text-align: left;">很实用</td></tr><tr class="odd"><td style="text-align: left;"><code>Ctrl + \</code></td><td style="text-align: left;"><strong>创建多个编辑器</strong></td><td style="text-align: left;">【重要】抄代码利器</td></tr><tr class="even"><td style="text-align: left;"><strong>Ctrl + 1、2</strong></td><td style="text-align: left;">聚焦到第 1、第 2 个编辑器</td><td style="text-align: left;">同上重要</td></tr><tr class="odd"><td style="text-align: left;"><strong>ctrl +/-</strong></td><td style="text-align: left;">将工作区放大/缩小（包括代码字体、左侧导航栏）</td><td style="text-align: left;">在投影仪场景经常用到</td></tr><tr class="even"><td style="text-align: left;">Ctrl + J</td><td style="text-align: left;">显示/隐藏控制台</td><td style="text-align: left;"></td></tr><tr class="odd"><td style="text-align: left;"><strong>Ctrl + Shift + N</strong></td><td style="text-align: left;">重新开一个软件的窗口</td><td style="text-align: left;">很常用</td></tr><tr class="even"><td style="text-align: left;">Ctrl + Shift + W</td><td style="text-align: left;">关闭软件的当前窗口</td><td style="text-align: left;"></td></tr><tr class="odd"><td style="text-align: left;">Ctrl + N</td><td style="text-align: left;">新建文件</td><td style="text-align: left;"></td></tr><tr class="even"><td style="text-align: left;">Ctrl + W</td><td style="text-align: left;">关闭当前文件</td><td style="text-align: left;"></td></tr></tbody></table><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      才考完优化方法，折腾一下微软神器windows terminal
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-09-19-一周论文分享（第2期）</title>
    <link href="http://yoursite.com/2020/09/19/2020-09-19-%E4%B8%80%E5%91%A8%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB%EF%BC%88%E7%AC%AC2%E6%9C%9F%EF%BC%89/"/>
    <id>http://yoursite.com/2020/09/19/2020-09-19-%E4%B8%80%E5%91%A8%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB%EF%BC%88%E7%AC%AC2%E6%9C%9F%EF%BC%89/</id>
    <published>2020-09-19T02:58:48.000Z</published>
    <updated>2020-10-17T13:37:14.066Z</updated>
    
    <content type="html"><![CDATA[<h4 id="联邦学习">联邦学习</h4><p>参考 <a href="https://zhuanlan.zhihu.com/p/79284686" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/79284686</a></p><p>背景</p><p>1.现实生活中，除了少数巨头公司能够满足，绝大多数企业都存在数据量少，数据质量差的问题，不足以支撑人工智能技术的实现；</p><p>2.同时国内外监管环境也在逐步加强数据保护，因此数据在安全合规的前提下自由流动，成了大势所趋，所以不能获取很多涉及用户隐私的信息。</p><p>3.数据的不充分交流，同时也导致即使在同一个公司内，数据也往往以孤岛形式出现。</p><p>基于以上不足以支撑实现、不允许粗暴交换、不愿意贡献价值三点，</p><p>现在大量存在的<strong>数据孤岛</strong>，以及隐私保护问题，联邦学习被提出。</p><p>概念</p><p>本质：联邦学习本质上是一种<strong>分布式</strong>机器学习技术，或机器学习<strong>框架</strong>。</p><p>目标：联邦学习的目标是在保证数据隐私安全及合法合规的基础上，实现共同建模，提升AI模型的效果。</p><p>前身：联邦学习最早在 2016 年由谷歌提出，原本用于解决安卓手机终端用户在本地更新模型的问题；</p><p><img src="E:\myBlog\source_posts\v2-657a9f63512351691e60af9d88a34605_720w.jpg" alt="img"></p><h2 id="横向联邦学习">3.1 横向联邦学习</h2><p><strong>适用场景：</strong></p><p>横向联邦学习的本质是<strong>样本的联合</strong>，适用于参与者间业态相同但触达客户不同，即特征重叠多，用户重叠少时的场景，比如不同地区的银行间，他们的业务相似（特征相似），但用户不同（样本不同）</p><p><strong>学习过程：</strong></p><p><img src="E:\myBlog\source_posts\v2-23616816b92a6d62be206b0aa28ba393_720w.jpg" alt="img"></p><p>step1：参与方各自从服务器A下载最新模型；</p><p>step2：每个参与方利用本地数据训练模型，加密梯度上传给服务器A，服务器A聚合各用户的梯度更新模型参数；</p><p>step3：服务器A返回更新后的模型给各参与方；</p><p>step4：各参与方更新各自模型。</p><p><strong>步骤解读：</strong>在传统的机器学习建模中，通常是把模型训练需要的数据集合到一个数据中心然后再训练模型，之后预测。在横向联邦学习中，可以看作是<strong>基于样本的分布式模型训练</strong>，分发全部数据到不同的机器，每台机器从服务器下载模型，然后利用本地数据训练模型，之后返回给服务器需要更新的参数；服务器聚合各机器上的返回的参数，更新模型，再把最新的模型反馈到每台机器。</p><p>在这个过程中，每台机器下都是<strong>相同且完整的模型</strong>，且机器之间不交流不依赖，在预测时每台机器也可以<strong>独立预测</strong>，可以把这个过程看作成基于样本的分布式模型训练。谷歌最初就是采用横向联邦的方式解决安卓手机终端用户在本地更新模型的问题的。</p><h2 id="简介"><strong>简介</strong></h2><p>NAS</p><p>深度学习可以自动学习出有用的特征，脱离了对特征工程的依赖，在图像、语音等任务上取得了超越其他算法的结果。这种成功很大程度上得益于新神经网络结构的出现，如ResNet、Inception、DenseNet等。但设计出高性能的神经网络需要大量的专业知识与反复试验，成本极高，限制了神经网络在很多问题上的应用。神经结构搜索（Neural Architecture Search，简称NAS）是一种自动设计神经网络的技术，可以通过算法根据样本集自动设计出高性能的网络结构，在某些任务上甚至可以媲美人类专家的水准，甚至发现某些人类之前未曾提出的网络结构，这可以有效的降低神经网络的使用和实现成本。</p><p>NAS的原理是给定一个称为搜索空间的候选神经网络结构集合，用某种策略从中搜索出最优网络结构。神经网络结构的优劣即性能用某些指标如精度、速度来度量，称为性能评估。这一过程如下图所示。</p><p><img src="E:\myBlog\source_posts\v2-261f4e89d5c60e5d336052e7fc6d116d_720w.png" alt="img"></p><p>在搜索过程的每次迭代中，从搜索空间产生“样本”即得到一个神经网络结构，称为“子网络”。在训练样本集上训练子网络，然后在验证集上评估其性能。逐步优化网络结构，直至找到最优的子网络。</p><p>搜索空间，搜索策略，性能评估策略是NAS算法的核心要素。搜索空间定义了可以搜索的神经网络结构的集合，即解的空间。搜索策略定义了如何在搜索空间中寻找最优网络结构。性能评估策略定义了如何评估搜索出的网络结构的性能。对这些要素的不同实现得到了各种不同的NAS算法，本节将选择有代表性的进行介绍。</p><h3 id="fisher-information">Fisher Information</h3><p>反映了我们对参数估计的准确度，它越大，对参数估计的准确度越高，即代表了越多的信息。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      记录每周值得分享的论文，周一发布、
《reformer-the eficient transformer》、
《Transformer-XL-Attentive Language Models Beyond a Fixed-Length Context》

    
    </summary>
    
    
      <category term="论文分享" scheme="http://yoursite.com/categories/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"/>
    
    
      <category term="论文" scheme="http://yoursite.com/tags/%E8%AE%BA%E6%96%87/"/>
    
      <category term="论文分享" scheme="http://yoursite.com/tags/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/"/>
    
  </entry>
  
  <entry>
    <title>2020-09-14-修改pytorch版transformer代码</title>
    <link href="http://yoursite.com/2020/09/14/2020-09-14-%E4%BF%AE%E6%94%B9pytorch%E7%89%88transformer%E4%BB%A3%E7%A0%81/"/>
    <id>http://yoursite.com/2020/09/14/2020-09-14-%E4%BF%AE%E6%94%B9pytorch%E7%89%88transformer%E4%BB%A3%E7%A0%81/</id>
    <published>2020-09-14T08:26:17.000Z</published>
    <updated>2020-09-14T09:14:07.600Z</updated>
    
    <content type="html"><![CDATA[<p><del>源代码是<code>main3.py</code> ,在此基础上进行修改，修改后文件为<code>main3-2.py</code></del></p><p>740中<code>annotated-transformer</code>中<code>main.py</code>和哈佛的一样</p><p>复制到了本地<code>main.py</code> ,再复制到<code>annotated-transformer1</code>中的main.py</p><p><strong>所以改前的代码是<code>main.py</code> ，改后的代码是<code>main-1.py</code></strong></p><p>注：</p><p><strong><code>python main.py &gt;main.txt 2&gt;&amp;1</code>，在将结果重定向到main.txt中，会覆盖main.txt之前的内容</strong></p><p><strong>每次跑实验的预测都是不一样的，但是都是和输入差不多</strong></p><ol type="1"><li>将<code>attention</code>函数去掉，合并到<code>MultiHeadedAttention</code>中，服务器上测试<strong>可行</strong></li></ol><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      记录修改过程
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-09-11-gpu实验加速</title>
    <link href="http://yoursite.com/2020/09/11/2020-09-11-gpu%E5%AE%9E%E9%AA%8C%E5%8A%A0%E9%80%9F/"/>
    <id>http://yoursite.com/2020/09/11/2020-09-11-gpu%E5%AE%9E%E9%AA%8C%E5%8A%A0%E9%80%9F/</id>
    <published>2020-09-11T00:48:00.000Z</published>
    <updated>2020-09-16T08:00:54.184Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言">前言</h3><p>将深度学习应用到实际问题中，一个非常大的问题在于训练深度学习模型需要的计算量太大。为了加速训练过程，本文将介绍如何如何在TensorFlow中使用单个GPU进行计算加速</p><h3 id="简介">简介</h3><h4 id="cuda">CUDA</h4><p><a href="https://developer.nvidia.com/cuda-toolkit-archive" target="_blank" rel="noopener">CUDA</a>（Compute Unified Device Architecture,点击进入安装网站），是显卡厂商NVIDIA推出的运算平台。 CUDA™是一种由NVIDIA推出的通用并行计算架构，该架构使GPU能够解决复杂的计算问题。安装GPU版tensorflow,必须有这个环境。</p><p>CUDA是NVIDIA推出的用于自家GPU的并行计算框架，也就是说CUDA只能在NVIDIA的GPU上运行，而且只有当要解决的计算问题是可以大量并行计算的时候才能发挥CUDA的作用。</p><h4 id="cudnn">cuDNN</h4><p>NVIDIA <a href="https://developer.nvidia.com/cuda-toolkit-archive" target="_blank" rel="noopener">cuDNN</a>是用于深度神经网络的GPU加速库。它强调性能、易用性和低内存开销。</p><h3 id="安装">安装</h3><p>必须要安装对应版本的CUDA、cuDNN和tensorflow</p><p>我在实验室服务器R740上的安装版本如下，是可以运行的</p><blockquote><p>CUDA： V10.1 # nvcc --version</p><p>cuDNN：V7</p><p>tensorflow-gpu：1.14.0</p></blockquote><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">conda install python==3.6.10  <span class="comment">#这样可以 ，但是为什么 python==3.6.1 和  python==3.6.0 是不可以的呢？？</span></span><br><span class="line">pip install tensorflow-gpu==1.14.0  <span class="comment">#安装成功gpu版本</span></span><br><span class="line"><span class="comment">#用conda装tensorflow时候，会自动下载cuda和cudnn，所以推荐用pip安装</span></span><br><span class="line"></span><br><span class="line">pip install tensorflow-gpu==1.2 <span class="comment">#如果安装错误，可以用pip卸载，没测试过。 或者直接再新建一个虚拟环境也可以</span></span><br></pre></td></tr></tbody></table></figure><h3 id="测试tensorflow-gpu">测试tensorflow-gpu</h3><p>测试安装的tensorflow是否可用GPU，测试如下：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pyhton <span class="comment">#进入python操作环境</span></span><br><span class="line"></span><br><span class="line">import tensorflow as tf </span><br><span class="line">sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))</span><br></pre></td></tr></tbody></table></figure><p>显示如下则表示tensorflow支持的，输出如下：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">2020-09-11 08:30:54.735834: Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA</span><br><span class="line">2020-09-11 08:30:54.821023:  Successfully opened dynamic library libcuda.so.1</span><br><span class="line">2020-09-11 08:30:55.698894:  XLA service 0x5654b4f86600 executing computations on platform CUDA. Devices:</span><br><span class="line">2020-09-11 08:30:55.699000:   StreamExecutor device (0): Tesla M60, Compute Capability 5.2</span><br><span class="line">2020-09-11 08:30:55.699022:   StreamExecutor device (1): Tesla M60, Compute Capability 5.2</span><br><span class="line">2020-09-11 08:30:55.699042:   StreamExecutor device (2): Tesla M60, Compute Capability 5.2</span><br><span class="line">2020-09-11 08:30:55.699062:   StreamExecutor device (3): Tesla M60, Compute Capability 5.2</span><br><span class="line">2020-09-11 08:30:55.732911:   CPU Frequency: 2100000000 Hz</span><br><span class="line">2020-09-11 08:30:55.738953: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5654b54aa810 executing computations on platform Host. Devices:</span><br><span class="line">2020-09-11 08:30:55.739001: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;</span><br><span class="line">2020-09-11 08:30:55.741878: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: </span><br><span class="line">name: Tesla M60 major: 5 minor: 2 memoryClockRate(GHz): 1.1775</span><br><span class="line">pciBusID: 0000:b1:00.0</span><br><span class="line">2020-09-11 08:30:55.742665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: </span><br><span class="line">name: Tesla M60 major: 5 minor: 2 memoryClockRate(GHz): 1.1775</span><br><span class="line">pciBusID: 0000:b2:00.0</span><br><span class="line">2020-09-11 08:30:55.743420: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties: </span><br><span class="line">name: Tesla M60 major: 5 minor: 2 memoryClockRate(GHz): 1.1775</span><br><span class="line">pciBusID: 0000:da:00.0</span><br><span class="line">2020-09-11 08:30:55.744263: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties: </span><br><span class="line">name: Tesla M60 major: 5 minor: 2 memoryClockRate(GHz): 1.1775</span><br><span class="line">pciBusID: 0000:db:00.0</span><br><span class="line">2020-09-11 08:30:55.744692: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library <span class="string">'libcudart.so.10.0'</span>; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/dell2/.mujoco/mjpro150/bin:/usr/<span class="built_in">local</span>/cuda-10.1/lib64:</span><br><span class="line">2020-09-11 08:30:55.744798: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library <span class="string">'libcublas.so.10.0'</span>; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/dell2/.mujoco/mjpro150/bin:/usr/<span class="built_in">local</span>/cuda-10.1/lib64:</span><br><span class="line">2020-09-11 08:30:55.744891: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library <span class="string">'libcufft.so.10.0'</span>; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/dell2/.mujoco/mjpro150/bin:/usr/<span class="built_in">local</span>/cuda-10.1/lib64:</span><br><span class="line">2020-09-11 08:30:55.744980: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library <span class="string">'libcurand.so.10.0'</span>; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/dell2/.mujoco/mjpro150/bin:/usr/<span class="built_in">local</span>/cuda-10.1/lib64:</span><br><span class="line">2020-09-11 08:30:55.745070: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library <span class="string">'libcusolver.so.10.0'</span>; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/dell2/.mujoco/mjpro150/bin:/usr/<span class="built_in">local</span>/cuda-10.1/lib64:</span><br><span class="line">2020-09-11 08:30:55.745166: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library <span class="string">'libcusparse.so.10.0'</span>; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/dell2/.mujoco/mjpro150/bin:/usr/<span class="built_in">local</span>/cuda-10.1/lib64:</span><br><span class="line">2020-09-11 08:30:55.750141: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7</span><br><span class="line">2020-09-11 08:30:55.750170: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1663] Cannot dlopen some GPU libraries. Skipping registering GPU devices...</span><br><span class="line">2020-09-11 08:30:55.750542: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:</span><br><span class="line">2020-09-11 08:30:55.750706:       0 1 2 3 </span><br><span class="line">2020-09-11 08:30:55.750797:  0:   N Y Y Y </span><br><span class="line">2020-09-11 08:30:55.750887:  1:   Y N Y Y </span><br><span class="line">2020-09-11 08:30:55.750974:  2:   Y Y N Y </span><br><span class="line">2020-09-11 08:30:55.751059:  3:   Y Y Y N </span><br><span class="line">Device mapping:</span><br><span class="line">/job:localhost/replica:0/task:0/device:XLA_GPU:0 -&gt; device: XLA_GPU device</span><br><span class="line">/job:localhost/replica:0/task:0/device:XLA_GPU:1 -&gt; device: XLA_GPU device</span><br><span class="line">/job:localhost/replica:0/task:0/device:XLA_GPU:2 -&gt; device: XLA_GPU device</span><br><span class="line">/job:localhost/replica:0/task:0/device:XLA_GPU:3 -&gt; device: XLA_GPU device</span><br><span class="line">/job:localhost/replica:0/task:0/device:XLA_CPU:0 -&gt; device: XLA_CPU device</span><br><span class="line">2020-09-11 08:30:55.757190: I tensorflow/core/common_runtime/direct_session.cc:296] Device mapping:</span><br><span class="line">/job:localhost/replica:0/task:0/device:XLA_GPU:0 -&gt; device: XLA_GPU device</span><br><span class="line">/job:localhost/replica:0/task:0/device:XLA_GPU:1 -&gt; device: XLA_GPU device</span><br><span class="line">/job:localhost/replica:0/task:0/device:XLA_GPU:2 -&gt; device: XLA_GPU device</span><br><span class="line">/job:localhost/replica:0/task:0/device:XLA_GPU:3 -&gt; device: XLA_GPU device</span><br><span class="line">/job:localhost/replica:0/task:0/device:XLA_CPU:0 -&gt; device: XLA_CPU device</span><br></pre></td></tr></tbody></table></figure><p>表示tensorflow支持device：CPU：0 ，支持device：<code>GPU：0,1,2,3</code>，共4块GPU</p><p>比如CPU在TensorFlow中的名称为/cpu:0。<strong>在默认情况下，即使机器有多个CPU，TensorFlow也不会区分它们，所有的CPU都使用/cpu:0作为名称。</strong></p><p>而一台机器上不同GPU的名称是不同的，第n个GPU在TensorFlow中的名称为/gpu:n。比如第一个GPU的名称为/gpu:0，第二个GPU名称为/gpu:1，以此类推。</p><p>作者：博文视点 链接：https://www.jianshu.com/p/26ac409dfb38 来源：简书 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p><h3 id="ubuntu中查看显卡信息">Ubuntu中查看显卡信息</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lspci | grep -i vga <span class="comment">#显卡</span></span><br></pre></td></tr></tbody></table></figure><p>显示结果如下：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">03:00.0 VGA compatible controller: Matrox Electronics Systems Ltd. Integrated Matrox G200eW3 Graphics Controller (rev 04)</span><br><span class="line">b1:00.0 VGA compatible controller: NVIDIA Corporation GM204GL [Tesla M60] (rev a1)</span><br><span class="line">b2:00.0 VGA compatible controller: NVIDIA Corporation GM204GL [Tesla M60] (rev a1)</span><br><span class="line">da:00.0 VGA compatible controller: NVIDIA Corporation GM204GL [Tesla M60] (rev a1)</span><br><span class="line">db:00.0 VGA compatible controller: NVIDIA Corporation GM204GL [Tesla M60] (rev a1)</span><br></pre></td></tr></tbody></table></figure><h3 id="ubuntu中查看nvidia-gpu">Ubuntu中查看nvidia GPU</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lspci | grep -i nvidia <span class="comment">#查看gpu信息</span></span><br></pre></td></tr></tbody></table></figure><p>显示结果如下：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">b1:00.0 VGA compatible controller: NVIDIA Corporation GM204GL [Tesla M60] (rev a1)</span><br><span class="line">b2:00.0 VGA compatible controller: NVIDIA Corporation GM204GL [Tesla M60] (rev a1)</span><br><span class="line">da:00.0 VGA compatible controller: NVIDIA Corporation GM204GL [Tesla M60] (rev a1)</span><br><span class="line">db:00.0 VGA compatible controller: NVIDIA Corporation GM204GL [Tesla M60] (rev a1)</span><br></pre></td></tr></tbody></table></figure><h3 id="查看nvidia的显卡信息和使用情况">查看Nvidia的显卡信息和使用情况</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvidia-smi</span><br></pre></td></tr></tbody></table></figure><p>显示如下：</p><p><img src="https://i.loli.net/2020/09/11/OQZjHpocke5S9FE.png" alt="image-20200910221947824"></p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ps aux | grep train.py #我的实验名称为train.py</span><br></pre></td></tr></tbody></table></figure><p><img src="https://i.loli.net/2020/09/11/7lujWDbiqnV6zwG.png" alt="image-20200910222011793"></p><p>可以看到，我的实验进程号是<code>21195</code>，在<code>processes</code>中可以看到使用了<code>GPU1,2</code></p><h3 id="指定gpu实验加速">指定GPU实验加速</h3><p>如果电脑有多个GPU，tensorflow默认全部使用。</p><p>如果想只使用部分GPU，可以设置<code>CUDA_VISIBLE_DEVICES</code>。</p><h4 id="命令行指定">命令行指定</h4><p>在执行python程序时，可以通过：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=1 python train.py <span class="comment">#只使用GPU1</span></span><br></pre></td></tr></tbody></table></figure><p>以下为一些使用指导：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Environment Variable Syntax      Results</span><br><span class="line"></span><br><span class="line">CUDA_VISIBLE_DEVICES=1           Only device 1 will be seen</span><br><span class="line">CUDA_VISIBLE_DEVICES=0,1         Devices 0 and 1 will be visible</span><br><span class="line">CUDA_VISIBLE_DEVICES=<span class="string">"0,1"</span>       Same as above, quotation marks are optional</span><br><span class="line">CUDA_VISIBLE_DEVICES=0,2,3       Devices 0, 2, 3 will be visible; device 1 is masked</span><br><span class="line">CUDA_VISIBLE_DEVICES=<span class="string">""</span>          No GPU will be visible1234567</span><br></pre></td></tr></tbody></table></figure><h4 id="代码中指定">代码中指定</h4><p>在Python代码中添加以下内容：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="string">"1"</span> <span class="comment">#只使用GPU1</span></span><br></pre></td></tr></tbody></table></figure><h3 id="设置tensorflow使用的显存大小">设置tensorflow使用的显存大小</h3><h4 id="定量设置显存">定量设置显存</h4><p>默认tensorflow是使用GPU尽可能多的显存（内存）。</p><p>用Tensorflow创建session的时候要注意设置内存使用情况，特别是内存资源不够而且要和别人共享一块GPU的时候（留一点给别人用）</p><p>可以通过下面的方式，来设置使用的GPU显存：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=<span class="number">0.7</span>)</span><br><span class="line">sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))</span><br></pre></td></tr></tbody></table></figure><p>上面分配给tensorflow的GPU显存大小为：GPU实际显存*0.7。 可以按照需要，设置不同的值，来分配显存。</p><h4 id="按需设置显存">按需设置显存</h4><p>上面的只能设置固定的大小。如果想按需分配，可以使用allow_growth参数</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">gpu_options = tf.GPUOptions(allow_growth=<span class="literal">True</span>)</span><br><span class="line">sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))  </span><br><span class="line"><span class="comment"># 使用allow_growth option，刚一开始分配少量的GPU容量，然后按需慢慢的增加，由于不会释放内存，所以会导致碎片</span></span><br></pre></td></tr></tbody></table></figure><p>如果一个 TensorFlow 的 operation 中兼有 CPU 和 GPU 的实现, 当这个算子被指派设备时, GPU 有优先权. 比如<code>matmul</code>中 CPU 和 GPU kernel 函数都存在. 那么在 <code>cpu:0</code> 和 <code>gpu:0</code> 中, <code>matmul</code> operation 会被指派给 <code>gpu:0</code> .</p><h4 id="记录设备指派情况">记录设备指派情况</h4><p>为了获取你的 operations 和 Tensor 被指派到哪个设备上运行, 用 <code>log_device_placement</code> 新建一个 <code>session</code>, 并设置为 <code>True</code>.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 新建一个 graph.</span></span><br><span class="line">a = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], shape=[<span class="number">2</span>, <span class="number">3</span>], name=<span class="string">'a'</span>)</span><br><span class="line">b = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], shape=[<span class="number">3</span>, <span class="number">2</span>], name=<span class="string">'b'</span>)</span><br><span class="line">c = tf.matmul(a, b)</span><br><span class="line"><span class="comment"># 新建session with log_device_placement并设置为True.</span></span><br><span class="line">sess = tf.Session(config=tf.ConfigProto(log_device_placement=<span class="literal">True</span>))</span><br><span class="line"><span class="comment"># 运行这个 op.</span></span><br><span class="line"><span class="keyword">print</span> sess.run(c)</span><br></pre></td></tr></tbody></table></figure><p>你应该能看见以下输出:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Device mapping:</span><br><span class="line">/job:localhost/replica:0/task:0/gpu:0 -&gt; device: 0, name: Tesla K40c, pci bus</span><br><span class="line">id: <span class="number">0000</span>:<span class="number">05</span>:<span class="number">00.0</span></span><br><span class="line">b: /job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/gpu:<span class="number">0</span></span><br><span class="line">a: /job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/gpu:<span class="number">0</span></span><br><span class="line">MatMul: /job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/gpu:<span class="number">0</span></span><br><span class="line">[[ <span class="number">22.</span>  <span class="number">28.</span>]</span><br><span class="line"> [ <span class="number">49.</span>  <span class="number">64.</span>]]</span><br></pre></td></tr></tbody></table></figure><h4 id="section"></h4><h3 id="gpu和cpu">GPU和CPU</h3><p>一个GPU被多个实验使用，但是如果实验超过显存大小，就会都被挂掉，会显示<code>stopped</code>字样</p><p>一个实验可以用多个GPU，但是需要更改部分代码，让其支持多GPU</p><p>不要tensorflow-gpu和tensorflow(cpu版)一起装，因为这样装有个先后顺序问题，先安装tensorflow-gpu再安装tensorflow，gpu版本直接不能用了。</p><p>如果想测试cpu和gpu版本性能的，最好创建两个python的虚拟环境，一个装tensorflow-gpu，另一个装tensorflow。</p><hr><p>在Tensorflow中使用gpu和cpu是有很大的差别的。在小数据集的情况下，cpu和gpu的性能差别不大。不过在大数据集的情况下，cpu的时间显著增加，而gpu变化并不明显。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> timeit</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cpu_run</span><span class="params">(num)</span>:</span></span><br><span class="line">  <span class="keyword">with</span> tf.device(<span class="string">'/cpu:0'</span>):</span><br><span class="line">    cpu_a=tf.random.normal([<span class="number">1</span>,num])</span><br><span class="line">    cpu_b=tf.random.normal([num,<span class="number">1</span>])</span><br><span class="line">    c=tf.matmul(cpu_a,cpu_b)</span><br><span class="line">  <span class="keyword">return</span> c</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gpu_run</span><span class="params">(num)</span>:</span></span><br><span class="line">  <span class="keyword">with</span> tf.device(<span class="string">'/gpu:0'</span>):</span><br><span class="line">    gpu_a=tf.random.normal([<span class="number">1</span>,num])</span><br><span class="line">    gpu_b=tf.random.normal([num,<span class="number">1</span>])</span><br><span class="line">    c=tf.matmul(gpu_a,gpu_b)</span><br><span class="line">  <span class="keyword">return</span> c</span><br><span class="line">k=<span class="number">10</span></span><br><span class="line">m=<span class="number">7</span></span><br><span class="line">cpu_result=np.arange(m,dtype=np.float32)</span><br><span class="line">gpu_result=np.arange(m,dtype=np.float32)</span><br><span class="line">x_time=np.arange(m)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">  k=k*<span class="number">10</span></span><br><span class="line">  x_time[i]=k</span><br><span class="line">  cpu_str=<span class="string">'cpu_run('</span>+str(k)+<span class="string">')'</span></span><br><span class="line">  gpu_str=<span class="string">'gpu_run('</span>+str(k)+<span class="string">')'</span></span><br><span class="line">  <span class="comment">#print(cpu_str)</span></span><br><span class="line">  cpu_time=timeit.timeit(cpu_str,<span class="string">'from __main__ import cpu_run'</span>,number=<span class="number">10</span>)</span><br><span class="line">  gpu_time=timeit.timeit(gpu_str,<span class="string">'from __main__ import gpu_run'</span>,number=<span class="number">10</span>)</span><br><span class="line">  <span class="comment"># 正式计算10次，取平均时间</span></span><br><span class="line">  cpu_time=timeit.timeit(cpu_str,<span class="string">'from __main__ import cpu_run'</span>,number=<span class="number">10</span>)</span><br><span class="line">  gpu_time=timeit.timeit(gpu_str,<span class="string">'from __main__ import gpu_run'</span>,number=<span class="number">10</span>)</span><br><span class="line">  cpu_result[i]=cpu_time</span><br><span class="line">  gpu_result[i]=gpu_time</span><br><span class="line"></span><br><span class="line">print(cpu_result)</span><br><span class="line">print(gpu_result)</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.set_xscale(<span class="string">"log"</span>)</span><br><span class="line">ax.set_adjustable(<span class="string">"datalim"</span>)</span><br><span class="line">ax.plot(x_time,cpu_result)</span><br><span class="line">ax.plot(x_time,gpu_result)</span><br><span class="line">ax.grid()</span><br><span class="line">plt.draw()</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="https://i.loli.net/2020/09/11/tRxAb2wY5qKGF4D.png" alt="在这里插入图片描述"> 蓝线是cpu的耗时，而红线是gpu的耗时。</p><p>更多gpu内容可参考</p><blockquote><p><a href="https://docs.pythontab.com/tensorflow/how_tos/using_gpu/" target="_blank" rel="noopener">tensorflow官方文档，使用 GPUs</a></p><p><a href="https://www.cnblogs.com/nxf-rabbit75/p/10639833.html" target="_blank" rel="noopener">Tensorflow检验GPU是否安装成功 及 使用GPU训练注意事项</a></p><p><a href="https://www.jianshu.com/p/26ac409dfb38" target="_blank" rel="noopener">TensorFlow：实战Google深度学习框架（第2版）:GPU加速</a></p></blockquote><h3 id="tensorflow匹配的关系">tensorflow匹配的关系</h3><p><img src="https://i.loli.net/2020/09/13/FqJ1cXThMzKHvA5.png" alt="image-20200913144848843"></p><p><img src="E:\myBlog\source_posts\FqJ1cXThMzKHvA5.png"></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      记录加速过程以及知识点
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-09-07-实验日志</title>
    <link href="http://yoursite.com/2020/09/07/2020-09-07-%E5%AE%9E%E9%AA%8C%E6%97%A5%E5%BF%97/"/>
    <id>http://yoursite.com/2020/09/07/2020-09-07-%E5%AE%9E%E9%AA%8C%E6%97%A5%E5%BF%97/</id>
    <published>2020-09-07T12:44:43.000Z</published>
    <updated>2020-10-29T14:20:58.455Z</updated>
    
    <content type="html"><![CDATA[<h3 id="虚拟环境配置">虚拟环境配置</h3><h4 id="笔记本">笔记本</h4><table><thead><tr class="header"><th>名称</th><th>配置</th><th>用处</th></tr></thead><tbody><tr class="odd"><td>sijian36</td><td>tf 1.9.0</td><td>普通跑实验</td></tr><tr class="even"><td>python714</td><td>tf 1.14.0</td><td>RKN</td></tr><tr class="odd"><td>ronghe</td><td>tf 1.14.0</td><td>transformer和RKN</td></tr></tbody></table><h4 id="r740服务器">R740服务器</h4><p>cuda-10.2</p><table><thead><tr class="header"><th>名称</th><th>配置</th><th>用处</th></tr></thead><tbody><tr class="odd"><td>sijian1</td><td>tf 1.13.0</td><td>一般实验</td></tr><tr class="even"><td>pytorth030</td><td>torch 0.3</td><td>哈佛torch版transformer</td></tr><tr class="odd"><td>lsjRKN</td><td>tf 1.14.0</td><td>RKN</td></tr><tr class="even"><td>ronghe</td><td>tf 1.14.0</td><td>transformer和RKN</td></tr><tr class="odd"><td>ronghe6</td><td>tf-gpu1.14.0</td><td>gpu加速融合</td></tr></tbody></table><h3 id="日志">🚀日志</h3><h4 id="section"><strong>2020-09-06</strong></h4><h5 id="主要内容">主要内容</h5><p>笔记本的RKN实验</p><p>跑通是在RKNmaster文件跑</p><hr><p>哈佛torch版transformer实验</p><p>R740中 LSJ/annotated-transformer1/main5.py(pytorch030)</p><p>是之前上传到R740跑的实验</p><p>LSJ/annotated-transformer是前一阵为了将函数改成直通型流程而从笔记本上上传的</p><h5 id="出现问题">出现问题</h5><p>在R740跑RKN实验</p><p><code>attributeerror: module 'tensorflow.keras.initializers'  has no attribute 'normal'</code> 解决</p><p>将RKN.py 77行 normal 改为 <strong>RandomNormal</strong> 还是出错</p><p>再次出错 keep.dim出错</p><p>修改 将keep.dim=True参数去掉 再运行</p><p>​ <img src="https://i.loli.net/2020/09/08/h7Ma3lm82eFSuWX.png" alt="img"></p><ul><li>运行结果是没有tf.matrix_band_part 这个参数，于是百度发现，</li><li>新版本：tf.matrix_band_part变成tf.linalg.band_part 于是修改再运行</li></ul><p>运行结果显示</p><p>​ <img src="https://i.loli.net/2020/09/08/h7Ma3lm82eFSuWX.png" alt="img"></p><p>于是百度，原因是</p><p>The image from your input pipeline is of type 'uint8', you need to type cast it to 'float32', You can do this after the image jpeg decoder:</p><p>以下更改，在RKN.py中插入h = tf.cast(h, tf.float32)</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def _prop_through_layers(inputs, layers):</span><br><span class="line"></span><br><span class="line">​    """propagates inputs through layers"""</span><br><span class="line"></span><br><span class="line">​    h = inputs</span><br><span class="line"></span><br><span class="line">h = tf.cast(h, tf.float32)</span><br><span class="line"></span><br><span class="line">​    for layer in layers:</span><br><span class="line"></span><br><span class="line">​        h = layer(h)</span><br><span class="line"></span><br><span class="line">​    return h</span><br></pre></td></tr></tbody></table></figure><p>还是报错</p><p><strong>放弃使用sijian1 以及刚刚对RKNmaser的修改</strong></p><p>将笔记本中的RKNmaster 复制为rknmas上传到R740 名字为<strong>rknmas</strong></p><p>参考了笔记本中的虚拟环境，在R740新建lsjRKN的虚拟环境，<strong>tf版本为1.14 python：3.6</strong></p><p>可以跑通实验</p><p>实验可以在R740跑起来，但是为什么论文作者的github代码上tensorflow版本是1.13 不好使，但是在tensorflow1.14就可以跑起来？？？？</p><p>在笔记本上跑的 设置epoch=5</p><p>​ <img src="https://i.loli.net/2020/09/08/h7Ma3lm82eFSuWX.png" alt="img"></p><h4 id="section-1"><strong>2020-09-07</strong></h4><h5 id="主要内容-1">主要内容</h5><p>配置transformer和RKN融合的实验虚拟环境 测试代码</p><p>下载的是<a href="https://github.com/Kyubyong/transformer" target="_blank" rel="noopener">Kyubyong/transformer</a> 代码，准备融合RKN</p><p>具体的配置如下：</p><hr><p><strong>Requirements</strong></p><ul><li>python==3.x (Let's move on to python 3 if you still use python 2)</li><li>tensorflow==1.12.0</li><li>numpy&gt;=1.15.4</li><li>sentencepiece==0.1.8</li><li>tqdm&gt;=4.28.1 #显示进度条的包</li></ul><hr><p>github下载代码，放到<code>C:\Users\Administrator\PycharmProjects</code>目录下，文件名为 <code>transformer-master</code></p><p><code>python714</code>是可以运行RKN的，在笔记本上，根据<code>python714</code> clone了<code>ronghe</code> ，并添加所需要的包</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda install tqdm</span><br><span class="line"></span><br><span class="line">pip install  sentencepiece==0.1.8 <span class="comment"># conda 安装出错 ，于是用pip安装</span></span><br></pre></td></tr></tbody></table></figure><h5 id="出现问题-1"><strong>出现问题</strong></h5><p>此代码不是官方代码，虽然可以实现transformer，但是使用的数据集是小型的<code>IWSLT 2016 de-en</code>，而不是transformer论文中使用的大型数据集WMT，但是官方代码又很难读，而且有很多用不到的接口</p><p>在纠结，要用目前的代码进行融合，还是用官方的代码呢？</p><p>问过师兄，现在还是不用官方的transformer代码，就用目前的代码，只是验证，不用管实验数据集，先将现在的代码结合RKN再说</p><h4 id="section-2"><strong>2020-09-08</strong></h4><h5 id="主要内容-2">主要内容</h5><p>阅读整理RKN的代码</p><p>将昨天的transformer数据集无法读取的问题解决</p><p>将RKN在R740上跑，并保存在<code>test1.txt</code>文件中，可以用<code>less</code> 查看</p><h5 id="遇到问题">遇到问题</h5><p>RKN代码读的一脸懵</p><p>transformer代码bug还未修复 <span class="github-emoji" style="display:inline;vertical-align:middle;color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f62d.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f62d.png?v8">😭</span></p><h4 id="section-3"><strong>2020-09-09</strong></h4><h5 id="主要内容-3">主要内容</h5><p>在R740新建环境<code>ronghe</code>，根据虚拟环境<code>lsjRKN</code>来建的</p><p>第三方包也安装成功</p><h5 id="遇到问题-1"><strong>遇到问题</strong></h5><ol type="1"><li></li><li>添加上encoding='ascii',error='ignore'就可以解决</li></ol><p><img src="https://i.loli.net/2020/09/11/vCfMGWJ975VEiRl.png" alt="image-20200909094144959"></p><h6 id="注">注</h6><p>在解决完之后，一定要看报错的位置，可能这个已解决，但是其它相同的问题不同位置也会报错，同样解决就可以了</p><ol start="2" type="1"><li>在笔记本上跑此实验，发现内存不够，超出内容超过10%</li></ol><p><code>Allocation of 1196032000 exceeds 10% of system memory</code></p><p><strong>解决</strong></p><p>减少<code>banch_size</code> ， 但是还是超出，但是应该是在现有环境下实验可以跑通的，于是想着在R740上跑</p><p>在R740跑<code>prepro.py</code>实验，如下输出，并<code>INFO：done</code> （表示完成）</p><p><img src="https://i.loli.net/2020/09/11/BAb9krnJ8dCYKTX.png" alt="image-20200909132740936"></p><p>开始跑<code>train.py</code> 并将输出保存在train99.txt中（9月9日）</p><p><img src="https://i.loli.net/2020/09/11/7UpXhTBcnGNILtH.png" alt="image-20200909134011883"></p><p>这个WARNING是什么意思呢？</p><p>猜想：源代码需要的是<code>tf1.12</code>版本 我配置的是<code>tf 1.14</code>版本，不知道是不是这个原因 。晚上回寝百度一下</p><ol start="3" type="1"><li>在740中跑的太慢了，不知道具体原因。在看源代码进行修改</li></ol><h4 id="section-4">2020-09-10</h4><h5 id="主要内容-4">主要内容</h5><p>更改虚拟环境，可以使用GPU对实验进行加速</p><p>阅读transformer的代码，明天融合</p><p>对跑实验的一些warning都已经修改了，复制项目名字为<code>transformer-mas</code></p><p>上传到R740中，命名<code>transformer-mas</code></p><h5 id="遇到问题-2">遇到问题</h5><p><img src="https://i.loli.net/2020/09/11/HUMOjGrYSiZlPo5.png" alt="image-20200910200948426" style="zoom:200%;"></p><p>新建<code>ronghe3</code>虚拟环境</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda install python==3.6.10  <span class="comment">#这样可以 ，但是为什么 python==3.6.1 和  python==3.6.0 是不可以的呢？？</span></span><br><span class="line">pip install tensorflow-gpu==1.13.1</span><br></pre></td></tr></tbody></table></figure><p>在已经安装了tensorflow-gpu的<code>ronghe3</code>基础上，克隆了<code>ronghe4</code>，进行接下来的操作</p><h6 id="注-1">注</h6><p>如果执行<code>conda install tensorflow==1.13.1</code></p><p>安装错误 ，导入不了tensorflow-gpu，应该是和CUDA版本不匹配</p><p><img src="https://i.loli.net/2020/09/11/CgXZf5vQzdODpyU.png" alt="image-20200910211644840"></p><h5 id="参考">参考</h5><p><a href="https://www.tensorflow.org/install/gpu?hl=zh-cn" target="_blank" rel="noopener">tensorflow官方，GPU 支持</a></p><p><code>ronghe5</code></p><p><code>pip install tensorflow-gpu==1.12.0</code></p><p>还是跑不通</p><p><code>ronghe6</code></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda install python==3.6.10  <span class="comment">#这样可以 ，但是为什么 python==3.6.1 和  python==3.6.0 是不可以的呢？？</span></span><br><span class="line">pip install tensorflow-gpu==1.14.0</span><br></pre></td></tr></tbody></table></figure><p>终于可以跑通了，不会报错了！！！</p><p>测试安装的tensorflow是否可用GPU，可以使用。测试如下：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pyhton <span class="comment">#进入python操作环境</span></span><br><span class="line"></span><br><span class="line">import tensorflow as tf </span><br><span class="line">sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))</span><br></pre></td></tr></tbody></table></figure><p>RKN实验跑完了，保存在test2.txt中</p><blockquote><p>tensorflow-gpu 1.5版本及以上要求CUDA版本为9.0</p><p>tensorflow-gpu 1.3及以上版本要求cudnn版本为V6及以上</p></blockquote><h4 id="section-5">2020-09-11</h4><h5 id="主要内容-5">主要内容</h5><p>解决在linux显示图形的问题</p><p>解决transformer实验报错</p><h5 id="遇到问题-3">遇到问题</h5><ol type="1"><li>用xshell在服务器linux端只能显示控制台输出，如果想要显示图像，比如<code>matplotlib</code>包，则要下载<code>xmanage</code></li></ol><p>由于需要收费，没有下载</p><p><img src="https://i.loli.net/2020/09/11/zkqMvhElw2ubg9U.png" alt="image-20200911212759446"></p><p>解决方法： 可以用<code>plt.savafig</code>保存到服务器，再保存在本地笔记本</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line">matplotlib.use(<span class="string">'Agg'</span>)</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.font_manager <span class="keyword">import</span> FontProperties</span><br><span class="line">X = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</span><br><span class="line">dataY = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>]</span><br><span class="line">plt.xlabel(<span class="string">"x轴"</span>);</span><br><span class="line">plt.ylabel(<span class="string">"y轴"</span>);</span><br><span class="line">plt.savefig(<span class="string">"./lisijian.png"</span>,dpi=<span class="number">100</span>) <span class="comment">#保存在本文件夹下的lisijian.png</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p>报错<code>_tkinter.TclError: couldn't connect to display "localhost:32.0"</code></p><p>原因： 问题在于，您使用的是一个交互式后端，它试图为您创建图形窗口，但由于您断开了启动模拟时可用的x服务器，所以失败了。</p><p>解决方法：使用非交互式后端(请参见<a href="https://matplotlib.org/faq/usage_faq.html#what-is-a-backend" target="_blank" rel="noopener">后端</a>？)比如：Agg(用于Png格式，PDF, SVG或PS。在生成图形的脚本中，只需在import matplotlib.pyplot as plt之前调用matplotlib.use(）即可</p><p>比如<code>matplotlib.use('Agg')</code></p><ol start="2" type="1"><li><p>在transformer实验中，才开始没注意，今天才发现有一个错误，如下:</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">AssertionError: Bad argument number <span class="keyword">for</span> Name: 3, expecting 4</span><br></pre></td></tr></tbody></table></figure></li></ol><p>解决方法：因为对结果的影响不可观,所以就没去在意 ,后面发现用其他docker并没有多少问题,而且每次都出现一堆warning很影响美观性,于是百度准备解决这个问题</p><p><strong>后来发现是有个gast的库版本太高,导致不兼容的问题,降级gast即可解决</strong></p><p>使用pip进行降级</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install --user gast==0.2.2</span><br></pre></td></tr></tbody></table></figure><p><strong>待解决：</strong></p><p><strong>tensorflow的兼容性问题 cuda的兼容性问题 ？？</strong></p><p><strong>一般如果要对服务器上的实验进行更改的话，怎能会简单一些？？</strong></p><h4 id="section-6">2020-09-13</h4><h5 id="主要内容-6">主要内容</h5><p>解决transformer报错的问题</p><p>解决tensorflow目前不支持CUDA10.1的问题</p><p>修改：</p><p>将batch 由 128 改为 32</p><p>将maxlen1 和maxlen2 由100改为101</p><h5 id="遇到问题-4">遇到问题</h5><ol type="1"><li><p>在运行transformer代码的时候，程序报错如下（部分内容，具体参考<code>train911.txt</code>）：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">"train.py"</span>, line 81, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    hypotheses = get_hypotheses(num_eval_batches, num_eval_samples, sess, y_hat, m.idx2token)</span><br><span class="line">  File <span class="string">"/home/dell2/LSJ/transformer-master/utils.py"</span>, line 144, <span class="keyword">in</span> get_hypotheses</span><br><span class="line">    h = sess.run(tensor)</span><br><span class="line">  File <span class="string">"/home/dell2/anaconda3/envs/ronghe6/lib/python3.6/site-packages/tensorflow/python/client/session.py"</span>, line 950, <span class="keyword">in</span> run</span><br><span class="line">    run_metadata_ptr)</span><br><span class="line">  File <span class="string">"/home/dell2/anaconda3/envs/ronghe6/lib/python3.6/site-packages/tensorflow/python/client/session.py"</span>, line 1173, <span class="keyword">in</span> _run</span><br><span class="line">    feed_dict_tensor, options, run_metadata)</span><br><span class="line">  File <span class="string">"/home/dell2/anaconda3/envs/ronghe6/lib/python3.6/site-packages/tensorflow/python/client/session.py"</span>, line 1350, <span class="keyword">in</span> _do_run</span><br><span class="line">    run_metadata)</span><br><span class="line">  File <span class="string">"/home/dell2/anaconda3/envs/ronghe6/lib/python3.6/site-packages/tensorflow/python/client/session.py"</span>, line 1370, <span class="keyword">in</span> _do_call</span><br><span class="line">    raise <span class="built_in">type</span>(e)(node_def, op, message)</span><br><span class="line">tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[111,100] = 100 is not <span class="keyword">in</span> [0, 100)</span><br><span class="line"> [[node encoder_1/positional_encoding/embedding_lookup (defined at /home/dell2/LSJ/transformer-master/modules.py:290) ]]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Original stack trace <span class="keyword">for</span> <span class="string">'encoder_1/positional_encoding/embedding_lookup'</span>:</span><br><span class="line">  File <span class="string">"train.py"</span>, line 48, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    y_hat, eval_summaries = m.eval(xs, ys)</span><br><span class="line">  File <span class="string">"/home/dell2/LSJ/transformer-master/model.py"</span>, line 176, <span class="keyword">in</span> <span class="built_in">eval</span></span><br><span class="line">    memory, sents1, src_masks = self.encode(xs, False)</span><br><span class="line">  File <span class="string">"/home/dell2/LSJ/transformer-master/model.py"</span>, line 53, <span class="keyword">in</span> encode</span><br><span class="line">    enc += positional_encoding(enc, self.hp.maxlen1)</span><br><span class="line"></span><br><span class="line">  File <span class="string">"/home/dell2/LSJ/transformer-master/modules.py"</span>, line 290, <span class="keyword">in</span> positional_encoding</span><br><span class="line">     outputs = tf.nn.embedding_lookup(position_enc, position_ind)</span><br></pre></td></tr></tbody></table></figure><p>可以追溯到位置编码部分，出现了<code>InvalidArgumentError: indices[111,100] = 100 is not in [0, 100)</code>的错误</p><p>于是在我将超参数maxlen由100改为101，可以正常运行</p><h4 id="参考-1">参考</h4></li><li><p>在rognhe6中安装的tensorflow-gpu：1.14是不支持CUDA10.1版本的，只支持到CUDA10.0版本。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">tf.test.is_gpu_available()</span><br></pre></td></tr></tbody></table></figure><p>输出如下：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pciBusID: 0000:db:00.0</span><br><span class="line">2020-09-13 09:32:43.541828: Could not dlopen library <span class="string">'libcudart.so.10.0'</span>; </span><br><span class="line">dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory;</span><br><span class="line"></span><br><span class="line">False</span><br></pre></td></tr></tbody></table></figure></li></ol><p>可见是不支持目前ubuntu中的CUDA环境，参考了博客，修改如下：</p><p>将<code>cudatoolkit=10.0</code>安装到当前环境下</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install cudatoolkit=10.0</span><br></pre></td></tr></tbody></table></figure><p>问题解决</p><h5 id="参考-2">参考</h5><blockquote><p><a href="https://blog.csdn.net/qq_28193019/article/details/103146116" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/qq_28193019/article/details/103146116</a></p><p><a href="https://zhuanlan.zhihu.com/p/115611908" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/115611908</a></p></blockquote><ol start="3" type="1"><li>可以继续跑实验，可以用GPU，但是还是出现了一些问题</li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Resource exhausted: OOM when allocating tensor with shape[1024,98,64] and <span class="built_in">type</span> <span class="built_in">float</span> on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc</span><br></pre></td></tr></tbody></table></figure><p>显示内存不够，于是我将batch_size从128改为32 ，可以正常运行了</p><p>或者可以考虑使用多个GPU呢？</p><h5 id="参考-3">参考</h5><blockquote><p><a href="https://blog.csdn.net/Will_Ye/article/details/89878588" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/Will_Ye/article/details/89878588</a></p><p><a href="https://blog.csdn.net/Will_Ye/article/details/89878588" target="_blank" rel="noopener">OOM ResourceExhaustedError 的完美解决方法</a></p></blockquote><h4 id="section-7">2020-09-13</h4><p>transformer-mas训练部分已经跑了8个epoch，只用了一个GPU，跑的有点慢，于是暂停，以后再跑。</p><p>开始跑test.py文件，但是在跑的时候，<code>TypeError: stat: path should be string, bytes, os.PathLike or integer, &gt; not NoneType</code></p><p>路径写的不对，在ckpt中添加路径即可</p><h4 id="section-8">2020-09-29</h4><p>利用最新的ckpt进行测试，显示的是</p><p>想着可能最新的epoch的图和数据没有完全写入文件中，所以我在log/1文件夹中将最新的ckpt删除了，让次新的ckpt来进行测试。</p><p>发现结果还是unk ，不知道为什么？ 是不是因为我一个epoch保存了多个ckpt</p><h4 id="section-9">2020-09-30</h4><p>今天的实验终于解决了，可以有好的结果了。这段时间真的太煎熬了。不过还是学到了不少东西。</p><p>之前修改的其它地方是没有问题的，不需要再变，是在epoch</p><p>计划以及疑问：</p><p>如何锁死进程， 多个的话，会显示显存不足</p><p>为什么必须要跑完才能显示结果呢？ 在哪体现的呢</p><p>平时想要快速测试代码是否好用？ 有什么办法</p><p>哪些人tensorflow用的好，以后经常请教</p><p>模型验证的作用是啥？ 在代码中没有体现出来啊</p><p>总结遇到的困难以及学到的知识</p><p>解决onetab保存的标签</p><h3 id="讨论">🚀讨论</h3><h4 id="section-10">2020.10.16</h4><p>1.transformer和图神经网络/图卷积网络的联系 2.传统方法处理小样本学习 论文 + 代码 3.目标检测和小样本学习的联系 论文+ 代码 4.根据读的上述论文，多跑小样本实验，了解基本思路框架逻辑 5.根据已经读过的论文，改进的transformer应用到图像处理中（transformer可以处理目标检测，那么也可以解决小样本学习），验证 6.将图像处理中flatten变换为1*1卷积 ，验证 7.transformer做时间序列聚类 ，验证</p><h4 id="section-11">2020.10.23</h4><p>任务（做完讨论）： 1.阅读BERT论文+跑代码</p><ol start="2" type="1"><li>nlp领域的小样本学习研究情况</li></ol><p>研究方向: 1.参考BERT，将transformer-XL 改为双向模型，结合小样本学习的思想，用于nlp任务</p><p>2.在transformer处理图像的DETR模型中，加入小样本框架（MAML等），用于目标检测。 参考论文《Frustratingly Simple Few-Shot Object Detection》</p><p>3.可以用transformer来替换传统小样本学习模型框架中的特征提取器 （relation network、Adaptive Subspace等模型）</p><p>其它：</p><ol type="1"><li>将论文《Adaptive Subspaces for Few-Shot Learning》发给翟滨</li></ol><h4 id="section-12">2020.10.28</h4><p>任务： 1. NLP + 小样本学习的论文详细阅读 ★ 2. 将BERT思想融合到transformer-XL代码中 ，处理transformer-XL模型的任务；熟练掌握这两个模型 ★</p><ol start="3" type="1"><li>看GPT系列模型论文，要对transformer系列有清晰的认识</li><li>查找文本处理方面transformer系列最新的模型</li><li>查找基于对抗的文本处理模型</li></ol><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      记录在跑实验的一些配置以及遇到的问题解决，保持更新
    
    </summary>
    
    
    
      <category term="故障排除" scheme="http://yoursite.com/tags/%E6%95%85%E9%9A%9C%E6%8E%92%E9%99%A4/"/>
    
  </entry>
  
  <entry>
    <title>2020-09-06-学习工作杂记</title>
    <link href="http://yoursite.com/2020/09/06/2020-09-06-%E5%AD%A6%E4%B9%A0%E5%B7%A5%E4%BD%9C%E6%9D%82%E8%AE%B0/"/>
    <id>http://yoursite.com/2020/09/06/2020-09-06-%E5%AD%A6%E4%B9%A0%E5%B7%A5%E4%BD%9C%E6%9D%82%E8%AE%B0/</id>
    <published>2020-09-06T06:38:39.000Z</published>
    <updated>2020-09-19T14:02:42.167Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言">前言</h3><p>多做总结，提高效率，不要拖延</p><h3 id="怎样从熬夜中恢复过来">怎样从熬夜中恢复过来</h3><p>1． 不要打盹，5min不能够得到休息</p><p>2． 吃早餐 一个小时内吃早餐（全谷物和蛋白质） ，可以充满活力，认知能力可以提高，最好不摄入糖，会让人发困</p><p>3． 出去走。自然光可以让身体发热</p><p>4． 开始办公时候，喝一杯咖啡</p><p>5． 工作：首先完成最困难的部分，最开始的几个小时是一天中效率最高的时候</p><p>6． 会议之前可以喝一杯咖啡，有效时间是半小时</p><p>7． 午饭不吃过多的糖，会犯困</p><p>8． 下午可以喝一杯咖啡这时候是最困的时候。三点之后不能摄入咖啡，有效时间7 小时</p><p>9． 下午可以做一下简单的事情</p><h3 id="在家寝室学习">在家&amp;寝室学习</h3><ol type="1"><li><p>行为影响态度。换掉睡衣，接近类似学校的状态。希望自己成为什么样子， 就穿成什么样子</p></li><li></li></ol><p><img src="https://i.loli.net/2020/09/06/QDutkAwlLGgKTBo.png"></p><ol start="3" type="1"><li>先做一道题再说。（计划太多，无从下手。过分犹豫）不要想太多，直接动手</li></ol><p><img src="https://i.loli.net/2020/09/06/CerZI164jAU85qO.png"></p><ol start="4" type="1"><li>拒绝含糖食物</li></ol><p>自控力需要能量的供给，学习前可以吃块糖，可以补充能量。但是减少高gi食物，如酸奶果汁薯片，或者碳水类食物（米饭面条土豆）。多吃瘦肉、蔬菜、水果，能够增强自控力，还能瘦</p><ol start="5" type="1"><li><p>不要用时间做计划，用学习量做计划。（因为会拖延）拒绝整点学习、计划学习时间，采用今天背多少单词等，一个清晰明确的目标会事半功倍</p></li><li><p>保持工作区的整洁，不放无关的物品。使手机飞行模式、黑白模式</p></li></ol><p><img src="https://i.loli.net/2020/09/06/UO5JydTCmLIKa12.png"></p><h3 id="戒掉手机避免用意志力来克制">戒掉手机（避免用意志力来克制）</h3><p>1． 替代法 并不是真正想做，而是习惯了某种行为。可以买一个手机模型，终止大脑的无意识行为，给大脑一个选择的机会</p><p>2． 心理暗示。‘我不玩手机‘ 而不是’我不能玩手机‘</p><p>3． 优化环境。环境的影响很大。搭建一个良好的环境。睡觉前把手机放在客厅，学习时增加获得手机的难度</p><p>4． 负面反馈。人们对于损失和负面事件的敏感度高于正面事件的敏感</p><p>5． 看实时学习视频，看到别人学习 自己也不好意思玩</p><p>休息放空自己，会使得注意力更集中</p><p>把社交软件放在小文件夹里再放到手指不容易碰到的地方，如果一段时间又习惯了点这个位置的社交软件，就再更换桌面排布</p><h3 id="自己习惯">自己习惯</h3><p>对于我自己来说，习惯睡觉前进行一些文字记录的工作，比如写博客做总结，就是不会再去接触一些新知识。把第二天要做的事情列好，或者直接找好第二天最难工作内容的参考资料，对第二天工作内容有一个大概的印象，这样第二天一早就可以直攻克艰难的部分，避免其它琐碎的事情</p><p>起床的时候，提前找好第二天要穿的衣服，同时可以适量补充水分</p><p>在进行学习的时候，先设置5分钟，休息5分钟，再逐渐增加时间，进入状态，多学习时间不休息</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      之前看的一个b站up主关于如何高效学习的视频，觉得受到了启发，于是记录了下来，以此找到更适合自己的学习习惯
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-09-06-不确定性研究</title>
    <link href="http://yoursite.com/2020/09/06/2020-09-06-%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7%E7%A0%94%E7%A9%B6/"/>
    <id>http://yoursite.com/2020/09/06/2020-09-06-%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7%E7%A0%94%E7%A9%B6/</id>
    <published>2020-09-05T16:06:55.000Z</published>
    <updated>2020-10-10T14:04:27.458Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言">前言</h3><p>近日，旷视上海研究院长危夷晨在将门技术社群做了一次题为《Uncertainty Learning for Visual Recognition》(不确定性学习在视觉计算中的应用) Online Talk，共分为4个部分：</p><ol type="1"><li>Preliminary（基础知识）</li><li>Uncertainty in Deep Learning（深度学习中的不确定性问题）</li><li>Uncertainty in Computer Vision（不确定性的计算机视觉应用）</li><li>Summary（总结）</li></ol><p>我主要参考了前三部分的内容</p><h3 id="基础知识">基础知识</h3><h4 id="何为不确定性估计">何为不确定性估计</h4><p>要理解何为不确定性估计，我们可以先从<strong>确定性预测（deterministic prediction）</strong>开始。假设要对两张人脸进行对比，验证是否是同一个人的照片，那么可以使用人脸识别系统分别对这两张人脸图片提取特征，并使用某种度量指标衡量所提取的两个特征的相似程度，根据所预测出的相似程度来判断两张人脸图像是否从属同一个人。如果相似度很高（比如95%），则可以判断这两张人脸属于同一个人。这种通过预测一个确定性的人脸特征用来判断的方式被称为确定性预测（deterministic prediction）。</p><p>然而这个<strong>相似度分数并不总是有效</strong>，以下图中第二个例子为例，可以看到在输入图像中，一张非常清晰，另一张十分模糊，然而这个时候人脸识别系统依然给二者打出很高的相似度分数，那么面对这种情况，我们是否要相信系统给出的答案，我们是否有办法来判断系统给出这个分数的可靠程度？</p><p>为此，人们提出了另一个<strong>辅助判断的指标</strong>，即判断机器给出的答案是否可信，可信程度多少的分数被称为<strong>confidence score（置信度分数）</strong>。如下图第二行中，系统给出相似度95%，然而confidence score却只有10%，表明<strong>系统给出的相似度分数的可信度很低，因此我们在采纳系统给出的这个判断答案的时候需要十分谨慎。</strong></p><p>从这个案例可以知道，<strong>在confidence score分数背后存在一个核心思想，即很多时候机器学习系统给出的判断不一定是靠谱的，即，系统对于给出的判断具有一定程度的“不确定性”。</strong>那么此时人们就需要知道系统给出这个判断到底有几成把握，因此我们需要诸如置信度分数或者“不确定性”分数这样的额外信息来帮助我们做出更好的决策。</p><p><img src="https://i.loli.net/2020/09/06/xBqNOnrsRiM7o85.jpg" alt="img"></p><h4 id="为何不确定性重要">为何不确定性重要</h4><p>上面介绍完之后，我们再来谈谈它为什么重要。简单来讲，不确定性估计在深度学习之中有着广泛的应用场景，为其落地发挥着不可替代的重要作用，下面讲一些比较要代表性的场景：</p><ol type="1"><li><strong>高风险应用场景</strong>。这类场景<strong>需要非常精确的估计</strong>，因为一旦估计错误，可能出现严重的后果，例如医疗图像诊断、自动驾驶。</li><li><strong>大量机器学习场景</strong>。比如，在主动学习（Active Learning）这种技术框架中，模型需要确定哪些样本更值得被打标签。这也涉及到系统对于估计样本“价值程度”不确定性。同时，研究人员往往也会发现单纯使用机器学习系统进行判断时，会存在少量样本系统无法做出很好的判断，因此这时人们会邀请专家来标记这部分困难样本，以训练模型。</li><li><strong>强化学习</strong>。强化学习由于经常要权衡exploration和exploitation操作，因此如何确定每一台机器的概率分布是否被准确估计，就是对这台机器模型参数的不确定性估计。</li><li><strong>对处于训练数据分布之外情况的检测</strong>。由于很多时候测试数据并不在训练数据中，因此如果测试数据超出了训练数据的数据分布，那这样的预测是没有准确度可言的，这时候就需要一个额外的不确定性估计来确认对当前的预测有多大把握。</li></ol><h4 id="两种不确定性">两种不确定性</h4><p>接下来，我们界定一下不确定性的分类问题。一般来讲，不确定性可以分为两类：</p><ol type="1"><li><strong>数据的不确定性</strong>：也被称为偶然（Aleatoric）不确定性，它描述的是<strong>数据中内在的噪声，即无法避免的误差，这个现象不能通过增加采样数据来削弱。</strong>例如有时候拍照的手稍微颤抖画面便会模糊，这种数据是不能通过增加拍照次数来消除的。因此解决这个问题的方法一般是提升数据采集时候的稳定性，或者提升衡量指标的精度以囊括各类客观影响因素。</li><li><strong>模型的不确定性</strong>：也被称为认知（Epistemic）不确定性。它指出，<strong>模型自身对输入数据的估计可能因为训练不佳、训练数据不够等原因而不准确，与某一单独的数据无关</strong>。因此，认知不确定性测量的，是训练过程本身所估计的模型参数的不确定性。这种不确定性是可以通过有针对性的调整（增加训练数据等方式）来缓解甚至解决的。</li></ol><h3 id="深度学习中的不确定性问题">深度学习中的不确定性问题</h3><p><strong>如果单看深度学习网络本身，它是确定性的，例如简单的多层前馈网络，在训练好以后，其结构、权重以及对某一个样本所输出类别的概率都是确定的。因此，在深度神经网络中引入不确定性的一个方法就是引入贝叶斯法则，从而得到贝叶斯神经网络（BNN）。</strong></p><p>简单而言，如下图，贝叶斯神经网络的<strong>权重不像普通神经网络是一个具体数值，而是一个概率分布，表示每一个权重w遵循一个分布，而非之前是一个确定的数值</strong>。因此在训练和推理中，网络的权重会变化，<strong>根据分布来随机采样</strong>。通过这种方法可以建模各个参数本身存在的不确定性。</p><p><img src="https://i.loli.net/2020/09/06/6IRjbGw4cfT8mSB.jpg" alt="img"></p><p>然而，由于在实际应用中参数量十分巨大，要严格根据贝叶斯公式计算后验概率几乎不现实，因此为了将网络应用于大型数据集，就<strong>需要高效的近似计算方法</strong>。早期比较有名的方法是通过马尔科夫链蒙特卡洛采样法（MCMC-sampling）来逼近假定的参数分布，但是由于这种方法很慢，因此发展出了一系列更好的<strong>近似计算后验概率</strong>的方法，如下：</p><h4 id="变分推断">变分推断</h4><p>变分推断的基本方法就是<strong>引入变分分布对BNN优化过程中涉及到的后验概率进行近似估计，这种方法较为高效。</strong></p><p><img src="https://i.loli.net/2020/09/06/nQzKO2i1PYNkvuU.jpg"></p><h4 id="dropoutbnnvi">Dropout=BNN+VI</h4><p><img src="https://i.loli.net/2020/09/06/aQRWjzwDKIJl6eL.jpg" alt="img"></p><p>这种<strong>dropout方法</strong>也称为蒙特卡洛dropout，进一步简化了对后验概率分布的近似计算，它认为常见的dropout技术实际上等于在贝叶斯网络中进行变分推断。通过上图的对比，我们可以直观理解标准神经网络经过dropout之后，在每一层随机取消一些神经元，把连接变稀疏的网络是什么样子。</p><p>可以证明，<strong>在假设每一个神经元都服从一个离散的伯努利分布的情况下，经dropout方法处理的神经网络的优化过程实际上等价于在一个贝叶斯网络中进行变分推断。</strong>由于这种结构中每个节点的权重是被多个子网络共享的，因此它的训练和推理相对高效。这项理论成果近年来得到了较多的应用。</p><p>我们在前向传播的时候，让某个神经元的激活值以<strong>一定的概率p停止工作</strong>（每一个批次都是随机），这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征。</p><p><strong>dropout掉不同的隐藏神经元就类似在训练不同的网络，随机删掉一半隐藏神经元导致网络结构已经不同，整个dropout过程就相当于对很多个不同的神经网络取平均。而不同的网络产生不同的过拟合，一些互为“反向”的拟合相互抵消就可以达到整体上减少过拟合。</strong></p><h5 id="dropout具体工作流程">Dropout具体工作流程</h5><p>假设我们要训练这样一个神经网络，如图所示。</p><p><img src="https://i.loli.net/2020/09/06/SetbpsjYxEX1yQZ.jpg" alt="标准的神经网络"></p><p>输入是x输出是y，正常的流程是：我们首先把x通过网络前向传播，然后把误差反向传播以决定如何更新参数让网络进行学习。使用Dropout之后，过程变成如下：</p><p>（1）首先<strong>随机（临时）</strong>删掉网络中一半（dropout=0.5时）的隐藏神经元，输入输出神经元保持不变（图中虚线为部分临时被删除的神经元）</p><p><img src="https://i.loli.net/2020/09/06/GnirX4u39lSmyUg.jpg" alt="部分临时被删除的神经元"></p><p>（2） 然后把输入x通过修改后的网络前向传播，然后把得到的损失结果通过修改的网络反向传播。一小批训练样本执行完这个过程后，在<strong>没有被删除的神经元上</strong>按照随机梯度下降法更新对应的参数（w，b）。</p><p>（3）然后继续重复这一过程：</p><ul><li><strong>恢复被删掉的神经元</strong>（此时被删除的神经元保持原样，而没有被删除的神经元已经有所更新）</li><li>从隐藏层神经元中<strong>随机选择</strong>一个一半大小的子集临时删除掉（备份被删除神经元的参数）。</li><li>对一小批训练样本，先前向传播然后反向传播损失并根据随机梯度下降法更新参数（w，b） （没有被删除的那一部分参数得到更新，删除的神经元参数保持被删除前的结果）。</li></ul><p>不断重复这一过程。</p><h5 id="参考">参考</h5><blockquote><p><a href="https://zhuanlan.zhihu.com/p/38200980" target="_blank" rel="noopener">深度学习中Dropout原理解析</a></p></blockquote><h4 id="模型融合">模型融合</h4><p>这也是一种进行不确定性估计的基本方法，其大致思路是，<strong>从一个数据集中进行多次随机采样，分别训练模型，然后再将这些模型的推理结果综合，其均值作为预测结果，方差作为预测的不确定性。</strong>另外需要强调的是，蒙特卡洛dropout可以认为是一种特殊的模型融合方法。</p><p><img src="https://i.loli.net/2020/09/06/noXFIR2ACtwpmk1.jpg" alt="img"></p><h3 id="回归问题中的数据不确定性">回归问题中的数据不确定性</h3><p>这是一种数据估计的标准做法。<strong>给定输入x_i，解一个函数f(x_i)，使得它逼近ground truth y_i。假设这个函数f(x_i)遵循一个高斯分布，那么其均值就是y_i，方差就是σ（也依赖于x_i）。</strong></p><p><strong>这时，如果对这个高斯分布取似然度，再取负的log值，那么就可以得到下图中的损失函数L。因此在优化的时候，除了希望优化f(x_i)逼近y_i，同时也需要优化σ(x_i)，它表示这个高斯分布的不确定性，σ越大越不确定。</strong></p><p>因此当f很容易逼近y的时候，那么公式中第一项L2范数就会很小，这时即便σ也小，但结果依然不会很大；当f很难逼近y，即f很难学习的时候，第一项中的L2范数就会很大，这时优化过程就会使得σ也变大，从而使得整个第一项减小，因此学到的σ会随着数据学习的难度做自我调整。</p><p><img src="https://i.loli.net/2020/09/06/nlD62kW1pXygAV4.jpg" alt="img"></p><h4 id="简单例子">简单例子</h4><p>我们借助一个直观例子来理解模型不确定性与数据不确定性。首先这里的<strong>ground truth函数为一个正弦函数</strong>，即图中橙色的点是测试数据，而<strong>训练数据是从[-5，+5]区间采样的蓝色点，研究人员对每一个蓝色点都添加了高斯噪声，因此可以看到这些蓝色点明显偏离ground truth。</strong></p><p>下方左图是用贝叶斯网络加dropout进行的<strong>模型不确定性估计</strong>。<strong>红色曲线为估计出来的预测值，延其上下分布的黄色面积则为每一个点对应的方差</strong>。在进行模型不确定性估计时，系统会对每个输入点估计多次，每次会随机采样模型的权重，以求出对每个输入点多次预测所得到的均值和方差。可以发现，蓝色点区域之外的部分预测的方差很大，这是因为模型没有见过这样的数据。（<strong>因为蓝色是训练数据</strong>，其它是测试数据，没见过的，所以方差就会较大，也就是不确定性较高）</p><p>下方右图中红色曲线为估计出来的预测值，是<strong>数据不确定性估计</strong>，曲线上下的黄色跨度就是每一个点通过数据不确定性估计方法所学出的方差。可以发现，原本输入数据中有噪声的部分，其预测出的方差比较大，反映出模型对这样的输出拥有较大的不确定性。</p><p><img src="https://i.loli.net/2020/09/06/EjFQnPko6pVhYH5.jpg" alt="img"></p><h3 id="不确定性的计算机视觉应用">不确定性的计算机视觉应用</h3><p><img src="https://i.loli.net/2020/09/06/Gw6pNAvuisDe18a.png" alt="img"></p><p>尽管不确定性在机器学习中已经有很长历史，但是直到2017年（就我所知）随着NeurlPS论文<em>What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision</em>的提出，它才开始真正应用在基于深度学习的视觉预测任务中。这篇论文本身没有太多方法创新，通过将已知的方法 用于语义分割与深度回归任务，取得了不错的结果。<strong>通过在模型中引入不确定性估计的已有理论成果，使得原本任务的精度得到了显著提升。</strong></p><p>通过论文给出的定性结果可以较为直观的理解模型不确定性和数据不确定性。如下图，系统估计出来的不确定性是有明确含义即很容易理解的，图中上半部分做语义分割，下半部分做深度估计。</p><p><img src="https://picb.zhimg.com/80/v2-cf9cf0d7715af33b38dc8fa8b71b8aa0_1440w.jpg" alt="img"></p><p><strong>整张图的第4、5列分别是数据不确定性和模型不确定性的结果。红色部分表示不确定程度较大，蓝色部分表示较为确定。从数据不确定性结果（第4列）可以看到，红色部分往往出现在物体边界处，表示这些区域的像素更加具有二义性，系统不太清楚这部分像素究竟属于前景还是背景，另外这部分信息在训练数据中（即ground truth）往往也较模糊。可以发现，系统给出的数据不确定结果符合人类直观理解。</strong></p><p><strong>从模型不确定性结果（第5列）可以看到，模型对出现人的部分给出了很高的不确定性，这是因为模型在训练中很少遇到人的数据，因此模型很难估计出人所处位置的深度，将该区域标记为高度不确定。</strong></p><h4 id="物体检测中的数据不确定性">物体检测中的数据不确定性</h4><p><img src="https://i.loli.net/2020/09/06/RrKDqx7kFG8mJyl.jpg" alt="img"></p><p>在物体检测任务中，很大一部分不确定性来源于标注数据的不确定。上图给出了几个典型例子，可以看到，在标注边界框的时候，由于存在各种物体角度、遮挡，所以往往很难评价一个边界框标注的好坏。由于标注规则不一、数据本身存在的各种不确定性，因此具有二义性的数据标注会导致具有二义性的学习结果，从而将不确定性引入了模型，进一步输出结果也是不确定的。</p><p>针对这个问题，有研究人员在CVPR 2019、ICCV 2019提出了两篇颇有价值的论文，其核心思想类似，将每一个边界框的4个坐标均认为呈高斯分布，然后分别估计其均值和方差。用上述介绍的数据不确定性回归公式来替代传统的L1损失，将原来所需要预测的4个变量扩充为8个变量。</p><p>因此，这种方法除了可以估计边界框每一个坐标之外，还让它们都带有了一个不确定性参数。利用这些不确定性数据，可以进一步做很多事情（比如在NMS中作为权重来对边界框位置进行投票）。</p><h4 id="人脸识别中的模型不确定性">人脸识别中的模型不确定性</h4><p><img src="https://i.loli.net/2020/09/06/nWDBsXI2dNfaHTV.jpg" alt="img"></p><p>对于在人脸识别任务中如何估计模型不确定性，推荐大家上图中的论文工作，其核心思想是，将BNN+dropout用到人脸识别任务中，如图所示，dropout层（红色）被加在每一个卷积block之后，从而构建了一个蒙特卡洛Dropout网络。在训练过程中，每当流程到达这些层的时候，就会随机丢掉一些神经元，从而实现模拟参数分布的效果。在测试过程中，每一个图像都会经过该网络多次，进而可以多这些结果估计均值与方差，将方差作为预测结果的不确定性。</p><h4 id="人脸识别中的数据不确定性pfe方法">人脸识别中的数据不确定性：PFE方法</h4><p><img src="https://i.loli.net/2020/09/06/qPMvAkWN1UHK2e9.jpg" alt="img"></p><p>PFE方法全称为Probabilistic Face Embeddings，其核心思想是用概率分布来替代传统确定的人脸嵌入特征。传统的方法会将输入图像映射到一个高维空间中，得到一个表示特征的向量，然而对于这些方法而言，输出的向量都是确定的，因此被称为deterministic embedding。PFE引入了不确定性，将输出向量认为是一个概率分布，而不再是一个确定的向量，它有一个均值μ、方差σ。</p><p>均值代表对图像的嵌入，方差描述了模型对输入数据的不确定性。该方法希望同时估计μ和σ，并且σ能够根据输入图像的质量、包含噪声的程度进行自适应的学习。在上图右方的示例中可以看到，每一个输出的特征z不再是一个点，而是一个高斯形状的概率分布（椭圆），椭圆的大小就描述了估计的不确定性。</p><p><img src="https://i.loli.net/2020/09/06/O8mvS1VTIHodxZ5.png" alt="img"></p><p>从具体实现方法来看，PFE的创新值得借鉴，它并不直接去估计每一个均值μ，而是通过一个事先已经训练好的人脸识别模型来抽取每个样本的特征μ_i，然后研究人员再在网络中加入一个小分支，来对每个样本输出一个方差（比如假设μ_i是一个512维的向量，那么此时也会输出一个对μ_i的每一维度单独估计方差的512维方差向量）。</p><p>进一步，论文提出了一种新的metric——mutual likelihood score（MLS），来衡量两个分布间的距离。上图公式中x_i和x_j是两个样本在特定空间中的高斯分布，两个分布所得到的MLS数值就代表了其相似度。在训练过程中，针对所有positive 样本，计算负的MLS数值作为损失，并最小化该损失目标函数，进而可以估计新增加分支（估计方差的分支）的参数。</p><p><img src="https://picb.zhimg.com/80/v2-b2a72843a1b2199fb213df0e93e9d570_1440w.jpg" alt="img"></p><p>上图是论文对方差的解释，较为直观。可以发现红框标注出来的（方差超过一定阈值）图片都是姿态有较大变化、模糊、或者有遮挡的图片，系统都认为它们识别起来有较大不确定性；而正面、高清的图片不确定性普遍较小。为了进一步验证学习出来的不确定性是否能够有效解释图像质量，PFE在下方左图中进行了有关在低质量图像之间使用传统cosine相似度计算是否可靠的研究。</p><p>研究人员对清晰图片添加了不同程度的噪声，蓝色线代表原图与模糊图之间的相似度分数，而红色代表两张来自不同ID的图随模糊程度的增加所计算的相似度。可以发现对于同一ID（蓝线），随着模糊程度增加，相似度也逐渐降低；而对于不同ID，随着模糊程度增加相似度却在增加。这说明依据该相似度可能会将两张来自不同ID的模糊图像错认为是同一张图。这一现象在其它很多论文中也同样被观测到。</p><p><img src="https://picb.zhimg.com/80/v2-e38487eb964f4dc56b7ba89689ea5158_1440w.jpg" alt="img"></p><p>然而在经过PFE论文提出的MLS相似度修正之后，情况得到了很大改善。如右图，当图片模糊度增加时，对同样ID的图来说，其相似度没有变得太小，而不同ID图像的相似度也没有变得太大。这个实验证明这种计算图像相似度的新metric在面对低质量图片时更加鲁棒。</p><h4 id="pfe方法的缺陷">PFE方法的缺陷</h4><p>虽然PFE方法取得了重要进展，但是缺点也很明显，因为它并没有学习身份特征（identity-feature），每一个identity的特征嵌入是确定的，PFE只是增加了一个估计方差的小网络分支，这导致必须用一个新的metric（即MLS）来计算所有样本对的距离。而使用MLS这个度量函数带来的缺陷在实际工业应用中是代价较高的：第一，我们需要额外存储方差向量；第二，相比传统的余弦相似度，MLS相似度的计算资源消耗也更大。</p><p>受此启发，我们团队在投递给CVPR 2020的新论文中不仅做到了估计方差，同时也能更新每个样本的特征。下图为传统方法、PFE与我们团队方法的对比。</p><p><img src="https://i.loli.net/2020/09/06/pYSInMZ9R64euEC.jpg" alt="img"></p><p>可以发现，在图（a）中，虚线框出的蓝色椭圆代表一个类别，圈外存在一个正样本和负样本，而对于传统相似度计算方法来说，很难将负样本和正样本区分开来；而（b）中PFE方法对每个样本估计了一个分布，在带有分布的特征表示下，利用MLS就能够有效将正样本和负样本区分开来，但是PFE中正负样本本身是确定的；在（c）中，我们团队方法能够在估计正负样本方差的同时，也让特征本身修正得更好。</p><p><img src="https://i.loli.net/2020/09/06/ayVXfSxm1DYk5dG.png" alt="img"></p><p>上图是三种方法的对比，可以看到在最后计算相似度的时候，由于特征本身经过了调整，只需要使用cosine相似度来计算两个均值向量就可以得出答案。具体而言，我们团队提出了两种实现方法，如下：</p><h3 id="法1从头学习一个分类模型"><strong>法1：从头学习一个分类模型</strong></h3><p><img src="https://i.loli.net/2020/09/06/yZWVciHGd8bwK74.jpg" alt="img"></p><p>这种方法的主要部分与通用识别模型的结构一致，区别在于，在输出特征的位置，我们让模型输出一个有关每个样本特征的均值μ，以及一个方差σ。进一步，对于每个样本的每一次迭代而言，都随机采样一个ε（如上图最下方）。</p><p>通过这种方式得到的新样本特征s_i就是遵从均值μ、方差为σ的高斯分布采出的值，它可以模拟一个服从高斯分布的特征。通过这种简单的重新采样的技巧，就可以很好进行模型训练。在测试的时候不再需要采样，仅需要将已经得到的均值μ作为特征来计算相似度即可。</p><p><img src="https://i.loli.net/2020/09/06/8PcKyqBlErgUQAm.png" alt="img"></p><p>该方法的损失函数除了包含softmax以及其一切合理变种之外，还有一个KL损失，它使得每一个学出来特征的分布尽可能逼近单位高斯分布。这个损失项的引入来自于2016年一篇名为<em>Deep variational information bottleneck</em>的论文。进一步整个损失函数就可以用标准SGD方法来优化。下图解释了整个损失函数中softmax与kl损失是如何起到平衡的作用的。</p><p><img src="https://i.loli.net/2020/09/06/9wxoyQuj8M2AWem.jpg" alt="img"></p><h3 id="法2从现有模型出发学习回归模型"><strong>法2：从现有模型出发学习回归模型</strong></h3><p><img src="https://i.loli.net/2020/09/06/k8rDhcJY5ZaMQnU.jpg" alt="img"></p><p>这种方法假设输出的特征μ遵循高斯分布，目的是让它逼近期望的特征w。与PFE类似，假设输入的模型已经固定，且输出的特征μ属于类别c，则让μ逼近这个类别c的特征中心w_c（w_c来自事先训练好的人脸分类模型）。这种方法适用于当已经有一个训练好的模型，但依然希望做不确定性估计的情况。相对于PFE而言，它多做了样本特征的学习。下图解释了该损失函数中σ起到的平衡作用。</p><p><img src="https://i.loli.net/2020/09/06/3qWYVXC4QIx1vh8.png" alt="img"></p><p><strong>实验结果：</strong>在三种损失函数上的对比测试结果显示，我们团队提出的分类方法（HUM_cls）在最困难的数据集IJB-C（具有大量模糊、噪声图像）上效果最佳；在LFW、CFP-FP、YTF这些较成熟的数据集上我们提出的两种方法同其他方法区别不大；在较困难的MegFace(R1)数据集上我们团队的分类方法效果最佳。</p><p><img src="https://i.loli.net/2020/09/06/9e3k4ImOhET5Wgo.jpg" alt="img"></p><p>下图展示了在三种数据集上学习出来的方差分布情况，展示了位于不同方差位置的图像的样子。</p><p><img src="https://i.loli.net/2020/09/06/81X3Pk4o9IHZTxE.jpg" alt="img"></p><p>进一步，我们团队使用了ResNet-64作为backbone（与PFE的SOTA模型backbone深度一致），来将本文方法同SOTA方法在最困难的数据集IJB-C上进行性能对比，结果显示在每一个指标上我们团队方法均实现了领先。为了测试本文方法对噪声信息干扰的鲁棒性，团队对图片人工施加了高斯噪声（从0到40%），可以发现，当噪声越明显的时候，本文引入的不确定估计方法的优越性也约高。</p><p><img src="https://i.loli.net/2020/09/06/4VLqFkPgIvaYSm2.jpg" alt="img"></p><h3 id="参考-1">🚀参考</h3><blockquote><p><a href="https://zhuanlan.zhihu.com/p/95774787" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/95774787</a></p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      深度学习模型的不确定性估计，摘自几篇不错的博客
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>2020-09-05-tf&amp;&amp;torch知识点杂</title>
    <link href="http://yoursite.com/2020/09/05/2020-09-05-tf&amp;&amp;torch%E7%9F%A5%E8%AF%86%E7%82%B9%E6%9D%82%E7%9F%A5%E8%AF%86%E7%82%B9%E6%9D%82/"/>
    <id>http://yoursite.com/2020/09/05/2020-09-05-tf&amp;&amp;torch%E7%9F%A5%E8%AF%86%E7%82%B9%E6%9D%82%E7%9F%A5%E8%AF%86%E7%82%B9%E6%9D%82/</id>
    <published>2020-09-05T11:56:22.000Z</published>
    <updated>2020-10-27T02:26:39.411Z</updated>
    
    <content type="html"><![CDATA[<h2 id="others">🚀others</h2><h3 id="python的einops-rearrange函数">python的einops rearrange()函数</h3><p>例子：</p><p>假设我有一个3-D数组：</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[[[0,1,2],</span><br><span class="line">  [0,1,2],</span><br><span class="line">  [0,1,2]],</span><br><span class="line"></span><br><span class="line"> [[3,4,5],</span><br><span class="line">  [3,4,5],</span><br><span class="line">  [3,4,5]]]</span><br></pre></td></tr></tbody></table></figure><p>我想按列重新排列：</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[0,1,2,3,4,5],</span><br><span class="line"> [0,1,2,3,4,5],</span><br><span class="line"> [0,1,2,3,4,5]]</span><br></pre></td></tr></tbody></table></figure><p>使用einops：</p><figure class="highlight py"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">einops.rearrange(a, <span class="string">'x y z -&gt; y (x z) '</span>)</span><br></pre></td></tr></tbody></table></figure><p>并且我建议根据上下文（例如时间，高度等）为轴指定有意义的名称（而不是xyz）。 这将使您易于理解代码的作用</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In : einops.rearrange(a, 'x y z -&gt; y (x z) ')</span><br><span class="line">Out:</span><br><span class="line">array([[0, 1, 2, 3, 4, 5],</span><br><span class="line">       [0, 1, 2, 3, 4, 5],</span><br><span class="line">       [0, 1, 2, 3, 4, 5]])</span><br></pre></td></tr></tbody></table></figure><h3 id="epochiterationbatch_size">Epoch、Iteration、Batch_size</h3><blockquote><p><a href="https://blog.csdn.net/program_developer/article/details/78597738" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/program_developer/article/details/78597738</a></p></blockquote><h2 id="tensorflow">🚀tensorflow</h2><h3 id="tf.tile用法">tf.tile()用法</h3><blockquote><p><a href="https://blog.csdn.net/tsyccnh/article/details/82459859" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/tsyccnh/article/details/82459859</a></p></blockquote><h3 id="dataset-api-和-iterator">Dataset API 和 Iterator</h3><blockquote><p>Dataset API 和 Iterator</p><p><a href="https://blog.csdn.net/briblue/article/details/80962728" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/briblue/article/details/80962728</a></p><p>TensorFlow中的Dataset API</p><p><a href="https://blog.csdn.net/dqcfkyqdxym3f8rb0/article/details/79342369" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/dqcfkyqdxym3f8rb0/article/details/79342369</a></p><p>TensorFlow data模块详解</p><p><a href="https://www.weaf.top/posts/cd5ba0c4/" target="_blank" rel="noopener" class="uri">https://www.weaf.top/posts/cd5ba0c4/</a></p><p>使用Tensorflow的DataSet和Iterator读取数据</p><p><a href="https://www.jianshu.com/p/bcff8a99b15b" target="_blank" rel="noopener" class="uri">https://www.jianshu.com/p/bcff8a99b15b</a></p><p>tensorflow数据读取机制（附代码）</p><p><a href="https://zhuanlan.zhihu.com/p/27238630" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/27238630</a></p><p>Dataset API入门教程</p><p><a href="https://zhuanlan.zhihu.com/p/30751039" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/30751039</a></p><p>Dataset.from_generator</p><p><a href="https://blog.csdn.net/foreseerwang/article/details/80572182" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/foreseerwang/article/details/80572182</a></p></blockquote><p>看个简单的示例：</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">#创建一个Dataset对象</span><br><span class="line">dataset = tf.data.Dataset.from_tensor_slices([1,2,3,4,5,6,7,8,9])</span><br><span class="line"></span><br><span class="line">#创建一个迭代器</span><br><span class="line">iterator = dataset.make_one_shot_iterator()</span><br><span class="line"></span><br><span class="line">#get_next()函数可以帮助我们从迭代器中获取元素</span><br><span class="line">element = iterator.get_next()</span><br><span class="line"></span><br><span class="line">#遍历迭代器，获取所有元素</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">   for i in range(9):</span><br><span class="line">       print(sess.run(element))</span><br></pre></td></tr></tbody></table></figure><p>以上打印结果为：1 2 3 4 5 6 7 8 9</p><p>from_generator</p><p>创建Dataset由其生成元素的元素generator。</p><p>函数形式：from_generator(generator,output_types,output_shapes=None,args=None)</p><p>参数generator:一个可调用对象，它返回支持该iter()协议的对象 。如果args未指定，generator则不得参数; 否则它必须采取与有值一样多的参数args。 参数output_types：tf.DType对应于由元素生成的元素的每个组件的对象的嵌套结构generator。 参数output_shapes:tf.TensorShape 对应于由元素生成的元素的每个组件的对象 的嵌套结构generator 参数args:tf.Tensor将被计算并将generator作为NumPy数组参数传递的对象元组。</p><p>具体例子</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">#定义一个生成器</span><br><span class="line">def data_generator():</span><br><span class="line">    dataset = np.array(range(9))</span><br><span class="line">    for i in dataset:</span><br><span class="line">        yield i</span><br><span class="line"></span><br><span class="line">#接收生成器，并生产dataset数据结构</span><br><span class="line">dataset = tf.data.Dataset.from_generator(data_generator, (tf.int32))</span><br><span class="line"></span><br><span class="line">iterator = concat_dataset.make_one_shot_iterator()</span><br><span class="line"></span><br><span class="line">element = iterator.get_next()</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">   for i in range(3):</span><br><span class="line">       print(sess.run(element))</span><br></pre></td></tr></tbody></table></figure><p>以上代码运行结果：0 1 2</p><h3 id="tf-strip-和-split">tf strip() 和 split()</h3><blockquote><p><a href="https://blog.csdn.net/hjxu2016/article/details/78676859" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/hjxu2016/article/details/78676859</a></p></blockquote><h3 id="summary用法--tensorborad可视化">Summary用法 -tensorborad可视化</h3><blockquote><p><a href="https://www.cnblogs.com/lyc-seu/p/8647792.html" target="_blank" rel="noopener" class="uri">https://www.cnblogs.com/lyc-seu/p/8647792.html</a></p></blockquote><h3 id="math.ceil">math.ceil()</h3><blockquote><p><a href="https://www.runoob.com/python/func-number-ceil.html" target="_blank" rel="noopener" class="uri">https://www.runoob.com/python/func-number-ceil.html</a></p></blockquote><h3 id="format-格式化函数">.format() 格式化函数</h3><blockquote><p><a href="https://www.runoob.com/python/att-string-format.html" target="_blank" rel="noopener" class="uri">https://www.runoob.com/python/att-string-format.html</a></p></blockquote><h3 id="tf.shapea-和-a.get_shape.as_list-和-tf.split">tf.shape(A) 和 A.get_shape().as_list() 和 tf.split()</h3><blockquote><p><a href="https://www.itread01.com/content/1544436557.html" target="_blank" rel="noopener" class="uri">https://www.itread01.com/content/1544436557.html</a></p><p><a href="https://blog.csdn.net/xc_zhou/article/details/85632109" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/xc_zhou/article/details/85632109</a></p></blockquote><ul><li>tf.shape(A) # 獲取張量A（陣列，list, tensor張量）的大小，返回的是一個list</li><li>x.get_shape()，只有<strong>tensor</strong>才可以使用這種方法，返回的是一個元組</li><li>tf.split(dimension, num_split, input)：dimension的意思就是輸入張量的哪一個維度，如果是0就表示對第0維度進行切割。num_split就是切割的數量，如果是2就表示輸入張量被切成2份，每一份是一個列表。</li></ul><h3 id="tf.range">tf.range()</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">w=tf.range(<span class="number">3</span>)</span><br><span class="line"><span class="keyword">print</span> (sess.run(w))<span class="comment">#输出[0 1 2]</span></span><br></pre></td></tr></tbody></table></figure><h3 id="os.path">os.path（）</h3><table><thead><tr class="header"><th style="text-align: left;">方法</th><th style="text-align: left;">说明</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;">os.path.abspath(path)</td><td style="text-align: left;">返回绝对路径</td></tr><tr class="even"><td style="text-align: left;">os.path.basename(path)</td><td style="text-align: left;">返回文件名</td></tr><tr class="odd"><td style="text-align: left;">os.path.join(path1[, path2[, ...]])</td><td style="text-align: left;">把目录和文件名合成一个路径</td></tr><tr class="even"><td style="text-align: left;">os.path.dirname(path)</td><td style="text-align: left;">返回文件路径</td></tr><tr class="odd"><td style="text-align: left;">os.path.exists(path)</td><td style="text-align: left;">如果路径 path 存在，返回 True；如果路径 path 不存在，返回 False。</td></tr><tr class="even"><td style="text-align: left;">os.path.split(path)</td><td style="text-align: left;">把路径分割成 dirname 和 basename，返回一个元组</td></tr></tbody></table><blockquote><p><a href="https://www.runoob.com/python/python-os-path.html" target="_blank" rel="noopener" class="uri">https://www.runoob.com/python/python-os-path.html</a></p></blockquote><h3 id="embedding_lookup">embedding_lookup()</h3><p>tf.nn.embedding_lookup()就是根据input_ids中的id，寻找embeddings中的第id行。比如input_ids=[1,3,5]，则找出embeddings中第1，3，5行，组成一个tensor返回。</p><blockquote><p><a href="https://www.jianshu.com/p/7bb87873f89e" target="_blank" rel="noopener" class="uri">https://www.jianshu.com/p/7bb87873f89e</a></p><p><a href="https://www.zhihu.com/question/52250059" target="_blank" rel="noopener" class="uri">https://www.zhihu.com/question/52250059</a></p></blockquote><h3 id="模型保存和加载">模型保存和加载</h3><p>Saver的作用是将我们训练好的模型的参数保存下来，以便下一次继续用于训练或测试；Restore的用法是将训练好的参数提取出来。</p><p>1.Saver类训练完后，是以<strong>checkpoints文件形式</strong>保存。提取的时候也是从checkpoints文件中恢复变量。 Checkpoints文件是一个二进制文件，它把变量名映射到对应的tensor值 。</p><p>2.通过for循环，Saver类可以自动的生成checkpoint文件。这样我们就可以<strong>保存多个训练结果</strong>。例如，我们可以保存每一步训练的结果。但是为了避免填满整个磁盘，<strong>Saver可以自动的管理Checkpoints文件</strong>。例如，我们可以指定保存最近的N个Checkpoints文件。</p><h3 id="tensorflow模型保存和读取tf.train.saver">Tensorflow模型保存和读取tf.train.Saver</h3><p>目标：训练网络后想保存训练好的模型，以及在程序中读取以保存的训练好的模型。</p><p>首先，保存和恢复都需要实例化一个 tf.train.Saver。</p><blockquote><p>saver = tf.train.Saver()</p></blockquote><p>然后，在训练循环中，定期调用 saver.save() 方法，向文件夹中写入包含了当前模型中所有可训练变量的 checkpoint 文件。</p><blockquote><p>saver.save(sess, save_path, global_step=step)</p></blockquote><p>之后，就可以使用 saver.restore() 方法，重载模型的参数，继续训练或用于测试数据。</p><blockquote><p>saver.restore(sess, save_path)</p></blockquote><p>模型的恢复用的是restore()函数，它需要两个参数restore(sess, save_path)，save_path指的是保存的模型路径。我们可以使用<code>tf.train.latest_checkpoint（）</code>来自动获取最后一次保存的模型。如：</p><figure class="highlight javascript"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model_file=tf.train.latest_checkpoint(<span class="string">'ckpt/'</span>)</span><br><span class="line">saver.restore(sess,model_file)</span><br></pre></td></tr></tbody></table></figure><p>一次 saver.save() 后可以在文件夹中看到新增的四个文件，</p><p><img src="https://i.loli.net/2020/09/29/kRYmSZn8BbwJ4NK.png" alt="image-20200929102459806"></p><p>实际上每调用一次保存操作会创建后3个数据文件并创建一个检查点（checkpoint）文件，简单理解就是权重等参数被保存到 .ckpt.data 文件中，以字典的形式；图和元数据被保存到 .ckpt.meta 文件中，可以被 tf.train.import_meta_graph 加载到当前默认的图。</p><p>saver.restore()时填的文件名，因为在saver.save的时候，每个checkpoint会保存三个文件，如 <code>my-model-10000.meta</code>, <code>my-model-10000.index</code>, <code>my-model-10000.data-00000-of-00001</code></p><p>在<code>import_meta_graph</code>时填的就是<code>meta</code>文件名，我们知道权值都保存在my-model-10000.data-00000-of-00001这个文件中，但是如果在restore方法中填这个文件名，就会报错，应该填的是前缀，这个前缀可以使用<code>tf.train.latest_checkpoint(checkpoint_dir)</code>这个方法获取。</p><p>下面代码是简单的保存和读取模型：（不包括加载图数据）</p><figure class="highlight javascript"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"> </span><br><span class="line">#用numpy产生数据</span><br><span class="line">x_data = np.linspace(-1,1,300)[:, np.newaxis] #转置</span><br><span class="line">noise = np.random.normal(<span class="number">0</span>,<span class="number">0.05</span>, x_data.shape)</span><br><span class="line">y_data = np.square(x_data)<span class="number">-0.5</span>+noise</span><br><span class="line"> </span><br><span class="line">#输入层</span><br><span class="line">x_ph = tf.placeholder(tf.float32, [None, <span class="number">1</span>])</span><br><span class="line">y_ph = tf.placeholder(tf.float32, [None, <span class="number">1</span>])</span><br><span class="line"> </span><br><span class="line">#隐藏层</span><br><span class="line">w1 = tf.Variable(tf.random_normal([<span class="number">1</span>,<span class="number">10</span>]))</span><br><span class="line">b1 = tf.Variable(tf.zeros([<span class="number">1</span>,<span class="number">10</span>])+<span class="number">0.1</span>)</span><br><span class="line">wx_plus_b1 = tf.matmul(x_ph, w1) + b1</span><br><span class="line">hidden = tf.nn.relu(wx_plus_b1)</span><br><span class="line"> </span><br><span class="line">#输出层</span><br><span class="line">w2 = tf.Variable(tf.random_normal([<span class="number">10</span>,<span class="number">1</span>]))</span><br><span class="line">b2 = tf.Variable(tf.zeros([<span class="number">1</span>,<span class="number">1</span>])+<span class="number">0.1</span>)</span><br><span class="line">wx_plus_b2 = tf.matmul(hidden, w2) + b2</span><br><span class="line">y = wx_plus_b2</span><br><span class="line"> </span><br><span class="line">#损失</span><br><span class="line">loss = tf.reduce_mean(tf.reduce_sum(tf.square(y_ph-y),reduction_indices=[<span class="number">1</span>]))</span><br><span class="line">train_op = tf.train.GradientDescentOptimizer(<span class="number">0.1</span>).minimize(loss)</span><br><span class="line"> </span><br><span class="line">#保存模型对象saver</span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line"> </span><br><span class="line">#判断模型保存路径是否存在，不存在就创建</span><br><span class="line"><span class="keyword">if</span> not os.path.exists(<span class="string">'tmp/'</span>):</span><br><span class="line">    os.mkdir(<span class="string">'tmp/'</span>)</span><br><span class="line"> </span><br><span class="line">#初始化</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    if os.path.exists('tmp/checkpoint'):         #判断模型是否存在</span><br><span class="line">        saver.restore(sess, 'tmp/model.ckpt')    #存在就从模型中恢复变量</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        init = tf.global_variables_initializer() #不存在就初始化变量</span><br><span class="line">        sess.run(init)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">        _,loss_value = sess.run([train_op,loss], feed_dict={<span class="attr">x_ph</span>:x_data, <span class="attr">y_ph</span>:y_data})</span><br><span class="line">        <span class="keyword">if</span>(i%<span class="number">50</span>==<span class="number">0</span>):</span><br><span class="line">            save_path = saver.save(sess, <span class="string">'tmp/model.ckpt'</span>)</span><br><span class="line">            print(<span class="string">"迭代次数：%d , 训练损失：%s"</span>%(i, loss_value))</span><br></pre></td></tr></tbody></table></figure><p>注：</p><ul><li>saver 的操作必须在 sess 建立后进行。</li><li>model.ckpt 必须存在给定文件夹中，‘tmp/model.ckpt’ 这里至少要有一层文件夹，否则无法保存。</li><li>恢复模型时同保存时一样，是 ‘tmp/model.ckpt’，和那3个文件名都不一样。</li></ul><p>如果不用<code>tf.train.latest_checkpoint（）</code>来自动获取最后一次保存的模型，则怎么做呢？</p><blockquote><p><a href="https://www.jianshu.com/p/7ebee4d10e49" target="_blank" rel="noopener" class="uri">https://www.jianshu.com/p/7ebee4d10e49</a></p><p><a href="https://blog.csdn.net/mylove0414/article/details/55097486" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/mylove0414/article/details/55097486</a></p></blockquote><h3 id="saver中的max_to_keep-参数">Saver中的max_to_keep 参数</h3><h3 id="keras中的timedistributed函数">keras中的TimeDistributed函数</h3><blockquote><p><a href="https://blog.csdn.net/u012193416/article/details/79477220" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/u012193416/article/details/79477220</a></p><p><a href="https://keras.io/zh/layers/wrappers/" target="_blank" rel="noopener" class="uri">https://keras.io/zh/layers/wrappers/</a></p><p><a href="https://blog.csdn.net/zh_JNU/article/details/85160379" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/zh_JNU/article/details/85160379</a></p><p><a href="https://www.cnblogs.com/CheeseZH/p/13408658.html" target="_blank" rel="noopener" class="uri">https://www.cnblogs.com/CheeseZH/p/13408658.html</a></p></blockquote><h3 id="tf.concat详解">tf.concat()详解</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.concat([tensor1, tensor2, tensor3,...], axis)</span><br><span class="line"><span class="comment"># axis=0     代表在第0个维度拼接</span></span><br><span class="line"><span class="comment"># axis=1     代表在第1个维度拼接 </span></span><br><span class="line"><span class="comment">#axis=-1 代表倒数第一个维度</span></span><br></pre></td></tr></tbody></table></figure><blockquote><p><a href="https://blog.csdn.net/leviopku/article/details/82380118" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/leviopku/article/details/82380118</a></p></blockquote><h3 id="shape">shape</h3><p>numpy数据的形状：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.shape()</span><br></pre></td></tr></tbody></table></figure><p>list 数据的形状：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.shape(x)</span><br></pre></td></tr></tbody></table></figure><p><strong>注：</strong>如果写<code>x.shape()</code> , 则会报错<code>ValueError: invalid literal for int() with base 10</code></p><p>torsor形状：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.get_shape()</span><br></pre></td></tr></tbody></table></figure><h3 id="keras-的-fit函数">keras 的 fit函数</h3><p>fit中以call()方法的形式来run session</p><blockquote><p><a href="https://blog.csdn.net/u012526436/article/details/102488164" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/u012526436/article/details/102488164</a></p></blockquote><h3 id="model-类继承">Model 类继承</h3><p><strong>可以通过继承 <code>Model</code> 类并在 <code>call</code> 方法中实现你自己的前向传播，以创建你自己的完全定制化的模型，</strong>（<code>Model</code> 类继承 API 引入于 Keras 2.2.0）。</p><p>这里是一个用 <code>Model</code> 类继承写的简单的多层感知器的例子：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleMLP</span><span class="params">(keras.Model)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, use_bn=False, use_dp=False, num_classes=<span class="number">10</span>)</span>:</span></span><br><span class="line">        super(SimpleMLP, self).__init__(name=<span class="string">'mlp'</span>)</span><br><span class="line">        self.use_bn = use_bn</span><br><span class="line">        self.use_dp = use_dp</span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line"></span><br><span class="line">        self.dense1 = keras.layers.Dense(<span class="number">32</span>, activation=<span class="string">'relu'</span>)</span><br><span class="line">        self.dense2 = keras.layers.Dense(num_classes, activation=<span class="string">'softmax'</span>)</span><br><span class="line">        <span class="keyword">if</span> self.use_dp:</span><br><span class="line">            self.dp = keras.layers.Dropout(<span class="number">0.5</span>)</span><br><span class="line">        <span class="keyword">if</span> self.use_bn:</span><br><span class="line">            self.bn = keras.layers.BatchNormalization(axis=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        x = self.dense1(inputs)</span><br><span class="line">        <span class="keyword">if</span> self.use_dp:</span><br><span class="line">            x = self.dp(x)</span><br><span class="line">        <span class="keyword">if</span> self.use_bn:</span><br><span class="line">            x = self.bn(x)</span><br><span class="line">        <span class="keyword">return</span> self.dense2(x)</span><br><span class="line"></span><br><span class="line">model = SimpleMLP()</span><br><span class="line">model.compile(...)</span><br><span class="line">model.fit(...)</span><br></pre></td></tr></tbody></table></figure><p>网络层定义在 <code>__init__(self, ...)</code> 中，前向传播在 <code>call(self, inputs)</code> 中指定。在 <code>call</code> 中，你可以指定自定义的损失函数，通过调用 <code>self.add_loss(loss_tensor)</code> （就像你在自定义层中一样）。</p><p>在类继承模型中，模型的拓扑结构是由 Python 代码定义的（而不是网络层的静态图）。这意味着该模型的拓扑结构不能被检查或序列化。因此，以下方法和属性<strong>不适用于类继承模型</strong>：</p><ul><li><code>model.inputs</code> 和 <code>model.outputs</code>。</li><li><code>model.to_yaml()</code> 和 <code>model.to_json()</code>。</li><li><code>model.get_config()</code> 和 <code>model.save()</code>。</li></ul><p><strong>关键点</strong>：为每个任务使用正确的 API。<code>Model</code> 类继承 API 可以为实现复杂模型提供更大的灵活性，但它需要付出代价（比如缺失的特性）：它更冗长，更复杂，并且有更多的用户错误机会。如果可能的话，尽可能使用函数式 API，这对用户更友好。</p><blockquote><p><a href="https://blog.csdn.net/qq_27825451/article/details/90517036" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/qq_27825451/article/details/90517036</a></p></blockquote><h3 id="关于tensorflow的sessiontensorshape等基础知识整理">关于tensorflow的session、tensor、shape等基础知识（整理）</h3><p>在tensorflow程序中，tensor只是占位符，在会话层没有run出tensor的值之前，我们是无法获知tensor的值的</p><blockquote><p><a href="https://blog.csdn.net/jiongnima/article/details/78524551" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/jiongnima/article/details/78524551</a></p><p><a href="https://www.tensorflow.org/guide/tensor?hl=zh-cn" target="_blank" rel="noopener" class="uri">https://www.tensorflow.org/guide/tensor?hl=zh-cn</a></p><p><a href="https://www.jianshu.com/p/75a903a44cf2" target="_blank" rel="noopener" class="uri">https://www.jianshu.com/p/75a903a44cf2</a></p></blockquote><h3 id="tf.layers.flatten">tf.layers.flatten</h3><p>在保留第0轴的情况下对输入的张量进行Flatten(扁平化)</p><p>代码示例：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x=tf.placeholder(shape=(<span class="literal">None</span>,<span class="number">4</span>,<span class="number">4</span>),dtype=<span class="string">'float32'</span>)</span><br><span class="line"></span><br><span class="line">y=tf.layers.flatten(x)</span><br><span class="line"></span><br><span class="line">print(y)</span><br></pre></td></tr></tbody></table></figure><p>输出： 将后两维进行合并</p><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Tensor("flatten/Reshape:0", shape=(?, 16), dtype=float32)</span><br></pre></td></tr></tbody></table></figure><h3 id="tf.layers.dense">tf.layers.dense</h3><p>全连接层 ，相当于添加一个层。只<strong>改变输入的最后一维</strong></p><h3 id="python---tensorflow中-none-1和之间的区别">python - Tensorflow中 None，-1和？之间的区别</h3><p><code>None</code>表示未指定的维度。因此，如果您定义了一个占位符，您可以使用<code>None</code>来表示“这个维度可以有任何大小”。 占位符可以有多个<code>None</code>维度这仅仅意味着多个维度可以是不同的大小甚至整个形状都可以<code>None</code>来指定未知的维数。 <code>-1</code>是TensorFlow的一条指令，用于自行推断维度的大小。在<code>tf.reshape(input, [-1, input_size])</code>中，这意味着“重塑它，使第二个维度<code>input_size</code>，第一个维度是匹配元素总数所需的任何内容”。 这并不一定意味着维数是未知的，因为对于<code>None</code>如果输入张量的已知大小为10个元素，并且将其重塑为<code>[-1, 2]</code>，则张量流能够推断出完整的形状<code>[5, 2]</code>。 <code>-1</code>纯粹是为了方便。你可以把形状写下来，而不是让Tensorflow推断出来<code>None</code>另一方面，对于接受可变大小张量是必要的。 一个形状中只能有一个<code>-1</code>。多个是没有意义的，因为不可能推断出形状。例如，如果一个张量中有12个元素，则未定义将其重塑为<code>[-1, -1, 2]</code>——我们是否应该这样做？<code>[3, 2, 2]</code>？<code>[2, 3, 2]</code>？… 最后，问号正是tensorflow在打印张量和/或其形状时用来标记“未知”维度的内容。您发布的示例实际上会产生语法错误——您不能自己使用问号。未知维度的原因当然可以是具有<code>[6, 1, 2]</code>维度的占位符，并且通常根据占位符定义的张量（即应用于它们的某些运算的结果）也将具有未知维度。此外，有些操作可能没有指定（部分）它们的输出形状，这也可能导致未知。 这里可能还有一些我遗漏的技术细节，但根据经验：使用<code>None</code>作为占位符，使用<code>None</code>进行整形。这应该涵盖大多数用例。</p><blockquote><p><code>？</code>== <code>None</code> ，维度是未知的</p><p><code>-1</code>代表根据推断之后的维度</p><p><code>(3,)</code> 表明张量是一个一维数组，这个数组的长度为3</p></blockquote><blockquote><p><a href="https://www.coder.work/article/2032326" target="_blank" rel="noopener" class="uri">https://www.coder.work/article/2032326</a></p></blockquote><h3 id="keras的-call-函数build-函数">keras的 call 函数、build 函数</h3><p>build() 用来初始化定义weights, 这里可以用父类的self.add_weight() 函数来初始化数据, 该函数必须将 self.built 设置为True, 以保证该 Layer 已经成功 build , 通常如上所示, 使用 super(MyLayer, self).build(input_shape) 来完成。</p><p>call() 用来执行 Layer 的职能, x就是该层的输入，x与权重kernel做点积，生成新的节点层，即当前 Layer 所有的计算过程均在该函数中完成。</p><p><code>__init__()</code>和<code>build()</code>函数都在对Layer进行初始化，都初始化了一些成员函数</p><p><code>__init__()</code>：保存成员变量的设置</p><p><code>build()</code>：在<code>call()</code>函数第一次执行时会被调用一次，这时候可以知道输入数据的<code>shape</code>。返回去看一看，果然是<code>__init__()</code>函数中只初始化了输出数据的<code>shape</code>，而输入数据的<code>shape</code>需要在<code>build()</code>函数中动态获取，这也解释了为什么在有<code>__init__()</code>函数时还需要使用<code>build()</code>函数</p><p><code>call()</code>函数则是在该layer被调用时执行。</p><blockquote><p><a href="https://blog.csdn.net/qq_32623363/article/details/104128497" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/qq_32623363/article/details/104128497</a></p><p><a href="https://blog.csdn.net/qq_27825451/article/details/90517036" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/qq_27825451/article/details/90517036</a></p></blockquote><h3 id="tf.expand_dims">tf.expand_dims（）</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.expand_dims(input, dim, name=<span class="literal">None</span>) <span class="comment">#在指定位置增加维度</span></span><br></pre></td></tr></tbody></table></figure><blockquote><p><a href="https://blog.csdn.net/jasonzzj/article/details/60811035" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/jasonzzj/article/details/60811035</a></p></blockquote><h3 id="tf.boolean_mask">tf.boolean_mask（）</h3><p>选择张量的特定维度的值</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tf.boolean_mask(tensor,mask,name=<span class="string">'boolean_mask'</span>,axis=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#1-D example</span></span><br><span class="line">tensor = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">mask = np.array([<span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">False</span>])</span><br><span class="line">boolean_mask(tensor, mask)  <span class="comment"># [0, 2]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#2-D example</span></span><br><span class="line">tensor = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>]]</span><br><span class="line">mask = np.array([<span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">True</span>])</span><br><span class="line">boolean_mask(tensor, mask)  <span class="comment"># [[1, 2], [5, 6]]</span></span><br></pre></td></tr></tbody></table></figure><blockquote><p><a href="https://blog.csdn.net/wuguangbin1230/article/details/81334544" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/wuguangbin1230/article/details/81334544</a></p></blockquote><h2 id="pytorch">🚀pytorch</h2><h3 id="pytorch-torch.nn.parameter">PyTorch torch.nn.Parameter()</h3><p><strong>作用</strong>：对于<code>self.v = torch.nn.Parameter(torch.FloatTensor(hidden_size))</code>，也就是将一个不可训练的类型<code>Tensor</code>转换成可以训练的类型<code>parameter</code>并将这个<code>parameter</code>绑定到这个<code>module</code>里面(<code>net.parameter()</code>中就有这个绑定的<code>parameter</code>，所以在参数优化的时候可以进行优化的)，所以经过类型转换这个<code>self.v</code>变成了模型的一部分，成为了模型中根据训练可以改动的参数了。</p><p>使用这个函数的目的也是想让某些变量在学习的过程中不断的修改其值以达到最优化。</p><blockquote><p><a href="https://www.jianshu.com/p/d8b77cc02410" target="_blank" rel="noopener" class="uri">https://www.jianshu.com/p/d8b77cc02410</a></p></blockquote><h3 id="pytorch-nn.linear">PyTorch nn.Linear（）</h3><p>用于设置网络中的<strong>全连接层的</strong></p><blockquote><p><a href="https://blog.csdn.net/qq_42079689/article/details/102873766" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/qq_42079689/article/details/102873766</a></p></blockquote><h3 id="pytorch-nn.embedding-词向量">pytorch nn.embedding() 词向量</h3><p>词嵌入在 pytorch 中非常简单，只需要调用 <code>torch.nn.Embedding(m, n)</code> 就可以了，m 表示单词的总数目，n 表示词嵌入的维度，其实词嵌入就相当于是一个大矩阵，矩阵的每一行表示一个单词。</p><p><strong>随机初始化</strong></p><blockquote><p><a href="https://blog.csdn.net/david0611/article/details/81090371" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/david0611/article/details/81090371</a></p></blockquote><h3 id="pytorch-torch.mean">pytorch torch.mean()</h3><p><strong>torch.mean(input, dim, keepdim=False, out=None)</strong></p><p>返回新的张量，其中包含输入张量input指定维度dim中每行的平均值。</p><p>若keepdim值为True，则在输出张量中，除了被操作的dim维度值降为1，其它维度与输入张量input相同。否则，dim维度相当于被执行torch.squeeze()维度压缩操作，导致此维度消失，最终输出张量会比输入张量少一个维度。</p><p><strong>参数：</strong></p><ul><li>input (Tensor) - 输入张量</li><li>dim (int) - 指定进行均值计算的维度</li><li>keepdim (bool, optional) - 输出张量是否保持与输入张量有相同数量的维度</li><li>out (Tensor) - 结果张量</li></ul><p><strong>例子：</strong></p><blockquote><p>a = torch.randn(4, 5) a 0.3168 0.4953 -0.6758 -0.5559 -0.6906 0.2241 2.2450 1.5735 -1.3815 -1.5199 0.0033 0.5236 -0.9070 -0.5961 -2.1281 0.9605 1.5314 -0.6555 -1.2584 -0.4160 [torch.FloatTensor of size 4x5] torch.mean(a, 1, True) -0.2220 0.2283 -0.6209 0.0324 [torch.FloatTensor of size 4x1]</p></blockquote><h3 id="np.triu-np.tril">np.triu() &amp; np.tril()</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">triu</span>（<span class="title">m</span>， <span class="title">k</span>）：</span></span><br><span class="line"><span class="function">#取上三角阵  </span></span><br><span class="line"><span class="function">#<span class="title">m</span>：表示一个矩阵</span></span><br><span class="line"><span class="function">#<span class="title">K</span>：表示对角线的起始位置（<span class="title">k</span>取值默认为0）</span></span><br><span class="line"><span class="function"></span></span><br><span class="line">#k=0表示正常的上三角矩阵</span><br><span class="line"><span class="comment">#k=-1表示对角线的位置下移1个对角线</span></span><br><span class="line"><span class="comment">#k=1表示对角线的位置上移1个对角线</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#同理，np.tril取下三角阵</span></span><br></pre></td></tr></tbody></table></figure><blockquote><p><a href="https://blog.csdn.net/weixin_37724529/article/details/102881776" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/weixin_37724529/article/details/102881776</a></p></blockquote><h3 id="pytorch-forward的使用以及原理---pytorch使用">pytorch forward的使用以及原理 --pytorch使用</h3><blockquote><p><a href="https://blog.csdn.net/u011501388/article/details/84062483" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/u011501388/article/details/84062483</a></p></blockquote><h3 id="pytorch-torch.nn.parameter详解">PyTorch torch.nn.Parameter()详解</h3><blockquote><p><a href="https://cloud.tencent.com/developer/article/1608348" target="_blank" rel="noopener" class="uri">https://cloud.tencent.com/developer/article/1608348</a></p></blockquote><h3 id="pytorch-view">pytorch view()</h3><p>PyTorch中<strong>view</strong>函数作用为重构张量的维度</p><blockquote><p>torch.view(参数a,参数b,.....)，其中参数a=3,参数b=2决定了将一维的tt1重构成3*2维的张量。 有时候会出现torch.view(-1)或者torch.view(参数a,-1)这种情况。则-1参数是需要估算的。</p></blockquote><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">tt3=torch.tensor([[<span class="number">-0.3623</span>,<span class="number">-0.6115</span>],[<span class="number">0.7283</span>,<span class="number">0.4699</span>],[<span class="number">2.3261</span>,<span class="number">0.1599</span>]])</span><br><span class="line">result2=tt3.view(<span class="number">2</span>,<span class="number">-1</span>).contiguous()</span><br></pre></td></tr></tbody></table></figure><p>则<code>result2</code>为</p><figure class="highlight css"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">tensor</span>(<span class="selector-attr">[[-0.3623, -0.6115,  0.7283]</span>,</span><br><span class="line">        <span class="selector-attr">[ 0.4699,  2.3261,  0.1599]</span>])</span><br></pre></td></tr></tbody></table></figure><h3 id="pytorch-model.parameters">pytorch model.parameters()</h3><p>这个方法会获得模型的参数信息 。</p><p>model.parameters()方法<strong>返回的是一个生成器generator，每一个元素是从开头到结尾的参数</strong>，parameters没有对应的key名称，是一个由纯参数组成的generator，查看Module的参数信息，<strong>用于更新参数，或者用于模型的保存。</strong></p><blockquote><p><a href="https://blog.csdn.net/qq_27825451/article/details/95888267" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/qq_27825451/article/details/95888267</a></p></blockquote><h3 id="pytorch-torch.optim.lr_scheduler">pytorch torch.optim.lr_scheduler</h3><p>用于设置学习率的衰减</p><blockquote><p><a href="https://zhuanlan.zhihu.com/p/69411064" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/69411064</a></p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      记录杂乱的知识点，持续更新
    
    </summary>
    
    
    
  </entry>
  
</feed>
