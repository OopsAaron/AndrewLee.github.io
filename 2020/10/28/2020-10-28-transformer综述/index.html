<!DOCTYPE html>
<html lang="zh-CN">

<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/robot-4291692_1280.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/robot-4291692_1280.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-barber-shop.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"valine","storage":true,"lazyload":false,"nav":null,"activeClass":"valine"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>


  <meta name="description" content="transformerç»¼è¿°">
<meta property="og:type" content="article">
<meta property="og:title" content="2020-10-28-transformerç»¼è¿°">
<meta property="og:url" content="http://yoursite.com/2020/10/28/2020-10-28-transformer%E7%BB%BC%E8%BF%B0/index.html">
<meta property="og:site_name" content="æ€å»ºçš„NLPä¹‹æ—…">
<meta property="og:description" content="transformerç»¼è¿°">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.loli.net/2020/10/28/6z5wSWXlaQnAF2E.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/zPU8Z9scbDNkS5A.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/K4lYeqtN7jUR5fx.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/C8lDMeBadc4yftg.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/ctFMhR8IEdPVSZ1.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/8YW7R4Q5J6zbmD1.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/KmazXviordxyZuw.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/38l7XhVjZnqyuBe.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/IKCVE7riwaQ9Zl1.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/Id9fV3bxSnmHyu2.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/IyoR924J3BQWrOu.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/oKQv3akDcXWOZ7B.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/g2TyZhWtRSEA7pB.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/4EeY78JcQojSuhD.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/KhcS2VIyvNEAwrg.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/hDmpknyqTSIUEW7.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/ZwAy1H3kB7SjIP5.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/ZThJDNXByQp2Lod.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/pMCoW2ZLv6gq3j4.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/IjGfH7Mi1LK2hsC.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/JdZUrh6L3QFBHIn.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/WlOZR3iaJSA2nIU.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/SZIDO7KcoGHqvhF.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/nOstVdScKG68CDz.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/yRVqrjfJPgBKlIk.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/hvoFmDIp1BjgV5f.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/EDsBxfipL4HcOwF.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/IDjAR2ucfELsegw.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/G8AVxR7fuCZdPHn.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/Z1fYDpO6BnLyHj4.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/YyTZDG1Bohkswit.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/gSHlhWRaoqfk9pN.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/ZS5dCT8nVcaiob4.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/L4BDoiwTfaRjInv.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/gKZT1mRFhEj48nU.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/hElWr4uHtUomVYG.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/rpzwVXOSUavEs18.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/5ixhnpvKeQHrBw1.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/8EtHVoBsfYxnmhO.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/9UfkunTVEpL5XBG.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/WQtJvpbP9rUczhs.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/w6b1LlqIpZgt8fX.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/JYzeocGBUr6jy4W.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/Lk3ymRQKZHpwN8e.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/6aOdB5QqIYKSlEb.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/tdJkSqcoZ4FGHNQ.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/pXaRS5Q7fVDuxyW.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/iT5HIsel3WjDCxA.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/LZTwqn9hjpBiAyr.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/AGKsp9WPLQ2julY.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/yYDIensgFQmv3HG.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/IKjLfbDRNgMd5rE.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/BzXjLlFERMe34x2.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/FnUxJrzqgL6eEks.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/lLPuxzv8IVRTAQO.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/ZmF3yXaiHPME1LQ.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/DeQYroznLq4cCkt.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/LhoxNVG5rF3McDK.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/ZI176zJekFciyGr.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/6cBzpUQ3J7e1Xd2.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/5tdJqBHve3XYuCo.png">
<meta property="og:image" content="https://i.loli.net/2020/10/28/jTUZBNqcrlm9hR2.png">
<meta property="article:published_time" content="2020-10-28T12:39:37.000Z">
<meta property="article:modified_time" content="2020-11-07T13:57:10.149Z">
<meta property="article:author" content="ææ€å»º">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2020/10/28/6z5wSWXlaQnAF2E.png">

<link rel="canonical" href="http://yoursite.com/2020/10/28/2020-10-28-transformer%E7%BB%BC%E8%BF%B0/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>2020-10-28-transformerç»¼è¿° | æ€å»ºçš„NLPä¹‹æ—…</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>



<link rel="alternate" href="/atom.xml" title="æ€å»ºçš„NLPä¹‹æ—…" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>




    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="åˆ‡æ¢å¯¼èˆªæ ">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">æ€å»ºçš„NLPä¹‹æ—…</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">æ²‰æ·€è‡ªå·±</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>é¦–é¡µ</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>å…³äº</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>æ ‡ç­¾<span class="badge">25</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>åˆ†ç±»<span class="badge">21</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>å½’æ¡£<span class="badge">78</span></a>

  </li>
        <li class="menu-item menu-item-tools">

    <a href="/tools/" rel="section"><i class="fas fa-location-arrow fa-fw"></i>åˆ©å™¨</a>

  </li>
        <li class="menu-item menu-item-onesentence">

    <a href="/onesentence" rel="section"><i class="fas fa-bowling-ball fa-fw"></i>ä¸€å¥è¯</a>

  </li>
        <li class="menu-item menu-item-links">

    <a href="/links" rel="section"><i class="fas fa-link fa-fw"></i>å¤§ä½¬</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>æœç´¢
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="æœç´¢..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/OopsAaron" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/10/28/2020-10-28-transformer%E7%BB%BC%E8%BF%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/touxiang.jpg">
      <meta itemprop="name" content="ææ€å»º">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="æ€å»ºçš„NLPä¹‹æ—…">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          2020-10-28-transformerç»¼è¿°
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">å‘è¡¨äº</span>

              <time title="åˆ›å»ºæ—¶é—´ï¼š2020-10-28 20:39:37" itemprop="dateCreated datePublished" datetime="2020-10-28T20:39:37+08:00">2020-10-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">æ›´æ–°äº</span>
                <time title="ä¿®æ”¹æ—¶é—´ï¼š2020-11-07 21:57:10" itemprop="dateModified" datetime="2020-11-07T21:57:10+08:00">2020-11-07</time>
              </span>

          
            <span id="/2020/10/28/2020-10-28-transformer%E7%BB%BC%E8%BF%B0/" class="post-meta-item leancloud_visitors" data-flag-title="2020-10-28-transformerç»¼è¿°" title="é˜…è¯»æ¬¡æ•°">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">é˜…è¯»æ¬¡æ•°ï¼š</span>
              <span class="leancloud-visitors-count"></span>
            </span>
            <span class="post-meta-item" title="é˜…è¯»æ¬¡æ•°" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">é˜…è¯»æ¬¡æ•°ï¼š</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valineï¼š</span>
    
    <a title="valine" href="/2020/10/28/2020-10-28-transformer%E7%BB%BC%E8%BF%B0/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2020/10/28/2020-10-28-transformer%E7%BB%BC%E8%BF%B0/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
            <span class="post-meta-item" title="æœ¬æ–‡å­—æ•°">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
              <span>36k</span>
            </span>
            <span class="post-meta-item" title="é˜…è¯»æ—¶é•¿">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
              <span>33 åˆ†é’Ÿ</span>
            </span>
            <div class="post-description">transformerç»¼è¿°</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h3 id="transformerå®¶æ—1----transformerè¯¦è§£å’Œæºç åˆ†æ">ğŸš€Transformerå®¶æ—1 -- Transformerè¯¦è§£å’Œæºç åˆ†æ</h3>
<h4 id="transformeræ€»ä½“ç»“æ„">1 Transformeræ€»ä½“ç»“æ„</h4>
<p>è¿‘å‡ å¹´NLPé¢†åŸŸæœ‰äº†çªé£çŒ›è¿›çš„å‘å±•ï¼Œé¢„è®­ç»ƒæ¨¡å‹åŠŸä¸å¯æ²¡ã€‚å½“å‰åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼ˆpretrain modelsï¼‰åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­è¿›è¡Œfine-tuneï¼Œå·²ç»æˆä¸ºäº†å¤§éƒ¨åˆ†NLPä»»åŠ¡çš„å›ºå®šèŒƒå¼ã€‚Transformeræ‘’å¼ƒäº†RNNçš„åºåˆ—ç»“æ„ï¼Œå®Œå…¨é‡‡ç”¨attentionå’Œå…¨è¿æ¥ï¼Œä¸¥æ ¼æ¥è¯´ä¸å±äºé¢„è®­ç»ƒæ¨¡å‹ã€‚ä½†å®ƒå´æ˜¯å½“å‰å‡ ä¹æ‰€æœ‰pretrain modelsçš„åŸºæœ¬ç»“æ„ï¼Œä¸ºpretrain modelsæ‰“ä¸‹äº†åšå®çš„åŸºç¡€ï¼Œå¹¶é€æ­¥å‘å±•å‡ºäº†transformer-XLï¼Œreformerç­‰ä¼˜åŒ–æ¶æ„ã€‚æœ¬æ–‡ç»“åˆè®ºæ–‡å’Œæºç ï¼Œå¯¹transformeråŸºæœ¬ç»“æ„ï¼Œè¿›è¡Œè¯¦ç»†åˆ†æã€‚</p>
<p>Transformeræ˜¯è°·æ­Œåœ¨2017å¹´6æœˆæå‡ºï¼Œå‘è¡¨åœ¨NIPS2017ä¸Šã€‚è®ºæ–‡åœ°å€ <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need</a>ã€‚ åˆ†æçš„ä»£ç ä¸ºHarvardnlpçš„ä»£ç ï¼ŒåŸºäºPyTorchï¼Œ åœ°å€ <a href="https://github.com/harvardnlp/annotated-transformer" target="_blank" rel="noopener">annotated-transformer</a></p>
<p>Transformerä¸»ä½“æ¡†æ¶æ˜¯ä¸€ä¸ª<strong>encoder-decoder</strong>ç»“æ„ï¼Œå»æ‰äº†RNNåºåˆ—ç»“æ„ï¼Œå®Œå…¨åŸºäºattentionå’Œå…¨è¿æ¥ã€‚åœ¨WMT2014è‹±è¯­ç¿»è¯‘å¾·è¯­ä»»åŠ¡ä¸Šï¼Œbleuå€¼è¾¾åˆ°äº†28.4ï¼Œè¾¾åˆ°å½“æ—¶çš„SOTAã€‚å…¶æ€»ä½“ç»“æ„å¦‚ä¸‹æ‰€ç¤º</p>
<p><img src="https://i.loli.net/2020/10/28/6z5wSWXlaQnAF2E.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></p>
<p>æ€»ä½“ä¸ºä¸€ä¸ªå…¸å‹çš„encoder-decoderç»“æ„ã€‚ä»£ç å¦‚ä¸‹</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># æ•´ä¸ªæ¨¡å‹å…¥å£</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_model</span><span class="params">(src_vocab, tgt_vocab, N=<span class="number">6</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">               d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, h=<span class="number">8</span>, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">    <span class="string">"Helper: Construct a model from hyperparameters."</span></span><br><span class="line">    c = copy.deepcopy</span><br><span class="line"></span><br><span class="line">    <span class="comment"># multiHead attention</span></span><br><span class="line">    attn = MultiHeadedAttention(h, d_model)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># feed-forward</span></span><br><span class="line">    ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># position-encoding</span></span><br><span class="line">    position = PositionalEncoding(d_model, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># æ•´ä½“ä¸ºä¸€ä¸ªencoder-decoder</span></span><br><span class="line">    model = EncoderDecoder(</span><br><span class="line">        <span class="comment"># encoderç¼–ç å±‚</span></span><br><span class="line">        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),</span><br><span class="line"></span><br><span class="line">        <span class="comment"># decoderè§£ç å±‚</span></span><br><span class="line">        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ç¼–ç å±‚è¾“å…¥ï¼Œè¾“å…¥è¯­å¥è¿›è¡Œtoken embeddingå’Œposition embedding</span></span><br><span class="line">        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),</span><br><span class="line"></span><br><span class="line">        <span class="comment"># è§£ç å±‚è¾“å…¥ï¼ŒåŒæ ·éœ€è¦åštoken embeddingå’Œposition embedding</span></span><br><span class="line">        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),</span><br><span class="line"></span><br><span class="line">        <span class="comment"># linear + softmaxï¼ŒæŸ¥æ‰¾vocabä¸­æ¦‚ç‡æœ€å¤§çš„å­—</span></span><br><span class="line">        Generator(d_model, tgt_vocab))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># This was important from their code. </span></span><br><span class="line">    <span class="comment"># Initialize parameters with Glorot / fan_avg.</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">            nn.init.xavier_uniform(p)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"><span class="number">1234567891011121314151617181920212223242526272829303132333435363738</span></span><br></pre></td></tr></tbody></table></figure>
<p>make_modelä¸ºTransformeræ¨¡å‹å®šä¹‰çš„å…¥å£ï¼Œå®ƒå…ˆå®šä¹‰äº†multi-head attentionã€feed-forwardã€position-encodingç­‰ä¸€ç³»åˆ—å­æ¨¡å—ï¼Œç„¶åå®šä¹‰äº†ä¸€ä¸ªencoder-decoderç»“æ„å¹¶è¿”å›ã€‚ä¸‹é¢æ¥çœ‹encoder-decoderå®šä¹‰ã€‚</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderDecoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    ä¸€ä¸ªæ ‡å‡†çš„encoderå’Œdecoderæ¡†æ¶ï¼Œå¯ä»¥è‡ªå®šä¹‰embeddingã€encoderã€decoderç­‰</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, encoder, decoder, src_embed, tgt_embed, generator)</span>:</span></span><br><span class="line">        super(EncoderDecoder, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># encoderå’Œdecoderé€šè¿‡æ„é€ å‡½æ•°ä¼ å…¥ï¼Œå¯çµæ´»æ›´æ”¹</span></span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line"></span><br><span class="line">        <span class="comment"># srcå’Œtargetçš„embeddingï¼Œä¹Ÿæ˜¯é€šè¿‡æ„é€ å‡½æ•°ä¼ å…¥ï¼Œæ–¹ä¾¿çµæ´»æ›´æ”¹</span></span><br><span class="line">        self.src_embed = src_embed</span><br><span class="line">        self.tgt_embed = tgt_embed</span><br><span class="line"></span><br><span class="line">        <span class="comment"># linear + softmax</span></span><br><span class="line">        self.generator = generator</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, src, tgt, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="string">"Take in and process masked src and target sequences."</span></span><br><span class="line">        <span class="comment"># å…ˆå¯¹è¾“å…¥è¿›è¡Œencodeï¼Œç„¶åå†é€šè¿‡decodeè¾“å‡º</span></span><br><span class="line">        <span class="keyword">return</span> self.decode(self.encode(src, src_mask), src_mask,</span><br><span class="line">                            tgt, tgt_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, src, src_mask)</span>:</span></span><br><span class="line">        <span class="comment"># å…ˆå¯¹è¾“å…¥è¿›è¡Œembeddingï¼Œç„¶åå†ç»è¿‡encoder</span></span><br><span class="line">        <span class="keyword">return</span> self.encoder(self.src_embed(src), src_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span><span class="params">(self, memory, src_mask, tgt, tgt_mask)</span>:</span></span><br><span class="line">        <span class="comment"># å…ˆå¯¹ç›®æ ‡è¿›è¡Œembeddingï¼Œç„¶åç»è¿‡decoder</span></span><br><span class="line">        <span class="keyword">return</span> self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)</span><br><span class="line"><span class="number">12345678910111213141516171819202122232425262728293031</span></span><br></pre></td></tr></tbody></table></figure>
<p>encoder-decoderå®šä¹‰äº†ä¸€ä¸ªæ ‡å‡†çš„ç¼–ç è§£ç æ¡†æ¶ï¼Œå…¶ä¸­ç¼–ç å™¨ã€è§£ç å™¨å‡å¯ä»¥è‡ªå®šä¹‰ï¼Œæœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚æ¨¡å—è¿è¡Œæ—¶ä¼šè°ƒç”¨forwardå‡½æ•°ï¼Œå®ƒå…ˆå¯¹è¾“å…¥è¿›è¡Œencodeï¼Œç„¶åå†é€šè¿‡decodeè¾“å‡ºã€‚æˆ‘ä»¬å°±ä¸è¯¦ç»†å±•å¼€äº†ã€‚</p>
<h4 id="encoder">2 encoder</h4>
<h5 id="encoderå®šä¹‰">2.1 encoderå®šä¹‰</h5>
<p>encoderåˆ†ä¸ºä¸¤éƒ¨åˆ†</p>
<ol type="1">
<li><strong>è¾“å…¥å±‚embedding</strong>ã€‚è¾“å…¥å±‚å¯¹inputsæ–‡æœ¬åštoken embeddingï¼Œå¹¶å¯¹æ¯ä¸ªå­—åšposition encodingï¼Œç„¶åå åŠ åœ¨ä¸€èµ·ï¼Œä½œä¸ºæœ€ç»ˆçš„è¾“å…¥ã€‚</li>
<li><strong>ç¼–ç å±‚encoding</strong>ã€‚ç¼–ç å±‚æ˜¯å¤šå±‚ç»“æ„ç›¸åŒçš„layerå †å è€Œæˆã€‚æ¯ä¸ªlayeråˆåŒ…æ‹¬ä¸¤éƒ¨åˆ†ï¼Œmulti-head self-attentionå’Œfeed-forwardå…¨è¿æ¥ï¼Œå¹¶åœ¨æ¯éƒ¨åˆ†åŠ å…¥äº†æ®‹å·®è¿æ¥å’Œå½’ä¸€åŒ–ã€‚</li>
</ol>
<p>ä»£ç å®ç°ä¸Šä¹ŸéªŒè¯äº†è¿™ä¸€ç‚¹ã€‚æˆ‘ä»¬çœ‹EncoderDecoderç±»ä¸­çš„encodeå‡½æ•°ï¼Œå®ƒå…ˆåˆ©ç”¨è¾“å…¥embeddingå±‚å¯¹åŸå§‹è¾“å…¥è¿›è¡Œembeddingï¼Œç„¶åå†é€šè¿‡ç¼–ç å±‚è¿›è¡Œencodingã€‚</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderDecoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, src, src_mask)</span>:</span></span><br><span class="line">        <span class="comment"># å…ˆå¯¹è¾“å…¥è¿›è¡Œembeddingï¼Œç„¶åå†ç»è¿‡encoder</span></span><br><span class="line">        <span class="keyword">return</span> self.encoder(self.src_embed(src), src_mask)</span><br><span class="line"><span class="number">1234</span></span><br></pre></td></tr></tbody></table></figure>
<h5 id="è¾“å…¥å±‚embedding">2.2 è¾“å…¥å±‚embedding</h5>
<p>åŸå§‹æ–‡æœ¬ç»è¿‡embeddingå±‚è¿›è¡Œå‘é‡åŒ–ï¼Œå®ƒåŒ…æ‹¬token embeddingå’Œposition embeddingä¸¤å±‚ã€‚</p>
<h6 id="token-embedding">2.2.1 token embedding</h6>
<p>token embeddingå¯¹æ–‡æœ¬è¿›è¡Œå‘é‡åŒ–ï¼Œä¸€èˆ¬æ¥è¯´æœ‰ä¸¤ç§æ–¹å¼</p>
<ol type="1">
<li>é‡‡ç”¨<strong>å›ºå®šè¯å‘é‡</strong>ï¼Œæ¯”å¦‚åˆ©ç”¨Word2vecé¢„å…ˆè®­ç»ƒå¥½çš„ã€‚è¿™ç§æ–¹å¼æ˜¯LSTMæ—¶ä»£å¸¸ç”¨çš„æ–¹å¼ï¼Œæ¯”è¾ƒç®€å•çœäº‹ï¼Œæ— éœ€è®­ç»ƒã€‚ä½†ç”±äºè¯å‘é‡æ˜¯å›ºå®šçš„ï¼Œä¸èƒ½è§£å†³ä¸€è¯å¤šä¹‰çš„é—®é¢˜ï¼Œè¯è¯­æœ¬èº«ä¹Ÿä¸æ˜¯contextualçš„ï¼Œæ²¡æœ‰ç»“åˆä¸Šä¸‹æ–‡è¯­å¢ƒä¿¡æ¯ï¼Œå¦å¤–å¯¹äºä¸åœ¨è¯å‘é‡ä¸­çš„è¯è¯­ï¼Œæ¯”å¦‚ç‰¹å®šé¢†åŸŸè¯è¯­æˆ–è€…æ–°è¯ï¼Œå®¹æ˜“å‡ºç°OOVé—®é¢˜ã€‚</li>
<li>éšæœºåˆå§‹åŒ–ï¼Œç„¶å<strong>è®­ç»ƒ</strong>ã€‚è¿™ç§æ–¹å¼æ¯”è¾ƒéº»çƒ¦ï¼Œéœ€è¦å¤§è§„æ¨¡è®­ç»ƒè¯­æ–™ï¼Œä½†èƒ½è§£å†³å›ºå®šè¯å‘é‡çš„ä¸€ç³»åˆ—é—®é¢˜ã€‚Transformeré‡‡ç”¨äº†è¿™ç§æ–¹å¼ã€‚</li>
</ol>
<p>å¦å¤–ï¼ŒåŸºäºTransformerçš„BERTæ¨¡å‹åœ¨ä¸­æ–‡å¤„ç†æ—¶ï¼Œç›´æ¥åŸºäºå­—åšembeddingï¼Œä¼˜ç‚¹æœ‰</p>
<ol type="1">
<li>æ— éœ€åˆ†è¯ï¼Œæ•…ä¸ä¼šå¼•å…¥åˆ†è¯è¯¯å·®ã€‚äº‹å®ä¸Šï¼Œåªè¦è®­ç»ƒè¯­æ–™å……åˆ†ï¼Œæ¨¡å‹è‡ªç„¶å°±å¯ä»¥å­¦åˆ°åˆ†è¯ä¿¡æ¯äº†ã€‚</li>
<li>ä¸­æ–‡å­—ä¸ªæ•°å›ºå®šï¼Œä¸ä¼šå¯¼è‡´OOVé—®é¢˜</li>
<li>ä¸­æ–‡å­—ç›¸å¯¹è¯ï¼Œæ•°é‡å°‘å¾ˆå¤šï¼Œembeddingå±‚å‚æ•°å¤§å¤§ç¼©å°ï¼Œå‡å°äº†æ¨¡å‹ä½“ç§¯ï¼Œå¹¶åŠ å¿«äº†è®­ç»ƒé€Ÿåº¦ã€‚</li>
</ol>
<p>äº‹å®ä¸Šï¼Œå°±ç®—åœ¨LSTMæ—¶ä»£ï¼Œå¾ˆå¤šcaseä¸­ï¼Œæˆ‘ä»¬ä¹Ÿç¢°åˆ°è¿‡åŸºäºå­—çš„embeddingçš„æ•ˆæœæ¯”åŸºäºè¯çš„è¦å¥½ä¸€äº›ã€‚</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Embeddings</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="comment"># token embeddingï¼Œéšæœºåˆå§‹åŒ–è®­ç»ƒï¼Œç„¶åæŸ¥è¡¨æ‰¾åˆ°æ¯ä¸ªå­—çš„embedding</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, vocab)</span>:</span></span><br><span class="line">        super(Embeddings, self).__init__()</span><br><span class="line">        <span class="comment"># æ„å»ºä¸€ä¸ªéšæœºåˆå§‹åŒ–çš„è¯å‘é‡è¡¨ï¼Œ[vocab_size, d_model]ã€‚ bertä¸­çš„è®¾ç½®ä¸º[21128, 768]</span></span><br><span class="line">        self.lut = nn.Embedding(vocab, d_model)</span><br><span class="line">        self.d_model = d_model</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># ä»è¯å‘é‡è¡¨ä¸­æŸ¥æ‰¾å­—å¯¹åº”çš„embeddingå‘é‡</span></span><br><span class="line">        <span class="keyword">return</span> self.lut(x) * math.sqrt(self.d_model)</span><br><span class="line"></span><br><span class="line"><span class="number">123456789101112</span></span><br></pre></td></tr></tbody></table></figure>
<p>ç”±ä»£ç å¯è§ï¼ŒTransformeré‡‡ç”¨çš„æ˜¯éšæœºåˆå§‹åŒ–ï¼Œç„¶åè®­ç»ƒçš„æ–¹å¼ã€‚è¯å‘é‡ç»´åº¦ä¸º[vocab_size, d_model]ã€‚ä¾‹å¦‚BERTä¸­ä¸º[21128, 768]ï¼Œå‚æ•°é‡è¿˜æ˜¯å¾ˆå¤§çš„ã€‚ALBerté’ˆå¯¹embeddingå±‚è¿›è¡ŒçŸ©é˜µåˆ†è§£ï¼Œå¤§å¤§å‡å°äº†embeddingå±‚ä½“ç§¯ã€‚</p>
<h6 id="position-encoding">2.2.2 position encoding</h6>
<p>é¦–å…ˆä¸€ä¸ªé—®é¢˜ï¼Œä¸ºå•¥è¦è¿›è¡Œä½ç½®ç¼–ç å‘¢ã€‚åŸå› åœ¨äºself-attentionï¼Œå°†ä»»æ„ä¸¤ä¸ªå­—ä¹‹é—´è·ç¦»ç¼©å°ä¸º1ï¼Œä¸¢å¤±äº†å­—çš„ä½ç½®ä¿¡æ¯ï¼Œæ•…æˆ‘ä»¬éœ€è¦åŠ ä¸Šè¿™ä¸€ä¿¡æ¯ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥æƒ³åˆ°ä¸¤ç§æ–¹æ³•</p>
<ol type="1">
<li><strong>å›ºå®šç¼–ç </strong>ã€‚Transformeré‡‡ç”¨äº†è¿™ä¸€æ–¹å¼ï¼Œé€šè¿‡å¥‡æ•°åˆ—coså‡½æ•°ï¼Œå¶æ•°åˆ—sinå‡½æ•°æ–¹å¼ï¼Œåˆ©ç”¨ä¸‰è§’å‡½æ•°å¯¹ä½ç½®è¿›è¡Œå›ºå®šç¼–ç ã€‚</li>
<li><strong>åŠ¨æ€è®­ç»ƒ</strong>ã€‚BERTé‡‡ç”¨äº†è¿™ç§æ–¹å¼ã€‚å…ˆéšæœºåˆå§‹åŒ–ä¸€ä¸ªembedding tableï¼Œç„¶åè®­ç»ƒå¾—åˆ°table å‚æ•°å€¼ã€‚predictæ—¶é€šè¿‡embedding_lookupæ‰¾åˆ°æ¯ä¸ªä½ç½®çš„embeddingã€‚è¿™ç§æ–¹å¼å’Œtoken embeddingç±»ä¼¼ã€‚</li>
</ol>
<p>å“ªä¸€ç§æ–¹æ³•å¥½å‘¢ï¼Ÿä¸ªäººä»¥ä¸ºå„æœ‰åˆ©å¼Š</p>
<ol type="1">
<li>å›ºå®šç¼–ç æ–¹å¼ç®€æ´ï¼Œä¸éœ€è¦è®­ç»ƒã€‚ä¸”ä¸å—embedding tableç»´åº¦å½±å“ï¼Œç†è®ºä¸Šå¯ä»¥æ”¯æŒä»»æ„é•¿åº¦æ–‡æœ¬ã€‚ï¼ˆä½†è¦å°½é‡é¿å…é¢„æµ‹æ–‡æœ¬å¾ˆé•¿ï¼Œä½†è®­ç»ƒé›†æ–‡æœ¬è¾ƒçŸ­çš„caseï¼‰</li>
<li>åŠ¨æ€è®­ç»ƒæ–¹å¼ï¼Œåœ¨è¯­æ–™æ¯”è¾ƒå¤§æ—¶ï¼Œå‡†ç¡®åº¦æ¯”è¾ƒå¥½ã€‚ä½†éœ€è¦è®­ç»ƒï¼Œä¸”æœ€è‡´å‘½çš„æ˜¯ï¼Œé™åˆ¶äº†è¾“å…¥æ–‡æœ¬é•¿åº¦ã€‚å½“æ–‡æœ¬é•¿åº¦å¤§äºposition embedding tableç»´åº¦æ—¶ï¼Œè¶…å‡ºçš„positionæ— æ³•æŸ¥è¡¨å¾—åˆ°embeddingï¼ˆå¯ä»¥ç†è§£ä¸ºOOVäº†ï¼‰ã€‚è¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆBERTæ¨¡å‹æ–‡æœ¬é•¿åº¦æœ€å¤§512çš„åŸå› ã€‚</li>
</ol>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="comment"># ä½ç½®ç¼–ç ã€‚transformeråˆ©ç”¨ç¼–ç æ–¹å¼å®ç°ï¼Œæ— éœ€è®­ç»ƒã€‚bertåˆ™é‡‡ç”¨è®­ç»ƒembedding_lookupæ–¹å¼</span></span><br><span class="line">    <span class="comment"># ç¼–ç æ–¹å¼æ–‡æœ¬è¯­å¥é•¿åº¦ä¸å—é™ï¼Œä½†å‡†ç¡®åº¦ä¸é«˜</span></span><br><span class="line">    <span class="comment"># è®­ç»ƒæ–¹å¼æ–‡æœ¬é•¿åº¦ä¼šå—positionç»´åº¦é™åˆ¶ï¼ˆè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆbertåªèƒ½å¤„ç†æœ€å¤§512ä¸ªå­—åŸå› ï¼‰ï¼Œä½†è®­ç»ƒæ•°æ®å¤šæ—¶ï¼Œå‡†ç¡®ç‡é«˜</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, dropout, max_len=<span class="number">5000</span>)</span>:</span></span><br><span class="line">        super(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># é‡‡ç”¨sinå’Œcosè¿›è¡Œposition encoding</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) *</span><br><span class="line">                             -(math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)        <span class="comment"># å¶æ•°åˆ—</span></span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)        <span class="comment"># å¥‡æ•°åˆ—</span></span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        self.register_buffer(<span class="string">'pe'</span>, pe)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># token embeddingå’Œposition encodingåŠ åœ¨ä¸€èµ·</span></span><br><span class="line">        x = x + Variable(self.pe[:, :x.size(<span class="number">1</span>)], </span><br><span class="line">                         requires_grad=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br><span class="line"></span><br><span class="line"><span class="number">123456789101112131415161718192021222324</span></span><br></pre></td></tr></tbody></table></figure>
<p>ç”±ä»£ç å¯è§ï¼Œposition encodingç›´æ¥é‡‡ç”¨äº†ä¸‰è§’å‡½æ•°ã€‚å¯¹å¶æ•°åˆ—é‡‡ç”¨sinï¼Œå¥‡æ•°åˆ—é‡‡ç”¨cosã€‚ <img src="https://i.loli.net/2020/10/28/zPU8Z9scbDNkS5A.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></p>
<h5 id="ç¼–ç å±‚">2.3 ç¼–ç å±‚</h5>
<p>Encoderå±‚æ˜¯Transformerçš„æ ¸å¿ƒï¼Œå®ƒç”±<strong>Nå±‚ç›¸åŒç»“æ„çš„layer</strong>ï¼ˆé»˜è®¤6å±‚ï¼‰å †å è€Œæˆã€‚</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Core encoder is a stack of N layers"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layer, N)</span>:</span></span><br><span class="line">        super(Encoder, self).__init__()</span><br><span class="line">        <span class="comment"># Nå±‚å †å è€Œæˆï¼Œæ¯ä¸€å±‚ç»“æ„éƒ½æ˜¯ç›¸åŒçš„ï¼Œè®­ç»ƒå‚æ•°ä¸åŒ</span></span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># layer normalization</span></span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        <span class="number">45</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask)</span>:</span></span><br><span class="line">        <span class="comment"># 1 ç»è¿‡Nå±‚å †å çš„multi-head attention + feed-forward</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2 å¯¹encoderæœ€ç»ˆè¾“å‡ºç»“æœè¿›è¡Œlayer-normå½’ä¸€åŒ–ã€‚å±‚é—´å’Œå±‚å†…å­æ¨¡å—éƒ½åšè¿‡ add + dropout + layer-norm</span></span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br><span class="line"><span class="number">1234567891011121314151617</span></span><br></pre></td></tr></tbody></table></figure>
<p>encoderçš„å®šä¹‰å¾ˆç®€æ´ã€‚å…ˆç»è¿‡Nå±‚ç›¸åŒç»“æ„çš„layerï¼Œç„¶åå†è¿›è¡Œå½’ä¸€åŒ–è¾“å‡ºã€‚é‡ç‚¹æˆ‘ä»¬æ¥çœ‹layerçš„å®šä¹‰ã€‚</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Encoder is made up of self-attn and feed forward (defined below)"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, self_attn, feed_forward, dropout)</span>:</span></span><br><span class="line">        super(EncoderLayer, self).__init__()</span><br><span class="line">        <span class="comment"># 1 self_attention</span></span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2 feed_forward</span></span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3 æ®‹å·®è¿æ¥ã€‚encoderå’Œdecoderï¼Œæ¯å±‚ç»“æ„ï¼Œæ¯ä¸ªå­ç»“æ„ï¼Œéƒ½æœ‰æ®‹å·®è¿æ¥ã€‚</span></span><br><span class="line">        <span class="comment"># add + drop-out + layer-norm</span></span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">2</span>)</span><br><span class="line">        self.size = size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask)</span>:</span></span><br><span class="line">        <span class="comment"># ç»è¿‡self_attention, ç„¶åå’Œè¾“å…¥è¿›è¡Œadd + layer-norm</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, mask))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ç»è¿‡feed_forwardï¼Œ æ­¤æ¨¡å—ä¹Ÿæœ‰add + layer-norm</span></span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">1</span>](x, self.feed_forward)</span><br><span class="line"></span><br><span class="line"><span class="number">12345678910111213141516171819202122</span></span><br></pre></td></tr></tbody></table></figure>
<p>encoder layeråˆ†ä¸ºä¸¤ä¸ªå­æ¨¡å—</p>
<ol type="1">
<li><strong>self attention</strong>, å¹¶å¯¹è¾“å…¥attentionå‰çš„å’Œç»è¿‡attentionè¾“å‡ºçš„ï¼Œåšæ®‹å·®è¿æ¥ã€‚æ®‹å·®è¿æ¥å…ˆç»è¿‡layer-normå½’ä¸€åŒ–ï¼Œç„¶åè¿›è¡Œdropoutï¼Œæœ€åå†åšaddã€‚åé¢æˆ‘ä»¬è¯¦ç»†åˆ†æ</li>
<li><strong>feed-forward</strong>å…¨è¿æ¥ï¼Œä¹Ÿæœ‰æ®‹å·®è¿æ¥çš„å­˜åœ¨ï¼Œæ–¹å¼å’Œself attentionç›¸åŒã€‚</li>
</ol>
<h6 id="multiheadedattention">2.3.1 MultiHeadedAttention</h6>
<p>MultiHeaded Attentioné‡‡ç”¨å¤šå¤´self-attentionã€‚å®ƒå…ˆå°†éšå‘é‡åˆ‡åˆ†ä¸ºhä¸ªå¤´ï¼Œç„¶åæ¯ä¸ªå¤´å†…éƒ¨è¿›è¡Œself-attentionè®¡ç®—ï¼Œæœ€åå†concatå†ä¸€èµ·ã€‚</p>
<p><img src="https://i.loli.net/2020/10/28/K4lYeqtN7jUR5fx.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"> ä»£ç å¦‚ä¸‹</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadedAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, h, d_model, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        super(MultiHeadedAttention, self).__init__()</span><br><span class="line">        <span class="keyword">assert</span> d_model % h == <span class="number">0</span></span><br><span class="line">        <span class="comment"># d_modelä¸ºéšå±‚ç»´åº¦ï¼Œä¹Ÿæ˜¯embeddingçš„ç»´åº¦ï¼Œhä¸ºå¤šå¤´ä¸ªæ•°ã€‚</span></span><br><span class="line">        <span class="comment"># d_kä¸ºæ¯ä¸ªå¤´çš„éšå±‚ç»´åº¦ï¼Œè¦é™¤ä»¥å¤šå¤´ä¸ªæ•°ã€‚ä¹Ÿå°±æ˜¯åŠ å…¥äº†å¤šå¤´ï¼Œæ€»éšå±‚ç»´åº¦ä¸å˜ã€‚</span></span><br><span class="line">        self.d_k = d_model // h</span><br><span class="line">        self.h = h</span><br><span class="line"></span><br><span class="line">        <span class="comment"># çº¿æ€§è¿æ¥</span></span><br><span class="line">        self.linears = clones(nn.Linear(d_model, d_model), <span class="number">4</span>)</span><br><span class="line">        self.attn = <span class="literal">None</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, mask=None)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># è¾“å…¥maskï¼Œåœ¨decoderçš„æ—¶å€™æœ‰ç”¨åˆ°ã€‚decodeæ—¶ä¸èƒ½çœ‹åˆ°è¦ç”Ÿæˆå­—ä¹‹åçš„å­—ï¼Œæ‰€ä»¥éœ€è¦mask</span></span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        nbatches = query.size(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 1) q, k, vå½¢çŠ¶å˜åŒ–ï¼ŒåŠ å…¥å¤šå¤´ï¼Œ [batch, L, d_model] =&gt; [batch, h, L, d_model/h]</span></span><br><span class="line">        query, key, value = [l(x).view(nbatches, <span class="number">-1</span>, self.h, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">             <span class="keyword">for</span> l, x <span class="keyword">in</span> zip(self.linears, (query, key, value))]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2) attentionè®¡ç®—</span></span><br><span class="line">        x, self.attn = attention(query, key, value, mask=mask, </span><br><span class="line">                                 dropout=self.dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3) å¤šå¤´ç»“æœconcatåœ¨ä¸€èµ·ï¼Œè¿˜åŸä¸ºåˆå§‹å½¢çŠ¶</span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(nbatches, <span class="number">-1</span>, self.h * self.d_k)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 4ï¼‰æœ€åç»è¿‡ä¸€ä¸ªçº¿æ€§å±‚</span></span><br><span class="line">        <span class="keyword">return</span> self.linears[<span class="number">-1</span>](x)</span><br><span class="line"><span class="number">123456789101112131415161718192021222324252627282930313233</span></span><br></pre></td></tr></tbody></table></figure>
<p>ä¸‹é¢é‡ç‚¹æ¥çœ‹å•ä¸ªå¤´çš„self-attentionã€‚ä¹Ÿå°±æ˜¯è®ºæ–‡ä¸­çš„â€œScaled Dot-Product Attentionâ€ã€‚attentionæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªå‘é‡çš„åŠ æƒæ±‚å’Œã€‚å®ƒæ¢è®¨çš„æ˜¯æ¯ä¸ªä½ç½®å¯¹å½“å‰ä½ç½®çš„è´¡çŒ®ã€‚æ­¥éª¤å¦‚ä¸‹</p>
<ol type="1">
<li>qå‘é‡å’Œæ¯ä¸ªä½ç½®çš„kå‘é‡è®¡ç®—ç‚¹ç§¯ï¼Œç„¶åé™¤ä»¥å‘é‡é•¿åº¦çš„æ ¹å·ã€‚è®¡ç®—ç‚¹ç§¯å¯ä»¥è®¤ä¸ºæ˜¯è¿›è¡Œæƒé‡è®¡ç®—ã€‚é™¤ä»¥å‘é‡é•¿åº¦åŸå› æ˜¯å‘é‡è¶Šé•¿ï¼Œq*kå€¼ç†è®ºä¸Šä¼šè¶Šå¤§ï¼Œæ•…éœ€è¦åœ¨å‘é‡é•¿åº¦ä¸Šåšå½’ä¸€åŒ–ã€‚</li>
<li><strong>attention-mask</strong>ã€‚maskå’Œè¾“å…¥çŸ©é˜µshapeç›¸åŒï¼ŒmaskçŸ©é˜µä¸­å€¼ä¸º0ä½ç½®å¯¹åº”çš„è¾“å…¥çŸ©é˜µçš„å€¼æ›´æ”¹ä¸º-1e9ï¼Œä¸€ä¸ªéå¸¸éå¸¸å°çš„æ•°ï¼Œç»è¿‡softmaxåè¶‹è¿‘äº0ã€‚decoderä¸­ä½¿ç”¨äº†maskï¼Œåé¢æˆ‘ä»¬è¯¦ç»†åˆ†æã€‚</li>
<li>softmaxå½’ä¸€åŒ–ï¼Œä½¿å¾—qå‘é‡å’Œæ¯ä¸ªä½ç½®çš„kå‘é‡çš„scoreåˆ†å¸ƒåˆ°ï¼ˆ0, 1ï¼‰ä¹‹é—´</li>
<li>åŠ æƒç³»æ•°ä¹˜ä»¥æ¯ä¸ªä½ç½®vå‘é‡ï¼Œç„¶ååŠ èµ·æ¥ã€‚</li>
</ol>
<p>å…¬å¼å¦‚ä¸‹ï¼š<img src="https://i.loli.net/2020/10/28/C8lDMeBadc4yftg.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"> ä»£ç å¦‚ä¸‹</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention</span><span class="params">(query, key, value, mask=None, dropout=None)</span>:</span></span><br><span class="line">    <span class="comment"># attentionè®¡ç®—ï¼Œself_attentionå’Œsoft-attentionéƒ½æ˜¯ä½¿ç”¨è¿™ä¸ªå‡½æ•°</span></span><br><span class="line">    <span class="comment"># self-attention, q k v å‡æ¥è‡ªåŒä¸€æ–‡æœ¬ã€‚è¦ä¹ˆæ˜¯encoderï¼Œè¦ä¹ˆæ˜¯decoder</span></span><br><span class="line">    <span class="comment"># soft-attention, qæ¥è‡ªdecoderï¼Œkå’Œvæ¥è‡ªencoderï¼Œä»è€ŒæŒ‰ç…§decoderå’Œencoderç›¸å…³æ€§ï¼Œå°†encoderä¿¡æ¯èåˆè¿›æ¥</span></span><br><span class="line">    d_k = query.size(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># åˆ©ç”¨q * kè®¡ç®—ä¸¤å‘é‡é—´ç›¸å…³åº¦ï¼Œç›¸å…³åº¦é«˜åˆ™æƒé‡å¤§ã€‚</span></span><br><span class="line">    <span class="comment"># é™¤ä»¥æ ¹å·dkçš„åŸå› æ˜¯ï¼Œå¯¹å‘é‡é•¿åº¦è¿›è¡Œå½’ä¸€åŒ–ã€‚qå’Œkçš„å‘é‡é•¿åº¦è¶Šé•¿ï¼Œq*kçš„å€¼è¶Šå¤§</span></span><br><span class="line">    scores = torch.matmul(query, key.transpose(<span class="number">-2</span>, <span class="number">-1</span>)) / math.sqrt(d_k)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># attention-maskï¼Œå°† maskä¸­ä¸º1çš„ å…ƒç´ æ‰€åœ¨çš„ç´¢å¼•ï¼Œåœ¨aä¸­ç›¸åŒçš„çš„ç´¢å¼•å¤„æ›¿æ¢ä¸º value</span></span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        scores = scores.masked_fill(mask == <span class="number">0</span>, <span class="number">-1e9</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># softmaxå½’ä¸€åŒ–</span></span><br><span class="line">    p_attn = F.softmax(scores, dim = <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># dropout</span></span><br><span class="line">    <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        p_attn = dropout(p_attn)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># æœ€ååˆ©ç”¨å½’ä¸€åŒ–åçš„åŠ æƒç³»æ•°ï¼Œä¹˜ä»¥æ¯ä¸€ä¸ªvå‘é‡ï¼Œå†åŠ å’Œåœ¨ä¸€èµ·ï¼Œä½œä¸ºattentionåçš„å‘é‡ã€‚æ¯ä¸ªå­—å¯¹åº”ä¸€ä¸ªå‘é‡</span></span><br><span class="line">    <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br><span class="line"><span class="number">1234567891011121314151617181920212223</span></span><br></pre></td></tr></tbody></table></figure>
<p>self-attentionå’Œsoft-attentionå…±ç”¨äº†è¿™ä¸ªå‡½æ•°ï¼Œä»–ä»¬ä¹‹é—´çš„å”¯ä¸€åŒºåˆ«æ˜¯<strong>q k vå‘é‡çš„æ¥æºä¸åŒ</strong>ã€‚self-attentionä¸­q k v å‡æ¥è‡ªåŒä¸€æ–‡æœ¬ã€‚è€Œdecoderçš„soft-attentionï¼Œqæ¥è‡ªäºdecoderï¼Œkå’Œvæ¥è‡ªäºencoderã€‚å®ƒä½“ç°çš„æ˜¯encoderå¯¹decoderçš„åŠ æƒè´¡çŒ®ã€‚</p>
<h6 id="positionwisefeedforward">2.3.2 PositionwiseFeedForward</h6>
<p>feed-forwardæœ¬è´¨æ˜¯ä¸€ä¸ªä¸¤å±‚çš„å…¨è¿æ¥ï¼Œå…¨è¿æ¥ä¹‹é—´åŠ å…¥äº†reluéçº¿æ€§å’Œdropoutã€‚æ¯”è¾ƒç®€å•ï¼Œä»£ç å¦‚ä¸‹</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionwiseFeedForward</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="comment"># å…¨è¿æ¥å±‚</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, d_ff, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        super(PositionwiseFeedForward, self).__init__()</span><br><span class="line">        <span class="comment"># ç¬¬ä¸€å±‚å…¨è¿æ¥  [d_model, d_ff]</span></span><br><span class="line">        self.w_1 = nn.Linear(d_model, d_ff)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ç¬¬äºŒå±‚å…¨è¿æ¥ [d_ff, d_model]</span></span><br><span class="line">        self.w_2 = nn.Linear(d_ff, d_model)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># dropout</span></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># å…¨è¿æ¥1 -&gt; relu -&gt; dropout -&gt; å…¨è¿æ¥2</span></span><br><span class="line">        <span class="keyword">return</span> self.w_2(self.dropout(F.relu(self.w_1(x))))</span><br><span class="line"><span class="number">12345678910111213141516</span></span><br></pre></td></tr></tbody></table></figure>
<p>æ€»ä½“è¿‡ç¨‹æ˜¯ï¼š<strong>å…¨è¿æ¥1 -&gt; relu -&gt; dropout -&gt; å…¨è¿æ¥2</strong>ã€‚ä¸¤å±‚å…¨è¿æ¥å†…éƒ¨æ²¡æœ‰shortcutï¼Œè¿™å„¿ä¸è¦ææ··äº†ã€‚</p>
<h6 id="sublayerconnection">2.3.3 SublayerConnection</h6>
<p>åœ¨æ¯å±‚çš„self-attentionå’Œfeed-forwardæ¨¡å—ä¸­ï¼Œå‡åº”ç”¨äº†æ®‹å·®è¿æ¥ã€‚æ®‹å·®è¿æ¥å…ˆå¯¹è¾“å…¥è¿›è¡ŒlayerNormå½’ä¸€åŒ–ï¼Œç„¶åé€å…¥attentionæˆ–feed-forwardæ¨¡å—ï¼Œç„¶åç»è¿‡dropoutï¼Œæœ€åå†å’ŒåŸå§‹è¾“å…¥ç›¸åŠ ã€‚è¿™æ ·åšçš„å¥½å¤„æ˜¯ï¼Œè®©æ¯ä¸€å±‚attentionå’Œfeed-forwardæ¨¡å—çš„è¾“å…¥å€¼ï¼Œå‡æ˜¯ç»è¿‡å½’ä¸€åŒ–çš„ï¼Œä¿æŒåœ¨ä¸€ä¸ªé‡çº§ä¸Šï¼Œä»è€Œå¯ä»¥åŠ å¿«æ”¶æ•›é€Ÿåº¦ã€‚</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SublayerConnection</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A residual connection followed by a layer norm.</span></span><br><span class="line"><span class="string">    Note for code simplicity the norm is first as opposed to last.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, dropout)</span>:</span></span><br><span class="line">        super(SublayerConnection, self).__init__()</span><br><span class="line">        <span class="comment"># layer-norm å½’ä¸€åŒ–</span></span><br><span class="line">        self.norm = LayerNorm(size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># dropout</span></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, sublayer)</span>:</span></span><br><span class="line">        <span class="comment"># å…ˆå¯¹è¾“å…¥è¿›è¡Œlayer-norm, ç„¶åç»è¿‡attentionç­‰ç›¸å…³æ¨¡å—ï¼Œå†ç»è¿‡dropoutï¼Œæœ€åå†å’Œè¾“å…¥ç›¸åŠ </span></span><br><span class="line">        <span class="keyword">return</span> x + self.dropout(sublayer(self.norm(x)))</span><br><span class="line"><span class="number">12345678910111213141516</span></span><br></pre></td></tr></tbody></table></figure>
<p>ä»forwardå‡½æ•°å¯è§ï¼Œå…ˆå¯¹è¾“å…¥è¿›è¡Œlayer-norm, ç„¶åç»è¿‡attentionç­‰ç›¸å…³æ¨¡å—ï¼Œå†ç»è¿‡dropoutï¼Œæœ€åå†å’Œè¾“å…¥ç›¸åŠ ã€‚æ®‹å·®è¿æ¥çš„ä½œç”¨å°±ä¸è¯´äº†ï¼Œå‚è€ƒResNetã€‚</p>
<h4 id="decoder">3 decoder</h4>
<p>decoderç»“æ„å’Œencoderå¤§ä½“ç›¸åŒï¼Œä¹Ÿæ˜¯å †å äº†Nå±‚ç›¸åŒç»“æ„çš„layerï¼ˆé»˜è®¤6å±‚ï¼‰ã€‚ä¸åŒçš„æ˜¯ï¼Œdecoderçš„æ¯ä¸ªå­å±‚åŒ…æ‹¬ä¸‰å±‚ã€‚</p>
<ol type="1">
<li><strong>masked multi-head self-attention</strong>ã€‚è¿™ä¸€éƒ¨åˆ†å’ŒencoderåŸºæœ¬ç›¸åŒï¼ŒåŒºåˆ«åœ¨äºdecoderä¸ºäº†ä¿è¯æ¨¡å‹ä¸èƒ½çœ‹è§è¦é¢„æµ‹å­—çš„åé¢ä½ç½®çš„å­—ï¼ŒåŠ å…¥äº†maskï¼Œä»è€Œé¿å…æœªæ¥ä¿¡æ¯çš„ç©¿è¶Šé—®é¢˜ã€‚maskä¸ºä¸€ä¸ªä¸Šä¸‰è§’çŸ©é˜µï¼Œä¸Šä¸‰è§’å…¨ä¸º1ï¼Œä¸‹ä¸‰è§’å’Œå¯¹è§’çº¿å…¨ä¸º0</li>
<li><strong>multi-head soft-attention</strong>ã€‚soft-attentionå’Œself-attentionç»“æ„åŸºæœ¬ç›¸åŒï¼Œç”šè‡³å®ç°å‡½æ•°éƒ½æ˜¯åŒä¸€ä¸ªã€‚å”¯ä¸€çš„åŒºåˆ«åœ¨äºï¼Œself-attentionçš„q k vçŸ©é˜µæ¥è‡ªåŒä¸€ä¸ªï¼Œæ‰€ä»¥å«self-attentionã€‚è€Œsoft-attentionçš„qæ¥è‡ªdecoderï¼Œkå’Œvæ¥è‡ªencoderã€‚è¡¨å¾çš„æ˜¯encoderçš„æ•´ä½“è¾“å‡ºå¯¹äºdecoderçš„è´¡çŒ®ã€‚</li>
<li><strong>feed-forward</strong>ã€‚è¿™ä¸€å—åŸºæœ¬ç›¸åŒã€‚</li>
</ol>
<p>å¦å¤–ä¸‰ä¸ªæ¨¡å—å‡ä½¿ç”¨äº†æ®‹å·®è¿æ¥ï¼Œæ­¥éª¤ä»ç„¶ä¸º layerNorm -&gt; attentionç­‰æ¨¡å— -&gt; dropout -&gt; å’Œè¾“å…¥è¿›è¡Œadd</p>
<p>decoderæ¯ä¸ªlayerä»£ç å¦‚ä¸‹</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Decoder is made of self-attn, src-attn, and feed forward (defined below)"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, self_attn, src_attn, feed_forward, dropout)</span>:</span></span><br><span class="line">        super(DecoderLayer, self).__init__()</span><br><span class="line">        self.size = size</span><br><span class="line"></span><br><span class="line">        <span class="comment"># self-attention è‡ªæ³¨æ„åŠ›</span></span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line"></span><br><span class="line">        <span class="comment"># soft-attenton, encoderçš„è¾“å‡ºå¯¹decoderçš„ä½œç”¨</span></span><br><span class="line">        self.src_attn = src_attn</span><br><span class="line"></span><br><span class="line">        <span class="comment"># feed-forward å…¨è¿æ¥</span></span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line"></span><br><span class="line">        <span class="comment"># æ®‹å·®è¿æ¥</span></span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, memory, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="comment"># memoryä¸ºencoderæœ€ç»ˆè¾“å‡º</span></span><br><span class="line">        m = memory</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1 å¯¹decoderè¾“å…¥åšself-attention, å†å’Œè¾“å…¥åšæ®‹å·®è¿æ¥</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, tgt_mask))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2 å¯¹encoderè¾“å‡ºå’Œdecoderå½“å‰è¿›è¡Œsoft-attentionï¼Œæ­¤å¤„ä¹Ÿæœ‰æ®‹å·®è¿æ¥</span></span><br><span class="line">        x = self.sublayer[<span class="number">1</span>](x, <span class="keyword">lambda</span> x: self.src_attn(x, m, m, src_mask))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3 feed-forwardå…¨è¿æ¥ï¼Œä¹Ÿæœ‰æ®‹å·®è¿æ¥</span></span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">2</span>](x, self.feed_forward)</span><br><span class="line"><span class="number">123456789101112131415161718192021222324252627282930</span></span><br></pre></td></tr></tbody></table></figure>
<h4 id="è¾“å‡ºå±‚">4 è¾“å‡ºå±‚</h4>
<p>decoderçš„è¾“å‡ºä½œä¸ºæœ€ç»ˆè¾“å‡ºå±‚çš„è¾“å…¥ï¼Œç»è¿‡ä¸¤æ­¥</p>
<ol type="1">
<li>linearçº¿æ€§è¿æ¥ï¼Œä¹Ÿå³æ˜¯w * x + b</li>
<li>softmaxå½’ä¸€åŒ–ï¼Œå‘é‡é•¿åº¦ç­‰äºvocabularyçš„é•¿åº¦ï¼Œå¾—åˆ°vocabularyä¸­æ¯ä¸ªå­—çš„æ¦‚ç‡ã€‚åˆ©ç”¨beam-searchç­‰æ–¹æ³•ï¼Œå³å¯å¾—åˆ°ç”Ÿæˆç»“æœã€‚</li>
</ol>
<p>è¿™ä¸€å±‚æ¯”è¾ƒç®€å•ï¼Œä»£ç å¦‚ä¸‹</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Define standard linear + softmax generation step."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, vocab)</span>:</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        self.proj = nn.Linear(d_model, vocab)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># å…ˆç»è¿‡linearçº¿æ€§å±‚ï¼Œç„¶åç»è¿‡softmaxå¾—åˆ°å½’ä¸€åŒ–æ¦‚ç‡åˆ†å¸ƒ</span></span><br><span class="line">        <span class="comment"># è¾“å‡ºå‘é‡é•¿åº¦ç­‰äºvocabularyçš„ç»´åº¦</span></span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(self.proj(x), dim=<span class="number">-1</span>)</span><br><span class="line"><span class="number">12345678910</span></span><br></pre></td></tr></tbody></table></figure>
<h4 id="æ€»ç»“">5 æ€»ç»“</h4>
<p>Transformerç›¸æ¯”LSTMçš„ä¼˜ç‚¹</p>
<ol type="1">
<li><strong>å®Œå…¨çš„å¹¶è¡Œè®¡ç®—</strong>ï¼ŒTransformerçš„attentionå’Œfeed-forwardï¼Œå‡å¯ä»¥å¹¶è¡Œè®¡ç®—ã€‚è€ŒLSTMåˆ™ä¾èµ–ä¸Šä¸€æ—¶åˆ»ï¼Œå¿…é¡»ä¸²è¡Œ</li>
<li><strong>å‡å°‘é•¿ç¨‹ä¾èµ–</strong>ï¼Œåˆ©ç”¨self-attentionå°†æ¯ä¸ªå­—ä¹‹é—´è·ç¦»ç¼©çŸ­ä¸º1ï¼Œå¤§å¤§ç¼“è§£äº†é•¿è·ç¦»ä¾èµ–é—®é¢˜</li>
<li><strong>æé«˜ç½‘ç»œæ·±åº¦</strong>ã€‚ç”±äºå¤§å¤§ç¼“è§£äº†é•¿ç¨‹ä¾èµ–æ¢¯åº¦è¡°å‡é—®é¢˜ï¼ŒTransformerç½‘ç»œå¯ä»¥å¾ˆæ·±ï¼ŒåŸºäºTransformerçš„BERTç”šè‡³å¯ä»¥åšåˆ°24å±‚ã€‚è€ŒLSTMä¸€èˆ¬åªæœ‰2å±‚æˆ–è€…4å±‚ã€‚ç½‘ç»œè¶Šæ·±ï¼Œé«˜é˜¶ç‰¹å¾æ•è·èƒ½åŠ›è¶Šå¥½ï¼Œæ¨¡å‹performanceä¹Ÿå¯ä»¥è¶Šé«˜ã€‚</li>
<li><strong>çœŸæ­£çš„åŒå‘ç½‘ç»œ</strong>ã€‚Transformerå¯ä»¥åŒæ—¶èåˆå‰åä½ç½®çš„ä¿¡æ¯ï¼Œè€ŒåŒå‘LSTMåªæ˜¯ç®€å•çš„å°†ä¸¤ä¸ªæ–¹å‘çš„ç»“æœç›¸åŠ ï¼Œä¸¥æ ¼æ¥è¯´ä»ç„¶æ˜¯å•å‘çš„ã€‚</li>
<li><strong>å¯è§£é‡Šæ€§å¼º</strong>ã€‚å®Œå…¨åŸºäºattentionçš„Transformerï¼Œå¯ä»¥è¡¨è¾¾å­—ä¸å­—ä¹‹é—´çš„ç›¸å…³å…³ç³»ï¼Œå¯è§£é‡Šæ€§æ›´å¼ºã€‚</li>
</ol>
<p>Transformerä¹Ÿä¸æ˜¯ä¸€å®šå°±æ¯”LSTMå¥½ï¼Œå®ƒçš„ç¼ºç‚¹å¦‚ä¸‹</p>
<ol type="1">
<li>æ–‡æœ¬é•¿åº¦å¾ˆé•¿æ—¶ï¼Œæ¯”å¦‚ç¯‡ç« çº§åˆ«ï¼Œ<strong>è®¡ç®—é‡çˆ†ç‚¸</strong>ã€‚self-attentionçš„è®¡ç®—é‡ä¸ºO(n^2), nä¸ºæ–‡æœ¬é•¿åº¦ã€‚Transformer-xlåˆ©ç”¨å±‚çº§æ–¹å¼ï¼Œå°†è®¡ç®—é€Ÿåº¦æå‡äº†1800å€</li>
<li>Transformerä½ç½®ä¿¡æ¯åªé <strong>position encoding</strong>ï¼Œæ•ˆæœæ¯”è¾ƒä¸€èˆ¬ã€‚å½“è¯­å¥è¾ƒçŸ­æ—¶ï¼Œæ¯”å¦‚å°äº10ä¸ªå­—ï¼ŒTransformeræ•ˆæœä¸ä¸€å®šæ¯”LSTMå¥½</li>
<li>Transformerå‚æ•°é‡è¾ƒå¤§ï¼Œåœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šï¼Œæ•ˆæœè¿œå¥½äºLSTMã€‚ä½†åœ¨<strong>å°è§„æ¨¡æ•°æ®é›†</strong>ä¸Šï¼Œå¦‚æœä¸æ˜¯åˆ©ç”¨pretrain modelsï¼Œæ•ˆæœä¸ä¸€å®šæœ‰LSTMå¥½ã€‚</li>
</ol>
<h3 id="transformerå®¶æ—2----ç¼–ç é•¿åº¦ä¼˜åŒ–transformer-xllongformer">ğŸš€Transformerå®¶æ—2 -- ç¼–ç é•¿åº¦ä¼˜åŒ–ï¼ˆTransformer-XLã€Longformerï¼‰</h3>
<h4 id="èƒŒæ™¯">1 èƒŒæ™¯</h4>
<p>NLPä¸­ç»å¸¸å‡ºç°é•¿ç¨‹ä¾èµ–é—®é¢˜ï¼Œæ¯”å¦‚ä¸€ä¸ªè¯è¯­å¯èƒ½å’Œå®ƒè·ç¦»ä¸Šåƒä½ç½®çš„å¦ä¸€ä¸ªè¯è¯­æœ‰å…³ç³»ã€‚é•¿ç¨‹å…³ç³»çš„å»ºç«‹ååˆ†å›°éš¾ã€‚å¸¸è§åºåˆ—ç»“æ„æ¨¡å‹éƒ½æœ‰ä¸€äº›éš¾ç‚¹ï¼Œå¦‚ä¸‹ã€‚</p>
<ol type="1">
<li>åœ¨RNNä¸­ï¼Œç”±äºåå‘ä¼ æ’­æ¢¯åº¦è¡°å‡å’Œæ¢¯åº¦çˆ†ç‚¸é—®é¢˜ï¼Œä½¿å¾—æ¨¡å‹åªèƒ½æ•è·è¾ƒçŸ­è·ç¦»ã€‚</li>
<li>LSTMåˆ©ç”¨é—¨é™æœºåˆ¶ï¼Œå°†è¿ä¹˜è½¬å˜äº†ä¸ºè¿åŠ ï¼Œæå‡äº†æ¨¡å‹é•¿ç¨‹æ•è·èƒ½åŠ›ï¼Œä½†æ¢¯åº¦å¼¥æ•£é—®é¢˜æ²¡æœ‰ä»æ ¹æœ¬ä¸Šå¾—åˆ°è§£å†³ï¼Œæ•…å…¶æœ€å¤§ç¨‹åº¦åªèƒ½åœ¨400å·¦å³ã€‚</li>
<li>Transformeråˆ©ç”¨self-attentionæœºåˆ¶è¿›è¡Œå»ºæ¨¡ï¼Œä½¿å¾—ä»»ä½•ä¸¤ä¸ªä½ç½®tokenè·ç¦»éƒ½ä¸º1ã€‚å¦‚æœæ²¡æœ‰å†…å­˜å’Œç®—åŠ›çš„é™åˆ¶ï¼ŒTransformerç†è®ºä¸Šå¯ä»¥ç¼–ç æ— é™é•¿çš„æ–‡æœ¬ã€‚ä½†ç”±äºattentionè®¡ç®—é‡ååˆ†å¤§ï¼Œè€Œä¸”è®¡ç®—å¤æ‚åº¦å’Œåºåˆ—é•¿åº¦ä¸ºO(n^2)å…³ç³»ï¼Œå¯¼è‡´åºåˆ—é•¿åº¦å¢åŠ ï¼Œå†…å­˜å’Œè®¡ç®—é‡æ¶ˆè€—é£å¿«å¢åŠ ã€‚å®é™…ä¸­ç”±äºå†…å­˜å’Œç®—åŠ›æœ‰é™ï¼Œä¸€èˆ¬åªèƒ½ç¼–ç ä¸€å®šé•¿åº¦ï¼Œä¾‹å¦‚512ã€‚</li>
</ol>
<p>ä¸ºäº†æå‡æ¨¡å‹çš„é•¿ç¨‹ç¼–ç èƒ½åŠ›ï¼Œä»è€Œæå‡æ¨¡å‹åœ¨é•¿æ–‡æœ¬ï¼Œç‰¹åˆ«æ˜¯document-levelè¯­æ–™ä¸Šçš„æ•ˆæœï¼Œæˆ‘ä»¬å¿…é¡»å¯¹Transformerç¼–ç é•¿åº¦è¿›è¡Œä¼˜åŒ–ã€‚æœ¬æ–‡å¸¦æ¥äº†Transformer-XLã€Longformerï¼Œè¯¦ç»†åˆ†æä»–ä»¬å¦‚ä½•å®ç°ç¼–ç é•¿åº¦ä¼˜åŒ–ã€‚</p>
<p>LongFormeré€šè¿‡é™ä½attentionè®¡ç®—æ‰€éœ€å†…å­˜å’Œç®—åŠ›ï¼Œæ¥å®ç°é•¿æ–‡æœ¬ç¼–ç ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥æŠŠå®ƒå½’å…¥åˆ°ç®—åŠ›ä¼˜åŒ–ä¸­ã€‚ä½†é‰´äºå…¶åå­—å°±é‡ç‚¹ä½“ç°äº†å®ƒçš„é•¿è·ç¦»èƒ½åŠ›ï¼Œæ•…è¿˜æ˜¯æ”¾åœ¨äº†ç¼–ç é•¿åº¦ä¼˜åŒ–ä¸­ï¼Œå’ŒTransformer-XLä¸€èµ·æ¥åˆ†æ</p>
<h4 id="transformer-xl">2 Transformer-XL</h4>
<p><img src="https://i.loli.net/2020/10/28/ctFMhR8IEdPVSZ1.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"> è®ºæ–‡ä¿¡æ¯ï¼š2019å¹´01æœˆï¼Œè°·æ­Œ &amp; CMUï¼ŒACL 2019 è®ºæ–‡åœ°å€ https://arxiv.org/abs/1901.02860 ä»£ç å’Œæ¨¡å‹åœ°å€ https://github.com/kimiyoung/transformer-xl</p>
<h5 id="ä¸ºä»€ä¹ˆéœ€è¦transformer-xl">2.1 ä¸ºä»€ä¹ˆéœ€è¦Transformer-XL</h5>
<p>ä¸ºäº†è§£å†³é•¿æ–‡æœ¬ç¼–ç é—®é¢˜ï¼ŒåŸç‰ˆTransformeré‡‡ç”¨äº†å›ºå®šç¼–ç é•¿åº¦çš„æ–¹æ¡ˆï¼Œä¾‹å¦‚512ä¸ªtokenã€‚å°†é•¿æ–‡æœ¬æŒ‰ç…§å›ºå®šé•¿åº¦ï¼Œåˆ‡åˆ†ä¸ºå¤šä¸ªsegmentã€‚æ¯ä¸ªsegmentå†…éƒ¨å•ç‹¬ç¼–ç ï¼Œsegmentä¹‹é—´ä¸äº§ç”Ÿäº¤äº’ä¿¡æ¯ã€‚è¿™ç§æ–¹å¼çš„é—®é¢˜å¦‚ä¸‹</p>
<ol type="1">
<li>æ¨¡å‹æ— æ³•å»ºæ¨¡è¶…è¿‡å›ºå®šç¼–ç é•¿åº¦çš„æ–‡æœ¬</li>
<li>segmentä¹‹é—´æ²¡æœ‰äº¤äº’ä¿¡æ¯ï¼Œå¯¼è‡´äº†æ–‡æœ¬ç¢ç‰‡åŒ–ã€‚é•¿è¯­å¥çš„ç¼–ç æ•ˆæœæœ‰å¾…æå‡ã€‚</li>
<li>predicté˜¶æ®µï¼Œdecoderæ¯ç”Ÿæˆä¸€ä¸ªå­—ï¼Œå°±å¾€åæŒªä¸€ä¸ªï¼Œæ²¡æœ‰é‡å¤åˆ©ç”¨ä¹‹å‰ä¿¡æ¯ï¼Œå¯¼è‡´è®¡ç®—é‡çˆ†ç‚¸</li>
</ol>
<p>trainå’Œevaluateè¿‡ç¨‹å¦‚ä¸‹<img src="https://i.loli.net/2020/10/28/8YW7R4Q5J6zbmD1.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></p>
<h5 id="å®ç°æ–¹æ³•">2.2 å®ç°æ–¹æ³•</h5>
<h6 id="segment-level-recurrence-with-state-reuse-ç‰‡æ®µçº§é€’å½’å’Œä¿¡æ¯å¤ç”¨">2.2.1 Segment-Level Recurrence with State Reuse ç‰‡æ®µçº§é€’å½’å’Œä¿¡æ¯å¤ç”¨</h6>
<p>Transformer-XLåœ¨ç¼–ç åä¸€ä¸ªsegmentæ—¶ï¼Œå°†å‰ä¸€ä¸ªsegmentçš„éšå±‚ç¼“å­˜ä¸‹æ¥ã€‚åä¸€ä¸ªsegmentçš„self-attentionè®¡ç®—ï¼Œä¼šä½¿ç”¨åˆ°å‰ä¸€ä¸ªsegmentçš„éšå±‚ã€‚åä¸€ä¸ªsegmentçš„ç¬¬n+1å±‚ï¼Œå¯¹å‰ä¸€ä¸ªsegmentçš„ç¬¬nå±‚éšå±‚è¿›è¡Œèåˆã€‚æ•…æœ€å¤§ç¼–ç é•¿åº¦ç†è®ºä¸Šä¸ºO(N Ã— L)ã€‚åœ¨é¢„æµ‹é˜¶æ®µï¼Œç”±äºå¯¹segmentéšå±‚ä½¿ç”¨äº†ç¼“å­˜ï¼Œæ•…æ¯é¢„æµ‹ä¸€ä¸ªè¯ï¼Œä¸éœ€è¦é‡æ–°å¯¹ä¹‹å‰çš„æ–‡æœ¬è¿›è¡Œè®¡ç®—ã€‚å¤§å¤§æå‡äº†é¢„æµ‹é€Ÿåº¦ï¼Œæœ€å¤§å¯è¾¾åˆ°åŸå§‹Transformerçš„1800å€ã€‚å¦‚ä¸‹å›¾æ‰€ç¤º <img src="https://i.loli.net/2020/10/28/KmazXviordxyZuw.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></p>
<h6 id="relative-positional-encodings-ç›¸å¯¹ä½ç½®ç¼–ç ">2.2.2 Relative Positional Encodings ç›¸å¯¹ä½ç½®ç¼–ç </h6>
<p>segmenté€’å½’ä¸­æœ‰ä¸ªæ¯”è¾ƒå¤§çš„é—®é¢˜ï¼Œå°±æ˜¯å¦‚ä½•åŒºåˆ†ä¸åŒsegmentä¸­çš„ç›¸åŒä½ç½®ã€‚å¦‚æœé‡‡ç”¨åŸç‰ˆTransformerä¸­çš„ç»å¯¹ç¼–ç æ–¹æ¡ˆï¼Œä¸¤è€…æ˜¯æ— æ³•åŒºåˆ†çš„ã€‚å¦‚ä¸‹ <img src="https://i.loli.net/2020/10/28/38l7XhVjZnqyuBe.png" alt="gh">ä¸åŒsegmentä¸­çš„ç›¸åŒä½ç½®ï¼Œå…¶position encodingä¼šç›¸åŒã€‚è¿™æ˜¾ç„¶æ˜¯æœ‰é—®é¢˜çš„ã€‚Transformer-XLå°†ç»å¯¹ä½ç½®ç¼–ç æ”¹ä¸ºäº†qå’Œkä¹‹é—´çš„ç›¸å¯¹ä½ç½®ç¼–ç ï¼Œä»£è¡¨äº†ä¸¤ä¸ªtokenä¹‹é—´çš„ç›¸å¯¹ä½ç½®ã€‚ä»è¯­ä¹‰ä¸Šè®²ï¼Œæ˜¯make senseçš„ã€‚æˆ‘ä»¬æ¥çœ‹çœ‹å…·ä½“å®ç°æ–¹å¼ã€‚</p>
<p>ç»å¯¹ä½ç½®ç¼–ç çš„attentionè®¡ç®—å¦‚ä¸‹ <img src="https://i.loli.net/2020/10/28/IKCVE7riwaQ9Zl1.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°">åˆ†ä¸ºå››éƒ¨åˆ†</p>
<ol type="1">
<li>queryçš„token encodingå’Œ keyçš„token encodingï¼Œä¹‹é—´çš„å…³è”ä¿¡æ¯</li>
<li>queryçš„token encodingå’Œ keyçš„position encodingï¼Œä¹‹é—´çš„å…³è”ä¿¡æ¯ã€‚Ujä¸ºç»å¯¹ä½ç½®jçš„ç¼–ç å‘é‡</li>
<li>queryçš„position encodingå’Œ keyçš„token encodingï¼Œä¹‹é—´çš„å…³è”ä¿¡æ¯ã€‚Uiä¸ºç»å¯¹ä½ç½®içš„ç¼–ç å‘é‡</li>
<li>queryçš„position encodingå’Œ keyçš„position encodingï¼Œä¹‹é—´çš„å…³è”ä¿¡æ¯</li>
</ol>
<p>è€Œé‡‡ç”¨ç›¸å¯¹ä½ç½®ç¼–ç åï¼Œattentionè®¡ç®—å¦‚ä¸‹ <img src="https://i.loli.net/2020/10/28/Id9fV3bxSnmHyu2.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°">åŒæ ·åŒ…å«å››éƒ¨åˆ†ï¼Œä»ç„¶ä¸ºäºŒè€…token encodingå’Œposition encodingä¹‹é—´çš„å…³è”å…³ç³»ã€‚åŒºåˆ«åœ¨äº</p>
<ol type="1">
<li>Ri-jä¸ºiå’Œjä¹‹é—´ç›¸å¯¹ä½ç½®ç¼–ç ï¼Œå…¶ä¸­Rä¸ºç›¸å¯¹ä½ç½®ç¼–ç çŸ©é˜µ</li>
<li>uå’Œvä¸ºqueryçš„ä½ç½®ç¼–ç ï¼Œé‡‡ç”¨ä¸€ä¸ªå›ºå®šå‘é‡ã€‚å› ä¸ºé‡‡ç”¨ç›¸å¯¹ä½ç½®ç¼–ç åï¼Œæ— æ³•å¯¹å•ä¸ªç»å¯¹ä½ç½®è¿›è¡Œç¼–ç äº†ã€‚æ–‡ä¸­ç§°ä¸ºglobal content biasï¼Œå’Œglobal positional bias</li>
</ol>
<p>ä¹Ÿæœ‰å…¶ä»–æ–‡ç« ï¼Œé‡‡ç”¨äº†ä¸åŒçš„ç›¸å¯¹ä½ç½®ç¼–ç æ–¹æ¡ˆã€‚æ¯”å¦‚"Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.2018. Self-attention with relative position representations. arXiv preprint arXiv:1803.02155." ä¸­åªæœ‰aå’Œbä¸¤éƒ¨åˆ†ï¼Œä¸¢æ‰äº†cå’Œdã€‚Transformer-XLå¯¹ä¸¤ç§æ–¹æ¡ˆè¿›è¡Œäº†å¯¹æ¯”å®éªŒï¼Œè¯æ˜å‰ä¸€ç§å¥½ã€‚</p>
<h5 id="å®éªŒç»“æœ">2.3 å®éªŒç»“æœ</h5>
<h6 id="é•¿æ–‡æœ¬ç¼–ç æ•ˆæœ">é•¿æ–‡æœ¬ç¼–ç æ•ˆæœ</h6>
<p><img src="https://i.loli.net/2020/10/28/IyoR924J3BQWrOu.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"> åœ¨WikiText-103ä¸Šçš„å®éªŒç»“æœã€‚WikiText-103åŒ…å«è¯çº§åˆ«çš„è¶…é•¿æ–‡æœ¬ï¼Œå¹³å‡æ¯ç¯‡æ–‡ç« é•¿åº¦ä¸º3.6K tokenã€‚åˆ©ç”¨å®ƒå¯ä»¥éªŒè¯æ¨¡å‹çš„é•¿æ–‡æœ¬ç¼–ç èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜Transformer-XL largeçš„PPLæœ€ä½ï¼Œæ•ˆæœæœ€å¥½ã€‚åŒæ—¶ä½œè€…åœ¨One Billion Wordã€enwik8ã€text8ä¸Šå‡è¿›è¡Œäº†å®éªŒï¼Œéƒ½è¡¨æ˜Transformer-XLæ•ˆæœæœ€å¥½ã€‚</p>
<h6 id="æœ‰æ•ˆç¼–ç é•¿åº¦">æœ‰æ•ˆç¼–ç é•¿åº¦</h6>
<p><img src="https://i.loli.net/2020/10/28/oKQv3akDcXWOZ7B.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"> æ¨¡å‹å¯ç¼–ç çš„æœ‰æ•ˆé•¿åº¦å¦‚ä¸Šï¼Œrä¸ºtop-réš¾åº¦æ ·æœ¬ä¸Šçš„è¡¨ç°ã€‚Transformer-XLæ¯”RNNé•¿80%ï¼Œæ¯”Transformeré•¿450%ã€‚è¯æ˜Transformer-XLå¯ç¼–ç é•¿åº¦æœ€é•¿ï¼Œé•¿ç¨‹æ•è·èƒ½åŠ›æœ€å¼ºã€‚</p>
<h6 id="é¢„æµ‹é€Ÿåº¦">é¢„æµ‹é€Ÿåº¦</h6>
<p><img src="https://i.loli.net/2020/10/28/g2TyZhWtRSEA7pB.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"> åœ¨ä¸åŒsegmenté•¿åº¦ä¸‹ï¼Œæ¨¡å‹é¢„æµ‹é€Ÿåº¦çš„å¯¹æ¯”ã€‚å’ŒåŸå§‹Transformerç›¸æ¯”ï¼Œé¢„æµ‹é€Ÿåº¦æœ€å¤§å¯ä»¥æå‡1874å€ã€‚</p>
<h6 id="æ¶ˆèåˆ†æ">æ¶ˆèåˆ†æ</h6>
<p>æ–‡ç« å¯¹ç‰‡æ®µçº§é€’å½’å’Œç›¸å¯¹ä½ç½®ç¼–ç ä¸¤ä¸ªMethodè¿›è¡Œäº†æ¶ˆèåˆ†æï¼Œå¦‚ä¸‹ <img src="https://i.loli.net/2020/10/28/4EeY78JcQojSuhD.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"> ä¸¤ä¸ªæ”¹è¿›ç‚¹å‡æœ‰ä½œç”¨ï¼Œå…¶ä¸­ç‰‡æ®µçº§é€’å½’ä½œç”¨æ›´å¤§ã€‚</p>
<h4 id="longformer">3 Longformer</h4>
<p><img src="https://i.loli.net/2020/10/28/KhcS2VIyvNEAwrg.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"> è®ºæ–‡ä¿¡æ¯ï¼š2020å¹´04æœˆï¼Œallenai è®ºæ–‡åœ°å€ https://arxiv.org/abs/2004.05150 ä»£ç å’Œæ¨¡å‹åœ°å€ https://github.com/allenai/longformer</p>
<h5 id="æ”¹è¿›æ–¹æ³•">3.1 æ”¹è¿›æ–¹æ³•</h5>
<h6 id="attentionç¨€ç–åŒ–">3.1.1 attentionç¨€ç–åŒ–</h6>
<p>Transformerä¸èƒ½æ•è·é•¿è·ç¦»ä¿¡æ¯ï¼Œæœ¬è´¨åŸå› è¿˜æ˜¯å› ä¸ºè®¡ç®—é‡è¿‡å¤§å¯¼è‡´çš„ã€‚é‚£æˆ‘ä»¬é€šè¿‡é™ä½attentionè®¡ç®—é‡ï¼Œæ˜¯ä¸æ˜¯å°±å¯ä»¥æå‡é•¿è·ç¦»ç¼–ç èƒ½åŠ›å‘¢ã€‚ç­”æ¡ˆæ˜¯è‚¯å®šçš„ï¼ŒLongFormeræå‡ºäº†ä¸‰ç§attentionç¨€ç–åŒ–çš„æ–¹æ³•ï¼Œæ¥é™ä½è®¡ç®—é‡ã€‚ <img src="https://i.loli.net/2020/10/28/hDmpknyqTSIUEW7.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"> aæ˜¯åŸå§‹çš„å…¨è¿æ¥æ–¹å¼çš„attentionã€‚åé¢ä¸‰ç§ä¸ºæ–‡ç« ä½¿ç”¨çš„ç¨€ç–attentionã€‚</p>
<ol type="1">
<li>Sliding Window attentionã€‚æ»‘çª—æ–¹å¼çš„attentionã€‚å‡è®¾åºåˆ—é•¿åº¦ä¸ºnï¼Œæ»‘çª—å¤§å°wï¼Œåˆ™æ¯ä¸ªä½ç½®çš„attentionåªå’Œæ»‘çª—èŒƒå›´å†…çš„tokenè¿›è¡Œè®¡ç®—ï¼Œå¤æ‚åº¦ä»O(n^2)å˜ä¸ºäº†O(n * w)ã€‚å½“w &lt;&lt; næ—¶ï¼Œå¯ä»¥å¤§å¤§é™ä½è®¡ç®—é‡ã€‚</li>
<li>Dilated Sliding Window attentionã€‚å—åˆ°ç©ºæ´å·ç§¯çš„å¯å‘ï¼Œæå‡ºäº†ç©ºæ´æ»‘çª—attentionã€‚çœ‹ä¸‹é¢è¿™å¼ å›¾å°±æ˜ç™½äº†ã€‚</li>
</ol>
<p><img src="https://i.loli.net/2020/10/28/ZwAy1H3kB7SjIP5.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></p>
<ol type="1">
<li>Global Attention + sliding windowã€‚æŸäº›å…³é”®ä½ç½®é‡‡ç”¨å…¨å±€attentionï¼Œè¿™äº›ä½ç½®çš„attentionå’Œæ‰€æœ‰tokenè¿›è¡Œè®¡ç®—ã€‚è€Œå…¶ä»–ç›¸å¯¹ä¸å…³é”®çš„ä½ç½®åˆ™é‡‡ç”¨æ»‘çª—attentionã€‚é‚£ä»€ä¹ˆå«å…³é”®ä½ç½®å‘¢ï¼Ÿä½œè€…ä¸¾ä¾‹ï¼Œåˆ†ç±»é—®é¢˜ä¸­[CLS]ä¸ºå…³é”®ä½ç½®ï¼Œéœ€è¦è®¡ç®—å…¨å±€attentionã€‚QAä¸­questionæ¯ä¸ªä½ç½®å‡ä¸ºå…³é”®ä½ç½®ï¼ŒåŒæ ·è®¡ç®—å…¨å±€attentionã€‚</li>
</ol>
<h6 id="tensor-virtual-machine-tvm">3.1.2 Tensor Virtual Machine (TVM)</h6>
<p>ä½œè€…ä½¿ç”¨äº†TVMæ„å»ºCUDA kernelï¼ŒåŠ å¿«äº†longformerçš„é€Ÿåº¦ï¼Œå¹¶é™ä½äº†æ˜¾å­˜éœ€æ±‚ã€‚è¿™ä¸ªåˆæ˜¯å¦ä¸€ä¸ªæ¨¡å‹åŠ é€Ÿæ–¹é¢çš„è¯é¢˜ï¼Œæˆ‘ä»¬å°±å…ˆä¸å±•å¼€äº†ã€‚</p>
<h5 id="å®éªŒç»“æœ-1">3.2 å®éªŒç»“æœ</h5>
<h6 id="å¤§å°æ¨¡å‹æ•ˆæœ">å¤§å°æ¨¡å‹æ•ˆæœ</h6>
<p><img src="https://i.loli.net/2020/10/28/ZThJDNXByQp2Lod.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°">ä½œè€…åœ¨å¤§å°æ¨¡å‹ä¸Šå‡å®éªŒäº†LongFormerçš„æ•ˆæœã€‚å°æ¨¡å‹ä¸º12 layersï¼Œ512 hiddenã€‚å¤§æ¨¡å‹ä¸º30 layersï¼Œ512 hiddenã€‚åœ¨text8å’Œenwik8æ•°æ®é›†ä¸Šã€‚å°æ¨¡å‹è¾¾åˆ°äº†SOTAã€‚å¤§æ¨¡å‹æ¯”18å±‚çš„Transformer-XLå¥½ï¼Œè™½ç„¶ä¸å¦‚Adaptive-span-Transformerå’ŒCompressiveï¼Œä½†èƒœåœ¨å¯ä»¥pretrain-finetune</p>
<h6 id="æ¶ˆèåˆ†æ-1">æ¶ˆèåˆ†æ</h6>
<p><img src="https://i.loli.net/2020/10/28/pMCoW2ZLv6gq3j4.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°">æ¶ˆèåˆ†æä¸­ï¼Œå¯ä»¥å‘ç°</p>
<ol type="1">
<li>Dilationç©ºæ´ï¼Œæœ‰ä¸€å®šçš„æ”¶ç›Š</li>
<li>top layeræ»‘çª—å¤§å°æ¯”bottom layerå¤§æ—¶ï¼Œæ•ˆæœå¥½ä¸€äº›ã€‚è¿™ä¸ªä¹Ÿæ˜¯make senseçš„ã€‚å› ä¸ºtop layeræ•è·é«˜ç»´è¯­ä¹‰ï¼Œå…³è”ä¿¡æ¯è·ç¦»ç›¸å¯¹è¾ƒè¿œï¼Œçª—å£åº”è¯¥å°½é‡å¤§ä¸€äº›ã€‚</li>
</ol>
<h6 id="è¯­æ–™é•¿åº¦">è¯­æ–™é•¿åº¦</h6>
<p><img src="https://i.loli.net/2020/10/28/IjGfH7Mi1LK2hsC.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"> ä»ä¸Šè¡¨ä¸­æˆ‘ä»¬å‘ç°ï¼Œè¯­æ–™éƒ½æ˜¯ç‰¹åˆ«é•¿çš„é•¿æ–‡æœ¬ã€‚LongFormerçœŸçš„æ˜¯documentçº§åˆ«çš„Transformerã€‚</p>
<h6 id="ä¸‹æ¸¸ä»»åŠ¡finetuneæ•ˆæœ">ä¸‹æ¸¸ä»»åŠ¡finetuneæ•ˆæœ</h6>
<p><img src="https://i.loli.net/2020/10/28/JdZUrh6L3QFBHIn.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"> <img src="https://i.loli.net/2020/10/28/WlOZR3iaJSA2nIU.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"> ç¬¬ä¸€ä¸ªtableä¸ºRoBERTaå’ŒLongFormeråœ¨é—®ç­”ã€æŒ‡ä»£æ¶ˆè§£ã€åˆ†ç±»ä»»åŠ¡ä¸­çš„å¯¹æ¯”ã€‚ç¬¬äºŒä¸ªtableä¸ºè¿™å‡ ä¸ªä»»åŠ¡æ•°æ®é›†å¹³å‡æ–‡æœ¬é•¿åº¦ã€‚æ¯é¡¹ä»»åŠ¡éƒ½æ˜¯è¶…å‡ºRoBERTaï¼Œå½“æ–‡æœ¬é•¿åº¦å¤§äº512æ—¶ï¼Œperformanceæå‡ç‰¹åˆ«æ˜æ˜¾ã€‚æ›´åŠ è¯´æ˜äº†é•¿ç¨‹æ•è·èƒ½åŠ›åœ¨NLPä¸­çš„é‡è¦æ€§ã€‚</p>
<h3 id="transformerå®¶æ—3----è®¡ç®—æ•ˆç‡ä¼˜åŒ–adaptive-spanreformerlite-transformer">ğŸš€Transformerå®¶æ—3 -- è®¡ç®—æ•ˆç‡ä¼˜åŒ–ï¼ˆAdaptive-Spanã€Reformerã€Lite-Transformerï¼‰</h3>
<h4 id="èƒŒæ™¯-1">1 èƒŒæ™¯</h4>
<p>ä¸Šæ–‡æˆ‘ä»¬ä»ç¼–ç é•¿åº¦ä¼˜åŒ–çš„è§’åº¦ï¼Œåˆ†æäº†å¦‚ä½•å¯¹Transformerè¿›è¡Œä¼˜åŒ–ã€‚Transformer-XLã€LongFormerç­‰æ¨¡å‹ï¼Œé€šè¿‡ç‰‡æ®µé€’å½’å’Œattentionç¨€ç–åŒ–ç­‰æ–¹æ³•ï¼Œå°†é•¿æ–‡æœ¬ç¼–ç èƒ½åŠ›æå‡åˆ°äº†å¾ˆé«˜çš„é«˜åº¦ã€‚åŸºæœ¬å·²ç»å…‹æœäº†Transformeré•¿æ–‡æœ¬æ•è·èƒ½åŠ›åå¼±çš„é—®é¢˜ï¼Œä½¿å¾—ä¸‹æ¸¸ä»»åŠ¡æ¨¡å‹performanceå¾—åˆ°äº†è¾ƒå¤§æå‡ï¼Œç‰¹åˆ«æ˜¯æ–‡æœ¬è¾ƒé•¿ï¼ˆå¤§äº512ï¼‰çš„ä»»åŠ¡ä¸Šã€‚</p>
<p>ä½†Transformerè®¡ç®—é‡å’Œå†…å­˜æ¶ˆè€—è¿‡å¤§çš„é—®é¢˜ï¼Œè¿˜äºŸå¾…è§£å†³ã€‚äº‹å®ä¸Šï¼ŒTransformer-XLã€LongFormerå·²ç»å¤§å¤§é™ä½äº†å†…å­˜å’Œç®—åŠ›æ¶ˆè€—ã€‚æ¯•ç«ŸTransformerä¹‹æ‰€ä»¥é•¿è·ç¦»ç¼–ç èƒ½åŠ›åå¼±ï¼Œå°±æ˜¯å› ä¸ºå…¶è®¡ç®—é‡æ˜¯åºåˆ—é•¿åº¦çš„å¹³æ–¹å…³ç³»ï¼Œå¯¹ç®—åŠ›éœ€æ±‚è¿‡å¤§ï¼Œå¯¼è‡´å½“å‰GPU/TPUä¸èƒ½æ»¡è¶³éœ€æ±‚ã€‚ç¼–ç é•¿åº¦ä¼˜åŒ–å’Œè®¡ç®—é‡ä¼˜åŒ–ï¼ŒäºŒè€…æ˜¯ç›¸è¾…ç›¸æˆçš„ã€‚ä½†ç€çœ¼äºè®ºæ–‡çš„å‡ºå‘ç‚¹ï¼Œæˆ‘ä»¬è¿˜æ˜¯åˆ†ä¸ºä¸¤ä¸ªä¸åŒçš„ç« èŠ‚è¿›è¡Œåˆ†æã€‚æ¯•ç«Ÿæ€»ä¸èƒ½æ‰€æœ‰æ¨¡å‹éƒ½æ”¾åœ¨ä¸€ä¸ªç« èŠ‚å§ï¼ˆ_ï¼‰ã€‚</p>
<p>æœ¬æ–‡æˆ‘ä»¬å¸¦æ¥Adaptive-Span Transformerã€Reformerã€Lite-Transformerç­‰å‡ ç¯‡æ–‡ç« </p>
<h4 id="adaptive-span-transformer">2 Adaptive-Span Transformer</h4>
<p><img src="https://i.loli.net/2020/10/28/SZIDO7KcoGHqvhF.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"> è®ºæ–‡ä¿¡æ¯ï¼š2019å¹´5æœˆï¼ŒFaceBookï¼ŒACL2019 è®ºæ–‡åœ°å€ https://arxiv.org/pdf/1905.07799.pdf ä»£ç å’Œæ¨¡å‹åœ°å€ https://github.com/facebookresearch/adaptive-span</p>
<h5 id="ä¸ºä»€ä¹ˆéœ€è¦adaptive-span">2.1 ä¸ºä»€ä¹ˆéœ€è¦Adaptive-Span</h5>
<p>ä¹‹å‰Transformer-XLå°†é•¿æ–‡æœ¬ç¼–ç èƒ½åŠ›æå‡åˆ°äº†è¾ƒé«˜é«˜åº¦ï¼Œä½†æ˜¯å¦æ¯ä¸ªlayerçš„æ¯ä¸ªheadï¼Œéƒ½éœ€è¦è¿™ä¹ˆé•¿çš„attentionå‘¢ï¼Ÿå°½ç®¡ä½¿ç”¨äº†å¤šç§ä¼˜åŒ–æ‰‹æ®µï¼Œé•¿è·ç¦»attentionæ¯•ç«Ÿè¿˜æ˜¯éœ€è¦è¾ƒå¤§çš„å†…å­˜å’Œç®—åŠ›ã€‚ç ”ç©¶å‘ç°ï¼Œå¤§éƒ¨åˆ†headåªéœ€è¦50å·¦å³çš„attentioné•¿åº¦ï¼Œåªæœ‰å°‘éƒ¨åˆ†headéœ€è¦è¾ƒé•¿çš„attentionã€‚è¿™ä¸ªæ˜¯make senseçš„ï¼Œå¤§éƒ¨åˆ†tokenåªå’Œå®ƒé™„è¿‘çš„tokenæœ‰å…³è”ã€‚å¦‚ä¸‹å›¾ <img src="https://i.loli.net/2020/10/28/nOstVdScKG68CDz.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°">æˆ‘ä»¬æ˜¯å¦å¯ä»¥å®ç°attention spané•¿åº¦çš„è‡ªé€‚åº”å‘¢ï¼Ÿè®©ä¸åŒçš„layerçš„ä¸åŒçš„headï¼Œè‡ªå·±å»å­¦ä¹ è‡ªå·±çš„attention spané•¿åº¦å‘¢ï¼ŸAdaptive-Span Transformerç»™å‡ºäº†è‚¯å®šç­”æ¡ˆã€‚</p>
<h5 id="å®ç°æ–¹æ¡ˆ">2.2 å®ç°æ–¹æ¡ˆ</h5>
<p>æ–‡ç« è®¾å®šæ¯ä¸ªattention headå†…çš„tokenè®¡ç®—ï¼Œéƒ½ä½¿ç”¨åŒä¸€ä¸ªspané•¿åº¦ã€‚æˆ‘ä»¬å°±å¯ä»¥åˆ©ç”¨attention maskæ¥å®ç°è‡ªé€‚åº”spanã€‚å¯¹æ¯ä¸ªheadéƒ½æ·»åŠ ä¸€ä¸ªattention maskï¼Œmaskä¸º0çš„ä½ç½®ä¸è¿›è¡Œattentionè®¡ç®—ã€‚æ–‡ç« è®¾è®¡çš„maskå‡½æ•°å¦‚ä¸‹ <img src="https://i.loli.net/2020/10/28/yRVqrjfJPgBKlIk.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"> Rä¸ºè¶…å‚ï¼Œæ§åˆ¶æ›²çº¿å¹³æ»‘åº¦ã€‚å…¶ä¸ºå•è°ƒé€’å‡å‡½æ•°ï¼Œå¦‚ä¸‹å›¾ã€‚ <img src="https://i.loli.net/2020/10/28/hvoFmDIp1BjgV5f.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></p>
<h5 id="å®éªŒç»“æœ-2">2.3 å®éªŒç»“æœ</h5>
<p><img src="https://i.loli.net/2020/10/28/EDsBxfipL4HcOwF.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"> å’ŒTransformerå®¶æ—å…¶ä»–å¾ˆå¤šæ¨¡å‹ä¸€æ ·ï¼ŒAdaptive-spanä¹Ÿåœ¨å­—ç¬¦çº§åˆ«çš„è¯­è¨€æ¨¡å‹ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œæ•°æ®é›†ä¸ºtext8ã€‚å¦‚ä¸Šï¼ŒTransformeræ³¨æ„åŠ›é•¿åº¦å›ºå®šä¸º512ï¼Œç»“è®ºå¦‚ä¸‹</p>
<ol type="1">
<li>Transformer-XLé•¿ç¨‹ç¼–ç èƒ½åŠ›ç¡®å®å¾ˆå¼ºï¼Œå¹³å‡spanå¯è¾¾3800ã€‚</li>
<li>æ³¨æ„åŠ›é•¿åº¦ç¡®å®ä¸éœ€è¦æ€»é‚£ä¹ˆé•¿ï¼ŒAdaptive-Spanå¤§æ¨¡å‹ä¸Šï¼Œå¹³å‡é•¿åº¦åªæœ‰245</li>
<li>Adaptive-Spanåœ¨ç®—åŠ›éœ€æ±‚å¾ˆå°ï¼ˆåªæœ‰XLçš„1/3ï¼‰çš„æƒ…å†µä¸‹ï¼Œæ•ˆæœå¯ä»¥è¾¾åˆ°SOTAã€‚</li>
</ol>
<p><img src="https://i.loli.net/2020/10/28/IDjAR2ucfELsegw.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°">ä¸Šé¢æ˜¯åœ¨enwik8ä¸Šçš„ç»“æœã€‚Adaptive-Spanåˆä¸€æ¬¡åœ¨ç®—åŠ›å¾ˆå°çš„æƒ…å†µä¸‹ï¼Œè¾¾åˆ°äº†æœ€ä¼˜æ•ˆæœã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œ64å±‚çš„Transformerå±…ç„¶éœ€è¦120Gçš„è®¡ç®—é‡ï¼Œåˆä¸€æ¬¡è¯æ˜äº†åŸç‰ˆTransformeræ˜¯å¤šä¹ˆçš„åƒè®¡ç®—èµ„æºã€‚å¦å¤–Transformer-XLåœ¨èŠ‚çœè®¡ç®—èµ„æºä¸Šï¼Œå…¶å®ä¹Ÿç®—å¯åœˆå¯ç‚¹ã€‚</p>
<h4 id="reformer">3 Reformer</h4>
<p><img src="https://i.loli.net/2020/10/28/G8AVxR7fuCZdPHn.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"> è®ºæ–‡ä¿¡æ¯ï¼š2020å¹´1æœˆï¼Œè°·æ­Œï¼ŒICLR2020 è®ºæ–‡åœ°å€ https://arxiv.org/abs/2001.04451 ä»£ç å’Œæ¨¡å‹åœ°å€ https://github.com/google/trax/tree/master/trax/models/reformer</p>
<h5 id="ä¸ºä»€ä¹ˆéœ€è¦reformer">3.1 ä¸ºä»€ä¹ˆéœ€è¦Reformer</h5>
<p>Transformerå†…å­˜å’Œè®¡ç®—é‡æ¶ˆè€—å¤§çš„é—®é¢˜ï¼Œä¸€ç›´ä»¥æ¥å¹¿ä¸ºè¯Ÿç—…ï¼Œå¹¶å¯¼è‡´å…¶ä¸€ç›´ä¸èƒ½åœ¨é•¿æ–‡æœ¬ä¸Šè¿›è¡Œåº”ç”¨ã€‚ï¼ˆBERTã€RoBERTaå‡è®¾ç½®æœ€å¤§é•¿åº¦ä¸º512ï¼‰ã€‚Reformerè®¤ä¸ºTransformeræœ‰ä¸‰å¤§é—®é¢˜</p>
<ol type="1">
<li>attentionå±‚è®¡ç®—é‡å’Œåºåˆ—é•¿åº¦ä¸ºå¹³æ–¹å…³ç³»ï¼Œå¯¼è‡´æ— æ³•è¿›è¡Œé•¿è·ç¦»ç¼–ç </li>
<li>å†…å­˜å ç”¨å’Œæ¨¡å‹å±‚æ•°å‘ˆNå€å…³ç³»ï¼Œå¯¼è‡´åŠ æ·±Transformerå±‚æ•°ï¼Œæ¶ˆè€—çš„å†…å­˜ç‰¹åˆ«å¤§</li>
<li>feed-forwardçš„dffæ¯”éšå±‚dmodelä¸€èˆ¬å¤§å¾ˆå¤šï¼Œå¯¼è‡´FFå±‚å ç”¨çš„å†…å­˜ç‰¹åˆ«å¤§</li>
</ol>
<p>é’ˆå¯¹è¿™å‡ ä¸ªé—®é¢˜ï¼ŒReformeråˆ›æ–°æ€§çš„æå‡ºäº†ä¸‰ç‚¹æ”¹è¿›æ–¹æ¡ˆ</p>
<ol type="1">
<li>LOCALITY-SENSITIVE HASHING å±€éƒ¨æ•æ„Ÿhashï¼Œä½¿å¾—è®¡ç®—é‡ä» O(L^2)é™ä½ä¸ºO(L log L) ,Lä¸ºåºåˆ—é•¿åº¦</li>
<li>Reversible Transformer å¯é€†Transformerï¼Œä½¿å¾—Nå±‚layerså†…å­˜æ¶ˆè€—å˜ä¸ºåªéœ€è¦ä¸€å±‚ï¼Œä»è€Œä½¿å¾—æ¨¡å‹åŠ æ·±ä¸ä¼šå—å†…å­˜é™åˆ¶ã€‚</li>
<li>Feed-forward Chunking åˆ†å—å…¨è¿æ¥ï¼Œå¤§å¤§é™ä½äº†feed-forwardå±‚çš„å†…å­˜æ¶ˆè€—ã€‚</li>
</ol>
<p>Reformeræ˜¯Transformerå®¶æ—ä¸­æœ€ä¸ºå…³é”®çš„å‡ ä¸ªæ¨¡å‹ä¹‹ä¸€ï¼ˆå»æ‰ä¹‹ä¸€è²Œä¼¼éƒ½å¯ä»¥ï¼Œé¡¶å¤šTransformer-XLä¸ç­”åº”ï¼‰ï¼Œå…¶åˆ›æ–°æ–°ä¹Ÿç‰¹åˆ«æ–°é¢–ï¼Œå¾ˆå¤šæ€æƒ³å€¼å¾—æˆ‘ä»¬æ·±å…¥æ€è€ƒå’Œå€Ÿé‰´ã€‚å…¶æ•ˆæœä¹Ÿæ˜¯ç‰¹åˆ«æ˜æ˜¾ï¼Œå¤§å¤§æé«˜äº†å†…å­˜å’Œè®¡ç®—èµ„æºæ•ˆç‡ï¼Œç¼–ç é•¿åº¦å¯è¾¾64kã€‚ä¸‹é¢é’ˆå¯¹å®ƒçš„ä¸‰ç‚¹æ”¹è¿›æ–¹æ¡ˆè¿›è¡Œåˆ†æï¼Œæœ‰ç‚¹éš¾æ‡‚å“¦ã€‚</p>
<h5 id="å®ç°æ–¹æ¡ˆ-1">3.2 å®ç°æ–¹æ¡ˆ</h5>
<h6 id="locality-sensitive-hashing-å±€éƒ¨æ•æ„Ÿhash">3.2.1 LOCALITY-SENSITIVE HASHING å±€éƒ¨æ•æ„Ÿhash</h6>
<p>å±€éƒ¨æ•æ„Ÿhashæœ‰ç‚¹éš¾æ‡‚ï¼ŒReformeré’ˆå¯¹Transformerç»“æ„è¿›è¡Œäº†æ·±åº¦çµé­‚æ‹·é—®</p>
<h6 id="queryå’Œkeyå¿…é¡»ç”¨ä¸¤å¥—å—">Queryå’ŒKeyå¿…é¡»ç”¨ä¸¤å¥—å—</h6>
<p>Transformerä¸»ä½“ç»“æ„ä¸ºattentionï¼ŒåŸç‰ˆattentionè®¡ç®—æ–¹æ³•å¦‚ä¸‹ <img src="https://i.loli.net/2020/10/28/Z1fYDpO6BnLyHj4.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"> æ¯ä¸ªtokenï¼Œåˆ©ç”¨å…¶queryå‘é‡ï¼Œå’Œå…¶ä»–tokençš„keyå‘é‡è¿›è¡Œç‚¹ä¹˜ï¼Œä»è€Œä»£è¡¨ä¸¤ä¸ªtokenä¹‹é—´çš„ç›¸å…³æ€§ã€‚å½’ä¸€åŒ–åï¼Œåˆ©ç”¨å¾—åˆ°çš„ç›¸å…³æ€§æƒé‡ï¼Œå¯¹æ¯ä¸ªtokençš„valueå‘é‡è¿›è¡ŒåŠ æƒæ±‚å’Œã€‚é¦–å…ˆä¸€ä¸ªé—®é¢˜å°±æ˜¯ï¼Œqueryå’Œkeyå‘é‡å¯ä»¥æ˜¯åŒä¸€å¥—å—ï¼Ÿæˆ‘ä»¬å¯å¦åˆ©ç”¨keyå‘é‡å»å’Œå…¶ä»–tokençš„keyè®¡ç®—ç›¸å…³æ€§å‘¢ï¼Ÿ</p>
<p>ä¸ºæ­¤æ–‡ç« è¿›è¡Œå®éªŒåˆ†æï¼Œè¯æ˜æ˜¯å¯è¡Œçš„ã€‚ä¸ªäººè®¤ä¸ºè¿™ä¸€ç‚¹ä¹Ÿæ˜¯make senseçš„ã€‚ <img src="https://i.loli.net/2020/10/28/YyTZDG1Bohkswit.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°">åœ¨æ–‡æœ¬å’Œå›¾åƒä¸Šï¼ŒQ=Kçš„attentionï¼Œå’Œæ™®é€šattentionï¼Œæ•ˆæœå·®åˆ«ä¸å¤§ã€‚</p>
<h6 id="å¿…é¡»å’Œæ¯ä¸ªtokenè®¡ç®—ç›¸å…³æ€§å—">å¿…é¡»å’Œæ¯ä¸ªtokenè®¡ç®—ç›¸å…³æ€§å—</h6>
<p>åŸç‰ˆattentionä¸­ï¼Œä¸€ä¸ªtokenå¿…é¡»å’Œåºåˆ—ä¸­å…¶ä»–æ‰€æœ‰tokenè®¡ç®—ç›¸å…³æ€§ï¼Œå¯¼è‡´è®¡ç®—é‡éšåºåˆ—é•¿åº¦å‘ˆå¹³æ–¹å…³ç³»å¢é•¿ï¼Œå¤§å¤§åˆ¶çº¦äº†å¯ç¼–ç æœ€å¤§é•¿åº¦ã€‚é‚£å¿…é¡»å’Œæ¯ä¸ªtokenè®¡ç®—ç›¸å…³æ€§å—ï¼Ÿå…¶å®ä¹‹å‰Adaptive-Span Transformerä¹Ÿæ·±åº¦æ‹·é—®è¿‡è¿™ä¸ªè¯é¢˜ã€‚å®ƒå¾—å‡ºçš„ç»“è®ºæ˜¯ï¼Œå¯¹äºå¤§éƒ¨åˆ†layerçš„multi-headï¼Œé•¿åº¦50èŒƒå›´å†…è¿›è¡Œattentionå°±å·²ç»è¶³å¤Ÿäº†ã€‚ä¸è¿‡Adaptive-Spané‡‡å–çš„æ–¹æ³•è¿˜æ˜¯ç®€å•ç²—æš´äº†ä¸€ç‚¹ï¼Œå®ƒçº¦å®šæ¯ä¸ªheadçš„attention spané•¿åº¦æ˜¯å›ºå®šçš„ï¼Œå¹¶ä¸”attention spanä¸ºå½“å‰tokené™„è¿‘çš„å…¶ä»–tokenã€‚</p>
<p>Adaptive-Span Transformerçš„è¿™ç§æ–¹æ³•æ˜¾ç„¶è¿˜æ˜¯æ²¡æœ‰æŠ“ä½Attentionè®¡ç®—å†—ä½™çš„ç—›ç‚¹ã€‚Attentionæœ¬è´¨æ˜¯åŠ æƒæ±‚å’Œï¼Œæƒé‡ä¸ºä¸¤ä¸ªtokené—´çš„ç›¸å…³æ€§ã€‚æœ€ç»ˆç»“æœå–å†³äºè¾ƒå¤§çš„topkæƒé‡ï¼Œå…¶ä»–æƒå€¼è¾ƒå°çš„åŸºæœ¬å°±æ˜¯ç‚®ç°ã€‚å¹¶ä¸”softmaxå½’ä¸€åŒ–æ›´æ˜¯åŠ å‰§äº†è¿™ä¸€ç‚¹ã€‚å°è€…æ›´å°ï¼Œå¤§è€…æ›´å¤§ã€‚ä¸ºäº†å‡å°‘è®¡ç®—å†—ä½™ï¼Œæˆ‘ä»¬å¯ä»¥åªå¯¹ç›¸å…³æ€§å¤§çš„å…¶ä»–tokençš„keyå‘é‡è®¡ç®—Attentionã€‚</p>
<h6 id="æ€ä¹ˆæ‰¾åˆ°ç›¸å…³æ€§å¤§çš„å‘é‡å‘¢">æ€ä¹ˆæ‰¾åˆ°ç›¸å…³æ€§å¤§çš„å‘é‡å‘¢</h6>
<p>æˆ‘ä»¬ç°åœ¨è¦ä»åºåˆ—ä¸­æ‰¾åˆ°ä¸æœ¬tokenç›¸å…³æ€§æœ€å¤§çš„tokenï¼Œä¹Ÿå°±æ˜¯å½“å‰keyå‘é‡ä¸å“ªäº›keyå‘é‡ç›¸å…³æ€§å¤§ã€‚æç«¯ä¾‹å­ï¼Œå¦‚æœä¸¤ä¸ªå‘é‡å®Œå…¨ç›¸åŒï¼Œä»–ä»¬çš„ç›¸å…³æ€§æ˜¯æœ€é«˜çš„ã€‚ç¡®å®šä¸¤ä¸ªé«˜ç»´å‘é‡çš„ç›¸å…³æ€§ç¡®å®æ¯”è¾ƒå›°éš¾ï¼Œå¥½åœ¨æˆ‘ä»¬å¯ä»¥åˆ©ç”¨å‘é‡Hashæ¥è®¡ç®—ã€‚</p>
<p>Reformeré‡‡ç”¨äº†å±€éƒ¨æ•æ„Ÿhashã€‚æˆ‘ä»¬è®©ä¸¤ä¸ªkeyå‘é‡åœ¨éšæœºå‘é‡ä¸ŠæŠ•å½±ï¼Œå°†å®ƒä»¬åˆ’åˆ†åˆ°æŠ•å½±åŒºé—´å†…ã€‚ <img src="https://i.loli.net/2020/10/28/gSHlhWRaoqfk9pN.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"> å¦‚å›¾æ‰€ç¤ºï¼Œåˆ’åˆ†äº†å››ä¸ªåŒºé—´ï¼ˆ4ä¸ªæ¡¶bucketï¼‰ï¼Œè¿›è¡Œäº†ä¸‰æ¬¡Hashã€‚ç¬¬ä¸€æ¬¡Hashï¼Œä½¿å¾—ä¸Šé¢ä¸¤ä¸ªå‘é‡åˆ†åˆ«å½’å…¥bucket0å’Œbucket3ä¸­ï¼Œä¸‹é¢ä¸¤ä¸ªå‘é‡éƒ½å½’å…¥bucket0ã€‚ç¬¬äºŒæ¬¡Hashï¼Œä¸Šé¢ä¸¤ä¸ªå‘é‡å’Œä¸‹é¢ä¸¤ä¸ªï¼Œå‡å½’å…¥åˆ°bucket2ä¸­äº†ã€‚æˆ‘ä»¬å¯ä»¥å‘ç°</p>
<ol type="1">
<li>ç›¸ä¼¼çš„å‘é‡ï¼Œä¹Ÿå°±æ˜¯ç›¸å…³æ€§å¤§çš„ï¼Œå®¹æ˜“å½’å…¥åˆ°ä¸€ä¸ªbucketä¸­</li>
<li>å±€éƒ¨æ•æ„ŸHashè¿˜æ˜¯æœ‰ä¸€å®šçš„é”™è¯¯ç‡çš„ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨å¤šè½®Hashæ¥ç¼“è§£ã€‚è¿™ä¹Ÿæ˜¯Reformerçš„åšæ³•ï¼Œå®ƒé‡‡å–äº†4è½®å’Œ8è½®çš„Hashã€‚</li>
</ol>
<h6 id="æ•´ä¸ªæµç¨‹">æ•´ä¸ªæµç¨‹</h6>
<p>ç»è¿‡å±€éƒ¨æ•æ„ŸHashåï¼Œæˆ‘ä»¬å¯ä»¥å°†ç›¸å…³æ€§å¤§çš„keyå½’å…¥åŒä¸€ä¸ªbucketä¸­ã€‚è¿™æ ·åªç”¨åœ¨bucketå†…è¿›è¡Œæ™®é€šAttentionå³å¯ï¼Œå¤§å¤§é™ä½äº†è®¡ç®—å†—ä½™åº¦ã€‚ä¸ºäº†å®ç°å¹¶è¡Œè®¡ç®—ï¼Œè€ƒè™‘åˆ°æ¯ä¸ªbucketåŒ…å«çš„å‘é‡æ•°ç›®å¯èƒ½ä¸åŒï¼Œå®é™…å¤„ç†ä¸­éœ€è¦å¤šçœ‹ä¸€ä¸ªbucketã€‚æ•´ä¸ªæµç¨‹å¦‚ä¸‹ <img src="https://i.loli.net/2020/10/28/ZS5dCT8nVcaiob4.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></p>
<ol type="1">
<li>è®©queryç­‰äºkey</li>
<li>å±€éƒ¨æ•æ„ŸHashï¼ˆLSHï¼‰åˆ†æ¡¶ã€‚ä¸Šå›¾åŒä¸€é¢œè‰²çš„ä¸ºåŒä¸€ä¸ªæ¡¶ï¼Œå…±4ä¸ªæ¡¶</li>
<li>æ¡¶æ’åºï¼Œå°†ç›¸åŒçš„æ¡¶æ”¾åœ¨ä¸€èµ·</li>
<li>ä¸ºäº†å®ç°å¹¶è¡Œè®¡ç®—ï¼Œå°†æ‰€æœ‰æ¡¶åˆ†å—ï¼ˆchunkï¼‰ï¼Œæ¯ä¸ªchunkå¤§å°ç›¸åŒ</li>
<li>æ¡¶å†…è®¡ç®—Attentionï¼Œç”±äºä¹‹å‰åšäº†åˆ†å—æ“ä½œï¼Œæ‰€ä»¥éœ€è¦å¤šçœ‹ä¸€ä¸ªå—ã€‚</li>
</ol>
<h6 id="å¤šè½®lsh">å¤šè½®LSH</h6>
<p>ä¸ºäº†å‡å°‘åˆ†æ¡¶é”™è¯¯ç‡ï¼Œæ–‡ç« é‡‡ç”¨äº†å¤šæ¬¡åˆ†æ¡¶ï¼Œè®¡ç®—LSH Attentionï¼ŒMulti-round LSH attentionã€‚å¯ä»¥æå‡æ•´ä½“å‡†ç¡®ç‡ã€‚å¦‚ä¸‹è¡¨ã€‚<img src="https://i.loli.net/2020/10/28/L4BDoiwTfaRjInv.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></p>
<h6 id="reversible-transformer-å¯é€†transformer">3.2.2 REVERSIBLE TRANSFORMER å¯é€†Transformer</h6>
<p>LSHå±€éƒ¨æ•æ„ŸHashç¡®å®æ¯”è¾ƒéš¾ç†è§£ï¼Œå¯é€†Transformerç›¸å¯¹å¥½æ‡‚ä¸€äº›ã€‚è¿™ä¸ªæ–¹æ¡ˆæ˜¯ä¸ºäº†è§£å†³Transformerå†…å­˜å ç”¨é‡ï¼Œéšlayerså±‚æ•°çº¿æ€§å¢é•¿çš„é—®é¢˜ã€‚ä¸ºä»€ä¹ˆä¼šçº¿æ€§å¢é•¿å‘¢ï¼ŸåŸå› æ˜¯åå‘ä¼ æ’­ä¸­ï¼Œæ¢¯åº¦ä¼šä»top layerå‘bottom layerä¼ æ’­ï¼Œæ‰€ä»¥å¿…é¡»ä¿å­˜ä½æ¯ä¸€å±‚çš„Q K Vå‘é‡ï¼Œä¹Ÿå°±å¯¼è‡´Nå±‚å°±éœ€è¦Nå¥—Q K Vã€‚</p>
<p>é‚£æœ‰æ²¡æœ‰åŠæ³•ä¸ä¿å­˜æ¯ä¸€å±‚çš„Q K Vå‘¢ï¼Ÿå¯é€†Transformeræ­£æ˜¯è¿™ä¸ªæ€è·¯ã€‚å®ƒåˆ©ç”¨æ—¶é—´æ¢ç©ºé—´çš„æ€æƒ³ï¼Œåªä¿ç•™ä¸€å±‚çš„å‘é‡ï¼Œåå‘ä¼ æ’­æ—¶ï¼Œå®æ—¶è®¡ç®—å‡ºä¹‹å‰å±‚çš„å‘é‡ã€‚æ‰€ä»¥å«åšReversibleã€‚Reformeræ¯ä¸€å±‚åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼Œx1å’Œx2ã€‚è¾“å‡ºä¹Ÿä¸¤éƒ¨åˆ†ï¼Œy1å’Œy2ã€‚è®¡ç®—å¦‚ä¸‹</p>
<p><img src="https://i.loli.net/2020/10/28/gKZT1mRFhEj48nU.png" alt="image-20201028214552153"></p>
<p>é‡‡ç”¨å¯é€†æ®‹å·®è¿æ¥åï¼Œæ¨¡å‹æ•ˆæœåŸºæœ¬æ²¡æœ‰ä¸‹é™ã€‚è¿™ä¹Ÿæ˜¯make senseçš„ï¼Œæ¯•ç«Ÿå¯é€†æ˜¯ä»è®¡ç®—è§’åº¦æ¥è§£å†³é—®é¢˜çš„ï¼Œå¯¹æ¨¡å‹æœ¬èº«æ²¡æœ‰æ”¹å˜ã€‚ <img src="https://i.loli.net/2020/10/28/hElWr4uHtUomVYG.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></p>
<h6 id="feed-forward-chunking-ffå±‚åˆ†å—">3.2.3 Feed-Forward chunking FFå±‚åˆ†å—</h6>
<p>é’ˆå¯¹fead-forwardå±‚å†…å­˜æ¶ˆè€—è¿‡å¤§çš„é—®é¢˜ï¼ŒReformerä¹Ÿç»™å‡ºäº†è§£å†³æ–¹æ¡ˆï¼Œå°±æ˜¯FFå±‚åˆ†å—ã€‚å¦‚ä¸‹ <img src="https://i.loli.net/2020/10/28/rpzwVXOSUavEs18.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></p>
<h5 id="å®éªŒç»“æœ-3">3.3 å®éªŒç»“æœ</h5>
<h6 id="å†…å­˜å’Œæ—¶é—´å¤æ‚åº¦">å†…å­˜å’Œæ—¶é—´å¤æ‚åº¦</h6>
<p>Reformerä¸‰ä¸ªåˆ›æ–°ç‚¹ï¼Œå¤§å¤§é™ä½äº†å†…å­˜å’Œæ—¶é—´å¤æ‚åº¦ï¼Œæ¶ˆèåˆ†æå¦‚ä¸‹ <img src="https://i.loli.net/2020/10/28/5ixhnpvKeQHrBw1.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></p>
<h6 id="æ¨¡å‹æ•ˆæœ">æ¨¡å‹æ•ˆæœ</h6>
<p>å¦‚ä¸‹ä¸ºåœ¨æœºå™¨ç¿»è¯‘ä¸Šçš„æ•ˆæœã€‚Reformerå‡å°‘äº†ç®—åŠ›æ¶ˆè€—ï¼ŒåŒæ—¶ä¹Ÿå¤§å¤§å¢åŠ äº†é•¿æ–‡æœ¬ç¼–ç èƒ½åŠ›ï¼Œæ•…æ¨¡å‹æ•ˆæœä¹Ÿå¾—åˆ°äº†æå‡ã€‚å¦‚ä¸‹ã€‚ <img src="https://i.loli.net/2020/10/28/8EtHVoBsfYxnmhO.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></p>
<h4 id="lite-transformer">4 Lite Transformer</h4>
<p><img src="https://i.loli.net/2020/10/28/9UfkunTVEpL5XBG.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"> è®ºæ–‡ä¿¡æ¯ï¼š2020å¹´4æœˆï¼ŒMIT &amp; ä¸Šæµ·äº¤å¤§ï¼ŒICLR2020 è®ºæ–‡åœ°å€ https://arxiv.org/abs/2004.11886 ä»£ç å’Œæ¨¡å‹åœ°å€ https://github.com/mit-han-lab/lite-transformer</p>
<h5 id="ä¸ºä»€ä¹ˆè¦åšlite-transformer">4.1 ä¸ºä»€ä¹ˆè¦åšLite Transformer</h5>
<p>ä¸»è¦å‡ºå‘ç‚¹ä»ç„¶æ˜¯Transformerè®¡ç®—é‡å¤ªå¤§ï¼Œè®¡ç®—å†—ä½™è¿‡å¤šçš„é—®é¢˜ã€‚è·ŸAdaptive-Span Transformerå’ŒReformeræƒ³æ³•ä¸€æ ·ï¼ŒLite Transformerä¹Ÿè§‰å¾—æ²¡å¿…è¦åšFull Attentionï¼Œå¾ˆå¤šAttentionè¿æ¥æ˜¯å†—ä½™çš„ã€‚ä¸ä¸€æ ·çš„æ˜¯ï¼Œå®ƒé€šè¿‡å‹ç¼©Attentioné€šé“çš„æ–¹å¼å®ç°ï¼Œå°†å¤šå¤´å‡å°‘äº†ä¸€åŠã€‚ä¸Base Transformerç›¸æ¯”ï¼Œè®¡ç®—é‡å‡å°‘äº†2.5å€ã€‚å¹¶ä¸”æ–‡ç« ä½¿ç”¨äº†é‡åŒ–å’Œå‰ªææŠ€æœ¯ï¼Œä½¿å¾—æ¨¡å‹ä½“ç§¯å‡å°äº†18.2å€ã€‚</p>
<h5 id="å®ç°æ–¹æ¡ˆ-2">4.2 å®ç°æ–¹æ¡ˆ</h5>
<p>å®ç°æ–¹æ¡ˆå¾ˆç®€å•ï¼Œä»ç„¶é‡‡ç”¨äº†åŸç‰ˆTransformerçš„seq2seqç»“æ„ï¼Œåˆ›æ–°ç‚¹ä¸º</p>
<ol type="1">
<li>multiHead self-attentionå˜ä¸ºäº†ä¸¤è·¯å¹¶è¡Œï¼Œåˆ†åˆ«ä¸ºä¸€åŠçš„é€šé“æ•°ï¼ˆå¤šå¤´ï¼‰ã€‚å¦‚ä¸‹å›¾aæ‰€ç¤ºã€‚å…¶ä¸­å·¦åŠéƒ¨åˆ†ä¸ºæ­£å¸¸çš„fully attentionï¼Œå®ƒç”¨æ¥æ•è·å…¨å±€ä¿¡æ¯ã€‚å³åŠéƒ¨åˆ†ä¸ºCNNå·ç§¯ï¼Œç”¨æ¥æ•è·å¸ƒå±€ä¿¡æ¯ã€‚æœ€ç»ˆäºŒè€…é€šè¿‡FFNå±‚èåˆã€‚è¿™ä¸ªæ¶æ„ç§°ä¸ºLONG-SHORT RANGE ATTENTION (LSRA)ï¼Œé•¿çŸ­æœŸAttentionã€‚</li>
<li>ä¸ºäº†è¿›ä¸€æ­¥é™ä½è®¡ç®—é‡ï¼Œä½œè€…å°†CNNè½¬å˜ä¸ºäº†ä¸€ä¸ªdepth wiseå·ç§¯å’Œä¸€ä¸ªçº¿æ€§å…¨è¿æ¥ã€‚dwå·ç§¯åœ¨mobileNetä¸­æœ‰è®²è¿‡ï¼Œä¸æ¸…æ¥šå¯è‡ªè¡Œè°·æ­Œã€‚</li>
</ol>
<p><img src="https://i.loli.net/2020/10/28/WQtJvpbP9rUczhs.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></p>
<h5 id="å®éªŒç»“æœ-4">4.3 å®éªŒç»“æœ</h5>
<h6 id="è®¡ç®—å¤æ‚åº¦">è®¡ç®—å¤æ‚åº¦</h6>
<p><img src="https://i.loli.net/2020/10/28/w6b1LlqIpZgt8fX.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"> å¦‚ä¸Šå›¾ï¼Œåœ¨æ–‡æœ¬æ‘˜è¦ä»»åŠ¡ä¸Šï¼ŒLite Transformerè®¡ç®—é‡ç›¸æ¯”Base Transformerï¼Œå‡å°‘äº†2.5å€ã€‚åŒæ—¶RougeæŒ‡æ ‡åŸºæœ¬æ²¡å˜ã€‚</p>
<h6 id="æ¨¡å‹ä½“ç§¯">æ¨¡å‹ä½“ç§¯</h6>
<p><img src="https://i.loli.net/2020/10/28/JYzeocGBUr6jy4W.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"> Lite Transformeræ¨¡å‹ä½“ç§¯åªæœ‰Transformerçš„2.5åˆ†ä¹‹ä¸€ï¼Œé€šè¿‡8bité‡åŒ–å’Œå‰ªæï¼Œæœ€ç»ˆæ¨¡å‹ä½“ç§¯ä¸‹é™äº†18.2å€ã€‚</p>
<h4 id="å…¶ä»–">5 å…¶ä»–</h4>
<p>å…¶ä»–å‡ ç¯‡æ–‡ç« ï¼Œä¹Ÿå»ºè®®æ‹œè¯»ä¸‹</p>
<ol type="1">
<li><a href="https://arxiv.org/abs/1904.10509" target="_blank" rel="noopener">Generating Long Sequences with Sparse Transformers</a> (OpenAI, 2019.04)</li>
<li><a href="https://arxiv.org/abs/1909.00015" target="_blank" rel="noopener">Adaptively Sparse Transformers</a> (EMNLP2019, 2019.09)</li>
<li><a href="https://arxiv.org/abs/1911.05507" target="_blank" rel="noopener">Compressive Transformers for Long-Range Sequence Modelling</a> (2019.11)</li>
<li><a href="https://arxiv.org/abs/2002.06170" target="_blank" rel="noopener">Transformer on a Diet</a> (2020.02)</li>
</ol>
<h3 id="transformerå®¶æ—4----é€šç”¨æ€§ä¼˜åŒ–universal-transformer">ğŸš€Transformerå®¶æ—4 -- é€šç”¨æ€§ä¼˜åŒ–ï¼ˆUniversal-Transformerï¼‰</h3>
<h4 id="èƒŒæ™¯-2">1 èƒŒæ™¯</h4>
<p>ä¹‹å‰è®²Transformerçš„æ—¶å€™ï¼Œä¹Ÿæåˆ°è¿‡å®ƒçš„é€šç”¨æ€§çš„ç¼ºç‚¹ã€‚ç›¸æ¯”äºRNNï¼ŒTransformerä¸æ˜¯å›¾çµå®Œå¤‡çš„ï¼Œè™½ç„¶å¤§å¤šæ•°ä»»åŠ¡éƒ½æ˜¯åŠæ‰“RNNï¼Œä½†åœ¨æŸäº›çœ‹èµ·æ¥æä¸ºç®€å•çš„ä»»åŠ¡ä¸Šï¼Œå´è¡¨ç°å¾ˆå·®ï¼Œæ¯”å¦‚å­—ç¬¦ä¸²æ‹·è´ç­‰ã€‚è¿™ä¸ªé—®é¢˜å…¶å®ä¹Ÿä¸ç®—å¤§ï¼Œä½†è°·æ­Œè¿˜æ˜¯ç»™å‡ºäº†ä»–çš„è§£å†³æ–¹æ¡ˆï¼Œä¹Ÿå°±æ˜¯Universal Transformerã€‚è¿™ç¯‡çœ‹çœ‹å°±å¥½äº†ï¼Œä¸ªäººæ„Ÿè§‰å®é™…åº”ç”¨ä¸­ä½œç”¨æœ‰é™ã€‚</p>
<h4 id="universal-transformer">2 Universal-Transformer</h4>
<p><img src="https://i.loli.net/2020/10/28/Lk3ymRQKZHpwN8e.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°">è®ºæ–‡ä¿¡æ¯ï¼š2018å¹´7æœˆï¼Œè°·æ­Œï¼ŒICLR2019 è®ºæ–‡åœ°å€ https://arxiv.org/abs/1807.03819 ä»£ç å’Œæ¨¡å‹åœ°å€ https://github.com/tensorflow/tensor2tensor</p>
<h5 id="ä¸ºä»€ä¹ˆéœ€è¦universal-transformer">2.1 ä¸ºä»€ä¹ˆéœ€è¦Universal-Transformer</h5>
<p>ä¸»è¦çš„å‡ºå‘ç‚¹æ˜¯åŸç‰ˆTransformerä¸æ˜¯å›¾çµå®Œå¤‡çš„ï¼Œæœ‰äº›å¾ˆç®€å•çš„ä»»åŠ¡è¡¨ç°å¾ˆå·®ï¼Œæ¯”å¦‚å­—ç¬¦ä¸²æ‹·è´ã€‚åºåˆ—ä»»åŠ¡è¿˜æ˜¯æ¯”è¾ƒåå¥½äºè¿­ä»£å’Œé€’å½’å˜æ¢ï¼ŒRNNæ­£å¥½æ»¡è¶³äº†è¿™ä¸€ç‚¹ï¼Œè€ŒTransformerä¸æ»¡è¶³ã€‚è¿™ä¸€ç‚¹æ–‡ç« ç§°ä½œå½’çº³åç½®ï¼ˆInductive Biasï¼‰ã€‚<a href="https://www.zhihu.com/question/41404496/answer/627673667" target="_blank" rel="noopener">æ·±åº¦å­¦ä¹ çš„å½’çº³åç½®æ˜¯ä»€ä¹ˆï¼Ÿ</a></p>
<h5 id="å®ç°æ–¹æ¡ˆ-3">2.2 å®ç°æ–¹æ¡ˆ</h5>
<h6 id="æ¨¡å‹ç»“æ„">æ¨¡å‹ç»“æ„</h6>
<p><img src="https://i.loli.net/2020/10/28/6aOdB5QqIYKSlEb.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"> å¦‚ä¸Šæ‰€ç¤ºä¸ºUniversal-Transformerçš„ç»“æ„ï¼Œä»ç„¶ä¸ºä¸€ä¸ªåŸºäºmultiHead self-attentionçš„seq2seqï¼Œå‡ ç‚¹ä¸åŒ</p>
<ol type="1">
<li>å¼•å…¥äº†æ—¶é—´æ­¥stepï¼Œä»è€Œå®ç°äº†å¾ªç¯é€’å½’ã€‚é™¤äº†ç¬¬ä¸€æ¬¡æ˜¯åŸå§‹ä¿¡æ¯ä½œä¸ºè¾“å…¥ï¼Œä¹‹åéƒ½æ˜¯ç”±å‰ä¸€ä¸ªstepçš„è¾“å‡ºä½œä¸ºåä¸€ä¸ªçš„è¾“å…¥ã€‚</li>
<li>Feed-forwardæ¢æˆäº†Transitionå‡½æ•°ã€‚æ ¹æ®taskä¸åŒï¼Œå¯é€‰æ‹©separable convolutionåˆ†è§£å·ç§¯å’Œfully-connected neural networkå…¨è¿æ¥ç¥ç»ç½‘ç»œã€‚</li>
<li>æ—¶é—´å’Œä½ç½®ç¼–ç ï¼ŒTimeStep embeddingå’ŒPosition embeddingï¼Œæ–°å¼•å…¥äº†TimeStep embeddingï¼ŒäºŒè€…çš„ç¼–ç å…¬å¼å’ŒTransformerä¸­çš„ä½ç½®ç¼–ç å¾ˆåƒï¼Œå¦‚ä¸‹</li>
</ol>
<p><img src="https://i.loli.net/2020/10/28/tdJkSqcoZ4FGHNQ.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></p>
<h6 id="adaptive-computation-timeact-è‡ªé€‚åº”è®¡ç®—æ—¶é—´">Adaptive Computation Timeï¼ˆACTï¼‰ è‡ªé€‚åº”è®¡ç®—æ—¶é—´</h6>
<p>å‰äººå·²ç»æåˆ°è¿‡ACTäº†ï¼Œä½œè€…åœ¨æ¨¡å‹ä¸­å¼•ç”¨äº†ã€‚åºåˆ—é—®é¢˜ä¸­ï¼Œæœ‰äº›è¯è¯­æ¯”å…¶ä»–çš„æ›´æ¨¡ç³Šã€‚ä»–ä»¬éœ€è¦è¿›è¡Œæ›´å¤šæ¬¡çš„è®¡ç®—ã€‚Universal-Transformeråˆ©ç”¨äº†ACTæœºåˆ¶ï¼Œå¯ä»¥å¯¹æ¯ä¸ªtokenè®¾ç½®è‡ªé€‚åº”è®¡ç®—æ—¶é—´ã€‚æ¨¡å‹ä¼šåŠ¨æ€è°ƒæ•´æ¯ä¸ªä½ç½®æ‰€éœ€çš„è®¡ç®—stepsã€‚å½“æŸä¸ªä½ç½®åœæ­¢è®¡ç®—åï¼Œç›´æ¥copyå®ƒçš„éšçŠ¶æ€åˆ°ä¸‹ä¸€stepã€‚å½“æ‰€æœ‰ä½ç½®éƒ½åœæ­¢è®¡ç®—åï¼Œæ•´ä¸ªè¿‡ç¨‹æ‰åœæ­¢ã€‚<img src="https://i.loli.net/2020/10/28/pXaRS5Q7fVDuxyW.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"> å¦‚ä¸Šï¼Œä¸åŒä½ç½®tokenæ‰€éœ€çš„è®¡ç®—stepsæ˜¯ä¸åŒçš„ã€‚</p>
<h5 id="å®éªŒç»“æœ-5">2.3 å®éªŒç»“æœ</h5>
<h6 id="å­—ç¬¦ä¸²ä»»åŠ¡">å­—ç¬¦ä¸²ä»»åŠ¡</h6>
<p><img src="https://i.loli.net/2020/10/28/iT5HIsel3WjDCxA.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°">å­—ç¬¦ä¸²å¤åˆ¶ã€ç¿»è½¬ã€æ·»åŠ æ“ä½œçš„æ•ˆæœã€‚å¯ä»¥å‘ç°</p>
<ol type="1">
<li>Transformeræ•ˆæœç¡®å®æ¯”è¾ƒå·®ï¼Œæ¯”LSTMå·®å¾ˆå¤šã€‚è¿™ä¹ŸéªŒè¯äº†Transformeré€šç”¨æ€§ç¡®å®æœ‰äº›é—®é¢˜ï¼Œä¹Ÿå°±æ˜¯æœ¬æ–‡çš„å‡ºå‘ç‚¹</li>
<li>Universal-Transformeræ•ˆæœå¾ˆå¥½ï¼Œè¶…è¿‡LSTMå¾ˆå¤šï¼ŒæˆåŠŸè§£å†³äº†åŸç‰ˆTransformerçš„é—®é¢˜</li>
</ol>
<h6 id="æœºå™¨ç¿»è¯‘">æœºå™¨ç¿»è¯‘</h6>
<p><img src="https://i.loli.net/2020/10/28/LZTwqn9hjpBiAyr.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°">æœºå™¨ç¿»è¯‘ä¸Šçš„ç»“æœï¼ŒUniversal-Transformerçš„BLEUæ¯”åŸç‰ˆTransformeræé«˜äº†0.9%</p>
<h3 id="transformerå®¶æ—5----æ¨ç†åŠ é€Ÿfaster-transformer-turbotransformers">ğŸš€Transformerå®¶æ—5 -- æ¨ç†åŠ é€Ÿï¼ˆFaster-Transformer ã€TurboTransformersï¼‰</h3>
<h4 id="èƒŒæ™¯-3">1 èƒŒæ™¯</h4>
<p>ä¹‹å‰ä»‹ç»äº†ä»ç¼–ç é•¿åº¦ã€è®¡ç®—æ•ˆç‡ã€é€šç”¨æ€§ç­‰è§’åº¦å¯¹Transformerè¿›è¡Œä¼˜åŒ–ï¼Œå¹¶ä»‹ç»äº†å‡ ä¸ªé‡è¦æ¨¡å‹ã€‚æœ¬æ–‡ä»‹ç»å¦‚ä½•è¿›è¡ŒTransformeræ¨ç†åŠ é€Ÿã€‚ç›¸æ¯”äºç¦»çº¿è®­ç»ƒï¼Œåœ¨çº¿æ¨ç†åŠ é€Ÿæ›´åŠ å…³é”®ã€‚ä¸€æ–¹é¢ç”±äºåœ¨çº¿æµé‡å¤§ï¼ŒåŠ é€Ÿå¯å¸¦æ¥ç¡¬ä»¶æˆæœ¬çš„èŠ‚çœã€‚å¦ä¸€æ–¹é¢åœ¨çº¿æ¨ç†åŠ é€Ÿï¼Œå¯å¤§å¤§æå‡AIåº”ç”¨çš„ç”¨æˆ·ä½“éªŒã€‚</p>
<p>äº‹å®ä¸Šï¼Œä¹‹å‰çš„å¤šç§æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯è®¡ç®—æ•ˆç‡ä¼˜åŒ–ï¼Œå¯¹æ¨ç†åŠ é€Ÿå¾ˆæœ‰å¸®åŠ©ã€‚è¿™äº›æ¨¡å‹ä»ç®—æ³•çš„è§’åº¦ï¼Œè¿›è¡Œäº†æ¨ç†é€Ÿåº¦ä¼˜åŒ–ã€‚æœ¬æ–‡ä¸»è¦ä»æ¡†æ¶å±‚çš„è§’åº¦ï¼Œè®²è§£å¦‚ä½•å¯¹æ¨ç†è¿›è¡ŒåŠ é€Ÿã€‚ä¸»è¦å¸¦æ¥NVIDIAçš„Faster-Transformeræ¡†æ¶å’Œè…¾è®¯çš„Turbo-Transformeræ¡†æ¶ã€‚</p>
<h4 id="faster-transformer">2 Faster-Transformer</h4>
<p>PPTèµ„æ–™ï¼šhttps://on-demand.gputechconf.com/gtc-cn/2019/pdf/CN9468/presentation.pdf ä»£ç åœ°å€ï¼šhttps://github.com/NVIDIA/DeepLearningExamples/tree/master/FasterTransformer</p>
<h5 id="å®ç°æ–¹æ¡ˆ-4">å®ç°æ–¹æ¡ˆ</h5>
<p>Faster-Transformerç®—æ³•ç»“æ„å’ŒåŸç‰ˆTransformeråŸºæœ¬ä¸€è‡´ï¼Œä¸»è¦æ˜¯ä»æ¡†æ¶å±‚è§’åº¦æ¥å®ç°è®¡ç®—åŠ é€Ÿã€‚ä¸»è¦æ–¹æ³•æœ‰</p>
<ol type="1">
<li>ç®—å­èåˆã€‚å¯¹é™¤çŸ©é˜µä¹˜æ³•å¤–çš„æ‰€æœ‰ç®—å­ï¼Œè¿›è¡Œäº†åˆå¹¶ã€‚æ¯”å¦‚Addã€Subã€‚ä»è€Œå‡å°‘äº†GPU kernelè°ƒåº¦å’Œæ˜¾å­˜è¯»å†™ã€‚</li>
<li>åŠç²¾åº¦F16ä¼˜åŒ–ã€‚</li>
<li>GELUæ¿€æ´»å‡½æ•°ã€å±‚æ­£åˆ™åŒ–ã€softmaxç­‰è°ƒç”¨é¢‘æ¬¡å¾ˆé«˜çš„æ“ä½œçš„ä¼˜åŒ–</li>
</ol>
<h5 id="æ•ˆæœ">æ•ˆæœ</h5>
<p><img src="https://i.loli.net/2020/10/28/AGKsp9WPLQ2julY.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"> Encoderæ•ˆæœå¯¹æ¯”å¦‚ä¸Šã€‚Faster-TransformeråŸºæœ¬åŠæ‰“TF XLAï¼Œæå‡é€Ÿåº¦ä¸€å€å¤šã€‚<img src="https://i.loli.net/2020/10/28/yYDIensgFQmv3HG.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"> Decoderæ•ˆæœå¯¹æ¯”å¦‚ä¸Šã€‚å¯¹æ¯”äº†32bitå’Œ16bitçš„ç»“æœã€‚Decoding FP32å’ŒDecoding FP16ä¸ºFaster-Transformer çš„ç»“æœï¼Œä¹Ÿæ˜¯åŠæ‰“åŸå§‹TensorFlowã€‚</p>
<h4 id="turbotransformers">3 <strong>TurboTransformers</strong></h4>
<p><img src="https://i.loli.net/2020/10/28/IKjLfbDRNgMd5rE.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°">ä»£ç åœ°å€ https://github.com/Tencent/TurboTransformers</p>
<h5 id="å®ç°æ–¹æ¡ˆ-5">å®ç°æ–¹æ¡ˆ</h5>
<ol type="1">
<li>å’ŒFaster-Transformerä¸€æ ·ï¼Œè¿›è¡Œäº†ç®—å­èåˆã€‚ä»è€Œå‡å°‘GPU kernelè°ƒç”¨å’Œæ˜¾å­˜å ç”¨</li>
<li>å¯¹äºLayerNormå’Œsoftmaxï¼Œç”±äºä¸é€‚åˆå¹¶è¡Œè®¡ç®—ï¼Œé‡æ–°å¼€å‘å¹¶å®ç°äº†å¹¶è¡Œè®¡ç®—ç‰ˆæœ¬ã€‚</li>
<li>å†…å­˜ç¼“å­˜ï¼Œé¿å…é¢‘ç¹é‡Šæ”¾å’Œåˆ†é…å†…å­˜ã€‚</li>
</ol>
<h5 id="å’Œå…¶ä»–æ–¹æ¡ˆçš„å¯¹æ¯”">å’Œå…¶ä»–æ–¹æ¡ˆçš„å¯¹æ¯”</h5>
<p><img src="https://i.loli.net/2020/10/28/BzXjLlFERMe34x2.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></p>
<h5 id="æ•ˆæœ-1">æ•ˆæœ</h5>
<p><img src="https://i.loli.net/2020/10/28/FnUxJrzqgL6eEks.png" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°">V100ä¸Šçš„QPSï¼Œè¶Šé«˜ä»£è¡¨æ¡†æ¶æ€§èƒ½è¶Šå¥½ã€‚å¯¹æ¯”äº†PyTorchã€TensorFlowã€Faster-Transformerã€turboTransformersçš„æ•ˆæœï¼Œå…¶ä¸­turboTransformersæ•ˆæœæœ€å¥½</p>
<h3 id="é‚±é”¡é¹æ•™æˆnlpé¢„è®­ç»ƒæ¨¡å‹ç»¼è¿°">ğŸš€é‚±é”¡é¹æ•™æˆï¼šNLPé¢„è®­ç»ƒæ¨¡å‹ç»¼è¿°</h3>
<h4 id="å¼•è¨€"><strong>1.å¼•è¨€</strong></h4>
<p>éšæ·±åº¦å­¦ä¹ çš„å‘å±•ï¼Œå¤šç§ç¥ç»ç½‘ç»œéƒ½è¢«åº”ç”¨åœ¨ NLP ä»»åŠ¡ä¸­ï¼Œæ¯”å¦‚ CNNã€RNNã€GNN å’Œ attention æœºåˆ¶ç­‰ï¼Œä½†ç”±äºç°æœ‰çš„æ•°æ®é›†å¯¹äºå¤§éƒ¨åˆ†æœ‰ç›‘ç£ NLP ä»»åŠ¡æ¥è¯´éƒ½å¾ˆå°ï¼Œå› æ­¤ï¼Œæ—©æœŸçš„æ¨¡å‹å¯¹ NLP ä»»åŠ¡æ¥è¯´éƒ½å¾ˆâ€œæµ…â€ï¼Œå¾€å¾€åªåŒ…å« 1-3 å±‚ã€‚</p>
<p>è€Œé¢„è®­ç»ƒæ¨¡å‹ï¼ˆPre-trained Models, PTMsï¼‰çš„å‡ºç°å°†NLPå¸¦å…¥ä¸€ä¸ªæ–°çš„æ—¶ä»£ï¼Œæ›´â€œæ·±â€çš„æ¨¡å‹å’Œè®­ç»ƒæŠ€å·§çš„å¢å¼ºä¹Ÿä½¿å¾— PTMs ç”±â€œæµ…â€å˜â€œæ·±â€ï¼Œåœ¨å¤šé¡¹ä»»åŠ¡éƒ½è¾¾åˆ°äº† SOTA æ€§èƒ½ã€‚</p>
<p>è¿‘æ—¥ï¼Œå¤æ—¦å¤§å­¦çš„é‚±é”¡é¹è€å¸ˆç­‰äººå‘å¸ƒäº†é¢„è®­ç»ƒæ¨¡å‹ç»¼è¿° *<strong>Pre-trained Models for Natural Language Processing: A Survey*</strong>ï¼Œä»èƒŒæ™¯ã€åˆ†ç±»åˆ°åº”ç”¨ä¸å‰æ™¯å¯¹ PTMs åšäº†è¯¦ç»†è€Œå…¨é¢çš„è°ƒç ”ã€‚</p>
<p><img src="https://i.loli.net/2020/10/28/lLPuxzv8IVRTAQO.png" alt="img"></p>
<p><strong>è®ºæ–‡æ ‡é¢˜ï¼š</strong>Pre-trained Models for Natural Language Processing: A Survey</p>
<p><strong>è®ºæ–‡é“¾æ¥ï¼š</strong> https://arxiv.org/abs/2003.08271</p>
<h4 id="èƒŒæ™¯-4"><strong>2.èƒŒæ™¯</strong></h4>
<h5 id="è¯­è¨€è¡¨ç¤ºå­¦ä¹ "><strong>2.1 è¯­è¨€è¡¨ç¤ºå­¦ä¹ </strong></h5>
<p>å¯¹äºè¯­è¨€æ¥è¯´ï¼Œä¸€ä¸ªå¥½çš„è¡¨ç¤ºåº”å½“æç»˜è¯­è¨€çš„å†…åœ¨è§„åˆ™æ¯”å¦‚è¯è¯­å«ä¹‰ã€å¥æ³•ç»“æ„ã€è¯­ä¹‰è§’è‰²ç”šè‡³è¯­ç”¨ã€‚</p>
<p><img src="https://i.loli.net/2020/10/28/ZmF3yXaiHPME1LQ.png" alt="img"></p>
<p>è€Œåˆ†å¸ƒå¼è¡¨ç¤ºçš„æ ¸å¿ƒæ€æƒ³å°±æ˜¯é€šè¿‡ä½ç»´å®å€¼å‘é‡æ¥æè¿°ä¸€æ®µæ–‡æœ¬çš„æ„ä¹‰ï¼Œè€Œå‘é‡çš„æ¯ä¸€ä¸ªç»´åº¦éƒ½æ²¡æœ‰å¯¹äºæ„ä¹‰ï¼Œæ•´ä½“åˆ™ä»£è¡¨ä¸€ä¸ªå…·ä½“çš„æ¦‚å¿µã€‚å›¾ 1 æ˜¯ NLP çš„é€šç”¨ç¥ç»ä½“ç³»æ¶æ„ã€‚</p>
<p>æœ‰ä¸¤ç§ embeddingï¼ˆè¯åµŒå…¥ï¼‰æ–¹å¼ï¼šä¸Šä¸‹æ–‡åµŒå…¥å’Œéä¸Šä¸‹æ–‡åµŒå…¥ï¼Œä¸¤è€…çš„åŒºåˆ«åœ¨äºè¯çš„ embedding æ˜¯å¦æ ¹æ®è¯å‡ºç°çš„ä¸Šä¸‹æ–‡åŠ¨æ€åœ°æ”¹å˜ã€‚</p>
<p><strong>éä¸Šä¸‹æ–‡åµŒå…¥ï¼š</strong>è¡¨ç¤ºè¯­è¨€çš„ç¬¬ä¸€æ­¥å°±æ˜¯å°†åˆ†ç¦»çš„è¯­è¨€ç¬¦å·æ˜ å°„åˆ°åˆ†å¸ƒå¼åµŒå…¥ç©ºé—´ä¸­ã€‚ä¹Ÿå°±æ˜¯å¯¹äºè¯æ±‡è¡¨ä¸­çš„æ¯ä¸ªå•è¯ï¼ˆè¯æ ¹ï¼‰ï¼Œé€šè¿‡ lookup table æ˜ å°„åˆ°ä¸€ä¸ªå‘é‡ã€‚</p>
<p>è¿™ç§åµŒå…¥æ–¹å¼æœ‰ä¸¤ä¸ªå±€é™ï¼šä¸€æ˜¯ä¸€ä¸ªè¯é€šè¿‡è¿™ç§æ–¹æ³•è·å¾—çš„è¯åµŒå…¥æ€»æ˜¯é™æ€ä¸”ä¸ä¸Šä¸‹æ–‡æ— å…³çš„ï¼Œæ— æ³•å¤„ç†å¤šä¹‰è¯ï¼›äºŒæ˜¯éš¾ä»¥è§£å†³ä¸åœ¨è¯æ±‡è¡¨ä¸­çš„è¯ï¼ˆé’ˆå¯¹è¿™ä¸ªé—®é¢˜ï¼Œå¾ˆå¤š NLP ä»»åŠ¡æå‡ºäº†å­—ç¬¦çº§æˆ–è¯æ ¹çº§çš„è¯è¡¨ç¤ºï¼Œå¦‚ CharCNNã€FastTextã€Byte-Pair Encoding (BPE)ï¼‰ã€‚</p>
<p><strong>ä¸Šä¸‹æ–‡åµŒå…¥ï¼š</strong>ä¸ºè§£å†³å¤šä¹‰æ€§å’Œä¸Šä¸‹æ–‡ç›¸å…³çš„é—®é¢˜ï¼Œå°†è¯åœ¨ä¸åŒä¸Šä¸‹æ–‡çš„è¯­ä¹‰åšåŒºåˆ†ã€‚é€šè¿‡å¯¹è¯ï¼ˆè¯æ ¹ï¼‰çš„ token åŠ ä¸€å±‚ Neural Contextual Encoderï¼ˆç¥ç»ä¸Šä¸‹æ–‡ç¼–ç å™¨ï¼‰å¾—åˆ°è¯çš„ä¸Šä¸‹æ–‡åµŒå…¥ã€‚</p>
<h3 id="section"></h3>
<h5 id="ç¥ç»ä¸Šä¸‹æ–‡ç¼–ç å™¨"><strong>2.2 ç¥ç»ä¸Šä¸‹æ–‡ç¼–ç å™¨</strong></h5>
<p><img src="https://i.loli.net/2020/10/28/DeQYroznLq4cCkt.png" alt="img"></p>
<p>å¦‚å›¾ 2 ä¸­æ‰€ç¤ºï¼Œå¤§éƒ¨åˆ†çš„ç¥ç»ä¸Šä¸‹æ–‡ç¼–ç å™¨éƒ½å¯ä»¥è¢«åˆ†ä¸ºä¸‰ç±»ï¼šå·ç§¯æ¨¡å‹ã€åºåˆ—æ¨¡å‹ã€åŸºäºå›¾çš„æ¨¡å‹ã€‚</p>
<p><strong>å·ç§¯æ¨¡å‹ ï¼š</strong>å·ç§¯æ¨¡å‹é€šè¿‡å·ç§¯æ“ä½œå°†è¾“å…¥å¥å­ä¸­çš„ embeddings ä¸å…¶ç›¸é‚»çš„å±€éƒ¨ä¿¡æ¯é›†æˆã€‚</p>
<p><strong>åºåˆ—æ¨¡å‹ ï¼š</strong>åºåˆ—æ¨¡å‹é€šå¸¸ä½¿ç”¨ RNNï¼ˆå¦‚ LSTM å’Œ GRUï¼‰æ¥æè¿°è¯çš„ä¸Šä¸‹æ–‡è¡¨ç¤ºã€‚å®è·µä¸­ï¼ŒåŒå‘ RNN å¸¸ç”¨äºæ”¶é›†è¯çš„ä¸¤è¾¹ä¿¡æ¯ï¼Œä½†è¡¨ç°å¾€å¾€ä¼šå—åˆ°é•¿ç¨‹ä¾èµ–é—®é¢˜çš„å½±å“ã€‚</p>
<p><strong>åŸºäºå›¾çš„æ¨¡å‹ ï¼š</strong>åŸºäºå›¾çš„æ¨¡å‹å°†è¯è§†åšèŠ‚ç‚¹ï¼Œé€šè¿‡é¢„å…ˆå®šä¹‰çš„è¯­è¨€ç»“æ„ï¼ˆå¦‚å¥æ³•ç»“æ„å’Œè¯­ä¹‰è”ç³»ï¼‰æ¥å­¦ä¹ ä¸Šä¸‹æ–‡è¡¨ç¤ºã€‚ä½†å¦‚ä½•æ„é€ ä¸€ä¸ªå¥½çš„å›¾ç»“æ„å¾€å¾€ä¸¥é‡ä¾èµ–äºä¸“å®¶çŸ¥è¯†å’Œå¤–éƒ¨ NLP å·¥å…·ï¼Œå¦‚ä¾å­˜åˆ†æå™¨ã€‚</p>
<p>å®é™…æ“ä½œä¸­å¾€å¾€ç›´æ¥é€šè¿‡ä¸€ä¸ªå…¨è¿æ¥å›¾æ¥å»ºæ¨¡å¹¶è®©æ¨¡å‹è‡ªå·±å­¦ä¹ ç»“æ„ï¼ˆä¸€èˆ¬é€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼‰ã€‚ä¸€ä¸ªå…¸å‹çš„æˆåŠŸè¿ç”¨å°±æ˜¯ Transformerã€‚</p>
<p><strong>åˆ†æï¼š</strong>å·ç§¯æ¨¡å‹å’Œåºåˆ—æ¨¡å‹éƒ½å¾ˆéš¾è§£å†³è¯ä¹‹é—´çš„é•¿ç¨‹ä¾èµ–é—®é¢˜ï¼Œè€Œ Transformer è™½ç„¶èƒ½æ›´å¥½åœ°æè¿°è¯ä¹‹é—´çš„æ·±å±‚è”ç³»ï¼Œå´å¾€å¾€éœ€è¦éå¸¸å¤§çš„è¯­æ–™æ¥è®­ç»ƒï¼Œä¸”å®¹æ˜“åœ¨ä¸­ç­‰è§„æ¨¡çš„æ•°æ®é›†ä¸Šè¿‡æ‹Ÿåˆã€‚</p>
<h5 id="ä¸ºä»€ä¹ˆè¦é¢„è®­ç»ƒ"><strong>2.3 ä¸ºä»€ä¹ˆè¦é¢„è®­ç»ƒï¼Ÿ</strong></h5>
<p>æ­£å¦‚ä¸Šæ–‡æåˆ°çš„ï¼Œæ¨¡å‹å‚æ•°çš„æ•°é‡å¢é•¿è¿…é€Ÿï¼Œè€Œä¸ºäº†è®­ç»ƒè¿™äº›å‚æ•°ï¼Œå°±éœ€è¦æ›´å¤§çš„æ•°æ®é›†æ¥é¿å…è¿‡æ‹Ÿåˆï¼Œè€Œå¤§è§„æ¨¡çš„æ ‡æ³¨æ•°æ®é›†æˆæœ¬åˆéå¸¸é«˜ã€‚è€Œç›¸æ¯”ä¹‹ä¸‹ï¼Œå¤§è§„æ¨¡æœªæ ‡æ³¨çš„è¯­æ–™å´å¾ˆå®¹æ˜“æ„å»ºã€‚</p>
<p>ä¸ºäº†åˆ©ç”¨å¤§é‡çš„æœªæ ‡æ³¨æ–‡æœ¬æ•°æ®ï¼Œæˆ‘ä»¬å¯ä»¥å…ˆä»å…¶ä¸­å­¦ä¹ ä¸€ä¸ªå¥½çš„è¡¨ç¤ºï¼Œå†å°†è¿™äº›è¡¨ç¤ºç”¨åœ¨åˆ«çš„ä»»åŠ¡ä¸­ã€‚è¿™ä¸€é€šè¿‡ PTMs ä»æœªæ ‡æ³¨å¤§è§„æ¨¡æ•°æ®é›†ä¸­æå–è¡¨ç¤ºçš„é¢„è®­ç»ƒè¿‡ç¨‹åœ¨å¾ˆå¤š NLP ä»»åŠ¡ä¸­éƒ½å–å¾—äº†å¾ˆå¥½çš„è¡¨ç°ã€‚</p>
<p>é¢„è®­ç»ƒçš„ä¼˜ç‚¹å¯ä»¥æ€»ç»“ä¸ºä»¥ä¸‹ä¸‰ç‚¹ï¼š1 åœ¨å¤§è§„æ¨¡è¯­æ–™ä¸Šé€šè¿‡é¢„è®­ç»ƒå­¦ä¹ é€šç”¨è¯­è¨€è¡¨ç¤ºå¯¹ä¸‹æ¸¸ä»»åŠ¡å¾ˆæœ‰å¸®åŠ©ï¼›2) é¢„è®­ç»ƒæä¾›äº†æ›´å¥½çš„æ¨¡å‹åˆå§‹åŒ–å‚æ•°ï¼Œä½¿å¾—åœ¨ç›®æ ‡ä»»åŠ¡ä¸Šæœ‰æ›´å¥½çš„æ³›åŒ–æ€§èƒ½å’Œæ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ï¼›3) é¢„è®­ç»ƒæ˜¯ä¸€ç§æœ‰æ•ˆçš„æ­£åˆ™åŒ–æ–¹æ³•ï¼Œèƒ½å¤Ÿé¿å…åœ¨å°æ•°æ®é›†ä¸Šè¿‡æ‹Ÿåˆã€‚</p>
<h4 id="ptmsæ¦‚è¿°"><strong>3.PTMsæ¦‚è¿°</strong></h4>
<p>PTMs çš„ä¸»è¦åŒºåˆ«åœ¨äºä¸Šä¸‹æ–‡ç¼–ç å™¨çš„ä½¿ç”¨ã€é¢„è®­ç»ƒä»»åŠ¡å’Œç›®æ ‡ã€‚ä¸Šä¸‹æ–‡ç¼–ç å™¨å·²åœ¨ 2.2 ä¸­åšäº†å™è¿°ï¼Œæ¥ä¸‹æ¥å¯¹é¢„è®­ç»ƒä»»åŠ¡è¿›è¡Œåˆ†æï¼Œå¹¶æå‡ºä¸€ç§ PTMs åˆ†ç±»æ–¹æ³•ã€‚</p>
<p><img src="https://i.loli.net/2020/10/28/LhoxNVG5rF3McDK.png" alt="img"></p>
<p>å¦‚å›¾ 3ï¼Œè¿™ä¸€éƒ¨åˆ†å†…å®¹ä½œè€…åœ¨æ–‡ä¸­æœ‰ä¸€å¼ éå¸¸è¯¦ç»†çš„åˆ†ç±»å›¾å¯ä¾›å‚è€ƒã€‚</p>
<p>è¡¨ 1 ä»å¤šä¸ªè§’åº¦åŒºåˆ†äº†æ–‡ä¸­æåˆ°çš„ä¸€äº› PTMsã€‚</p>
<p><img src="https://i.loli.net/2020/10/28/ZI176zJekFciyGr.png" alt="img"></p>
<h5 id="é¢„è®­ç»ƒä»»åŠ¡"><strong>3.1 é¢„è®­ç»ƒä»»åŠ¡</strong></h5>
<p>PTMs æŒ‰ç…§é¢„è®­ç»ƒä»»åŠ¡ç±»å‹å¯ä»¥è¢«åˆ†ä¸ºä¸¤ç±»ï¼šæœ‰ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ /è‡ªç›‘ç£å­¦ä¹ ã€‚</p>
<p>æœ‰ç›‘ç£å­¦ä¹ çš„é¢„è®­ç»ƒä»»åŠ¡ä¸»è¦æœ‰æœºå™¨ç¿»è¯‘ (MT)ï¼Œå…¸å‹çš„æ¨¡å‹æ˜¯ CoVeã€‚è€Œä¸‹æ–‡è¿›ä¸€æ­¥æ ¹æ®å®ç°æ€è·¯å°†è‡ªç›‘ç£/æ— ç›‘ç£ä»»åŠ¡åˆ†ä¸ºä¸¤ç±»ï¼Œä¸€æ˜¯åŸºäºä¸Šä¸‹æ–‡çš„ (LM, DAE, PLM)ï¼ŒäºŒæ˜¯åŸºäºå¯¹æ¯”çš„ (CTL)ã€‚</p>
<h4 id="section-1"></h4>
<h6 id="è¯­è¨€æ¨¡å‹-lm"><strong>3.1.1 è¯­è¨€æ¨¡å‹ (LM)</strong></h6>
<p>ä½œä¸º NLP ä¸­æœ€å¸¸è§çš„æ— ç›‘ç£ä»»åŠ¡ï¼ŒLM ä¸€èˆ¬æŒ‡è‡ªå›å½’ LM (auto-regressive LM) æˆ–è€…å•å‘ LM (unidirectional LM)ã€‚å…·ä½“è®­ç»ƒè¿‡ç¨‹æ˜¯åŸºäºä¸€ä¸ªå¤§çš„è¯­æ–™ï¼Œé€šè¿‡æœ€å¤§ä¼¼ç„¶ä¼°è®¡ (MLE) è®­ç»ƒè®¡ç®—ä¸€ä¸ªå¥å­å‡ºç°çš„æ¦‚ç‡ã€‚</p>
<p>ç„¶è€Œå•å‘ LM çš„ç¼ºç‚¹åˆ™æ˜¯åªèƒ½ç¼–ç ä¸€ä¸ªè¯å·¦ä¾§çš„æ–‡æœ¬å’Œå…¶è‡ªèº«ï¼Œè€Œæ›´å¥½çš„ä¸Šä¸‹æ–‡åº”è¯¥ç¼–ç å·¦å³ä¸¤ä¾§çš„æ–‡æœ¬ã€‚é’ˆå¯¹è¿™ä¸€ç¼ºç‚¹ï¼Œè§£å†³æ–¹æ¡ˆæ˜¯åŒå‘ LM (BiLM)ï¼Œå³ä¸€ä¸ªä»å·¦åˆ°å³å’Œä¸€ä¸ªä»å³åˆ°å·¦çš„æ¨¡å‹çš„ç»„åˆã€‚</p>
<h6 id="å»å™ªå£°è‡ªç¼–ç å™¨-denoising-autoencoder-dae"><strong>3.1.2 å»å™ªå£°è‡ªç¼–ç å™¨ (Denoising Autoencoder, DAE)</strong></h6>
<blockquote>
<p>è¿™é‡Œå°†åŸæ–‡ä¸­ Masked Language Modeling (MLM) ä¸ DAE åˆå¹¶ä¸ºä¸€ä¸ªéƒ¨åˆ†ï¼Œå› ä¸ºä¸€èˆ¬å°† BERT ä¸­æå‡ºçš„ MLM çœ‹ä½œæ˜¯åŸºäº DAE çš„æ€è·¯å®ç°çš„ã€‚</p>
</blockquote>
<p>DAE çš„ç›®çš„æ˜¯é€šè¿‡å‘è¾“å…¥æ–‡æœ¬ä¸­æ·»åŠ å™ªå£°ï¼Œåˆ©ç”¨å«å™ªå£°çš„æ ·æœ¬å»é‡æ„ä¸å«å™ªå£°çš„è¾“å…¥ã€‚ä¸»è¦æœ‰äº”ä¸ªå®ç°æ–¹å¼ï¼šæŒ¡ä½ (MASK) tokenã€åˆ é™¤ tokenã€å¡«å…… tokenã€å¥å­æ’åˆ—ã€æ–‡æœ¬è½®æ¢ã€‚</p>
<p>MLM éšæœºé€‰å‡ºä¸€äº›è¯ç”¨ [MASK] æ ‡è®°ï¼Œç„¶åå»é¢„æµ‹è¢« MASK çš„è¯ã€‚ä½†ç”±äºè¢« MASK çš„è¯å¹¶ä¸å‡ºç°åœ¨ fine-tuning çš„è¿‡ç¨‹ä¸­ï¼Œä¼šå¯¼è‡´é¢„è®­ç»ƒå’Œå¾®è°ƒçš„è¿‡ç¨‹å‡ºç°ä¸ä¸€è‡´æ€§ã€‚é’ˆå¯¹è¿™ç§æƒ…å†µï¼ŒBERT é€šè¿‡ 80% [MASK]ï¼Œ10% éšæœº token,10% åŸ token çš„æ–¹å¼æ¥è¿›è¡Œ maskã€‚</p>
<p>è€Œ MLM çš„ä¸€ç§å˜ä½“ï¼Œ<strong>Seq2SeqMLM</strong>ï¼Œåˆ™æ˜¯é€šè¿‡å°† encoder-decoder (Seq2Seq) åº”ç”¨åˆ° MLM ä¸Šï¼Œè¿™ç§å˜ä½“æœ‰åˆ©äº Seq2Seq ç±»å‹çš„ä¸‹æ¸¸ä»»åŠ¡ï¼Œæ¯”å¦‚ QAï¼Œæ€»ç»“å’Œæœºå™¨ç¿»è¯‘ã€‚è¿™ä¸€ç»“æ„ä¸»è¦ç”¨åœ¨ MASS å’Œ T5 ä¸­ã€‚</p>
<p>è€Œåœ¨ BERT ä¹‹åçš„å¾ˆå¤šè®ºæ–‡éƒ½å¯¹ MLM åšäº†ä¸€äº›æ”¹è¿›ä»¥å¢å¼ºæ€§èƒ½ï¼Œä½œè€…å°†å…¶æ€»ç»“ä¸º E-MLM (Enhanced Masked Language Modeling)ã€‚</p>
<p>å…¶ä¸­ RoBERTa ä½¿ç”¨åŠ¨æ€ maskingï¼ŒUniLM å°†å¯¹ mask çš„é¢„æµ‹æ‰©å±•åˆ°ä¸‰ç§ä»»åŠ¡ï¼šå•å‘ã€åŒå‘å’Œ Seq2Seqã€‚XLM é€šè¿‡ä¸€ç§ä¸²è”å¹¶è¡ŒåŒè¯­å¥å¯¹å«åš TLM (translation language modeling) çš„æ¨¡å‹å®ç° MLMã€‚</p>
<p>è€Œ SpanBERT å’Œ StructBERT åˆ™æ˜¯å¼•å…¥äº†ç»“æ„åŒ–ä¿¡æ¯ã€‚è€Œ ERINE (Baidu) åˆ™æ˜¯é€‰æ‹© MASK å®ä½“å’ŒçŸ­è¯­ï¼ŒE-BERT å’Œ ERINE (THU) åˆ™æ˜¯åˆ©ç”¨äº†å®ä½“ embedding æ–¹æ³•ï¼Œè¿™ä¸‰è€…éƒ½æ˜¯å€ŸåŠ©äº†å¤–éƒ¨çŸ¥è¯†æ¥ä¸°å¯Œ MLMã€‚</p>
<h6 id="æ’åˆ—è¯­è¨€æ¨¡å‹plm"><strong>3.1.3 æ’åˆ—è¯­è¨€æ¨¡å‹ï¼ˆPLMï¼‰</strong></h6>
<p>é’ˆå¯¹ MLM ä¸­ä½¿ç”¨ MASK å¯¼è‡´çš„é¢„è®­ç»ƒä¸å¾®è°ƒè¿‡ç¨‹çš„ä¸ä¸€è‡´ï¼ŒPermuted Language Modeling (PLM) å¯¹äºä¸€ä¸ªç»™å®šåºåˆ—ï¼Œç”Ÿæˆå…¶æ‰€æœ‰å¯èƒ½æ’åˆ—è¿›è¡Œé‡‡æ ·ä½œä¸ºè®­ç»ƒçš„ç›®æ ‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒPLM å¹¶ä¸æ”¹å˜åŸå§‹æ–‡æœ¬çš„ä½ç½®ï¼Œè€Œæ˜¯é‡æ–°å®šä¹‰ token é¢„æµ‹çš„é¡ºåºã€‚</p>
<h6 id="å¯¹æ¯”å­¦ä¹ ctl"><strong>3.1.4 å¯¹æ¯”å­¦ä¹ ï¼ˆCTLï¼‰</strong></h6>
<p>CTL (Contrastive Learning) åŸºäºä¸€ç§â€œlearning by comparisonâ€çš„æ€è·¯ï¼Œå‡è®¾æŸäº›è§‚æµ‹æ–‡æœ¬å¯¹æ¯”éšæœºé‡‡æ ·æ–‡æœ¬åœ¨è¯­ä¹‰ä¸Šæ›´ç›¸ä¼¼ï¼Œé€šè¿‡æ„å»ºæ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬å¹¶åº¦é‡è·ç¦»æ¥å®ç°å­¦ä¹ ã€‚CTL é€šå¸¸æ¯” LM å…·æœ‰æ›´å°‘çš„è®¡ç®—å¤æ‚åº¦ï¼Œä¹Ÿå› æ­¤æˆä¸ºä¸€ä¸ªå€¼å¾—é€‰æ‹©çš„ PTMs è®­ç»ƒæ ‡å‡†ã€‚</p>
<h6 id="deep-infomax-dim"><strong>3.1.5 Deep InfoMax (DIM)</strong></h6>
<p>DIM æœ€åˆæ˜¯åœ¨ CV é¢†åŸŸæå‡ºçš„ç”¨äºæœ€å¤§åŒ–å›¾åƒå…¨å±€ç‰¹å¾ä¸å±€éƒ¨ç‰¹å¾ä¹‹é—´çš„äº’ä¿¡æ¯ï¼ˆMutual Informationï¼‰çš„æ–¹æ³•ã€‚</p>
<p>InfoWord å°† DIM å¼•å…¥åˆ°è¯­ä¹‰è¡¨è¾¾å­¦ä¹ ä¸­ï¼Œæå‡ºç”¨ DIM objective ä»¥æœ€å¤§åŒ–å¥å­çš„å…¨å±€è¡¨ç¤ºå’Œä¸€ä¸ª N-gram çš„å…·å¤‡è¡¨ç¤ºä¹‹é—´çš„äº’ä¿¡æ¯ã€‚</p>
<p>å™ªå£°å¯¹æ¯”ä¼°è®¡ï¼ˆNoise-Contrastive Estimationï¼ŒNCEï¼‰é€šè¿‡è®­ç»ƒä¸€ä¸ªäºŒå…ƒåˆ†ç±»å™¨æ¥åŒºåˆ†çœŸå®æ ·æœ¬å’Œå‡æ ·æœ¬ï¼Œè®­ç»ƒè¯åµŒå…¥ã€‚NCE çš„æ€æƒ³ä¹Ÿè¢«ç”¨åœ¨ word2vec ä¸­ã€‚</p>
<h6 id="replaced-token-detection-rtd"><strong>3.1.6 Replaced Token Detection (RTD)</strong></h6>
<p>RTD å’Œ NCE å¤§ä½“ç›¸åŒï¼Œæ ¹æ®ä¸Šä¸‹æ–‡æ¥é¢„æµ‹ token æ˜¯å¦æ›¿æ¢ã€‚</p>
<p>CBOW çš„ negetive sampling å°±å¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ª RTD çš„ç®€å•ç‰ˆæœ¬ï¼Œå…¶ä¸­é‡‡æ ·æ˜¯æ ¹æ®è¯æ±‡è¡¨ä¸­çš„åˆ†å¸ƒè¿›è¡Œé‡‡æ ·ã€‚</p>
<p>ELECTRA åŸºäº RTD æå‡ºäº†ä¸€ç§æ–°çš„ generator-discriminator æ¡†æ¶ã€‚é¦–å…ˆç”¨ MLM ä»»åŠ¡è®­ç»ƒ generatorï¼Œå†ç”¨ generator çš„æƒé‡åˆå§‹åŒ– discriminatorï¼Œå†ç”¨åˆ¤åˆ«ä»»åŠ¡ï¼ˆåˆ¤åˆ«å“ªäº› token è¢« generator æ›¿æ¢è¿‡ï¼‰è®­ç»ƒ discriminatorã€‚</p>
<p>æœ€ç»ˆåœ¨ä¸‹æ¸¸ä»»åŠ¡åªéœ€è¦å¯¹ discriminator è¿›è¡Œ fine-tuningã€‚TRD ä¹Ÿæ˜¯ä¸€ç§å¾ˆå¥½çš„è§£å†³ MLM å¯¼è‡´çš„ä¸ä¸€è‡´é—®é¢˜çš„æ–¹æ³•ã€‚</p>
<p>WKLM åˆ™æ˜¯é€šè¿‡åœ¨å®ä½“å±‚é¢ï¼ˆentity-levelï¼‰è¿›è¡Œè¯æ›¿æ¢ï¼Œæ›¿æ¢ä¸ºåŒä¸€ä¸ªå®ä½“ç±»å‹çš„å®ä½“åã€‚</p>
<h5 id="section-2"></h5>
<h6 id="next-sentence-prediction-nsp"><strong>3.1.7 Next Sentence Prediction (NSP)</strong></h6>
<p>NSP è®­ç»ƒæ¨¡å‹åŒºåˆ†ä¸¤ä¸ªè¾“å…¥è¯­å¥æ˜¯å¦ä¸ºè®­ç»ƒè¯­æ–™ä¸­è¿ç»­çš„ç‰‡æ®µï¼Œåœ¨é€‰æ‹©é¢„è®­ç»ƒå¥å¯¹æ—¶ï¼Œç¬¬äºŒä¸ªå¥å­ 50% æ˜¯ç¬¬ä¸€ä¸ªå¥å­å®é™…çš„è¿ç»­ç‰‡æ®µï¼Œ50% æ˜¯è¯­æ–™ä¸­çš„éšæœºæ®µè½ã€‚NSP èƒ½å¤Ÿæ•™ä¼šæ¨¡å‹ç†è§£ä¸¤ä¸ªè¾“å…¥å¥å­ä¹‹é—´çš„è”ç³»ï¼Œä»è€Œä½¿å¾—å¦‚ QA å’Œ NLI è¿™ç§å¯¹æ­¤ç±»ä¿¡æ¯æ•æ„Ÿçš„ä¸‹æ¸¸ä»»åŠ¡å—ç›Šã€‚</p>
<p>ç„¶è€Œï¼Œè¿‘æ¥ NSP çš„å¿…è¦æ€§ä¹Ÿé­åˆ°äº†è´¨ç–‘ï¼ŒXLNet çš„ä½œè€…å‘ç°ä¸ç”¨ NSP loss çš„å•å¥è®­ç»ƒä¼˜äºä½¿ç”¨ NSP çš„å¥å¯¹è®­ç»ƒã€‚RoBERTa çš„ä½œè€…è¿›ä¸€æ­¥åˆ†æè¡¨æ˜ï¼šåœ¨å¯¹å•ä¸ªæ–‡æœ¬ä¸­çš„æ–‡æœ¬å—è®­ç»ƒæ—¶ï¼Œå»é™¤ NSP ä¼šåœ¨ä¸‹æ¸¸ä»»åŠ¡ç¨å¾®æé«˜æ€§èƒ½ã€‚</p>
<h6 id="sentence-order-prediction-sop"><strong>3.1.8 Sentence Order Prediction (SOP)</strong></h6>
<p>NSP ç»“åˆäº†ä¸»é¢˜é¢„æµ‹ç›¸å…³æ€§é¢„æµ‹ï¼Œè€Œå› ä¸ºä¸»é¢˜é¢„æµ‹æ›´å®¹æ˜“ï¼Œæ¨¡å‹å°†æ›´ä¾èµ–äºä¸»é¢˜é¢„æµ‹ã€‚ä¸ºæ›´å¥½å»ºæ¨¡å¥å­ä¹‹é—´çš„ç›¸å…³æ€§ï¼ŒALBERT æå‡ºä½¿ç”¨ SOP loss æ›¿æ¢ NSP lossï¼ŒSOP ä½¿ç”¨ä¸€ä¸ªæ–‡æ¡£ä¸­çš„ä¸¤ä¸ªè¿ç»­ç‰‡æ®µä½œä¸ºæ­£æ ·æœ¬ï¼Œå°†è¿™ä¸¤ä¸ªç‰‡æ®µäº¤æ¢é¡ºåºä½œä¸ºè´Ÿæ ·æœ¬ã€‚</p>
<p>é‡‡ç”¨äº† SOP çš„ ALBERT åœ¨å¤šé¡¹ä¸‹æ¸¸ä»»åŠ¡ä¸­ç»“æœéƒ½ä¼˜äº BERTã€‚StructBERT å’Œ BERTje ä¹Ÿä½¿ç”¨ SOP ä½œä¸ºè‡ªç›‘ç£å­¦ä¹ ä»»åŠ¡ã€‚</p>
<h3 id="section-3"></h3>
<h5 id="ptmsçš„æ‹“å±•"><strong>3.2 PTMsçš„æ‹“å±•</strong></h5>
<h6 id="å¼•å…¥çŸ¥è¯†çš„ptms"><strong>3.2.1 å¼•å…¥çŸ¥è¯†çš„PTMs</strong></h6>
<p>é€šå¸¸ PTMs éƒ½æ˜¯ç”¨å¤§é‡è¯­æ–™è®­ç»ƒé€šç”¨çš„è¯­è¨€è¡¨ç¤ºï¼Œè€Œå°†å¤–éƒ¨çš„é¢†åŸŸçŸ¥è¯†å¼•å…¥åˆ° PTMs è¢«è¯æ˜å¼æœ‰æ•ˆçš„ã€‚è‡ª BERT ä»¥æ¥ï¼Œå°±æœ‰å¾ˆå¤šé¢„è®­ç»ƒä»»åŠ¡ç”¨ä»¥å°†å¤–éƒ¨çŸ¥è¯†çº³å…¥ PTMsï¼Œå¦‚ï¼š</p>
<p><strong>LIBERTï¼š</strong>linguistically-informed BERT ï¼Œé€šè¿‡é™„åŠ è¯­è¨€çº¦æŸä»»åŠ¡çº³å…¥äº†è¯­è¨€çŸ¥è¯†ã€‚</p>
<p><strong>SentiLRï¼š</strong>é€šè¿‡å¯¹æ¯ä¸ªå•è¯æ·»åŠ æƒ…æ„Ÿææ€§ï¼Œå°† MLM æ‹“å±•è‡³ Label-Aware MLM (LA-MLM)ï¼Œåœ¨å¤šä¸ªæƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡è¾¾åˆ° SOTAã€‚</p>
<p><strong>SenseBERTï¼š</strong>ä¸ä»…èƒ½é¢„æµ‹è¢« mask çš„ tokenï¼Œè¿˜èƒ½é¢„æµ‹ WordNet ä¸­çš„ supersenseã€‚</p>
<p><strong>ERINE (THU)ï¼š</strong>å°†çŸ¥è¯†å›¾è°±ä¸­é¢„è®­ç»ƒçš„å®ä½“åµŒå…¥ä¸æ–‡æœ¬ä¸­ç›¸åº”çš„å®ä½“æåŠç›¸ç»“åˆï¼Œä»¥å¢å¼ºæ–‡æœ¬è¡¨ç¤ºã€‚</p>
<p><strong>KnowBERTï¼š</strong>ç«¯åˆ°ç«¯å°†å¸¦å®ä½“è¿æ¥æ¨¡å‹ä¸å®ä½“è¡¨ç¤ºé›†æˆã€‚</p>
<p><strong>KEPLERï¼š</strong>å°†çŸ¥è¯†åµŒå…¥å’Œè¯­è¨€æ¨¡å‹å¯¹è±¡è”åˆã€‚</p>
<p><strong>K-BERTï¼š</strong>ä¸åŒäºä»¥ä¸Šå‡ ä¸ªæ¨¡å‹é€šè¿‡å®ä½“åµŒå…¥å¼•å…¥çŸ¥è¯†å›¾è°±ä¸­çš„ç»“æ„åŒ–ä¿¡æ¯ï¼ŒK-BERT é€šè¿‡ç›´æ¥å°†çŸ¥è¯†å›¾è°±ä¸­ç›¸å…³ä¸‰å…ƒç»„å¼•å…¥å¥å­ï¼Œè·å¾—ä¸€ä¸ª BERT çš„æ‹“å±•çš„æ ‘å½¢è¾“å…¥ã€‚</p>
<p><strong>K-Adapterï¼š</strong>é’ˆå¯¹ä¸åŒé¢„è®­ç»ƒä»»åŠ¡ç‹¬ç«‹è®­ç»ƒä¸åŒçš„é€‚é…å™¨ä»¥å¼•å…¥å¤šç§çŸ¥è¯†ï¼Œä»¥è§£å†³ä¸Šè¿°æ¨¡å‹åœ¨æ³¨å…¥å¤šç§çŸ¥è¯†å‡ºç°çš„é—å¿˜é—®é¢˜ã€‚</p>
<h6 id="å¤šæ¨¡æ€ptms"><strong>3.2.2 å¤šæ¨¡æ€PTMs</strong></h6>
<p>éš PTMs åœ¨ NLP é¢†åŸŸçš„å¹¿æ³›åº”ç”¨ï¼Œä¸€äº›å¤šæ¨¡æ€ PTMs ä¹Ÿè¢«è®¾è®¡å‡ºæ¥ï¼Œåœ¨ä¸€äº›è¯­éŸ³ã€è§†é¢‘ã€å›¾åƒæ•°æ®é›†ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œæ¯”å¦‚ï¼š</p>
<ul>
<li><strong>è§†é¢‘-è¯­è¨€ï¼š</strong>VideoBERTã€CBT</li>
<li><strong>å›¾åƒ-è¯­è¨€ï¼š</strong>ç”¨äº visual question answering (VQA) and visual commonsense reasoning (VCR)ï¼Œå¦‚ ViLBERTã€LXMERTã€VisualBERTã€B2T2ã€VLBERTã€ Unicoder-VLã€UNITER</li>
<li><strong>éŸ³é¢‘-æ–‡æœ¬ï¼š</strong>ç”¨äºç«¯åˆ°ç«¯ Speech Question Answering (SQA) ä»»åŠ¡ï¼Œå¦‚ SpeechBERT</li>
</ul>
<h4 id="section-4"></h4>
<h6 id="é¢†åŸŸé¢„è®­ç»ƒptms"><strong>3.2.3 é¢†åŸŸé¢„è®­ç»ƒPTMs </strong></h6>
<p>å¤§å¤šæ•° PTMs éƒ½æ˜¯åœ¨ Wikipedia è¿™æ ·çš„é€šç”¨é¢†åŸŸè¯­æ–™åº“ä¸Šè®­ç»ƒçš„ï¼Œè¿™å°±é™åˆ¶äº†ä»–ä»¬åœ¨ç‰¹å®šé¢†åŸŸå†…çš„è¡¨ç°ã€‚</p>
<p>è¿‘æœŸæœ‰ä¸€äº›ç”¨ä¸“ä¸šé¢†åŸŸè¯­æ–™è®­ç»ƒçš„ PTMsï¼Œæ¯”å¦‚ï¼šç”Ÿç‰©åŒ»å­¦é¢†åŸŸçš„ BioBERTï¼Œç§‘å­¦é¢†åŸŸçš„ SciBERTï¼Œä¸´åºŠåŒ»å­¦é¢†åŸŸçš„ ClinicalBERTã€‚è¿˜æœ‰ä¸€äº›å·¥ä½œå°è¯•å°†é¢„è®­ç»ƒæ¨¡å‹æ›´å¥½åœ°ä½¿ç”¨ç›®æ ‡åº”ç”¨ï¼Œæ¯”å¦‚ç”Ÿç‰©åŒ»å­¦å®ä½“å½’ä¸€åŒ–ã€ä¸“åˆ©åˆ†ç±»ç­‰ã€‚</p>
<h6 id="å¤šè¯­è¨€ä¸ç‰¹å®šè¯­è¨€ptms"><strong>3.2.4 å¤šè¯­è¨€ä¸ç‰¹å®šè¯­è¨€PTMs </strong></h6>
<p>å­¦ä¹ å¤šè¯­è¨€æ–‡æœ¬è¡¨ç¤ºå¯¹äºè·¨è¯­è¨€ NLP ä»»åŠ¡æ˜¯å¾ˆé‡è¦çš„ã€‚æ—©æœŸå·¥ä½œç€åŠ›äºå­¦ä¹ æ¥è‡ªåŒä¸€è¯­ä¹‰ç¯å¢ƒä¸‹çš„å¤šè¯­è¨€è¯åµŒå…¥ï¼Œè¿™ä¸€æ–¹æ³•å¾€å¾€ç¼ºä¹è¯­è¨€é—´çš„æ ¡å‡†ã€‚è¿‘æœŸæœ‰å¦‚ä¸‹å‡ ä¸ªå¤šè¯­è¨€ PTMsï¼š</p>
<p><strong>Multilingual-BERTï¼š</strong>M-BERTï¼Œåœ¨ Wikipedia ä¸Š 104 ç§ç§è¯­è¨€çš„æ–‡æœ¬ä¸Šè¿›è¡Œ MLM è®­ç»ƒï¼Œæ¯ä¸ªè®­ç»ƒæ ·æœ¬éƒ½æ˜¯å•è¯­è¨€çš„ï¼Œä¹Ÿæ²¡æœ‰ä¸“é—¨è®¾è®¡è·¨è¯­è¨€ç›®æ ‡ï¼Œä½†å³ä¾¿å¦‚æ­¤ï¼ŒM-BERT åœ¨è·¨è¯­è¨€ä»»åŠ¡ä¸Šè¡¨ç°è¿˜æ˜¯éå¸¸å¥½ã€‚</p>
<p><strong>XLMï¼š</strong>é€šè¿‡ç»“åˆè·¨è¯­è¨€ä»»åŠ¡ TLM (translation language modeling)ï¼Œæå‡äº† M-BERT çš„æ€§èƒ½ã€‚</p>
<p><strong>Unicoderï¼š</strong>æå‡ºä¸‰ä¸ªè·¨è¯­è¨€é¢„è®­ç»ƒä»»åŠ¡ï¼š1) cross-lingual word recovery; 2) cross-lingual paraphrase classification; 3) cross-lingual masked language modelã€‚</p>
<p>é™¤æ­¤ä¹‹å¤–è¿˜æœ‰ä¸€äº›å•è¯­è¨€çš„ PTMsï¼šBERT-wwmï¼ŒZENï¼ŒNEZHAï¼ŒERNIE (Baidu)ï¼ŒBERTjeï¼ŒCamemBERTï¼Œ FlauBERT ï¼ŒRobBERT ã€‚</p>
<h5 id="å¦‚ä½•å‹ç¼©ptms"><strong>3.3 å¦‚ä½•å‹ç¼©PTMs</strong></h5>
<p>é¢„è®­ç»ƒæ¨¡å‹å¾€å¾€åŒ…å«è‡³å°‘å‡ åƒä¸‡ä¸ªå‚æ•°ï¼Œè¿™ä¹Ÿä½¿å¾—æ¨¡å‹éš¾ä»¥éƒ¨ç½²åˆ°ç”Ÿæ´»ä¸­çš„çº¿ä¸ŠæœåŠ¡ä»¥åŠèµ„æºæœ‰é™çš„è®¾å¤‡ä¸Šï¼Œè¿™å°±ä½¿å¾—æ¨¡å‹å‹ç¼©æˆä¸ºä¸€æ¡å¯èƒ½èƒ½å¤Ÿå‹ç¼©æ¨¡å‹å°ºå¯¸å¹¶æé«˜è®¡ç®—æ•ˆç‡çš„æ–¹æ³•ã€‚è¡¨ 2 å±•ç¤ºäº†ä¸€äº›å‹ç¼©çš„ PTMs çš„å¯¹æ¯”ã€‚</p>
<p><img src="https://i.loli.net/2020/10/28/6cBzpUQ3J7e1Xd2.png" alt="img"></p>
<p>å‹ç¼© PTMs ä¸€èˆ¬æœ‰å››ä¸ªæ–¹æ³•ï¼š</p>
<ul>
<li><strong>å‰ªæï¼ˆpruningï¼‰ï¼š</strong>å»é™¤ä¸é‚£ä¹ˆé‡è¦çš„å‚æ•°ï¼ˆe.g. æƒé‡ã€å±‚æ•°ã€é€šé“æ•°ã€attention headsï¼‰</li>
<li><strong>é‡åŒ–ï¼ˆweight quantizationï¼‰ï¼š</strong>ä½¿ç”¨å ä½æ›´å°‘ï¼ˆä½ç²¾åº¦ï¼‰çš„å‚æ•°</li>
<li><strong>å‚æ•°å…±äº«ï¼ˆparameter sharingï¼‰ï¼š</strong>ç›¸ä¼¼æ¨¡å‹å•å…ƒé—´å…±äº«å‚æ•°</li>
<li><strong>çŸ¥è¯†è’¸é¦ï¼ˆknowledge diistillationï¼‰ï¼š</strong>ç”¨ä¸€äº›ä¼˜åŒ–ç›®æ ‡ä»å¤§å‹ teacher æ¨¡å‹å­¦ä¹ ä¸€ä¸ªå°çš„ student æ¨¡å‹ï¼Œä¸€äº›åˆ©ç”¨çŸ¥è¯†è’¸é¦çš„ PTMs è§è¡¨ 3ã€‚</li>
</ul>
<p><img src="https://i.loli.net/2020/10/28/5tdJqBHve3XYuCo.png" alt="img"></p>
<h4 id="å¦‚ä½•å°†ptmsåº”ç”¨è‡³ä¸‹æ¸¸ä»»åŠ¡"><strong>4.å¦‚ä½•å°†PTMsåº”ç”¨è‡³ä¸‹æ¸¸ä»»åŠ¡</strong></h4>
<h5 id="è¿ç§»å­¦ä¹ "><strong>4.1 è¿ç§»å­¦ä¹ </strong></h5>
<p>è¿ç§»å­¦ä¹ å°±æ˜¯å°†æºä»»åŠ¡ä¸­çš„çŸ¥è¯†é€‚åº”åˆ°ç›®æ ‡ä»»åŠ¡ï¼Œå°† PTMs é€‚åº”åˆ°ä¸‹æ¸¸ä»»åŠ¡æ˜¯ä¸€ç§é¡ºåºè¿ç§»å­¦ä¹ ä»»åŠ¡ã€‚é‚£ä¹ˆï¼Œå¦‚ä½•è¿ç§»å‘¢ï¼Ÿæˆ‘ä»¬éœ€è¦è€ƒè™‘ä»¥ä¸‹å‡ ä¸ªé—®é¢˜ï¼š</p>
<ul>
<li><strong>é€‰æ‹©åˆé€‚çš„é¢„è®­ç»ƒä»»åŠ¡</strong>ï¼šè¿‘æœŸï¼ŒLM æ˜¯æœ€æµè¡Œçš„é¢„è®­ç»ƒä»»åŠ¡ï¼Œä¹Ÿæœ‰æ•ˆè§£å†³äº†å¾ˆå¤š NLP é—®é¢˜ã€‚ä½†ä¸åŒçš„é¢„è®­ç»ƒä»»åŠ¡åœ¨ä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡ä¸Šæœ‰ä¸åŒçš„æ•ˆæœï¼Œæ¯”å¦‚ NSP ä»»åŠ¡èƒ½å¸®åŠ© PTM ç†è§£å¥å­ä¹‹é—´çš„å…³ç³»ï¼Œå› æ­¤ PTM å¯¹äº QA å’Œ NLI è¿™æ ·çš„ä¸‹æ¸¸ä»»åŠ¡å¾ˆæœ‰å¸®åŠ©ã€‚</li>
<li><strong>é€‰æ‹©åˆé€‚çš„æ¨¡å‹æ¶æ„</strong>ï¼šæ¯”å¦‚ BERT ä½¿ç”¨çš„ MLM å’Œ Transformer ç»“æ„ä½¿å…¶æ“…é•¿ NLU ä»»åŠ¡ï¼Œå´å¾ˆéš¾ç”Ÿæˆè¯­è¨€ã€‚</li>
<li><strong>é€‰æ‹©åˆé€‚çš„è¯­æ–™</strong>ï¼šä¸‹æ¸¸ä»»åŠ¡çš„æ•°æ®åº”è¯¥æ¥è¿‘ PTMs çš„é¢„è®­ç»ƒä»»åŠ¡ã€‚</li>
<li><strong>é€‰æ‹©åˆé€‚çš„layers</strong>ï¼šåœ¨â€œæ·±â€çš„é¢„è®­ç»ƒæ¨¡å‹ä¸­ï¼Œä¸åŒçš„ layer å¾€å¾€æç»˜ä¸åŒç§ç±»çš„ä¿¡æ¯ã€‚æœ‰ä¸‰ç§é€‰æ‹© layers çš„æ–¹å¼ï¼š1) åªç”¨ Embeddingï¼Œå¦‚ word2vec å’Œ Gloveï¼›2) Top Layerï¼Œå¦‚ BERTï¼›3) All Layersï¼Œå¦‚ ELMoã€‚</li>
<li><strong>æ˜¯å¦è¿›è¡Œfine-tune</strong>ï¼šæ¨¡å‹è¿ç§»ä¸€èˆ¬æœ‰ä¸¤ç§æ–¹æ³•ï¼šç‰¹å¾æå–å’Œ fine-tuningã€‚ç‰¹å¾æå–çš„å‚æ•°æ˜¯å†»ç»“çš„ï¼Œä¸”å¾€å¾€éœ€è¦ç‰¹å®šä»»åŠ¡çš„ä½“ç³»ç»“æ„ã€‚fine-tunig çš„å‚æ•°æ˜¯éå†»ç»“çš„ï¼Œæ¯”ç‰¹å¾æå–æ–¹æ³•æ›´ä¸ºé€šç”¨ä¸”æ–¹ä¾¿ã€‚</li>
</ul>
<h5 id="fine-tuningçš„ç­–ç•¥"><strong>4.2 fine-tuningçš„ç­–ç•¥</strong></h5>
<p>è‡ª ULMFit å’Œ BERT èµ·ï¼Œfine-tuning å·²ç»æˆä¸º PTMs ä¸»è¦çš„é€‚é…æ–¹æ³•ã€‚è¿™é‡Œæœ‰ä¸€äº›å®ç”¨çš„ fine-tunig ç­–ç•¥ï¼š</p>
<ul>
<li>ä¸¤é˜¶æ®µ fine-tuningï¼šä¸¤é˜¶æ®µè¿ç§»çš„æ–¹æ³•åœ¨é¢„è®­ç»ƒå’Œ fine-tuning é˜¶æ®µå¼•å…¥äº†ä¸€ä¸ªä¸­é—´é˜¶æ®µã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œé€šè¿‡ä¸­é—´ä»»åŠ¡æˆ–è¯­æ–™æ¥å¾®è°ƒæ¨¡å‹ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œé€šè¿‡ç›®æ ‡ä»»åŠ¡å¾®è°ƒæ¨¡å‹ã€‚</li>
<li>å¤šä»»åŠ¡ fine-tuningï¼šliuç­‰äººåœ¨å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶ä¸‹å¯¹ BERT è¿›è¡Œäº†å¾®è°ƒï¼Œç»“æœæ˜¾ç¤ºå¤šä»»åŠ¡å­¦ä¹ å’Œé¢„è®­ç»ƒæ˜¯äº’è¡¥çš„æ–¹æ³•ã€‚</li>
<li>é‡‡ç”¨é¢å¤–çš„é€‚é…å™¨ fine-tuningï¼šfine-tuning çš„ä¸»è¦ç¼ºç‚¹æ˜¯å‚æ•°æ•ˆç‡ä½ï¼Œåœ¨æ¯ä¸€ä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸Šéƒ½æœ‰å„è‡ªçš„ dine-tuning å‚æ•°ã€‚å¯¹æ­¤çš„è§£å†³æ–¹æ¡ˆæ˜¯åœ¨å›ºå®šåŸå§‹å‚æ•°æ—¶å¼•å…¥ä¸€äº›å¯ä»¥ fine-tuning çš„é€‚é…å™¨ã€‚</li>
<li>å…¶ä»–ï¼šé€å±‚è§£å†»è€Œéè¿ç»­ fine-tune æ‰€æœ‰å±‚ï¼›self-ensemble å’Œ self-distillation</li>
</ul>
<h4 id="ä¸€äº›ptmsçš„èµ„æº"><strong>5.ä¸€äº›PTMsçš„èµ„æº</strong></h4>
<h5 id="ä¸€äº›å¼€æºçš„åº”ç”¨"><strong>ä¸€äº›å¼€æºçš„åº”ç”¨ï¼š</strong></h5>
<p><img src="https://i.loli.net/2020/10/28/jTUZBNqcrlm9hR2.png" alt="img"></p>
<p><strong>word2vec:</strong></p>
<p>https://github.com/tmikolov/word2vec</p>
<p><strong>GloVe:</strong></p>
<p>https://nlp.stanford.edu/projects/glove</p>
<p><strong>FastText:</strong></p>
<p>https://github.com/facebookresearch/fastText</p>
<p><strong>Transformers:</strong></p>
<p>https://github.com/huggingface/transformers</p>
<p><strong>Fairseq:</strong></p>
<p>https://github.com/pytorch/fairseq</p>
<p><strong>Flair:</strong></p>
<p>https://github.com/flairNLP/flair</p>
<p><strong>AllenNLP:</strong></p>
<p>https://github.com/allenai/allennlp</p>
<p><strong>FastNLP:</strong></p>
<p>https://github.com/fastnlp/fastNLP</p>
<p><strong>Chinese-BERT:</strong></p>
<p>https://github.com/ymcui/Chinese-BERT-wwm</p>
<p><strong>BERT:</strong></p>
<p>https://github.com/google-research/bert</p>
<p><strong>RoBERTa:</strong></p>
<p>https://github.com/pytorch/fairseq/tree/master/examples/roberta</p>
<p><strong>XLNet:</strong></p>
<p>https://github.com/zihangdai/xlnet/</p>
<p><strong>ALBERT:</strong></p>
<p>https://github.com/google-research/ALBERT</p>
<p><strong>T5:</strong></p>
<p>https://github.com/google-research/text-to-text-transfer-transformer</p>
<p><strong>ERNIE (Baidu):</strong></p>
<p>https://github.com/PaddlePaddle/ERNIE</p>
<p><strong>ç›¸å…³èµ„æºï¼š</strong></p>
<p><strong>è®ºæ–‡åˆ—è¡¨ï¼š</strong></p>
<p>https://github.com/thunlp/PLMpapers</p>
<p>https://github.com/tomohideshibata/BERT-related-papers</p>
<p>https://github.com/cedrickchee/awesome-bert-nlp</p>
<p><strong>BERT Lang Streetï¼ˆæ”¶é›† BERT åœ¨ä¸åŒæ•°æ®é›†å’Œä»»åŠ¡ä¸Šçš„è¡¨ç°ï¼‰ï¼š</strong></p>
<p>https://bertlang.unibocconi.it/</p>
<p><strong>BERTVizï¼ˆåº”ç”¨ transformer çš„æ¨¡å‹çš„æ³¨æ„åŠ›å¯è§†åŒ–ï¼‰ï¼š</strong></p>
<p>https://github.com/jessevig/bertviz</p>
<h4 id="åº”ç”¨"><strong>6.åº”ç”¨</strong></h4>
<h5 id="é€šç”¨è¯„ä¼°æ ‡å‡†"><strong>6.1 é€šç”¨è¯„ä¼°æ ‡å‡†</strong></h5>
<p>GLUE (The General Language Understanding Evaluation) æ ‡å‡†æ˜¯ä¸€ä¸ªé›†åˆäº† 9 ä¸ªè‡ªç„¶è¯­è¨€ç†è§£ä»»åŠ¡çš„æ ‡å‡†ã€‚</p>
<p>å…¶ä¸­åŒ…æ‹¬ï¼šå•ä¸ªå¥å­åˆ†ç±»ä»»åŠ¡ï¼ˆCoLAå’ŒSST-2ï¼‰ã€æ–‡æœ¬å¯¹åˆ†ç±»ä»»åŠ¡ï¼ˆMNLI, RTE, WNLI, QQP, MRPCï¼‰ã€æ–‡æœ¬ç›¸ä¼¼åº¦ä»»åŠ¡ï¼ˆSTSBï¼‰ã€ç›¸å…³æ€§æ’è¡Œä»»åŠ¡ï¼ˆQNLIï¼‰ã€‚GLUE æ ‡å‡†èƒ½å¤Ÿèƒ½å¤Ÿå¾ˆå¥½åœ°è¯„ä¼°æ¨¡å‹çš„é²æ£’æ€§å’Œé€šç”¨æ€§ã€‚</p>
<p>è€Œè¿‘æœŸ NLP çš„å¿«é€Ÿå‘å±•ä¿ƒä½¿äº†æ–°çš„æ ‡å‡† SuperGLUE çš„æå‡ºï¼Œç›¸æ¯” GLUEï¼ŒSuperGLUE æœ‰æ›´å¤šå¯Œæœ‰æŒ‘æˆ˜æ€§ä¸”å¤šç§å¤šæ ·çš„ä»»åŠ¡ï¼Œå¦‚æŒ‡ä»£æ¶ˆè§£å’Œ QAã€‚</p>
<h3 id="section-5"></h3>
<h5 id="æœºå™¨ç¿»è¯‘-1"><strong>6.2 æœºå™¨ç¿»è¯‘</strong></h5>
<p>æœºå™¨ç¿»è¯‘ï¼ˆMachine Translation, MTï¼‰ä¹Ÿæ˜¯ NLP çš„ä¸€é¡¹é‡è¦ä»»åŠ¡ã€‚å‡ ä¹æ‰€æœ‰ MT æ¨¡å‹éƒ½ä½¿ç”¨äº† encoder-decoder æ¡†æ¶ã€‚è€Œè¿‘æœŸéšé¢„è®­ç»ƒæ¨¡å‹çš„å‘å±•ï¼Œä¹Ÿæœ‰ä¸å°‘å°è¯•å°† BERT ä¹‹ç±»çš„é¢„è®­ç»ƒæ¨¡å‹ç”¨äºåˆå§‹åŒ– encoderï¼Œå–å¾—äº†ä¸€å®šæˆæ•ˆã€‚</p>
<h3 id="section-6"></h3>
<h5 id="é—®ç­”ç³»ç»Ÿ"><strong>6.3 é—®ç­”ç³»ç»Ÿ</strong></h5>
<p>é—®ç­”ç³»ç»Ÿï¼ˆQuestion answering, QAï¼‰æˆ–æ˜¯ç‹­ä¹‰æ¦‚å¿µçš„æœºå™¨é˜…è¯»ç†è§£ï¼ˆmachine reading comprehension, MRCï¼‰ä¹Ÿæ˜¯ NLP çš„é‡è¦ä»»åŠ¡ã€‚</p>
<p>ä»æ˜“åˆ°éš¾ï¼Œæœ‰ä¸‰ç§ç±»å‹çš„ QA ä»»åŠ¡ï¼šå•å›åˆæå– QA (single-round extractive QA, SQuAD)ã€å¤šå›åˆç”ŸæˆQA (multi-round generative QA, CoQA)ã€å¤šè·³é—®ç­” (multi-hop QA, HotpotQA)ã€‚</p>
<p>é’ˆå¯¹æå– QAï¼Œæœ‰é€šè¿‡ PTM åˆå§‹åŒ– encoder çš„å›æº¯é˜…è¯»æ¶æ„ï¼ˆretrospective reader architectureï¼‰ï¼›é’ˆå¯¹å¤šå›åˆç”Ÿæˆ QAï¼Œæœ‰â€œPTM+Adversarial Training+Rationale Tagging+Knowledge Distillationâ€æ¶æ„ï¼›é’ˆå¯¹å¤šè·³ QAï¼Œæœ‰â€œSelect, Answer, and Explainâ€ (SAE) ç³»ç»Ÿã€‚</p>
<h5 id="æƒ…æ„Ÿåˆ†æ"><strong>6.4 æƒ…æ„Ÿåˆ†æ</strong></h5>
<p>BERT é€šè¿‡åœ¨å¹¿æ³›ä½¿ç”¨çš„æƒ…æ„Ÿåˆ†ææ•°æ®é›† SST-2 ä¸Šè¿›è¡Œå¾®è°ƒåï¼Œè¡¨ç°è¶…è¿‡äº†å…ˆå‰çš„ SOTA æ¨¡å‹ã€‚è€Œååˆæœ‰å¾ˆå¤šå°† BERT è¿›è¡Œè°ƒæ•´ä»¥åº”ç”¨åœ¨ aspect çº§çš„æƒ…æ„Ÿåˆ†æï¼ˆABSAï¼‰ä»»åŠ¡ä¸Šã€‚</p>
<h5 id="æ€»ç»“-1"><strong>6.5 æ€»ç»“</strong></h5>
<p>ä»é•¿æ–‡æœ¬ä¸­æ€»ç»“å‡ºçŸ­æ–‡æœ¬ä¹Ÿæ˜¯è¿‘æœŸ NLP çš„çƒ­ç‚¹ã€‚ä¹Ÿæœ‰å¾ˆå¤šå°è¯•å°† PTM åº”ç”¨åœ¨æ€»ç»“æ–‡æœ¬ä»»åŠ¡ä¸Šï¼Œå¦‚å°† BERT é€šè¿‡æ’å…¥ [CLS] token æ¥å­¦ä¹ å¥å­è¡¨ç¤ºçš„æ¨¡å‹ BERTSUMã€‚</p>
<h5 id="å‘½åå®ä½“è¯†åˆ«"><strong>6.6 å‘½åå®ä½“è¯†åˆ«</strong></h5>
<p>å‘½åå®ä½“è¯†åˆ«ï¼ˆNamed Entity Recognition, NERï¼‰ä¹Ÿæ˜¯çŸ¥è¯†æå–çš„ä¸€ä¸ªåŸºç¡€ä»»åŠ¡ï¼Œåœ¨å¾ˆå¤š NLP ä»»åŠ¡ä¸Šéƒ½æœ‰é‡è¦ä½œç”¨ã€‚TagLM å’Œ ELMo åˆ©ç”¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„æœ€åä¸€å±‚çš„è¾“å…¥å’Œå„å±‚çš„åŠ æƒæ€»å’Œä½œä¸ºè¯åµŒå…¥çš„ä¸€éƒ¨åˆ†ã€‚</p>
<h4 id="æœªæ¥æ–¹å‘"><strong>7.æœªæ¥æ–¹å‘</strong></h4>
<h5 id="ptmsçš„ä¸Šç•Œ"><strong>7.1 PTMsçš„ä¸Šç•Œ</strong></h5>
<p>éš BERT çš„å‡ºç°ï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°ï¼Œå¾ˆå¤šæ¨¡å‹éƒ½å¯ä»¥é€šè¿‡æ›´é•¿çš„è®­ç»ƒæ­¥é•¿ä¸åœ¨å’Œæ›´å¤§çš„è¯­æ–™æ¥æå‡æ€§èƒ½ï¼Œæ¯”å¦‚å»å¹´çš„ T5 ä½¿ç”¨çš„ C4 æ•°æ®é›†ã€‚è€Œæˆ‘ä»¬ä¹Ÿå¯ä»¥é€šè¿‡åŠ æ·±æ¨¡å‹æ¥æå‡æ€§èƒ½ï¼Œæ¯”å¦‚ Turing-NLG ä½¿ç”¨äº† 72 ä¸ª transformer å±‚ã€‚</p>
<p>PTMs çš„å…±åŒç›®æ ‡éƒ½æ˜¯å­¦ä¹ è¯­è¨€çš„æœ¬è´¨é€šç”¨çŸ¥è¯†(æˆ–è€…è¯´æ˜¯ä¸–ç•Œçš„çŸ¥è¯†)ï¼Œç„¶è€Œï¼Œéšç€æ¨¡å‹çš„ä¸æ–­åŠ æ·±ï¼Œè¯­æ–™çš„ä¸æ–­å¢å¤§ï¼Œè®­ç»ƒæ¨¡å‹çš„èŠ±é”€ä¹Ÿè¶Šæ¥è¶Šå¤§ã€‚ä¸€ç§æ›´å¯è¡Œçš„è§£å†³æ–¹æ¡ˆæ˜¯è®¾è®¡æ›´æœ‰æ•ˆçš„æ¨¡å‹æ¶æ„ã€è‡ªç›‘ç£é¢„è®­ç»ƒä»»åŠ¡ã€ä¼˜åŒ–å™¨å’Œè½¯ç¡¬ä»¶æ–¹é¢çš„æŠ€å·§ç­‰ã€‚ELECTRA å°±æ˜¯è¿™ä¸ªæ–¹å‘ä¸Šä¸€ä¸ªå¾ˆå¥½çš„å°è¯•ã€‚</p>
<h5 id="é¢å‘ä»»åŠ¡çš„é¢„è®­ç»ƒä¸æ¨¡å‹å‹ç¼©"><strong>7.2 é¢å‘ä»»åŠ¡çš„é¢„è®­ç»ƒä¸æ¨¡å‹å‹ç¼©</strong></h5>
<p>åœ¨å®è·µä¸­ï¼Œä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡è¦æ±‚ PTMs æ‹¥æœ‰ä¸åŒçš„åŠŸèƒ½ã€‚è€Œ PTMs ä¸ä¸‹æ¸¸ç›®æ ‡ä»»åŠ¡é—´çš„å·®å¼‚é€šå¸¸è¡¨ç°åœ¨ä¸¤æ–¹é¢ï¼šæ¨¡å‹æ¶æ„ä¸æ•°æ®åˆ†å¸ƒã€‚è¾ƒå¤§çš„ PTMs é€šå¸¸æƒ…å†µä¸‹ä¼šæœ‰æ›´å¥½çš„æ€§èƒ½ï¼Œä½†å®é™…é—®é¢˜æ˜¯å¦‚ä½•åœ¨ä½å®¹é‡è®¾å¤‡å’Œä½æ—¶å»¶åº”ç”¨ä¸Šä½¿ç”¨å¦‚æ­¤åºå¤§çš„ PTMã€‚</p>
<p>é™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ¨¡å‹å‹ç¼©æ¥å°†é€šç”¨ PTMs æ•™ç»™é¢å‘å¯¹è±¡çš„ PTMã€‚å°½ç®¡ CV ä¸­å¯¹ CNNs çš„å‹ç¼©å·²ç»éå¸¸æˆç†Ÿï¼Œä½† Tansformer çš„å…¨è¿æ¥ç»“æ„ä½¿å¾—æ¨¡å‹å‹ç¼©éå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</p>
<h5 id="ptmsæ¶æ„"><strong>7.3 PTMsæ¶æ„</strong></h5>
<p>Transformer æ˜¯ PTMs çš„ä¸€ä¸ªé«˜æ•ˆçš„æ¡†æ¶ï¼Œä½† Transformer çš„å±€é™åœ¨äºè®¡ç®—å¤æ‚åº¦ã€‚ç”±äº GPU æ˜¾å­˜å¤§å°çš„é™åˆ¶ï¼Œç›®å‰å¤§å¤šæ•° PTM æ— æ³•å¤„ç†åºåˆ—é•¿åº¦è¶…è¿‡ 512 ä¸ª token çš„åºåˆ—ã€‚æ­é…è¿™ä¸€é™åˆ¶éœ€è¦æ”¹è¿› Transformer çš„ç»“æ„ï¼Œå¦‚ Transformer-XLã€‚å› æ­¤ï¼Œå¯»æ±‚æ›´æœ‰æ•ˆçš„æ¨¡å‹æ¶æ„å¯¹äºè§£å†³é•¿ç¨‹æ–‡æœ¬ä¿¡æ¯ä¹Ÿæ˜¯å¾ˆé‡è¦çš„ã€‚</p>
<h5 id="fine-tunigä¸­çš„çŸ¥è¯†è¿ç§»"><strong>7.4 Fine-tunigä¸­çš„çŸ¥è¯†è¿ç§» </strong></h5>
<p>Fine-tuning æ˜¯ç›®å‰å°† PTM çš„çŸ¥è¯†è¿ç§»è‡³ä¸‹æ¸¸ä»»åŠ¡çš„ä¸»è¦æ–¹æ³•ï¼Œä½†å‚æ•°æ•ˆç‡å´å¾ˆä½ï¼Œæ¯ä¸ªä¸‹æ¸¸ä»»åŠ¡éƒ½æœ‰ç‰¹å®šçš„ fine-tuned å‚æ•°ã€‚</p>
<p>ä¸€ä¸ªå¯ä»¥æ”¹è¿›çš„è§£å†³æ–¹æ¡ˆæ˜¯å›ºå®š PTMs çš„åŸå§‹å‚æ•°ï¼Œå¹¶ä¸ºç‰¹å®šä»»åŠ¡æ·»åŠ å°å‹çš„å¯å¾®è°ƒçš„é€‚é…å™¨ï¼Œè¿™æ ·å°±å¯ä»¥åœ¨ä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡ä½¿ç”¨å…±äº«çš„ PTMsã€‚ä» PTMâ€˜s ä¸­æŒ–æ˜çŸ¥è¯†ä¹Ÿå¯ä»¥æ›´çµæ´»ï¼Œæ¯”å¦‚ï¼šçŸ¥è¯†æå–ã€çŸ¥è¯†è’¸é¦ã€æ•°æ®å¢åŠ ã€å°† PTMs ä½œä¸ºå¤–éƒ¨çŸ¥è¯†ç­‰ç­‰ã€‚</p>
<h5 id="ptmsçš„å¯è§£é‡Šæ€§ä¸å¯é æ€§"><strong>7.5 PTMsçš„å¯è§£é‡Šæ€§ä¸å¯é æ€§ </strong></h5>
<p>PTMs çš„æ·±ä¸”éçº¿æ€§çš„æ¶æ„ä½¿å¾—å†³ç­–åˆ¶å®šçš„è¿‡ç¨‹éå¸¸ä¸é€æ˜ã€‚è¿‘æœŸï¼Œå¯è§£é‡Šäººå·¥æ™ºèƒ½ï¼ˆexplainable artificial intelligence, XAIï¼‰æˆä¸ºçƒ­ç‚¹ã€‚é€šè¿‡å¯¹æ¨¡å‹è¯åµŒå…¥çš„ç ”ç©¶æˆ‘ä»¬å¯ä»¥åˆ†æ PTMs ä¸­çš„è¯­è¨€å’Œä¸–ç•ŒçŸ¥è¯†ï¼Œä½†æ›´å¤šæœ‰å…³æ³¨æ„åŠ›æœºåˆ¶çš„å¯è§£é‡Šæ€§çš„é—®é¢˜è¿˜å€¼å¾—æ¢è®¨ã€‚</p>
<p>PTMs è¿™ç§æ·±æ¨¡å‹å¾ˆå®¹æ˜“å—åˆ°å¯¹æŠ—æ ·æœ¬çš„æ‰°åŠ¨è€Œäº§ç”Ÿé”™è¯¯çš„é¢„æµ‹ã€‚åœ¨ CV é¢†åŸŸï¼Œå¯¹æŠ—æ”»å‡»ä¸é˜²å¾¡å·²ç»è¢«å¹¿æ³›å­¦ä¹ ï¼Œè€Œç”±äºè¯­è¨€çš„ç‰¹æ€§ï¼Œæ–‡æœ¬çš„å¯¹æŠ—è¿˜éå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ã€‚PTMs çš„å¯¹æŠ—é˜²å¾¡ä¹Ÿå¯¹äºæå‡ PTMs çš„é²æ£’æ€§å¾ˆé‡è¦ã€‚</p>
<h4 id="æ€»ç»“-2"><strong>8.æ€»ç»“</strong></h4>
<p>é‚±é”¡é¹è€å¸ˆçš„è¿™ç¯‡ç»¼è¿°å¾ˆå…¨é¢åœ°æ¦‚æ‹¬äº†é¢„è®­ç»ƒæ¨¡å‹ï¼Œä¹Ÿéå¸¸é€‚åˆåˆå­¦è€…å½“ä½œä¸€ä¸ª roadmap æ¥é˜…è¯»ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ° NLP çš„å‘å±•è¿‡ç¨‹æ˜¯éå¸¸ä»¤äººæ„ŸåŠ¨çš„ï¼Œä»æœ€å¼€å§‹çš„â€œè¦è¡¨ç¤ºè¯­è¨€â€çš„ç›®æ ‡ï¼Œä½¿ç”¨è¯è¢‹æ¨¡å‹å’Œ N-gramã€‚</p>
<p>å†æƒ³åˆ°â€œè¯è¯­å…·æœ‰å¤šä¹‰æ€§â€ï¼Œæ‰€ä»¥éœ€è¦æœ‰ä¸Šä¸‹æ–‡ï¼Œä½¿ç”¨ LSTMã€‚LSTM åªæœ‰å•å‘ï¼Œé‚£å°±ä½¿ç”¨åŒå‘ LSTMã€‚â€œæƒ³è¦æ›´å¤§èŒƒå›´çš„ä¸Šä¸‹æ–‡â€ï¼Œå°±äº§ç”Ÿäº† transformerã€‚</p>
<p>â€œå†å¤§ä¸€äº›â€ï¼Œæœ‰äº† transformer-XLã€‚è¿˜æ˜¯ä¸å¤Ÿå¥½ï¼Œæ€ä¹ˆåŠï¼Ÿâ€œæ›´å¤šçŸ¥è¯†â€ï¼Œäºæ˜¯ä¸æ–­åŠ å¤§è¯­æ–™åº“ï¼Œä¸æ–­å † GPUï¼Œç›´åˆ° T5 æ¢ç´¢äº†â€œLimits of Transfer Learning with a Uniï¬ed Text-to-Text Transformerâ€ã€‚</p>
<p>æ¨¡å‹å¤ªå¤§ï¼Œæˆæœ¬å¤ªé«˜ï¼Œé‚£å°±å‹ç¼©æ¨¡å‹ï¼Œæ”¹è¿›æ¡†æ¶ï¼Œäºæ˜¯æœ‰äº† ELECTRAã€‚é¢„è®­ç»ƒæ¨¡å‹ç¼ºä¹å°è¯•æ¨ç†èƒ½åŠ›ï¼Œé‚£å°±çŸ¥è¯†æå–ï¼Œäºæ˜¯æœ‰äº† COMETã€‚æ¯ä¸€æ­¥å°è¯•éƒ½æ˜¯åœ¨é è¿‘è¯­è¨€çš„æœ¬è´¨ä¸ä¸–ç•Œçš„çŸ¥è¯†ã€‚</p>
<p><em>â€œThe whole of science is nothing more than a refinement of everyday thinking.â€</em></p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>
    </div>

    
    
    

      <div style="text-align:center;color: #ccc;font-size:14px;">-------------æœ¬æ–‡ç»“æŸ<i class="fab fa-gratipay"></i>æ„Ÿè°¢é˜…è¯»-------------</div>
        <div class="reward-container">
  <div>å‘å¾®åšä¸»ï¼Œåœ¨çº¿æ±‚èµ</div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    æ‰“èµ
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.jpg" alt="ææ€å»º å¾®ä¿¡æ”¯ä»˜">
        <p>å¾®ä¿¡æ”¯ä»˜</p>
      </div>

  </div>
</div>

    
      
   
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/10/26/2020-10-26-BERT%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%8F%8A%E8%AF%A6%E8%A7%A3/" rel="prev" title="2020-10-26-BERTè®ºæ–‡é˜…è¯»åŠè¯¦è§£">
      <i class="fa fa-chevron-left"></i> 2020-10-26-BERTè®ºæ–‡é˜…è¯»åŠè¯¦è§£
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/10/29/2020-10-29-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%B0%83%E5%8F%82%E6%8A%80%E5%B7%A7%E6%B1%87%E6%80%BB/" rel="next" title="2020-10-29-æ·±åº¦å­¦ä¹ è°ƒå‚æŠ€å·§æ±‡æ€»">
      2020-10-29-æ·±åº¦å­¦ä¹ è°ƒå‚æŠ€å·§æ±‡æ€» <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  





          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          æ–‡ç« ç›®å½•
        </li>
        <li class="sidebar-nav-overview">
          ç«™ç‚¹æ¦‚è§ˆ
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#transformerå®¶æ—1----transformerè¯¦è§£å’Œæºç åˆ†æ"><span class="nav-number">1.</span> <span class="nav-text">ğŸš€Transformerå®¶æ—1 -- Transformerè¯¦è§£å’Œæºç åˆ†æ</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#transformeræ€»ä½“ç»“æ„"><span class="nav-number">1.1.</span> <span class="nav-text">1 Transformeræ€»ä½“ç»“æ„</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#encoder"><span class="nav-number">1.2.</span> <span class="nav-text">2 encoder</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#encoderå®šä¹‰"><span class="nav-number">1.2.1.</span> <span class="nav-text">2.1 encoderå®šä¹‰</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#è¾“å…¥å±‚embedding"><span class="nav-number">1.2.2.</span> <span class="nav-text">2.2 è¾“å…¥å±‚embedding</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#token-embedding"><span class="nav-number">1.2.2.1.</span> <span class="nav-text">2.2.1 token embedding</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#position-encoding"><span class="nav-number">1.2.2.2.</span> <span class="nav-text">2.2.2 position encoding</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#ç¼–ç å±‚"><span class="nav-number">1.2.3.</span> <span class="nav-text">2.3 ç¼–ç å±‚</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#multiheadedattention"><span class="nav-number">1.2.3.1.</span> <span class="nav-text">2.3.1 MultiHeadedAttention</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#positionwisefeedforward"><span class="nav-number">1.2.3.2.</span> <span class="nav-text">2.3.2 PositionwiseFeedForward</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#sublayerconnection"><span class="nav-number">1.2.3.3.</span> <span class="nav-text">2.3.3 SublayerConnection</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#decoder"><span class="nav-number">1.3.</span> <span class="nav-text">3 decoder</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#è¾“å‡ºå±‚"><span class="nav-number">1.4.</span> <span class="nav-text">4 è¾“å‡ºå±‚</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#æ€»ç»“"><span class="nav-number">1.5.</span> <span class="nav-text">5 æ€»ç»“</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#transformerå®¶æ—2----ç¼–ç é•¿åº¦ä¼˜åŒ–transformer-xllongformer"><span class="nav-number">2.</span> <span class="nav-text">ğŸš€Transformerå®¶æ—2 -- ç¼–ç é•¿åº¦ä¼˜åŒ–ï¼ˆTransformer-XLã€Longformerï¼‰</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#èƒŒæ™¯"><span class="nav-number">2.1.</span> <span class="nav-text">1 èƒŒæ™¯</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#transformer-xl"><span class="nav-number">2.2.</span> <span class="nav-text">2 Transformer-XL</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#ä¸ºä»€ä¹ˆéœ€è¦transformer-xl"><span class="nav-number">2.2.1.</span> <span class="nav-text">2.1 ä¸ºä»€ä¹ˆéœ€è¦Transformer-XL</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#å®ç°æ–¹æ³•"><span class="nav-number">2.2.2.</span> <span class="nav-text">2.2 å®ç°æ–¹æ³•</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#segment-level-recurrence-with-state-reuse-ç‰‡æ®µçº§é€’å½’å’Œä¿¡æ¯å¤ç”¨"><span class="nav-number">2.2.2.1.</span> <span class="nav-text">2.2.1 Segment-Level Recurrence with State Reuse ç‰‡æ®µçº§é€’å½’å’Œä¿¡æ¯å¤ç”¨</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#relative-positional-encodings-ç›¸å¯¹ä½ç½®ç¼–ç "><span class="nav-number">2.2.2.2.</span> <span class="nav-text">2.2.2 Relative Positional Encodings ç›¸å¯¹ä½ç½®ç¼–ç </span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#å®éªŒç»“æœ"><span class="nav-number">2.2.3.</span> <span class="nav-text">2.3 å®éªŒç»“æœ</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#é•¿æ–‡æœ¬ç¼–ç æ•ˆæœ"><span class="nav-number">2.2.3.1.</span> <span class="nav-text">é•¿æ–‡æœ¬ç¼–ç æ•ˆæœ</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#æœ‰æ•ˆç¼–ç é•¿åº¦"><span class="nav-number">2.2.3.2.</span> <span class="nav-text">æœ‰æ•ˆç¼–ç é•¿åº¦</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#é¢„æµ‹é€Ÿåº¦"><span class="nav-number">2.2.3.3.</span> <span class="nav-text">é¢„æµ‹é€Ÿåº¦</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#æ¶ˆèåˆ†æ"><span class="nav-number">2.2.3.4.</span> <span class="nav-text">æ¶ˆèåˆ†æ</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#longformer"><span class="nav-number">2.3.</span> <span class="nav-text">3 Longformer</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#æ”¹è¿›æ–¹æ³•"><span class="nav-number">2.3.1.</span> <span class="nav-text">3.1 æ”¹è¿›æ–¹æ³•</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#attentionç¨€ç–åŒ–"><span class="nav-number">2.3.1.1.</span> <span class="nav-text">3.1.1 attentionç¨€ç–åŒ–</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#tensor-virtual-machine-tvm"><span class="nav-number">2.3.1.2.</span> <span class="nav-text">3.1.2 Tensor Virtual Machine (TVM)</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#å®éªŒç»“æœ-1"><span class="nav-number">2.3.2.</span> <span class="nav-text">3.2 å®éªŒç»“æœ</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#å¤§å°æ¨¡å‹æ•ˆæœ"><span class="nav-number">2.3.2.1.</span> <span class="nav-text">å¤§å°æ¨¡å‹æ•ˆæœ</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#æ¶ˆèåˆ†æ-1"><span class="nav-number">2.3.2.2.</span> <span class="nav-text">æ¶ˆèåˆ†æ</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#è¯­æ–™é•¿åº¦"><span class="nav-number">2.3.2.3.</span> <span class="nav-text">è¯­æ–™é•¿åº¦</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#ä¸‹æ¸¸ä»»åŠ¡finetuneæ•ˆæœ"><span class="nav-number">2.3.2.4.</span> <span class="nav-text">ä¸‹æ¸¸ä»»åŠ¡finetuneæ•ˆæœ</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#transformerå®¶æ—3----è®¡ç®—æ•ˆç‡ä¼˜åŒ–adaptive-spanreformerlite-transformer"><span class="nav-number">3.</span> <span class="nav-text">ğŸš€Transformerå®¶æ—3 -- è®¡ç®—æ•ˆç‡ä¼˜åŒ–ï¼ˆAdaptive-Spanã€Reformerã€Lite-Transformerï¼‰</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#èƒŒæ™¯-1"><span class="nav-number">3.1.</span> <span class="nav-text">1 èƒŒæ™¯</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#adaptive-span-transformer"><span class="nav-number">3.2.</span> <span class="nav-text">2 Adaptive-Span Transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#ä¸ºä»€ä¹ˆéœ€è¦adaptive-span"><span class="nav-number">3.2.1.</span> <span class="nav-text">2.1 ä¸ºä»€ä¹ˆéœ€è¦Adaptive-Span</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#å®ç°æ–¹æ¡ˆ"><span class="nav-number">3.2.2.</span> <span class="nav-text">2.2 å®ç°æ–¹æ¡ˆ</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#å®éªŒç»“æœ-2"><span class="nav-number">3.2.3.</span> <span class="nav-text">2.3 å®éªŒç»“æœ</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#reformer"><span class="nav-number">3.3.</span> <span class="nav-text">3 Reformer</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#ä¸ºä»€ä¹ˆéœ€è¦reformer"><span class="nav-number">3.3.1.</span> <span class="nav-text">3.1 ä¸ºä»€ä¹ˆéœ€è¦Reformer</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#å®ç°æ–¹æ¡ˆ-1"><span class="nav-number">3.3.2.</span> <span class="nav-text">3.2 å®ç°æ–¹æ¡ˆ</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#locality-sensitive-hashing-å±€éƒ¨æ•æ„Ÿhash"><span class="nav-number">3.3.2.1.</span> <span class="nav-text">3.2.1 LOCALITY-SENSITIVE HASHING å±€éƒ¨æ•æ„Ÿhash</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#queryå’Œkeyå¿…é¡»ç”¨ä¸¤å¥—å—"><span class="nav-number">3.3.2.2.</span> <span class="nav-text">Queryå’ŒKeyå¿…é¡»ç”¨ä¸¤å¥—å—</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#å¿…é¡»å’Œæ¯ä¸ªtokenè®¡ç®—ç›¸å…³æ€§å—"><span class="nav-number">3.3.2.3.</span> <span class="nav-text">å¿…é¡»å’Œæ¯ä¸ªtokenè®¡ç®—ç›¸å…³æ€§å—</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#æ€ä¹ˆæ‰¾åˆ°ç›¸å…³æ€§å¤§çš„å‘é‡å‘¢"><span class="nav-number">3.3.2.4.</span> <span class="nav-text">æ€ä¹ˆæ‰¾åˆ°ç›¸å…³æ€§å¤§çš„å‘é‡å‘¢</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#æ•´ä¸ªæµç¨‹"><span class="nav-number">3.3.2.5.</span> <span class="nav-text">æ•´ä¸ªæµç¨‹</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#å¤šè½®lsh"><span class="nav-number">3.3.2.6.</span> <span class="nav-text">å¤šè½®LSH</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#reversible-transformer-å¯é€†transformer"><span class="nav-number">3.3.2.7.</span> <span class="nav-text">3.2.2 REVERSIBLE TRANSFORMER å¯é€†Transformer</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#feed-forward-chunking-ffå±‚åˆ†å—"><span class="nav-number">3.3.2.8.</span> <span class="nav-text">3.2.3 Feed-Forward chunking FFå±‚åˆ†å—</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#å®éªŒç»“æœ-3"><span class="nav-number">3.3.3.</span> <span class="nav-text">3.3 å®éªŒç»“æœ</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#å†…å­˜å’Œæ—¶é—´å¤æ‚åº¦"><span class="nav-number">3.3.3.1.</span> <span class="nav-text">å†…å­˜å’Œæ—¶é—´å¤æ‚åº¦</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#æ¨¡å‹æ•ˆæœ"><span class="nav-number">3.3.3.2.</span> <span class="nav-text">æ¨¡å‹æ•ˆæœ</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#lite-transformer"><span class="nav-number">3.4.</span> <span class="nav-text">4 Lite Transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#ä¸ºä»€ä¹ˆè¦åšlite-transformer"><span class="nav-number">3.4.1.</span> <span class="nav-text">4.1 ä¸ºä»€ä¹ˆè¦åšLite Transformer</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#å®ç°æ–¹æ¡ˆ-2"><span class="nav-number">3.4.2.</span> <span class="nav-text">4.2 å®ç°æ–¹æ¡ˆ</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#å®éªŒç»“æœ-4"><span class="nav-number">3.4.3.</span> <span class="nav-text">4.3 å®éªŒç»“æœ</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#è®¡ç®—å¤æ‚åº¦"><span class="nav-number">3.4.3.1.</span> <span class="nav-text">è®¡ç®—å¤æ‚åº¦</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#æ¨¡å‹ä½“ç§¯"><span class="nav-number">3.4.3.2.</span> <span class="nav-text">æ¨¡å‹ä½“ç§¯</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#å…¶ä»–"><span class="nav-number">3.5.</span> <span class="nav-text">5 å…¶ä»–</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#transformerå®¶æ—4----é€šç”¨æ€§ä¼˜åŒ–universal-transformer"><span class="nav-number">4.</span> <span class="nav-text">ğŸš€Transformerå®¶æ—4 -- é€šç”¨æ€§ä¼˜åŒ–ï¼ˆUniversal-Transformerï¼‰</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#èƒŒæ™¯-2"><span class="nav-number">4.1.</span> <span class="nav-text">1 èƒŒæ™¯</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#universal-transformer"><span class="nav-number">4.2.</span> <span class="nav-text">2 Universal-Transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#ä¸ºä»€ä¹ˆéœ€è¦universal-transformer"><span class="nav-number">4.2.1.</span> <span class="nav-text">2.1 ä¸ºä»€ä¹ˆéœ€è¦Universal-Transformer</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#å®ç°æ–¹æ¡ˆ-3"><span class="nav-number">4.2.2.</span> <span class="nav-text">2.2 å®ç°æ–¹æ¡ˆ</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#æ¨¡å‹ç»“æ„"><span class="nav-number">4.2.2.1.</span> <span class="nav-text">æ¨¡å‹ç»“æ„</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#adaptive-computation-timeact-è‡ªé€‚åº”è®¡ç®—æ—¶é—´"><span class="nav-number">4.2.2.2.</span> <span class="nav-text">Adaptive Computation Timeï¼ˆACTï¼‰ è‡ªé€‚åº”è®¡ç®—æ—¶é—´</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#å®éªŒç»“æœ-5"><span class="nav-number">4.2.3.</span> <span class="nav-text">2.3 å®éªŒç»“æœ</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#å­—ç¬¦ä¸²ä»»åŠ¡"><span class="nav-number">4.2.3.1.</span> <span class="nav-text">å­—ç¬¦ä¸²ä»»åŠ¡</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#æœºå™¨ç¿»è¯‘"><span class="nav-number">4.2.3.2.</span> <span class="nav-text">æœºå™¨ç¿»è¯‘</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#transformerå®¶æ—5----æ¨ç†åŠ é€Ÿfaster-transformer-turbotransformers"><span class="nav-number">5.</span> <span class="nav-text">ğŸš€Transformerå®¶æ—5 -- æ¨ç†åŠ é€Ÿï¼ˆFaster-Transformer ã€TurboTransformersï¼‰</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#èƒŒæ™¯-3"><span class="nav-number">5.1.</span> <span class="nav-text">1 èƒŒæ™¯</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#faster-transformer"><span class="nav-number">5.2.</span> <span class="nav-text">2 Faster-Transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#å®ç°æ–¹æ¡ˆ-4"><span class="nav-number">5.2.1.</span> <span class="nav-text">å®ç°æ–¹æ¡ˆ</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#æ•ˆæœ"><span class="nav-number">5.2.2.</span> <span class="nav-text">æ•ˆæœ</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#turbotransformers"><span class="nav-number">5.3.</span> <span class="nav-text">3 TurboTransformers</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#å®ç°æ–¹æ¡ˆ-5"><span class="nav-number">5.3.1.</span> <span class="nav-text">å®ç°æ–¹æ¡ˆ</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#å’Œå…¶ä»–æ–¹æ¡ˆçš„å¯¹æ¯”"><span class="nav-number">5.3.2.</span> <span class="nav-text">å’Œå…¶ä»–æ–¹æ¡ˆçš„å¯¹æ¯”</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#æ•ˆæœ-1"><span class="nav-number">5.3.3.</span> <span class="nav-text">æ•ˆæœ</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#é‚±é”¡é¹æ•™æˆnlpé¢„è®­ç»ƒæ¨¡å‹ç»¼è¿°"><span class="nav-number">6.</span> <span class="nav-text">ğŸš€é‚±é”¡é¹æ•™æˆï¼šNLPé¢„è®­ç»ƒæ¨¡å‹ç»¼è¿°</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#å¼•è¨€"><span class="nav-number">6.1.</span> <span class="nav-text">1.å¼•è¨€</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#èƒŒæ™¯-4"><span class="nav-number">6.2.</span> <span class="nav-text">2.èƒŒæ™¯</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#è¯­è¨€è¡¨ç¤ºå­¦ä¹ "><span class="nav-number">6.2.1.</span> <span class="nav-text">2.1 è¯­è¨€è¡¨ç¤ºå­¦ä¹ </span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#section"><span class="nav-number">7.</span> <span class="nav-text"></span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#ç¥ç»ä¸Šä¸‹æ–‡ç¼–ç å™¨"><span class="nav-number">7.0.1.</span> <span class="nav-text">2.2 ç¥ç»ä¸Šä¸‹æ–‡ç¼–ç å™¨</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#ä¸ºä»€ä¹ˆè¦é¢„è®­ç»ƒ"><span class="nav-number">7.0.2.</span> <span class="nav-text">2.3 ä¸ºä»€ä¹ˆè¦é¢„è®­ç»ƒï¼Ÿ</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ptmsæ¦‚è¿°"><span class="nav-number">7.1.</span> <span class="nav-text">3.PTMsæ¦‚è¿°</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#é¢„è®­ç»ƒä»»åŠ¡"><span class="nav-number">7.1.1.</span> <span class="nav-text">3.1 é¢„è®­ç»ƒä»»åŠ¡</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#section-1"><span class="nav-number">7.2.</span> <span class="nav-text"></span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#è¯­è¨€æ¨¡å‹-lm"><span class="nav-number">7.2.0.1.</span> <span class="nav-text">3.1.1 è¯­è¨€æ¨¡å‹ (LM)</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#å»å™ªå£°è‡ªç¼–ç å™¨-denoising-autoencoder-dae"><span class="nav-number">7.2.0.2.</span> <span class="nav-text">3.1.2 å»å™ªå£°è‡ªç¼–ç å™¨ (Denoising Autoencoder, DAE)</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#æ’åˆ—è¯­è¨€æ¨¡å‹plm"><span class="nav-number">7.2.0.3.</span> <span class="nav-text">3.1.3 æ’åˆ—è¯­è¨€æ¨¡å‹ï¼ˆPLMï¼‰</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#å¯¹æ¯”å­¦ä¹ ctl"><span class="nav-number">7.2.0.4.</span> <span class="nav-text">3.1.4 å¯¹æ¯”å­¦ä¹ ï¼ˆCTLï¼‰</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#deep-infomax-dim"><span class="nav-number">7.2.0.5.</span> <span class="nav-text">3.1.5 Deep InfoMax (DIM)</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#replaced-token-detection-rtd"><span class="nav-number">7.2.0.6.</span> <span class="nav-text">3.1.6 Replaced Token Detection (RTD)</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#section-2"><span class="nav-number">7.2.1.</span> <span class="nav-text"></span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#next-sentence-prediction-nsp"><span class="nav-number">7.2.1.1.</span> <span class="nav-text">3.1.7 Next Sentence Prediction (NSP)</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#sentence-order-prediction-sop"><span class="nav-number">7.2.1.2.</span> <span class="nav-text">3.1.8 Sentence Order Prediction (SOP)</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#section-3"><span class="nav-number">8.</span> <span class="nav-text"></span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#ptmsçš„æ‹“å±•"><span class="nav-number">8.0.1.</span> <span class="nav-text">3.2 PTMsçš„æ‹“å±•</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#å¼•å…¥çŸ¥è¯†çš„ptms"><span class="nav-number">8.0.1.1.</span> <span class="nav-text">3.2.1 å¼•å…¥çŸ¥è¯†çš„PTMs</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#å¤šæ¨¡æ€ptms"><span class="nav-number">8.0.1.2.</span> <span class="nav-text">3.2.2 å¤šæ¨¡æ€PTMs</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#section-4"><span class="nav-number">8.1.</span> <span class="nav-text"></span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#é¢†åŸŸé¢„è®­ç»ƒptms"><span class="nav-number">8.1.0.1.</span> <span class="nav-text">3.2.3 é¢†åŸŸé¢„è®­ç»ƒPTMs </span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#å¤šè¯­è¨€ä¸ç‰¹å®šè¯­è¨€ptms"><span class="nav-number">8.1.0.2.</span> <span class="nav-text">3.2.4 å¤šè¯­è¨€ä¸ç‰¹å®šè¯­è¨€PTMs </span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#å¦‚ä½•å‹ç¼©ptms"><span class="nav-number">8.1.1.</span> <span class="nav-text">3.3 å¦‚ä½•å‹ç¼©PTMs</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#å¦‚ä½•å°†ptmsåº”ç”¨è‡³ä¸‹æ¸¸ä»»åŠ¡"><span class="nav-number">8.2.</span> <span class="nav-text">4.å¦‚ä½•å°†PTMsåº”ç”¨è‡³ä¸‹æ¸¸ä»»åŠ¡</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#è¿ç§»å­¦ä¹ "><span class="nav-number">8.2.1.</span> <span class="nav-text">4.1 è¿ç§»å­¦ä¹ </span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#fine-tuningçš„ç­–ç•¥"><span class="nav-number">8.2.2.</span> <span class="nav-text">4.2 fine-tuningçš„ç­–ç•¥</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ä¸€äº›ptmsçš„èµ„æº"><span class="nav-number">8.3.</span> <span class="nav-text">5.ä¸€äº›PTMsçš„èµ„æº</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#ä¸€äº›å¼€æºçš„åº”ç”¨"><span class="nav-number">8.3.1.</span> <span class="nav-text">ä¸€äº›å¼€æºçš„åº”ç”¨ï¼š</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#åº”ç”¨"><span class="nav-number">8.4.</span> <span class="nav-text">6.åº”ç”¨</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#é€šç”¨è¯„ä¼°æ ‡å‡†"><span class="nav-number">8.4.1.</span> <span class="nav-text">6.1 é€šç”¨è¯„ä¼°æ ‡å‡†</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#section-5"><span class="nav-number">9.</span> <span class="nav-text"></span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#æœºå™¨ç¿»è¯‘-1"><span class="nav-number">9.0.1.</span> <span class="nav-text">6.2 æœºå™¨ç¿»è¯‘</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#section-6"><span class="nav-number">10.</span> <span class="nav-text"></span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#é—®ç­”ç³»ç»Ÿ"><span class="nav-number">10.0.1.</span> <span class="nav-text">6.3 é—®ç­”ç³»ç»Ÿ</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#æƒ…æ„Ÿåˆ†æ"><span class="nav-number">10.0.2.</span> <span class="nav-text">6.4 æƒ…æ„Ÿåˆ†æ</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#æ€»ç»“-1"><span class="nav-number">10.0.3.</span> <span class="nav-text">6.5 æ€»ç»“</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#å‘½åå®ä½“è¯†åˆ«"><span class="nav-number">10.0.4.</span> <span class="nav-text">6.6 å‘½åå®ä½“è¯†åˆ«</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#æœªæ¥æ–¹å‘"><span class="nav-number">10.1.</span> <span class="nav-text">7.æœªæ¥æ–¹å‘</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#ptmsçš„ä¸Šç•Œ"><span class="nav-number">10.1.1.</span> <span class="nav-text">7.1 PTMsçš„ä¸Šç•Œ</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#é¢å‘ä»»åŠ¡çš„é¢„è®­ç»ƒä¸æ¨¡å‹å‹ç¼©"><span class="nav-number">10.1.2.</span> <span class="nav-text">7.2 é¢å‘ä»»åŠ¡çš„é¢„è®­ç»ƒä¸æ¨¡å‹å‹ç¼©</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#ptmsæ¶æ„"><span class="nav-number">10.1.3.</span> <span class="nav-text">7.3 PTMsæ¶æ„</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#fine-tunigä¸­çš„çŸ¥è¯†è¿ç§»"><span class="nav-number">10.1.4.</span> <span class="nav-text">7.4 Fine-tunigä¸­çš„çŸ¥è¯†è¿ç§» </span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#ptmsçš„å¯è§£é‡Šæ€§ä¸å¯é æ€§"><span class="nav-number">10.1.5.</span> <span class="nav-text">7.5 PTMsçš„å¯è§£é‡Šæ€§ä¸å¯é æ€§ </span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#æ€»ç»“-2"><span class="nav-number">10.2.</span> <span class="nav-text">8.æ€»ç»“</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="ææ€å»º"
      src="/images/touxiang.jpg">
  <p class="site-author-name" itemprop="name">ææ€å»º</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">78</span>
          <span class="site-state-item-name">æ—¥å¿—</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">21</span>
        <span class="site-state-item-name">åˆ†ç±»</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">25</span>
        <span class="site-state-item-name">æ ‡ç­¾</span></a>
      </div>
  </nav>
</div>
  <div class="sidebar-button motion-element">
    <a onclick="tidioChatApi.open();"><i class="fa fa-comment"></i>
    Chat
  </a>
  </div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/OopsAaron" title="GitHub â†’ https:&#x2F;&#x2F;github.com&#x2F;OopsAaron" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:li1574468139@outlook.com" title="E-Mail â†’ mailto:li1574468139@outlook.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/u/5871697996" title="Weibo â†’ https:&#x2F;&#x2F;weibo.com&#x2F;u&#x2F;5871697996" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i>Weibo</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/AaronLe48891280" title="Twitter â†’ https:&#x2F;&#x2F;twitter.com&#x2F;AaronLe48891280" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://ife.baidu.com/" title="http:&#x2F;&#x2F;ife.baidu.com&#x2F;" rel="noopener" target="_blank">ç™¾åº¦å‰ç«¯æŠ€æœ¯å­¦é™¢</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://wf.uisdc.com/cn/" title="http:&#x2F;&#x2F;wf.uisdc.com&#x2F;cn&#x2F;" rel="noopener" target="_blank">googleå‰ç«¯å¼€å‘åŸºç¡€</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>

 


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fas fa-atom"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ææ€å»º</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="ç«™ç‚¹æ€»å­—æ•°">416k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="ç«™ç‚¹é˜…è¯»æ—¶é•¿">6:19</span>
</div>
  <div class="powered-by">ç”± <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> å¼ºåŠ›é©±åŠ¨
  </div>


        
<div class="busuanzi-count">
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="æ€»è®¿å®¢é‡">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="æ€»è®¿é—®é‡">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>
  
        
<script type="text/javascript" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"></script>



  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/pjax/pjax.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>

  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  




  
<script src="/js/local-search.js"></script>






  <script src="//code.tidio.co/6hx5cq5nmltu08pt7pfkqy85wvgnhr1r.js"></script>



<script data-pjax>
document.querySelectorAll('.pdfobject-container').forEach(element => {
  let url = element.dataset.target;
  let pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  let pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  let fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});
</script>




    <div id="pjax">
  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'V06xtKF2qonsMYpPA6OqiRH0-gzGzoHsz',
      appKey     : 'Q8gXivQlmWVnr48ahETG2v5X',
      placeholder: "Just go go",
      avatar     : 'monsterid',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

    </div>
  


</body>


</html>
