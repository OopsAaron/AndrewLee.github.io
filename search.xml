<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>2020-08-16-服务器检修</title>
    <url>/2020/08/16/2020-08-16-%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%A3%80%E4%BF%AE/</url>
    <content><![CDATA[<h3 id="前言">前言</h3>
<p>实验室的服务器总是出故障，于是和师兄一起考虑将实验室重装系统，并进行一系列操作。因为之前接触的少，这次是一个很好的实践机会，过程中记录笔记如下</p>
<h3 id="服务器检修">服务器检修</h3>
<h4 id="实验室的服务器">实验室的服务器</h4>
<p>实验室有三台机架式服务器</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>Dell poweredge R730</th>
<th>Dell poweredge R740</th>
<th>thinkserver rd650</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>系统</td>
<td>ubuntu 16.04</td>
<td>ubuntu 18.04</td>
<td>windows server 2008</td>
</tr>
</tbody>
</table>
<h4 id="制作ubuntu安装u盘">制作ubuntu安装U盘</h4>
<p>大部分内容参考自<a href="https://blog.csdn.net/zjx2014430/article/details/49303785" target="_blank" rel="noopener">使用UltraISO制作ubuntu安装u盘启动盘图文教程</a>，内容很详细，我的操作就是按照博客里的步骤</p>
<p>Ubuntu基于Debian发行版和GNOME桌面环境，在下载得到Ubuntu的光盘镜像后，可以选择刻盘引导安装或利用unetbootin工具用U盘引导安装。</p>
<p><strong>如何用u盘装ubuntu？</strong></p>
<p>先在网上下载<code>ubuntu16.04镜像</code> 和 <code>UltraISO软件</code></p>
<p>1、首先打开UltraISO软件，尽量下载最新版的，旧版可能会不能识别磁盘，安装失败!</p>
<p><img src="https://i.loli.net/2020/08/26/xim8TtON9MqyIYH.jpg" alt="点击查看大图" style="zoom:67%;"></p>
<p>2、点击工具栏中的第二个打开镜像文件工具，如图红色方框标志按钮，然后在打开的“打开ISO文件”对话框中找到我们下载好的Ubuntu镜像文件，之后点右下方的“打开”按钮</p>
<p><img src="https://i.loli.net/2020/08/26/hpUG4TbYoAHdDLQ.jpg" alt="点击查看大图" style="zoom:67%;"></p>
<p>3、打开镜像文件之后，在上方的列表中就会出现对打开的镜像文件的预览左边显示的是具体的目录，右边显示的目录和具体文件</p>
<p><img src="https://i.loli.net/2020/08/26/kzGMHAhBT1Y3tVg.jpg" alt="点击查看大图" style="zoom:67%;"></p>
<p>4、下面就开始制作启动盘了，点击菜单栏的“启动”，然后再弹出才按中选择“写入硬盘映像...”，打开“写入硬盘映像”对话框</p>
<p><img src="https://i.loli.net/2020/08/26/PuEeokUWv7qyhrd.jpg" alt="点击查看大图" style="zoom:67%;"></p>
<p>5、在写入硬盘映像对话框中，硬盘驱动器选择我们要写入的U盘，写入方式可以选择USB-HDD也可以选择USB-HDD+，两种方式小编都有尝试过，均可以</p>
<p><img src="https://i.loli.net/2020/08/26/28JSil4sBk9pCxO.jpg" alt="点击查看大图" style="zoom:67%;"></p>
<p>6、现在的这一步是非常关键的，关系到我们最后制作的硬盘映像能否启动电脑并安装系统，点击“便捷启动”，然后再弹出的菜单中依次选择“写入新的驱动器引导扇区”，再选择“Syslinux”，这一步的没有选择的话，那么我们最后制作的U盘映像将不能识别硬盘，不能安装系统</p>
<p><img src="https://i.loli.net/2020/08/26/HEQYiB7gqk4ep61.jpg" alt="点击查看大图" style="zoom:67%;"></p>
<p>7、在选择“Syslinux”后，会弹出如下图所示的提示框，毫无疑问，这里我们应该选择“是”</p>
<p><img src="https://i.loli.net/2020/08/26/tdT2jmYGVAMQhpN.jpg" alt="点击查看大图" style="zoom:67%;"></p>
<p>8、将Syslinux引导神曲写入设置的过程非常快，写入完成后，会弹出写入成功的提示框，若是没有写入成功，那么我们要重复上面的6、7步</p>
<p><img src="https://i.loli.net/2020/08/26/I8rhwpq3iVfCFsv.jpg" alt="点击查看大图" style="zoom:67%;"></p>
<p>9、现在就到了将ISO内的文件写入到U盘的时候了，点击下面的“写入”按钮，会弹出警告提示框，点击“是”就开始U盘安装盘的写入了</p>
<p><img src="https://i.loli.net/2020/08/26/HrdAoOFwvIg8SmP.jpg" alt="点击查看大图" style="zoom:67%;"></p>
<p>10、做完上面一些设置后，点击下面的“写入”按钮，这样就开始了U盘安装盘的制作过程，小编这里用的DVD的镜像文件，文件比较大，所以也比较耗时，在制作完成后，会自动关闭这个“写入硬盘映像”的对话框</p>
<p><img src="https://i.loli.net/2020/08/26/1DnsEjZBrzQtAc8.jpg" alt="点击查看大图" style="zoom:67%;"></p>
<p>11、制作完成，打开我的电脑，我们可以看到U盘的磁盘图标和名称都已经改变，其实这些信息是从我们的镜像文件中提取出来的</p>
<p>制作完成，现在安全弹出U盘，重启电脑，设置冲U盘启动就可以从U盘安装Ubuntu了，具体安装过程请看小编手续的经验文档</p>
<p><img src="https://i.loli.net/2020/08/26/OEzD8hnmBUrkj2J.jpg" alt="点击查看大图" style="zoom:67%;"></p>
<ol type="1">
<li><p>在进行U盘安装系统之前，我们还需要设置BIOS选项，因为默认的是硬盘启动，因此我们需要进行设置为U盘启动，不同的主板设置U盘启动的方式也不同，因此小编就不在此详述怎么更改BIOS设置，大家查找自己的主板型号然后在网上找相关的设置教程即可。</p>
<p><a href="http://jingyan.baidu.com/album/a3761b2b66fe141577f9aa51.html?picindex=8" target="_blank" rel="noopener"><img src="https://i.loli.net/2020/08/26/pD2yrl37uQwefsa.jpg" alt="怎么用u盘安装ubuntu"></a></p></li>
<li><p>完成BIOS设置后我们就可以插入U盘，重启电脑了，我们就可以使用U盘进行Ubuntu操作系统的安装了，具体的安装步骤小编就不在详述了，网上有很多相关的教程，大家可以参考下。</p>
<p><a href="http://jingyan.baidu.com/album/a3761b2b66fe141577f9aa51.html?picindex=9" target="_blank" rel="noopener"><img src="https://i.loli.net/2020/08/26/wbGayoYzurPMnkR.png" alt="怎么用u盘安装ubuntu"></a></p></li>
</ol>
<h4 id="安装系统流程">安装系统流程</h4>
<p>在bios界面选择USB：data traveler字样的设备，就是U盘，点击就可以进入U盘的ubuntu系统 在ubuntu界面里，选择try ubuntu，就可以在U盘里暂时不安装系统就可以体验。 在try ubuntu中，左侧文件夹目录会显示各个硬盘和U盘的项目。其中computer选项，/home就是U盘里的。可以正常的进行ubuntu操作。 好像会重启格式化还原，也就是向里面拷数据，重启之后再通过U盘进入ubuntu界面，拷的数据就不存在了。而且在自己笔记本上打开U盘，里面的目录是和ubuntu目录不一样的，也没有拷的数据。</p>
<p>因为要将系统安装在新的硬盘里，所以考虑将新硬盘里的数据拷出来。然而拷进U盘会重置无法读取，旧的硬盘又无法操作（无法在旧硬盘新建文件夹）所以最后就用了另一个数据U盘，找到U盘路径，最终将数据拷到U盘里，再对新硬盘格式化。</p>
<p>安装ubuntu过程中，在ubuntu主界面正常选择硬盘安装即可。 我们选择的是清空硬盘数据安装（对硬盘格式化）。安装完毕会重启</p>
<p>在bios界面里，可以调整开机默认启动项（默认进入的系统）。我们想要将新装的系统设为默认。</p>
<p>F11： boot manager。一般在这里面进行操作。 选择one-shot 启动（U盘系统启动），以及调整默认启动项。</p>
<ul>
<li>显示不出挂载的硬盘。 硬盘没插好。需要用劲将硬盘按进去 ，使其完全固定，才会插好。并且在开机的时候，硬盘位置处会亮灯的</li>
</ul>
<p>服务器上硬盘是有顺序的。根据服务器版面上的提示，按照从上到下，从左到右依次编号为0,1,2...， 优先级也是依次降低的。所以在启动界面，会优先加载优先级高的硬盘里的系统。 因为旧硬盘之前在上面。新硬盘在下面，后来更换位置，就可以正常加载新装的ubuntu系统。</p>
<p>sudo -i ： 升级到最高权限。 一些提示没有权限的操作需要进行升级 在文件/夹中， 右键属性，可以看到绝对位置路径。这样方便进行命令行操作。</p>
<h4 id="配置网络以及远程ssh连接">配置网络以及远程ssh连接</h4>
<p>本章节的大部分内容参考自<a href="https://blog.51cto.com/tangyade/2330627" target="_blank" rel="noopener">ubuntu16.04的网络配置</a></p>
<p>截图来自于实际服务器操作</p>
<p>参考博客 <a href="https://blog.csdn.net/weixin_43162402/article/details/88419024" target="_blank" rel="noopener">远程ssh连接ubuntu</a></p>
<h5 id="配置网络">配置网络</h5>
<p>打开ubuntu网络设置</p>
<p>在IPv4设置中，增加地址和DNS服务器</p>
<p><img src="https://i.loli.net/2020/08/25/zFQvdOa4rJZ8NCM.png" alt="image-20200825000649093"></p>
<p>重启系统 <code>shutdown  -r  now</code></p>
<p>如下 <code>ping www.baidu.com</code> , 如果可以ping通，则网络配置正确</p>
<p><img src="https://i.loli.net/2020/08/25/sNwuUrzBJ354xAd.png" alt="image-20200825000941898"></p>
<h5 id="配置ssh">配置ssh</h5>
<h6 id="检查ssh服务并安装">检查ssh服务，并安装</h6>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">ps -e|grep ssh <span class="comment">#抓取是否有ssh运行程序</span></span><br><span class="line">sudo apt-get update <span class="comment">#更新依赖</span></span><br><span class="line">sudo apt-get install openssh-sever  <span class="comment"># 安装ssh服务</span></span><br></pre></td></tr></tbody></table></figure>
<p><img src="https://i.loli.net/2020/08/25/cfUBayIj59q7OSY.png" alt="image-20200825002257791"></p>
<p><img src="https://i.loli.net/2020/08/25/Xn8RufD3AlKEhBC.png" alt="image-20200825002523612"></p>
<h6 id="启动ssh">启动ssh</h6>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">/etc/init.d/ssh start  <span class="comment">#启动ssh</span></span><br><span class="line">sudo netstat -tlp</span><br></pre></td></tr></tbody></table></figure>
<p><img src="https://i.loli.net/2020/08/25/HJWKQDbFqnUCk9f.png" alt="image-20200825002331284"></p>
<p><img src="https://i.loli.net/2020/08/25/1zbncv7DyP43mHN.png" alt="image-20200825002625207"></p>
<h6 id="重启系统">重启系统</h6>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">shutdown  -r  now</span><br></pre></td></tr></tbody></table></figure>
<h4 id="语言设置">语言设置</h4>
<p>参考<a href="https://zhuanlan.zhihu.com/p/40755318" target="_blank" rel="noopener">ubuntu的语言设置（中文-&gt;英文）</a></p>
<h4 id="安装软件">安装软件</h4>
<h5 id="下载">下载</h5>
<p>浏览器搜狗输入法的linux版本安装包。 选择<code>save file</code>选项</p>
<h5 id="安装">安装</h5>
<ul>
<li><p>首先找到安装包所在文件夹，复制路径 。如路径<code>/home/dell2/Downloads/</code></p></li>
<li><p>在安装包右键属性，复制文件名，如 ：<code>sougou_64.deb</code></p></li>
<li><p>在terminal中命令行 <code>cd /home/dell2/Downloads/</code> 切换到当前文件夹，便于操作</p></li>
<li><p>继续执行 <code>sudo dpkg -i sougou_64.deb</code> 需要root权限，所以要输入密码</p>
<blockquote>
<p>dpkg是linux的deb包管理。</p>
<p>dpkg： 是Debian packager的简称，是由Debian开发出来的包管理器，软件包在发布时打包成.deb格式</p>
<p>适用于Dpkg (Debian系)：Ubuntu 注：RPM (Red Hat系)：CentOS、Fedora</p>
<p>dpkg支持 tar 包。 tar 只是一种压缩文件格式，所以，它只是把文件压缩打包而已</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">dpkg -i *.deb     deb文件的安装</span><br><span class="line">dpkg -r *.deb     deb文件的卸载</span><br><span class="line">dpkg -l           查看当前系统中已经安装的软件包的信息</span><br></pre></td></tr></tbody></table></figure>
</blockquote></li>
</ul>
<h5 id="更新依赖">更新依赖</h5>
<blockquote>
<p>dpkg常用命令行dpkg和rpm命令虽然可以解决安装，卸载和查询，但是对于软件包直接的依赖，比如安装的软件包依赖于很多其他的软件包，这两个软件只会将依赖打印出来告诉用户，需要用户一个一个的手动去先安装依赖，当依赖包又依赖其他包时，对于用户实在是不够友好，于是apt和yum出现了，他们的能够自动将依赖下载安装</p>
<p>apt的全称是Advanced Packaging Tool是Linux系统下的一款安装包管理工具。</p>
</blockquote>
<p>一般如果提示软件安装失败，那么应该就是缺少依赖。这时候应该更新依赖并且修复依赖。</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">sudo apt-get update  <span class="comment">#更新依赖</span></span><br><span class="line">sudo apt-get install -f  <span class="comment">#修复依赖.使用此命令可修复依赖关系，假如有软件因依赖关系不满足而无法安装，就可以运行此命令自动修复安装程序包所依赖的包。特别是在使用dpkg命令安装deb软件包时出现依赖问题常需要此命令来修复。</span></span><br><span class="line"><span class="comment">#修复依赖之后如果还是出错，那么就再次运行此命令。注意提示</span></span><br></pre></td></tr></tbody></table></figure>
<h5 id="安装完毕">安装完毕</h5>
<p>打开ubuntu搜索栏就可以搜索到软件，就可以使用啦</p>
<h4 id="卸载软件">卸载软件</h4>
<p>参考自<a href="https://blog.csdn.net/luckydog612/article/details/80877179" target="_blank" rel="noopener">ubuntu命令卸载</a></p>
<p>打开终端，输入<code>dpkg --list</code> ,按下Enter键，终端输出以下内容，显示的是你电脑上安装的所有软件。</p>
<p>2.在终端中找到你需要卸载的软件的名称，列表是按照首字母排序的。 <img src="https://i.loli.net/2020/08/25/I8ctzH3LJlbNS4p.jpg" alt="找到要卸载的软件包"> 3.在终端上输入命令<code>sudo apt-get --purge remove 包名</code>（<code>--purge</code>是可选项，写上这个属性是将软件及其配置文件一并删除，如不需要删除配置文件，可执行<code>sudo apt-get remove 包名</code>） ，此处我要删除的是<code>polipo</code> ，那么在终端输入<code>sudo apt-get --purge remove polipo</code>，按下回车，输入密码，再次回车。</p>
<p>4.执行过程中，会提示你是否真的要删除（继续执行删除命令），在终端输入<code>y</code> ，然后回车，删除程序继续执行。 <img src="https://i.loli.net/2020/08/25/zt1kIJoTsn7AlFf.png" alt="确认删除"></p>
<p>5.正常情况下，再次出现输入命令行删除成功。 <img src="https://i.loli.net/2020/08/25/PRcrVmlbD3NOf65.png" alt="删除成功"></p>
<p>总结如下：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">dpkg --list  <span class="comment"># 找到要删除的软件 按顺序排列</span></span><br><span class="line">sudo apt-get --purge remove polipo  <span class="comment">#配置文件一起删除</span></span><br></pre></td></tr></tbody></table></figure>
<h4 id="解决向日葵连接断开问题">解决向日葵连接断开问题</h4>
<p>实验室740服务器本来是ubuntu16.04，之后升级到ubuntu18.04，在windows上连接ubuntu的向日葵，总是显示正在连接，马上就是连接已断开，于是记录下解决方案。向日葵的客服的官方解答如下：</p>
<p>1、检查桌面环境是否有启动，若没有请先启动。需开启显示器使用</p>
<p>2、需要安装lightdm插件否则会提示连接停止</p>
<p>3、设备终端运行 xhost +再重新发起远程桌面测试能否显示画面</p>
<p>本次是方案2解决的。</p>
<blockquote>
<p>猜测：之前版本是ubuntu16.04支持的是lightdm，所以向日葵是可以正常运行的。而Ubuntu 16.10和更高版本中的默认显示管理器gdm，导致向日葵总是连接断开。所以切换到lightdm就可以了。</p>
</blockquote>
<p>将Display Manager切换为lightdm，<strong>重新启动系统</strong>即可：</p>
<figure class="highlight shell"><table><tbody><tr><td class="code"><pre><span class="line">sudo apt-get update </span><br><span class="line">sudo apt-get upgrade </span><br><span class="line">sudo apt-get install lightdm #安装lightdm</span><br><span class="line">sudo dpkg-reconfigure lightdm # 将Display Manage从gdm3切换为lightdm</span><br></pre></td></tr></tbody></table></figure>
<p>输入用户名和密码后，将出现以下窗口，大致了解显示管理器在系统中的运行方式。</p>
<p><img src="https://i.loli.net/2020/08/26/VXeSndcL2ybKsW5.jpg" alt="Switch to gdm3" style="zoom:67%;"></p>
<p>按Enter键确定；将出现以下窗口。可以通过向上和向下箭头键配置新的显示管理器，然后按Enter进行确定。</p>
<p><img src="https://i.loli.net/2020/08/26/C6mfYuHpz1JyxX8.jpg" alt="Set default display manager"></p>
<p>重新启动系统时，选择的显示管理器将被配置为默认显示管理器。</p>
<hr>
<p>也可再切换为gdm3，并将lightdm删除</p>
<figure class="highlight shell"><table><tbody><tr><td class="code"><pre><span class="line">sudo dpkg-reconfigure gdm3 #也可切换为gdm3</span><br><span class="line">sudo apt-get remove lightdm #删除lightdm</span><br></pre></td></tr></tbody></table></figure>
<p>要检查当前正在使用哪个显示管理器，请运行以下命令：</p>
<figure class="highlight shell"><table><tbody><tr><td class="code"><pre><span class="line">cat /etc/X11/default-display-manager</span><br></pre></td></tr></tbody></table></figure>
<blockquote>
<p><code>gdm3</code>，<code>kdm</code>和<code>lightdm</code>都是<strong>显示管理器</strong>（Display Manager），它们提供图形化登录，并且处理用户身份验证。</p>
<p>显示管理器：向用户显示登录屏幕，当用户成功输入用户名和密码的有效组合时，会话开始。</p>
<p><img src="https://i.loli.net/2020/08/26/uoEHl4bhSxaF2gj.jpg" alt="Ubuntu LightDM Display manager" style="zoom: 67%;"></p>
<p>LightDM的登录屏幕</p>
</blockquote>
<h4 id="将向日葵设置为开机自启动">将向日葵设置为开机自启动</h4>
<p>想将向日葵设置为自启动，这样以后重启服务器之后，就可以直接连接向日葵。在向日葵软件里设置并没有效果，于是想着在ubuntu开机启动项里设置。参考自<a href="https://www.cnblogs.com/end/archive/2012/10/12/2721059.html" target="_blank" rel="noopener">linux开机自启动</a></p>
<blockquote>
<p>linux随机启动的服务程序都在/etc/init.d这个文件夹里，里面的文件全部都是脚本文件（脚本程序简单的说就是把要运行的程序写到一个文件里让系统能够按顺序执行，类似windows下的autorun.dat文件）</p>
<p>另外在/etc这个文件夹里还有诸如名为rc1.d, rc2.d一直到rc6.d的文件夹，这些都是linux不同的runlevel，我们一般进入的X windows多用户的运行级别是第5级，也就是rc5.d，在这个文件夹下的脚本文件就是运行第5级时要随机启动的服务程序。</p>
<p>需要注意的是，在每个rc (1-6).d文件夹下的文件其实都是/etc/init.d文件夹下的文件的一个软连接（类似windows中的快捷方式），也就是说，<strong>在 /etc/init.d文件夹下是全部的服务程序，而每个rc(1-6).d只链接它自己启动需要的相应的服务程序！</strong></p>
<p>在本次操作中，目的就是写入运行向日葵脚本到/etc/init.d，然后软链接到rc5.d中即可开机自启动</p>
</blockquote>
<h5 id="找到系统中名字">找到系统中名字</h5>
<p>我们不确定向日葵（sunlogin）在系统中的名字，于是如下操作：</p>
<p>打开终端，输入<code>dpkg --list</code> ,按下Enter键，终端输出以下内容，显示的是你电脑上安装的所有软件。（按照首字母排列的）</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">dpkg --list <span class="comment"># 显示所有的软件名称</span></span><br></pre></td></tr></tbody></table></figure>
<p>找到向日葵名称，为<code>sunloginclient</code></p>
<h5 id="确定向日葵的位置">确定向日葵的位置</h5>
<p>要知道<code>sunloginclient</code>在哪里，用locate命令可以找到。</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">locate sunloginclient</span><br></pre></td></tr></tbody></table></figure>
<p><img src="https://i.loli.net/2020/08/26/AGgtMOySf2jd5ca.png" alt="image-20200826221019492"></p>
<p>选择<code>/usr/local/sunlogin/bin/sunloginclient</code>,这就是向日葵执行文件位置所在。</p>
<p>其中usr表 示是属于用户的，bin在linux里表示可以执行的程序。</p>
<h5 id="验证文件位置可忽略">验证文件位置（可忽略）</h5>
<p>验证是否这个位置可以打开向日葵，ubuntu用命令行的方式启动向日葵</p>
<ul>
<li>绝对路径：</li>
</ul>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">/usr/<span class="built_in">local</span>/sunlogin/bin/sunloginclient <span class="comment">#直接在终端输入绝对路径即可</span></span><br></pre></td></tr></tbody></table></figure>
<p>如果可以启动向日葵，则表明路径正确</p>
<ul>
<li>如果已经在执行文件所在的文件夹，如<code>/usr/local/sunlogin/bin</code>，则</li>
</ul>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">./sunloginclient  <span class="comment"># 执行文件</span></span><br></pre></td></tr></tbody></table></figure>
<h5 id="编写sh脚本">编写sh脚本</h5>
<p>这样，我就可以编写一个脚本程序，把它放到<code>/etc/init.d</code>里，然后在<code>rc5.d</code>里做一个相应的软链接就可以了。</p>
<p>在<code>/etc/init.d</code>里新建sunlogin.sh脚本，</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /etc/init.d <span class="comment">#cd到该目录下</span></span><br><span class="line">sudo vim sunlogin.sh <span class="comment">#新建脚本</span></span><br></pre></td></tr></tbody></table></figure>
<p>脚本内容如下：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/sh </span></span><br><span class="line">/usr/<span class="built_in">local</span>/sunlogin/bin</span><br></pre></td></tr></tbody></table></figure>
<p>第一行<strong>#!/bin/sh</strong>是指此脚本使用<strong>/bin/sh</strong>来解释执行，<strong>#!</strong>是特殊的表示符，其后面根的是此解释此脚本的shell的路径。</p>
<p>第二行就是要运行的命令，也就是打开向日葵。</p>
<blockquote>
<p>才开始用的是<code>#!/bin/bash</code> ，发现没有效果。后参照rc5.d里sh文件里格式是sh，于是将其改为<code>#!/bin/sh</code> 。重启有效果。</p>
<p>#!/bin/sh 和 #!/bin/bash 的区别可以参考<a href="https://www.cnblogs.com/EasonJim/p/6850319.html" target="_blank" rel="noopener">区别</a></p>
</blockquote>
<h5 id="建立软链接">建立软链接</h5>
<p>建立启动项从<code>/etc/init.d</code>到<code>/etc/rc5.d</code>的软链接</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">ln -s  /etc/init.d/sunlogin.sh  /etc/rc5.d/S04sunlogin.sh</span><br></pre></td></tr></tbody></table></figure>
<p><img src="https://i.loli.net/2020/08/26/Xeo5zuxMsv8nkba.png" alt="rc5.d中内容"></p>
<blockquote>
<p>建立软链接： ln -s 原目录 映射目录</p>
<p>删除软链接的方法： sudo rm -rf 映射目录</p>
</blockquote>
<blockquote>
<p>软链接相当于windows中的快捷方式，不必重复的占用磁盘空间</p>
<p>ln命令会保持每一处链接文件的同步性，和快捷方式一样</p>
<p>当我们需要在不同的目录，用到相同的文件时，我们不需要在每一个需要的目录下都放一个必须相同的文件，我们只要在某个固定的目录，放上该文件，然后在 其它的目录下用ln命令链接（link）它就可以，不必重复的占用磁盘空间。</p>
<p>具体参考<a href="https://www.runoob.com/linux/linux-comm-ln.html" target="_blank" rel="noopener">链接</a></p>
<p>还需要注意的一点是，在rc5.d里，每个链接的名字都是以S或者K开头的，S开头的表示是系统启动是要随机启动的，K开头的是不随机启动的。</p>
<p>如果我要哪个服务随机启动，就把它名字第一个字母K改成S就可以了，当然，把S改成K后，这个服务就不能随机启动了。因此，我这个链接 还要起名为SXXX，这样系统才能让它随机启动。</p>
</blockquote>
<h5 id="完成开机自启动">完成开机自启动</h5>
<p>重启系统后，等一下就可以启动向日葵，完成操作</p>
<h4 id="挂载硬盘">挂载硬盘</h4>
<h3 id="计算机启动过程boot">计算机启动过程（boot）</h3>
<p>计算机启动过程分成四个阶段。 大部分内容参考自博客<a href="http://www.ruanyifeng.com/blog/2013/02/booting.html" target="_blank" rel="noopener">计算机是如何启动的？</a></p>
<h4 id="一第一阶段bios"><strong>一、第一阶段：BIOS</strong></h4>
<p>是一组<strong>固化到计算机内主板上一个ROM芯片上的程序</strong>，计算机通电后，第一件事就是读取它。</p>
<p>它保存着计算机最重要的基本输入输出的程序、系统设置信息、开机后自检程序和系统自启动程序。其主要功能是为计算机提供最底层的、最直接的硬件设置和控制。</p>
<p>一般设置都是在这个过程中进行的</p>
<p><img src="https://i.loli.net/2020/08/25/KVNMtAZLJSs5c3H.jpg" alt="img"></p>
<p>这块芯片里的程序叫做"<strong>基本输入输出系统</strong>"（Basic Input/Output System），简称为<a href="http://en.wikipedia.org/wiki/BIOS" target="_blank" rel="noopener">BIOS</a>。</p>
<h5 id="硬件自检"><strong>1.1 硬件自检</strong></h5>
<p>BIOS程序首先检查，计算机硬件能否满足运行的基本条件，这叫做"硬件自检"（Power-On Self-Test），缩写为<a href="http://en.wikipedia.org/wiki/Power-on_self-test" target="_blank" rel="noopener">POST</a>。</p>
<p>如果硬件出现问题，主板会发出不同含义的<a href="http://en.wikipedia.org/wiki/Power-on_self-test#Original_IBM_POST_beep_codes" target="_blank" rel="noopener">蜂鸣</a>，启动中止。<strong>如果没有问题，屏幕就会显示出CPU、内存、硬盘等信息。</strong></p>
<p><img src="https://i.loli.net/2020/08/25/Atdr1chebHjRwpM.png" alt="img"></p>
<h5 id="启动顺序"><strong>1.2 启动顺序</strong></h5>
<p>硬件自检完成后，BIOS把控制权转交给下一阶段的启动程序。</p>
<p>这时，BIOS需要知道，"下一阶段的启动程序"具体存放在哪一个设备。也就是说，BIOS需要有一个<strong>外部储存设备</strong>的排序，排在前面的设备就是优先转交控制权的设备。这种排序叫做<strong>"启动顺序"（Boot Sequence）</strong>。</p>
<p>打开BIOS的操作界面，里面有一项就是"设定启动顺序"。 （可以自己设置）</p>
<p><img src="http://www.ruanyifeng.com/blogimg/asset/201302/bg2013021504.jpg" alt="img"></p>
<h4 id="二第二阶段主引导记录mbr"><strong>二、第二阶段：主引导记录</strong>（MBR）</h4>
<p>BIOS按照"启动顺序"，把控制权转交给排在第一位的储存设备。（已安装的硬盘/U盘）</p>
<p>这时，计算机读取该设备的第一个扇区，也就是读取最前面的512个字节。如果这512个字节的最后两个字节是0x55和0xAA，表明这个设备可以用于启动；如果不是，表明设备不能用于启动，控制权于是被转交给"启动顺序"中的下一个设备。</p>
<p>这最前面的512个字节，就叫做<a href="http://en.wikipedia.org/wiki/Master_boot_record" target="_blank" rel="noopener">"主引导记录"</a>（Master boot record，缩写为MBR）。</p>
<h5 id="主引导记录的结构"><strong>2.1 主引导记录的结构</strong></h5>
<p>"主引导记录"只有512个字节，放不了太多东西。它的<strong>主要作用是，告诉计算机到硬盘的哪一个位置去找操作系统。</strong></p>
<p>主引导记录由三个部分组成：</p>
<blockquote>
<p>　　（1） 第1-446字节：调用操作系统的机器码。</p>
<p>　　（2） 第447-510字节：分区表（Partition table）。</p>
<p>　　（3） 第511-512字节：主引导记录签名（0x55和0xAA）。</p>
</blockquote>
<p>其中，第二部分"分区表"的作用，是将硬盘分成若干个区。</p>
<h5 id="分区表"><strong>2.2 分区表</strong></h5>
<p>硬盘分区有很多<a href="http://en.wikipedia.org/wiki/Disk_partitioning#Benefits_of_multiple_partitions" target="_blank" rel="noopener">好处</a>。考虑到<strong>每个区可以安装不同的操作系统</strong>，"主引导记录"因此必须知道将控制权转交给哪个区。</p>
<p>分区表的长度只有64个字节，里面又分成四项，每项16个字节。所以，<strong>一个硬盘最多只能分四个一级分区</strong>，又叫做<strong>"主分区"</strong>。</p>
<p>每个主分区代表一个操作系统，最多只能装4个操作系统。 在操作系统中的区划分是在该主分区下进行的。</p>
<p>每个主分区的16个字节，由6个部分组成：</p>
<blockquote>
<p>　　（1） 第1个字节：如果为0x80，就表示该主分区是激活分区，控制权要转交给这个分区。四个主分区里面只能有一个是激活的。</p>
<p>　　（2） 第2-4个字节：主分区第一个扇区的物理位置（柱面、磁头、扇区号等等）。</p>
<p>　　（3） 第5个字节：<a href="http://en.wikipedia.org/wiki/Partition_type" target="_blank" rel="noopener">主分区类型</a>。</p>
<p>　　（4） 第6-8个字节：主分区最后一个扇区的物理位置。</p>
<p>　　（5） 第9-12字节：该主分区第一个扇区的逻辑地址。</p>
<p>　　（6） 第13-16字节：主分区的扇区总数。</p>
</blockquote>
<p>最后的四个字节（"主分区的扇区总数"），决定了这个主分区的长度。也就是说，一个主分区的扇区总数最多不超过2的32次方。</p>
<p>如果每个扇区为512个字节，就意味着单个分区最大不超过2TB。再考虑到扇区的逻辑地址也是32位，所以单个硬盘可利用的空间最大也不超过2TB。如果想使用更大的硬盘，只有2个方法：一是提高每个扇区的字节数，二是<a href="http://en.wikipedia.org/wiki/GUID_Partition_Table" target="_blank" rel="noopener">增加扇区总数</a>。</p>
<h4 id="三第三阶段硬盘启动"><strong>三、第三阶段：硬盘启动</strong></h4>
<p>这时，计算机的控制权就要转交给硬盘的某个分区了，这里又分成三种情况。</p>
<h5 id="情况a卷引导记录"><strong>3.1 情况A：卷引导记录</strong></h5>
<p>上一节提到，四个主分区里面，只有一个是激活的。计算机会<strong>读取激活分区的第一个扇区</strong>，叫做<a href="http://en.wikipedia.org/wiki/Volume_Boot_Record" target="_blank" rel="noopener">"卷引导记录</a>"（Volume boot record，缩写为VBR）。</p>
<p>"卷引导记录"的主要作用是，告诉计算机，操作系统在这个分区里的位置。然后，计算机就会加载操作系统了。</p>
<h5 id="情况b扩展分区和逻辑分区"><strong>3.2 情况B：扩展分区和逻辑分区</strong></h5>
<p>主分区的其中一个被定义为<strong>扩展分区</strong>，扩展分区下可以设置多个分区，被称为<strong>逻辑分区</strong></p>
<p>随着硬盘越来越大，四个主分区已经不够了，需要更多的分区。但是，<strong>分区表只有四项，因此规定有且仅有一个区可以被定义成"扩展分区"（Extended partition）。</strong></p>
<p>所谓<strong>"扩展分区"，就是指这个区里面又分成多个区</strong>。这种分区里面的分区，就叫做"<strong>逻辑分区</strong>"（logical partition）。</p>
<p>计算机先读取扩展分区的第一个扇区，叫做<a href="http://en.wikipedia.org/wiki/Extended_partition" target="_blank" rel="noopener">"扩展引导记录"</a>（Extended boot record，缩写为EBR）。它里面也包含一张64字节的分区表，但是最多只有两项（也就是两个逻辑分区）。</p>
<p>计算机接着读取第二个逻辑分区的第一个扇区，再从里面的分区表中找到第三个逻辑分区的位置，以此类推，直到某个逻辑分区的分区表只包含它自身为止（即只有一个分区项）。因此，<strong>扩展分区可以包含无数个逻辑分区。</strong></p>
<p>但是，似乎很少通过这种方式启动操作系统。如果操作系统确实安装在扩展分区，一般采用下一种方式启动。</p>
<h5 id="情况c启动管理器常用"><strong>3.3 情况C：启动管理器</strong>（常用）</h5>
<p>在这种情况下，计算机读取"主引导记录"前面446字节的机器码之后，不再把控制权转交给某一个分区，而是运行事先安装的<a href="http://en.wikipedia.org/wiki/Boot_loader#Modern_boot_loaders" target="_blank" rel="noopener">"启动管理器"</a>（boot loader），由用户选择启动哪一个操作系统。</p>
<p>Linux环境中，目前最流行的启动管理器是<a href="http://en.wikipedia.org/wiki/GNU_GRUB" target="_blank" rel="noopener">Grub</a>。</p>
<p><img src="https://i.loli.net/2020/08/25/Ao5JqKFpgmXWOfw.png" alt="img"></p>
<h4 id="四第四阶段操作系统"><strong>四、第四阶段：操作系统</strong></h4>
<p>控制权转交给操作系统后，操作系统的内核首先被载入内存。</p>
<p>以Linux系统为例，先载入<strong>/boot目录下面的kernel</strong>。内核加载成功后，第一个运行的程序是<strong>/sbin/init</strong>。它根据配置文件（Debian系统是/etc/initab）产生init进程。这是Linux启动后的第一个进程，pid进程编号为1，其他进程都是它的后代。</p>
<p><strong>然后，init线程加载系统的各个模块，比如窗口程序和网络程序，直至执行/bin/login程序，跳出登录界面，等待用户输入用户名和密码。</strong></p>
<p>至此，全部启动过程完成。</p>
<h3 id="bios和uefi的区别">BIOS和UEFI的区别</h3>
<blockquote>
<p>BOOT设置是说电脑一按开机键后，出现那段黑屏界面BOOT运行时，你按<strong>快捷键</strong>调出各种BOOT后的程序，例如BIOS的这个按键过程。</p>
<p><a href="http://www.udangjia.com/bios/" target="_blank" rel="noopener">BIOS设置</a>是，你必须按对快捷键，<strong>从BOOT进入BIOS程序后</strong>，在BIOS程序中的设置的这个过程。</p>
</blockquote>
<blockquote>
<p><strong>UEFI</strong>它其实和BIOS是同一个性质的东西，同一种程序，是随着发展出现的BIOS升级版。因为硬件发展迅速，传统式（<strong>Legacy</strong>）BIOS成为进步的包袱，现在已发展出最新的UEFI。理论上说是比BIOS更先进的UEFI，却还是诸多支持不足，往往很多是UEFI启动电脑，到头来还是切换回BIOS。</p>
</blockquote>
<p><img src="https://i.loli.net/2020/08/17/dkj8VXRMyTQ1UsI.png" alt="image-20200817235333172"></p>
<p><strong>现在的笔记本默认是UEFI+GPT</strong>，主流趋势也是使用UEFI进行引导。如果改legacy，必须在Security选项，找到Secure Boot，关闭。</p>
<h3 id="bios设置">BIOS设置</h3>
<h4 id="u盘启动">U盘启动</h4>
<p>按下电源键后,按照显示器上的提示进入BIOS，例如：按F2、F9、F11、F12或者Delete键。</p>
<p>方法1.直接选择带有USB：data traveler字样的enter进入即可。只是一次进入，下次还需要选择</p>
<p>方法2.将U盘项设置为First Boot，默认进入。一般在<strong>boot（启动）选项</strong>中进入设置</p>
<p>有的需要<strong>F10保存</strong>并退出</p>
<blockquote>
<p>参考</p>
<p><a href="https://blog.csdn.net/yuk1007/article/details/95217457" target="_blank" rel="noopener">基础的BIOS操作</a></p>
<p><a href="http://www.kqidong.com/bios/2771.html" target="_blank" rel="noopener">常见bios设置操作教程</a></p>
</blockquote>
<h3 id="计算机存储术语-扇区磁盘块页">计算机存储术语: 扇区，磁盘块，页</h3>
<blockquote>
<p><strong>扇区（sector）</strong>：硬盘的读写以扇区为基本单位。</p>
<p>磁盘上的每个磁道被等分为若干个弧段，这些弧段称之为扇区。</p>
<p>通常情况下每个扇区的大小是 512 字节。linux 下可以使用 <code>fdisk -l</code> 了解扇区大小</p>
<p>注意，扇区是磁盘物理层面的概念，操作系统是不直接与扇区交互的，而是与多个连续扇区组成的磁盘块交互。由于扇区是物理层面的概念，所以无法在系统中进行大小的更改。</p>
</blockquote>
<blockquote>
<p>簇：由于操作系统无法对数目众多的扇区进行寻址，所以操作系统就将相邻的扇区组合在一起，形成一个<strong>簇</strong>，然后再对簇进行管理。每个簇可以包括2、4、8、16、32或64个扇区。<strong>操作系统是通过块簇来做为单位读取等操作数据的</strong>。</p>
<p>为了更好地管理磁盘空间和更高效地从硬盘读取数据，操作系统规定<strong>一个簇中只能放置一个文件的内容</strong>，因此文件所占用的空间，只能是簇的整数倍；而如果文件实际大小小于一簇，它也要占一簇的空间。</p>
<p>所以，一般情况下文件所占空间要略大于文件的实际大小</p>
</blockquote>
<blockquote>
<p><strong>磁盘块（IO Block）</strong>：<strong>文件系统</strong>读写数据的最小单位，也叫磁盘簇。磁盘块的大小可以通过命令 <code>stat /boot</code> 来查看。</p>
<p>在Windows下如NTFS等文件系统中叫做簇；在Linux下如Ext4等文件系统中叫做块（block）。</p>
</blockquote>
<blockquote>
<p><strong>页，page</strong></p>
<p>内存的最小存储单位。页的大小通常为磁盘块大小的 2^n 倍，可以通过命令 <code>getconf PAGE_SIZE</code> 来获取页的大小</p>
<p>总结也就是</p>
<ul>
<li>页，内存操作的基本单位</li>
<li>磁盘块，磁盘操作的基本单位</li>
</ul>
</blockquote>
<blockquote>
<p><strong>命令索引</strong></p>
<ul>
<li>扇区大小，<code>fdisk -l</code> 查看磁盘分区情况</li>
<li>磁盘块大小，<code>stat /boot</code></li>
<li>内存页大小，<code>getconf PAGE_SIZE</code></li>
</ul>
</blockquote>
<h3 id="linux磁盘分区">linux磁盘分区</h3>
<p>更多内容参考自</p>
<p><a href="https://wizardforcel.gitbooks.io/vbird-linux-basic-4e/content/20.html" target="_blank" rel="noopener">鸟哥的linux私房菜 | 磁盘分区</a></p>
<p><a href="https://jasonhzy.github.io/2019/02/07/linux-mount/" target="_blank" rel="noopener">Linux分区与挂载</a></p>
<h3 id="poweredge-r740-机架式服务器基本操作">PowerEdge R740 机架式服务器基本操作</h3>
<h4 id="设置开机启动顺序">设置开机启动顺序</h4>
<p>开机按F2进入系统启动设置，也可以<strong>按F11进入快速启动配置</strong></p>
<p><img src="https://i.loli.net/2020/08/18/XZFykAOdgmHjJDe.png" alt="image-20200818151126884"></p>
<p><img src="https://i.loli.net/2020/08/18/OCK5cRzksAWIhyp.png" alt="image-20200818151216211"></p>
<p><img src="https://i.loli.net/2020/08/18/FpgEyq8vGzBlPXD.png" alt="image-20200818151323442"></p>
<p><img src="https://i.loli.net/2020/08/18/Rdf19rWcvAOHSp7.png" alt="image-20200818151824019"></p>
<p>在<strong>Boot Sequence</strong>处将<strong>Hard drive C</strong>设置到第一位，即优先级最高</p>
<h4 id="运行硬件检测">运行硬件检测</h4>
<p>怀疑硬件故障了，运行了一下硬件检测。可以看到有哪些硬件。来验证所连接的硬件是否正常工作，排除故障。</p>
<p>1.开机出现DELL LOGO标志时按2下F10键，等待大概5分钟会进入lifecycle controller界面；</p>
<p>2.鼠标单击选择左侧的“Hardware Diagnostics”硬件诊断，再单击右侧的“Run Hardware Diagnostics”运行硬件诊断；</p>
<p><img src="https://i.loli.net/2020/08/18/FNegH5CnmJiRWAa.jpg" alt="img"></p>
<p>3.自动进入检测</p>
<p><img src="https://i.loli.net/2020/08/18/oEsxlPruV6DmSw2.jpg" alt="img"></p>
<p>4.大概5分钟后完成快速检测，出现如下界面，再单击“YES”继续完整检测，大概需要几个小时</p>
<p><img src="https://i.loli.net/2020/08/18/DKRFHVjpEZo27kW.jpg" alt="img"></p>
<ol start="5" type="1">
<li>检测完成后请单击“Result”结果一列，拍照这个页面，可能需要拖动滚动条拍照未在一屏显示出的其他内容。</li>
</ol>
<p><img src="https://i.loli.net/2020/08/18/Yij4DdNxZ3w6RBF.jpg" alt="img"></p>
<p>6、如果检测出现问题，会弹出红框，点击继续，最后查看一下原因。</p>
<p><img src="https://i.loli.net/2020/08/18/tYB8e7lsuUyP9fc.jpg" alt="img"></p>
<p>查一下ERROR CODE，应该是事件日志有历史告警导致的，清除告警之后，再次运行检测程序，没有告警了。</p>
<h3 id="交换空间">交换空间</h3>
<p>当今无论什么操作系统 <em>交换(Swap)</em>空间是非常常见的。Linux 使用交换空间来增加主机可用的虚拟内存。</p>
<p>典型计算机中有<strong>两种基本类型的内存</strong>。第一种类型，<strong>随机存取存储器 (RAM)</strong>，用于存储计算机使用的数据和程序。只有程序和数据存储在 RAM 中，计算机才能使用它们。</p>
<p><strong>交换空间</strong>是现代 Linux 系统中的第二种内存类型。交换空间的主要功能是当全部的 RAM 被占用并且需要更多内存时，用磁盘空间代替 RAM 内存。</p>
<p>例如，假设你有一个 8GB RAM 的计算机。如果你启动的程序没有填满 RAM，一切都好，不需要交换。假设你在处理电子表格，当添加更多的行时，你电子表格会增长，加上所有正在运行的程序，将会占用全部的 RAM 。如果这时没有可用的交换空间，你将不得不停止处理电子表格，直到关闭一些其他程序来释放一些 RAM 。</p>
<p>内核使用一个内存管理程序来检测最近没有使用的内存块（内存页）。内存管理程序将这些相对不经常使用的内存页交换到硬盘上专门指定用于“分页”或交换的特殊分区。这会释放 RAM，为输入电子表格更多数据腾出了空间。那些换出到硬盘的内存页面被内核的内存管理代码跟踪，如果需要，可以被分页回 RAM。</p>
<p>Linux 计算机中的内存总量是 RAM + 交换分区，交换分区被称为虚拟内存.</p>
<h4 id="什么是swap"><strong>什么是swap?</strong></h4>
<p>swap space是磁盘上的一块区域，可以是一个分区，也可以是一个文件，或者是他们的组合。简单点说，当系统物理内存吃紧时，Linux会将内存中不常访问的数据保存到swap上，这样系统就有更多的物理内存为各个进程服务，而当系统需要访问swap上存储的内容时，再将swap上的数据加载到内存中，这就是我们常说的swap out和swap in。</p>
<p>很多发行版(如ubuntu)的休眠功能依赖于swap分区，当系统休眠的时候，会将内存中的数据保存到swap分区上，等下次系统启动的时候，再将数据加载到内存中，这样可以加快系统的启动速度，所以如果要使用休眠的功能，必须要配置swap分区，并且大小一定要大于等于物理内存</p>
<p>swap是存放在磁盘上的，磁盘的速度和内存比较起来慢了好几个数量级，如果不停的读写swap，那么对系统的性能肯定有影响，尤其是当系统内存很吃紧的时候，读写swap空间发生的频率会很高，导致系统运行很慢，像死了一样，这个时候添加物理内存是唯一的解决办法。</p>
<h4 id="参考">参考</h4>
<blockquote>
<p><a href="https://segmentfault.com/a/1190000008125116" target="_blank" rel="noopener">Linux交换空间（swap space）</a></p>
</blockquote>
<h3 id="服务器ubuntu基本操作">服务器ubuntu基本操作</h3>
<h4 id="关机">关机</h4>
<p>立即关机</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line"># shutdown -h now</span><br></pre></td></tr></tbody></table></figure>
<p>指定 10 分钟后关机</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line"># shutdown -h 10</span><br></pre></td></tr></tbody></table></figure>
<p>重新启动计算机</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line"># shutdown -r now</span><br></pre></td></tr></tbody></table></figure>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>服务器</category>
      </categories>
      <tags>
        <tag>服务器</tag>
        <tag>故障排除</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-07-28-transformer解读-pytorch版本</title>
    <url>/2020/07/28/2020-07-28-transformer-pytorch/</url>
    <content><![CDATA[<h3 id="前言">前言</h3>
<p>最近几天都在阅读哈佛pytorch实现transformer的代码，代码风格很好，很值得参考和研读。和实验室师兄又在一起讨论了几次，代码思路和实现过程基本都了解了，对于原论文 <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">“Attention is All You Need”</a> 中关于transformer模型的理解又深入了许多。要想了解模型，还是要好好研读实现代码。以便于后面自己结合模型的研究。</p>
<p>本篇是对实现代码的注释，加上了自己的理解，也会有一些函数的介绍扩充。</p>
<h4 id="参考链接">参考链接</h4>
<blockquote>
<p>解读的是哈佛的一篇transformer的pytorch版本实现</p>
<p>http://nlp.seas.harvard.edu/2018/04/03/attention.html</p>
<p>参考另一篇博客</p>
<p>http://fancyerii.github.io/2019/03/09/transformer-codes/</p>
<p>Transformer注解及PyTorch实现（上）</p>
<p>https://www.jiqizhixin.com/articles/2018-11-06-10</p>
<p>Transformer注解及PyTorch实现（下）</p>
<p>https://www.jiqizhixin.com/articles/2018-11-06-18</p>
<p>训练过程中的 Mask实现</p>
<p>https://www.cnblogs.com/wevolf/p/12484972.html</p>
<p>transformer综述</p>
<p><a href="https://libertydream.github.io/2020/05/03/Transformer-综述/" target="_blank" rel="noopener">https://libertydream.github.io/2020/05/03/Transformer-%E7%BB%BC%E8%BF%B0/</a></p>
</blockquote>
<h3 id="the-annotated-transformer">The Annotated Transformer</h3>
<p><img src="https://i.loli.net/2020/07/28/NUAyXWJ5DzHmjuv.png" alt="这是一张图片"></p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># !pip install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl numpy matplotlib spacy torchtext seaborn </span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> math, copy, time</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn</span><br><span class="line">seaborn.set_context(context=<span class="string">"talk"</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>Transformer使用了Self-Attention机制，它在编码每一词的时候都能够注意(attend to)整个句子，从而可以解决长距离依赖的问题，同时<strong>计算Self-Attention可以用矩阵乘法一次计算所有的时刻</strong>，因此可以充分利用计算资源(CPU/GPU上的矩阵运算都是充分优化和高度并行的)。</p>
<h3 id="模型结构">模型结构</h3>
<p>Most competitive neural sequence transduction models have an encoder-decoder structure <a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="noopener">(cite)</a>. Here, <code>the encoder maps an input sequence of symbol representations(x1,…,xn) to a sequence of continuous representations z=(z1,…,zn). Given z, the decoder then generates an output sequence (y1,…,ym) of symbols one element at a time.</code> At each step the model is auto-regressive <a href="https://arxiv.org/abs/1308.0850" target="_blank" rel="noopener">(cite)</a>, consuming the previously generated symbols as additional input when generating the next.</p>
<p><strong>EncoderDecoder定义了一种通用的Encoder-Decoder架构</strong>，具体的Encoder、Decoder、src_embed、target_embed和generator都是构造函数传入的参数。这样我们<strong>做实验更换不同的组件就会更加方便</strong>。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderDecoder</span><span class="params">(nn.Module)</span>:</span> <span class="comment">#定义的是整个模型 ，不包括generator</span></span><br><span class="line">    </span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">   标准的Encoder-Decoder架构。这是很多模型的基础</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    class里， init函数是实例化一个对象的时候用于初始化对象用的</span></span><br><span class="line"><span class="string">    forward函数是在执行调用对象的时候使用， 需要传入正确的参数 </span></span><br><span class="line"><span class="string">    在执行时候调用__call__方法，然后再call里再调用forward</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, encoder, decoder, src_embed, tgt_embed, generator)</span>:</span></span><br><span class="line">        super(EncoderDecoder, self).__init__()</span><br><span class="line">        <span class="comment"># encoder和decoder都是构造的时候传入的，这样会非常灵活</span></span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line">        <span class="comment"># 源语言和目标语言的embedding，包括embedding层和position encode层</span></span><br><span class="line">        self.src_embed = src_embed <span class="comment">#源数据集的嵌入</span></span><br><span class="line">        self.tgt_embed = tgt_embed <span class="comment">#目标数据集的嵌入，作为decoder的输入</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        generator后面会讲到，就是根据Decoder的隐状态输出当前时刻的词</span></span><br><span class="line"><span class="string">	    基本的实现就是隐状态输入一个全连接层，全连接层的输出大小是词的个数</span></span><br><span class="line"><span class="string">		然后接一个softmax变成概率</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.generator = generator</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, src, tgt, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="comment">#首先调用encode方法对输入进行编码，然后调用decode方法解码</span></span><br><span class="line">        <span class="keyword">return</span> self.decode(self.encode(src, src_mask), src_mask,</span><br><span class="line">                            tgt, tgt_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, src, src_mask)</span>:</span></span><br><span class="line">        <span class="comment"># 调用encoder来进行编码，传入的参数embedding的src和src_mask</span></span><br><span class="line">        <span class="keyword">return</span> self.encoder(self.src_embed(src), src_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span><span class="params">(self, memory, src_mask, tgt, tgt_mask)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask) <span class="comment">#目标是输入的一部分</span></span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span>  <span class="comment">#decoder后面的linear+softmax</span></span><br><span class="line">    <span class="comment"># 根据Decoder的隐状态输出一个词</span></span><br><span class="line">	<span class="comment"># d_model是Decoder输出的大小，vocab是词典大小 （数据语料有多少词 ）</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, vocab)</span>:</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        self.proj = nn.Linear(d_model, vocab) <span class="comment">#全连接成vocab大小，作为softmax的输入。</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(self.proj(x), dim=<span class="number">-1</span>) <span class="comment">#softmax的log值</span></span><br></pre></td></tr></tbody></table></figure>
<p>注：<code>Generator返回的是softmax的log值</code>。在PyTorch里为了计算交叉熵损失，有两种方法。第一种方法是使用<strong>nn.CrossEntropyLoss()</strong>，一种是使用<strong>NLLLoss()</strong>。很多开源代码里第二种更常见</p>
<p>The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.</p>
<p><img src="https://i.loli.net/2020/07/28/P3fSgRhrmFtlpxY.png" alt="png"></p>
<h3 id="encoder-and-decoder-stacks">Encoder and Decoder Stacks</h3>
<h4 id="encoder">Encoder</h4>
<p>Encoder和Decoder都是由N个相同结构的Layer堆积(stack)而成。<strong>因此我们首先定义clones函数，用于克隆相同的SubLayer。</strong></p>
<p>这里使用了<strong>nn.ModuleList</strong>，ModuleList就像一个普通的Python的List，我们可以使用下标来访问它，它的好处是传入的ModuleList的所有Module都会注册到PyTorch里，这样Optimizer就能找到这里面的参数，从而能够用梯度下降更新这些参数。但是nn.ModuleList并不是Module(的子类)，因此它没有forward等方法，我们通常把它放到某个Module里。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clones</span><span class="params">(module, N)</span>:</span>  <span class="comment">#克隆N层，是个层数的列表。 copy.deepcopy是深复制， 一个改变不会影响另一个</span></span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="keyword">for</span> _ <span class="keyword">in</span> range(N)]) <span class="comment">#复制N=6层</span></span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span><span class="params">(nn.Module)</span>:</span>  <span class="comment">#定义编码器 </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#Encoder是N个EncoderLayer的stack</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layer, N)</span>:</span> <span class="comment"># 根据make_model定义，layer = encoderlayer（sublayer）</span></span><br><span class="line">        super(Encoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N) <span class="comment">#编码器有6层编码层，根据上述函数的定义，module=layer</span></span><br><span class="line">        self.norm = LayerNorm(layer.size) <span class="comment">#调用下面的LayerNorm。 分开定义是因为 LayerNorm = 2* layer</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask)</span>:</span> </span><br><span class="line">      	 <span class="comment">#逐层进行处理</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers: <span class="comment"># x 在每一层中传递</span></span><br><span class="line">            x = layer(x, mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x) <span class="comment">#最终encoder的返回值</span></span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LayerNorm</span><span class="params">(nn.Module)</span>:</span> <span class="comment">#norm部分  作为每一个子层的输出</span></span><br><span class="line">    <span class="string">"Construct a layernorm module (See citation for details)."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, features, eps=<span class="number">1e-6</span>)</span>:</span> <span class="comment">#feature = layer.size layer的形状</span></span><br><span class="line">        super(LayerNorm, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.a_2 = nn.Parameter(torch.ones(features))  <span class="comment">#将后面的tensor转换为可优化的参数。初始化为全1</span></span><br><span class="line">        self.b_2 = nn.Parameter(torch.zeros(features))</span><br><span class="line">        self.eps = eps <span class="comment"># 很小的值</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span> <span class="comment"># 平均值和标准差</span></span><br><span class="line">        mean = x.mean(<span class="number">-1</span>, keepdim=<span class="literal">True</span>) <span class="comment"># 在最后一维进行操作，计算mean（也就是每一个样本的所有值）。保持原始的维度</span></span><br><span class="line">        std = x.std(<span class="number">-1</span>, keepdim=<span class="literal">True</span>) </span><br><span class="line">        <span class="keyword">return</span> self.a_2 * (x - mean) / (std + self.eps) + self.b_2 <span class="comment">#输出</span></span><br></pre></td></tr></tbody></table></figure>
<p><strong>不管是Self-Attention还是全连接层，都首先是LayerNorm，然后是Self-Attention/Dense，然后是Dropout，最后是残差连接。这里面有很多可以重用的代码，我们把它封装成SublayerConnection。</strong></p>
<hr>
<p>That is, <code>the output of each sub-layer is LayerNorm(x+Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself.</code> We apply dropout <a href="http://jmlr.org/papers/v15/srivastava14a.html" target="_blank" rel="noopener">(cite)</a> to the output of each sub-layer, before it is added to the sub-layer input and normalized.</p>
<p>To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension <code>dmodel=512</code>.</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SublayerConnection</span><span class="params">(nn.Module)</span>:</span> <span class="comment"># 每一个编码层中的两个子层</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">	LayerNorm + sublayer(Self-Attenion/Dense) + dropout + 残差连接</span></span><br><span class="line"><span class="string">	为了简单，把LayerNorm放到了前面，这和原始论文稍有不同，原始论文LayerNorm在最后。</span></span><br><span class="line"><span class="string">	"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, dropout)</span>:</span></span><br><span class="line">        super(SublayerConnection, self).__init__()</span><br><span class="line">        self.norm = LayerNorm(size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, sublayer)</span>:</span></span><br><span class="line">       <span class="comment">#sublayer是传入的参数，参考DecoderLayer，它可以当成函数调用，这个函数的有一个输入参数</span></span><br><span class="line">        <span class="keyword">return</span> x + self.dropout(sublayer(self.norm(x))) <span class="comment">#调用layernorm ，正则化之后再相加</span></span><br></pre></td></tr></tbody></table></figure>
<p>这个类会构造LayerNorm和Dropout，但是Self-Attention或者Dense并不在这里构造，还是放在了EncoderLayer里，在forward的时候由EncoderLayer传入。这样的好处是更加通用，比如Decoder也是类似的需要在Self-Attention、Attention或者Dense前面后加上LayerNorm和Dropout以及残差连接，我们就可以复用代码。但是这里要求传入的sublayer可以使用一个参数来调用的函数(或者有__call__)。</p>
<hr>
<p>forward调用sublayer[0] (这是SublayerConnection对象)的__call__方法，最终会调到它的forward方法，而这个方法需要两个参数，<strong>一个是输入Tensor，一个是一个callable，并且这个callable可以用一个参数来调用</strong>。而<strong>self_attn函数需要4个参数(Query的输入,Key的输入,Value的输入和Mask)</strong>，因此这里我们使用lambda的技巧把它变成一个参数x的函数(mask可以看成已知的数)。</p>
<p>Callable 类型是可以被执行调用操作的类型。包含自定义函数等。自定义的函数比如使用def、lambda所定义的函数</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderLayer</span><span class="params">(nn.Module)</span>:</span> <span class="comment">#每一个编码层，包含两个子层</span></span><br><span class="line">    <span class="string">"Encoder is made up of self-attn and feed forward (defined below)"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, self_attn, feed_forward, dropout)</span>:</span></span><br><span class="line">        super(EncoderLayer, self).__init__()</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">2</span>) <span class="comment">#每一层有2子层</span></span><br><span class="line">        self.size = size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask)</span>:</span></span><br><span class="line">      <span class="comment">#attention层，括号里面是参数。接收来自attention的输出</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">     lambda : atten()SublayerConnection里是作为sublayer出现的，而它的参数是norm(x),norm(x)的输出是一个向量x，</span></span><br><span class="line"><span class="string">   所以atten的参数是只有一个x， 而在muitihead里面，k、q、v在函数里是要被重新根据x计算的</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, mask)) </span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">1</span>](x, self.feed_forward) <span class="comment">#x是atten+norm之后的输出，再ff输出</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    可以理解为</span></span><br><span class="line"><span class="string">    z = lambda y: self.self_attn(y, y, y, mask)</span></span><br><span class="line"><span class="string">	x = self.sublayer[0](x, z)</span></span><br><span class="line"><span class="string">    """</span></span><br></pre></td></tr></tbody></table></figure>
<h4 id="decoder">Decoder</h4>
<p>The decoder is also composed of a stack of <code>N=6</code> identical layers.</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Generic N layer decoder with masking."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layer, N)</span>:</span></span><br><span class="line">        super(Decoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">     <span class="comment"># memory: 编码器的输出 x是输入</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, memory, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, memory, src_mask, tgt_mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderLayer</span><span class="params">(nn.Module)</span>:</span> <span class="comment">#每一层解码层</span></span><br><span class="line">    <span class="string">"Decoder is made of self-attn, src-attn, and feed forward (defined below)"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, self_attn, src_attn, feed_forward, dropout)</span>:</span></span><br><span class="line">        super(DecoderLayer, self).__init__()</span><br><span class="line">        self.size = size</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.src_attn = src_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">3</span>) <span class="comment">#每一层有3个子层</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, memory, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="string">"Follow Figure 1 (right) for connections."</span></span><br><span class="line">        m = memory</span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, tgt_mask)) <span class="comment">#第一子层</span></span><br><span class="line">        x = self.sublayer[<span class="number">1</span>](x, <span class="keyword">lambda</span> x: self.src_attn(x, m, m, src_mask)) <span class="comment">#第二子层 </span></span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">2</span>](x, self.feed_forward) <span class="comment">#第三子层</span></span><br></pre></td></tr></tbody></table></figure>
<p><strong>src-attn和self-attn的实现是一样的，只不过使用的Query，Key和Value的输入不同。</strong>普通的Attention(src-attn)的Query是下层输入进来的(来自self-attn的输出)，Key和Value是Encoder最后一层的输出memory；而Self-Attention的Query，Key和Value都是来自下层输入进来的。</p>
<hr>
<p>Decoder和Encoder有一个关键的不同：Decoder在解码第t个时刻的时候只能使用<strong>1…t时刻</strong>的输入，而不能使用t+1时刻及其之后的输入。因此我们需要一个函数来产生一个Mask矩阵，所以代码如下：</p>
<p>注意： t时刻包括t时刻的输入</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">subsequent_mask</span><span class="params">(size)</span>:</span>  <span class="comment">#将i后面的mask掉</span></span><br><span class="line">    <span class="string">"Mask out subsequent positions."</span></span><br><span class="line">    attn_shape = (<span class="number">1</span>, size, size)</span><br><span class="line">    subsequent_mask = np.triu(np.ones(attn_shape), k=<span class="number">1</span>).astype(<span class="string">'uint8'</span>) <span class="comment">#triu 上三角 对角线上移1位</span></span><br><span class="line">    <span class="keyword">return</span> torch.from_numpy(subsequent_mask) == <span class="number">0</span> <span class="comment">#将numpy转换为tensor格式，判断是否为0，输出布尔值</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#直接np.tril（）取下三角应该也是可以的</span></span><br></pre></td></tr></tbody></table></figure>
<p><img src="https://i.loli.net/2020/08/08/7brnPfDJxsLBtvh.png" alt="png"></p>
<p>它的输出：</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">print(subsequent_mask(5))</span><br><span class="line"># 输出</span><br><span class="line">  1  0  0  0  0</span><br><span class="line">  1  1  0  0  0</span><br><span class="line">  1  1  1  0  0</span><br><span class="line">  1  1  1  1  0</span><br><span class="line">  1  1  1  1  1</span><br></pre></td></tr></tbody></table></figure>
<p>我们发现它输出的是一个方阵，对角线和下面都是1。<strong>第一行只有第一列是1，它的意思是时刻1只能attend to输入1</strong>，第三行说明时刻3可以<code>attend to {1,2,3}</code>而不能<code>attend to{4,5}</code>的输入，因为在真正Decoder的时候这是属于Future的信息。代码首先使用triu产生一个上三角阵：</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">0 1 1 1 1</span><br><span class="line">0 0 1 1 1</span><br><span class="line">0 0 0 1 1</span><br><span class="line">0 0 0 0 1</span><br><span class="line">0 0 0 0 0</span><br></pre></td></tr></tbody></table></figure>
<p>然后需要把0变成1，把1变成0，这可以使用 matrix == 0来实现。</p>
<p>因为：布尔值True被索引求值为1，而False就等于0。</p>
<h4 id="attention">Attention</h4>
<p>An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.</p>
<p>We call our particular attention “<code>Scaled Dot-Product Attention</code>”. The input consists of queries and keys of dimension <code>dk</code>, and values of dimension <code>dv</code>. We compute the dot products of the query with all keys, divide each by <code>√dk</code>, and apply a softmax function to obtain the weights on the values.</p>
<p><img src="https://i.loli.net/2020/08/06/O3UNSGF7Poa1w4Q.png" alt="image-20200806015122441"></p>
<p><strong>Attention可以看成一个函数，它的输入是Query,Key,Value和Mask，输出是一个Tensor</strong>。其中输出是Value的加权平均，而权重来自Query和Key的计算。具体的计算如下图所示，计算公式为：</p>
<p><img src="https://i.loli.net/2020/07/28/WaSfHnNdt2L1AXU.png" alt="image-20200728212241453" style="zoom:50%;"></p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention</span><span class="params">(query, key, value, mask=None, dropout=None)</span>:</span></span><br><span class="line">    <span class="string">"Compute 'Scaled Dot Product Attention'"</span></span><br><span class="line">    d_k = query.size(<span class="number">-1</span>) <span class="comment"># query.size的最后一维</span></span><br><span class="line">    scores = torch.matmul(query, key.transpose(<span class="number">-2</span>, <span class="number">-1</span>)) / math.sqrt(d_k)</span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:<span class="comment"># 如果有mask</span></span><br><span class="line">        scores = scores.masked_fill(mask == <span class="number">0</span>, <span class="number">-1e9</span>)</span><br><span class="line">    p_attn = F.softmax(scores, dim = <span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: <span class="comment">#对p_attn进行dropout</span></span><br><span class="line">        p_attn = dropout(p_attn)</span><br><span class="line">    <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br></pre></td></tr></tbody></table></figure>
<p>我们知道, <strong>在训练的时候, 我们是以 batch_size 为单位的</strong>, 那么就会有 padding, 一般我们取 pad == 0, 那么就会造成在 Attention 的时候, query 的值为 0, query 的值为 0, 所以我们计算的对应的 scores 的值也是 0, 那么就会导致 softmax 很可能分配给该单词一个相对不是很小的比例, 因此, 我们将 pad 对应的 score 取值为<strong>负无穷</strong>（普通的计算，score可以为负数？不是，根据softmax的计算公式，就算pad=0，那么e^0=1,也是会占有一些概率值的）, 以此来减小 pad 的影响.</p>
<p>很容易想到, 在 decoder, <strong>未预测的单词</strong>也是用 padding 的方式加入到 batch 的, 所以使用的mask 机制与 padding 时mask 的机制是相同的, 本质上都是query 的值为0, 只是 mask 矩阵不同, 我们可以根据 decoder 部分的代码发现这一点.</p>
<hr>
<p>我们使用一个<strong>实际的例子跟踪一些不同Tensor的shape</strong>，然后对照公式就很容易理解。比如<strong>Q是(30,8,33,64)，其中30是batch，8是head个数，33是序列长度，64是每个时刻的特征数（size）。K和Q的shape必须相同的，而V可以不同，但是这里的实现shape也是相同的。</strong></p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">scores = torch.matmul(query, key.transpose(<span class="number">-2</span>, <span class="number">-1</span>)) \</span><br><span class="line">/ math.sqrt(d_k)</span><br></pre></td></tr></tbody></table></figure>
<p>上面的代码实现<img src="https://i.loli.net/2020/08/06/rLCJ7VFBAsmQb4x.png" alt="image-20200806014945713" style="zoom:50%;">，和公式里稍微不同的是，这里的Q和K都是4d的Tensor，包括batch和head维度。<strong>matmul会把query和key的最后两维进行矩阵乘法</strong>，这样效率更高，如果我们要用标准的矩阵(二维Tensor)乘法来实现，那么需要遍历batch维和head维：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">batch_num = query.size(<span class="number">0</span>) <span class="comment"># query.size(0)返回的是0维的数</span></span><br><span class="line">head_num = query.size(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(batch_num):</span><br><span class="line">	<span class="keyword">for</span> j <span class="keyword">in</span> range(head_num):</span><br><span class="line">		scores[i,j] = torch.matmul(query[i,j], key[i,j].transpose())</span><br></pre></td></tr></tbody></table></figure>
<p>而上面的写法一次完成所有这些循环，效率更高。<strong>输出的score是(30, 8, 33, 33)</strong>，前面两维不看，那<strong>么是一个(33, 33)的attention矩阵a，aij表示时刻 i关注 j 的得分</strong>(还没有经过softmax变成概率)。</p>
<p><strong>在编码器的attention中src_mask的作用！！！</strong></p>
<p>接下来是<code>scores.masked_fill(mask == 0, -1e9)</code>，用于<strong>把mask是0的变成一个很小的数</strong>，这样后面经过softmax之后的概率就很接近零(但是理论上还是用来很少一点点未来的信息)。</p>
<blockquote>
<p>masked_fill_(mask, value)：掩码操作 masked_fill方法有两个参数，maske和value，mask是一个pytorch张量（Tensor），<strong>元素是布尔值，value是要填充的值</strong>，填充规则是mask中取值为True位置对应于self的相应位置用value填充。</p>
<p>注：参数mask必须与score的size相同或者两者是可广播(broadcasting-semantics)的</p>
<p>pytorch masked_fill方法简单理解</p>
<p>https://blog.csdn.net/jianyingyao7658/article/details/103382654</p>
<p>pytorch 广播语义(Broadcasting semantics)</p>
<p>https://blog.csdn.net/qq_35012749/article/details/88308657</p>
</blockquote>
<p>这里<strong>mask是(30, 1, 1, 33)的tensor</strong>，因为8个head的mask都是一样的，所有第二维是1，masked_fill时使用broadcasting就可以了。这里是self-attention的mask，所以每个时刻都可以attend到所有其它时刻，所有第三维也是1，也使用broadcasting。如果是普通的mask，那么mask的shape是(30, 1, 33, 33)。</p>
<p>这样讲有点抽象，我们可以举一个例子，为了简单，我们假设batch=2, head=8。第一个序列长度为3，第二个为4，那么self-attention的mask为(2, 1, 1, 4)，我们可以用两个向量表示：</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">1 1 1 0</span><br><span class="line">1 1 1 1</span><br></pre></td></tr></tbody></table></figure>
<p>它的意思是在self-attention里，第一个序列的任一时刻可以attend to 前3个时刻(因为第4个时刻是padding的)；而第二个序列的可以attend to所有时刻的输入。而Decoder的src-attention的mask为(2, 1, 4, 4)，我们需要用2个矩阵表示：(一个序列对应一个一维src_mask（1×4）， 一个序列对应一个二维的tgt_mask（4×4）)</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">第一个序列的mask矩阵</span><br><span class="line">1 0 0 0</span><br><span class="line">1 1 0 0</span><br><span class="line">1 1 1 0</span><br><span class="line">1 1 1 0</span><br><span class="line"></span><br><span class="line">第二个序列的mask矩阵</span><br><span class="line">1 0 0 0</span><br><span class="line">1 1 0 0 </span><br><span class="line">1 1 1 0</span><br><span class="line">1 1 1 1</span><br></pre></td></tr></tbody></table></figure>
<p>接下来对score求softmax，把得分变成概率p_attn，如果有dropout还对p_attn进行Dropout(这也是原始论文没有的)。最后把p_attn和value相乘。p_attn是(30, 8, 33, 33)，value是(30, 8, 33, 64)，我们<strong>只看后两维，(33x33) x (33x64)最终得到33x64。</strong></p>
<hr>
<p>接下来就是输入怎么变成Q,K和V了，<strong>对于每一个Head，都使用三个矩阵WQ,WK,WV把输入转换成Q，K和V。</strong>然后<strong>分别用每一个Head进行Self-Attention的计算，最后把N个Head的输出拼接起来，最后用一个矩阵WO把输出压缩一下。</strong>具体计算过程为：</p>
<p><img src="https://i.loli.net/2020/08/06/1IbPcFJeK8tsHqN.png" alt="image-20200806023820900" style="zoom: 67%;"></p>
<p>详细结构如下图所示，<strong>输入Q，K和V经过多个线性变换后得到N(8)组Query，Key和Value</strong>，然后使用Self-Attention计算得到N个向量，然后拼接起来，<strong>最后使用一个线性变换进行降维。</strong></p>
<p><img src="https://i.loli.net/2020/08/08/a2gozSYGn8NOkpH.png" alt="png" style="zoom:67%;"></p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadedAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, h, d_model, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        <span class="string">"Take in model size and number of heads."</span></span><br><span class="line">        super(MultiHeadedAttention, self).__init__()</span><br><span class="line">        <span class="keyword">assert</span> d_model % h == <span class="number">0</span>  <span class="comment"># 不能整除就报错</span></span><br><span class="line">        <span class="comment"># We assume d_v always equals d_k</span></span><br><span class="line">        self.d_k = d_model // h</span><br><span class="line">        self.h = h</span><br><span class="line">        self.linears = clones(nn.Linear(d_model, d_model), <span class="number">4</span>) <span class="comment"># 构造4个(d_model ， d_model)的矩阵</span></span><br><span class="line">        self.attn = <span class="literal">None</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, mask=None)</span>:</span>  </span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        在编码器中，q、k、v都是x作为输入。</span></span><br><span class="line"><span class="string">        在解码器中，第一层是x输入；第二层是q是x输入，k、v是memory输入</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        <span class="string">"Implements Figure 2"</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># # 所有h个head的mask都是相同的 </span></span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>) <span class="comment">#在维度为1的位置添加一个维度，数字为1</span></span><br><span class="line">        nbatches = query.size(<span class="number">0</span>) <span class="comment">#就是有多少batch的值</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 1) 首先使用线性变换，然后把d_model分配给h个Head，每个head为d_k=d_model/h </span></span><br><span class="line">        query, key, value = \</span><br><span class="line">            [l(x).view(nbatches, <span class="number">-1</span>, self.h, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">             <span class="keyword">for</span> l, x <span class="keyword">in</span> zip(self.linears, (query, key, value))]</span><br><span class="line">           <span class="comment">#.view()表示重构张量的维度</span></span><br><span class="line">           <span class="comment">#对于l对应的是zip中的linears ， x分别对应的是query/key/value 。 然后分别进行l(x)运算</span></span><br><span class="line">           <span class="comment">#为什么要进行transpose(1, 2)？？？？</span></span><br><span class="line">         <span class="comment">#注：因为每个Linear学习到的参数是不一样的。所以qkv三个也是不一样的</span></span><br><span class="line">            </span><br><span class="line">        <span class="comment"># 2)使用attention函数计算 </span></span><br><span class="line">        x, self.attn = attention(query, key, value, mask=mask, </span><br><span class="line">                                 dropout=self.dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3) 把8个head的64维向量拼接成一个512的向量。然后再使用一个线性变换(512,521)，shape不变。 </span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous() \</span><br><span class="line">             .view(nbatches, <span class="number">-1</span>, self.h * self.d_k)</span><br><span class="line">        <span class="keyword">return</span> self.linears[<span class="number">-1</span>](x)</span><br></pre></td></tr></tbody></table></figure>
<p>我们先看构造函数，这里<strong>d_model(512)是Multi-Head的输出大小</strong>，因为有h(8)个head，因此每个head的d_k=512/8=64。接着我们构造4个(d_model ， d_model)的矩阵，后面我们会看到它的用处。最后是构造一个Dropout层。</p>
<p>然后我们来看forward方法。<strong>输入的mask是(batch, 1, time)的，因为每个head的mask都是一样的，所以先用unsqueeze(1)变成(batch, 1, 1, time)</strong>，mask我们前面已经详细分析过了。</p>
<p>接下来是<strong>根据输入query，key和value计算变换后的Multi-Head的query，key和value</strong>。这是通过下面的语句来实现的：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">query, key, value = \</span><br><span class="line">		[l(x).view(nbatches, <span class="number">-1</span>, self.h, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)	</span><br><span class="line">			<span class="keyword">for</span> l, x <span class="keyword">in</span> zip(self.linears, (query, key, value))] <span class="comment"># l(x): 调用nn.Linear函数</span></span><br></pre></td></tr></tbody></table></figure>
<p><strong>zip(self.linears, (query, key, value))是把(self.linears[0],self.linears[1],self.linears[2])和(query, key, value)放到一起然后遍历。我们只看一个self.linears[0] (query)。根据构造函数的定义，self.linears[0]是一个(512, 512)的矩阵，而query是(batch, time, 512)，相乘之后得到的新query还是512(d_model)维的向量，然后用view把它变成(batch, time, 8, 64)。然后transponse成(batch, 8,time,64)，这是attention函数要求的shape。分别对应8个Head，每个Head的Query都是64维。</strong></p>
<blockquote>
<p>1.一般来说，矩阵相乘，[a,b] x [b,c] = [a,c]</p>
<p>所以不同维度要进行处理，必须降维。</p>
<p>例如 A 矩阵 [a,b,c], B 矩阵是[c,d]，这个时候就需要将 A 矩阵看成是 [axb, c] 与 [c,d] 进行相乘，得到结果。</p>
<ol start="2" type="1">
<li>Linear函数l(x)，应该就是 (batch*time,512)**(512,512)</li>
</ol>
</blockquote>
<p>Key和Value的运算完全相同，因此我们也分别得到8个Head的64维的Key和64维的Value。接下来<strong>调用attention函数，得到x和self.attn。其中x的shape是(batch, 8, time, 64)，而attn是(batch, 8, time, time)。</strong></p>
<blockquote>
<p>time是倍数</p>
</blockquote>
<p><strong>x.transpose(1, 2)把x变成(batch, time, 8, 64)，然后把它view成(batch, time, 512)，其实就是把最后8个64维的向量拼接成512的向量。最后使用self.linears[-1]对x进行线性变换，self.linears[-1]是(512, 512)的，因此最终的输出还是(batch, time, 512)。我们最初构造了4个(512, 512)的矩阵，前3个用于对query，key和value进行变换，而最后一个对8个head拼接后的向量再做一次变换。</strong></p>
<h4 id="attention在模型中的应用">Attention在模型中的应用</h4>
<p>在Transformer里，有3个地方用到了MultiHeadedAttention：</p>
<ul>
<li><p>Encoder的Self-Attention层</p>
<p><strong>query，key和value都是相同的值</strong>，来自下层的输入。Mask都是1(当然padding的不算)。</p></li>
<li><p>Decoder的Self-Attention层</p>
<p><strong>query，key和value都是相同的值</strong>，来自下层的输入。但是Mask使得它不能访问未来的输入。</p></li>
<li><p>Encoder-Decoder的普通Attention</p>
<p><strong>query来自下层的输入，而key和value相同</strong>，是Encoder最后一层的输出，而Mask都是1。</p></li>
</ul>
<h3 id="position-wise-前馈网络">Position-wise 前馈网络</h3>
<p>In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. <code>This consists of two linear transformations with a ReLU activation in between.</code></p>
<p>全连接层有两个线性变换以及它们之间的ReLU激活组成：</p>
<p><img src="https://i.loli.net/2020/08/08/PU96rciRsWxOCKp.png" alt="image-20200728231445307" style="zoom:50%;"></p>
<p>全连接层的输入和输出都是d_model(512)维的，中间隐单元的个数是d_ff(2048)维</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionwiseFeedForward</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Implements FFN equation."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, d_ff, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        super(PositionwiseFeedForward, self).__init__()</span><br><span class="line">        self.w_1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        self.w_2 = nn.Linear(d_ff, d_model)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.w_2(self.dropout(F.relu(self.w_1(x))))</span><br></pre></td></tr></tbody></table></figure>
<h3 id="embeddings-和-softmax">Embeddings 和 Softmax</h3>
<p><strong>输入的词序列都是ID序列，我们需要Embedding</strong>。源语言和目标语言都需要Embedding，此外我们需要一个线性变换把隐变量变成输出概率，这可以通过前面的类Generator来实现。我们这里实现Embedding：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Embeddings</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, vocab)</span>:</span></span><br><span class="line">        super(Embeddings, self).__init__()</span><br><span class="line">        self.lut = nn.Embedding(vocab, d_model) <span class="comment">#将字典vocab大小映射到d_model大小</span></span><br><span class="line">        self.d_model = d_model</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.lut(x) * math.sqrt(self.d_model)</span><br></pre></td></tr></tbody></table></figure>
<p>注意的就是forward处理使用nn.Embedding对输入x进行Embedding之外，还除以了sqrt(d_model) （开方）</p>
<h3 id="位置编码">位置编码</h3>
<p>位置编码的公式为：</p>
<p><img src="https://i.loli.net/2020/08/08/WUpXhHsK3S1jCqn.png" alt="image-20200728232133981" style="zoom:50%;"></p>
<p><img src="https://i.loli.net/2020/08/08/XOZPy89KVi1xjTh.png" alt="image-20200728232255029" style="zoom:50%;"></p>
<p>where <code>pos</code> is the position and <code>i</code> is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid.</p>
<p>假设输入是ID序列长度为10，<strong>如果输入Embedding之后是(10, 512)，那么位置编码的输出也是(10, 512)。</strong>上式中pos就是位置(0-9)，512维的偶数维使用sin函数，而奇数维使用cos函数。这种位置编码的好处是：PE_pos+k可以表示成 PE_pos的线性函数，这样网络就能容易的学到相对位置的关系。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">15</span>, <span class="number">5</span>))</span><br><span class="line">pe = PositionalEncoding(<span class="number">20</span>, <span class="number">0</span>)</span><br><span class="line">y = pe.forward(Variable(torch.zeros(<span class="number">1</span>, <span class="number">100</span>, <span class="number">20</span>)))</span><br><span class="line">plt.plot(np.arange(<span class="number">100</span>), y[<span class="number">0</span>, :, <span class="number">4</span>:<span class="number">8</span>].data.numpy())</span><br><span class="line">plt.legend([<span class="string">"dim %d"</span>%p <span class="keyword">for</span> p <span class="keyword">in</span> [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>]])</span><br></pre></td></tr></tbody></table></figure>
<p>图是一个示例，向量的大小d_model=20，我们这里画出来第4、5、6和7维(下标从零开始)维的图像，最大的位置是100。我们可以看到它们都是正弦(余弦)函数，而且周期越来越长。</p>
<p><img src="https://i.loli.net/2020/08/08/TfDHKvnM3emYysL.png" alt="png"></p>
<p>前面我们提到位置编码的好处是PE_pos+k可以表示成 P_Epos的线性函数，我们下面简单的验证一下。我们以第i维为例，为了简单，我们把<img src="https://i.loli.net/2020/08/06/iEoDOvKzB42N6Xe.png" alt="image-20200806104700979" style="zoom: 67%;">记作Wi，这是一个常量。</p>
<p><img src="https://i.loli.net/2020/08/06/E9h2vXIDK1MAjUg.png" alt="image-20200806104725624" style="zoom:67%;"></p>
<p>我们发现PE_pos+k 确实可以表示成 PE_pos的线性函数。</p>
<p>In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of <code>Pdrop=0.1</code>.</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Implement the PE function."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, dropout, max_len=<span class="number">5000</span>)</span>:</span></span><br><span class="line">        super(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute the positional encodings once in log space.</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        <span class="comment">#之所以用log再exp,可能是考虑到数值过大溢出的问题</span></span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) * </span><br><span class="line">                             -(math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        self.register_buffer(<span class="string">'pe'</span>, pe)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = x + Variable(self.pe[:, :x.size(<span class="number">1</span>)], </span><br><span class="line">                         requires_grad=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></tbody></table></figure>
<p>代码可以参考公式，调用了<code>Module.register_buffer函数</code>。这个函数的作用是创建一个buffer，比如这里把pe保存下来。register_buffer通常用于保存一些模型参数之外的值，比如在BatchNorm中，我们需要保存running_mean(Moving Average)，它不是模型的参数(不用梯度下降)，但是模型会修改它，而且在预测的时候也要使用它。这里也是类似的，pe是一个提前计算好的常量，我们在forward要用到它。我们在构造函数里并没有把pe保存到self里，但是在forward的时候我们却可以直接使用它(self.pe)。如果我们保存(序列化)模型到磁盘的话，PyTorch框架也会帮我们保存buffer里的数据到磁盘，这样反序列化的时候能恢复它们</p>
<h3 id="完整模型">完整模型</h3>
<blockquote>
<p>Here we <code>define a function that takes in hyperparameters and produces a full model.</code></p>
</blockquote>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_model</span><span class="params">(src_vocab, tgt_vocab, N=<span class="number">6</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">               d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, h=<span class="number">8</span>, dropout=<span class="number">0.1</span>)</span>:</span> <span class="comment">#d_ff： feedforward的维度</span></span><br><span class="line">    <span class="string">"Helper: Construct a model from hyperparameters."</span></span><br><span class="line">    c = copy.deepcopy</span><br><span class="line">    attn = MultiHeadedAttention(h, d_model)</span><br><span class="line">    ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line">    position = PositionalEncoding(d_model, dropout)</span><br><span class="line">    model = EncoderDecoder(</span><br><span class="line">        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),</span><br><span class="line">        Decoder(DecoderLayer(d_model, c(attn), c(attn), </span><br><span class="line">                             c(ff), dropout), N),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),</span><br><span class="line">        Generator(d_model, tgt_vocab))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># This was important from their code. </span></span><br><span class="line">    <span class="comment"># Initialize parameters with Glorot / fan_avg. 随机初始化参数，这非常重要</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">            nn.init.xavier_uniform(p)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例: 对model简单输入参数</span></span><br><span class="line">tmp_model = make_model(<span class="number">10</span>, <span class="number">10</span>, <span class="number">2</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>首先把copy.deepcopy命名为c，这样使下面的代码简洁一点。然后构造MultiHeadedAttention，PositionwiseFeedForward和PositionalEncoding对象。接着就是构造EncoderDecoder对象。它需要5个参数：Encoder、Decoder、src-embed、tgt-embed和Generator。</p>
<p>我们先看后面三个简单的参数，Generator直接构造就行了，它的作用是把模型的隐单元变成输出词的概率。而src-embed是一个Embeddings层和一个位置编码层c(position)，tgt-embed也是类似的。</p>
<p>最后我们来看Decoder(Encoder和Decoder类似的)。Decoder由N个DecoderLayer组成，而DecoderLayer需要传入self-attn, src-attn，全连接层和Dropout。因为所有的MultiHeadedAttention都是一样的，因此我们直接deepcopy就行；同理所有的PositionwiseFeedForward也是一样的网络结果，我们可以deepcopy而不要再构造一个。</p>
<h3 id="训练">训练</h3>
<p>This section describes the training regime for our models.</p>
<blockquote>
<p>We stop for a quick interlude to introduce some of the tools needed to train a standard encoder decoder model. First <code>we define a batch object that holds the src and target sentences for training, as well as constructing the masks.</code></p>
</blockquote>
<h4 id="batches-和-masking">Batches 和 Masking</h4>
<p><code>mask 矩阵来自 batch</code></p>
<p><code>self.src_mask = (src != pad).unsqueeze(-2)</code> 也就是说, 源语言的 <strong>mask 矩阵的维度是 (batch_size, 1, length)</strong>, 那么为什么 <code>attn_shape = (batch_size, size, size)</code> 呢? 可以这么解释, <strong>在 encoder 阶段的 Self_Attention 阶段, 所有的 Attention 是可以同时进行的, 把所有的 Attention_result 算出来, 然后用同一个 mask vector * Attention_result 就可以了</strong>, 但是在 decoder 阶段却不能这么做, 我们需要关注的问题是:</p>
<blockquote>
<p>根据已经预测出来的单词预测下面的单词, 这一过程<strong>是序列的</strong>,</p>
<p>而我们的计算是<strong>并行</strong>的, 所以这一过程中, 必须要引入矩阵. 也就是上面的 subsequent_mask() 函数获得的矩阵.</p>
</blockquote>
<p>这个矩阵也很形象, 分别表示已经预测的单词的个数为, 1, 2, 3, 4, 5.</p>
<p>然后我们将以上过程反过来过一篇, 就很明显了, 在batch阶段获得 mask 矩阵, 然后和 batch 一起训练, 在 encoder 与 deocder 阶段实现 mask 机制.</p>
<blockquote>
<ul>
<li><p>mask在Batch中定义，src_mask.size (30,1,10) , trg_mask.size(30,10,10)</p></li>
<li><p>然后在MultiHeadedAttention中<code>mask = mask.unsqueeze(1)</code>又扩维了，</p>
<p>其中src_mask.size(30,1,1,10) ,trg_mask.size(30,1,10,10)</p></li>
<li><p>src_mask.size满足attention中的维度，所以可以对score进行mask</p>
<p>src_mask还在解码器的第1子层用到，相同的原理</p></li>
<li><p>trg_mask在解码器的第0子层用到，满足要求</p></li>
</ul>
</blockquote>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Batch</span>:</span> <span class="comment">#定义每一个batch中的src、tgt、mask</span></span><br><span class="line">    <span class="comment">#trg = tgt: 真实的标签序列  out ： 预测的单词  </span></span><br><span class="line">    <span class="string">"Object for holding a batch of data with mask during training."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, src, trg=None, pad=<span class="number">0</span>)</span>:</span> </span><br><span class="line">        self.src = src</span><br><span class="line">        self.src_mask = (src != pad).unsqueeze(<span class="number">-2</span>) <span class="comment">#扩充维度 倒数第二维增加值为1 size=(30,1,10)</span></span><br><span class="line">        <span class="comment">#并且非零值全部赋值为1</span></span><br><span class="line">        </span><br><span class="line">         <span class="comment"># 在预测的时候是没有 tgt 的,此时为 None 此时trg是tgt的形参</span></span><br><span class="line">        <span class="keyword">if</span> trg <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.trg = trg[:, :<span class="number">-1</span>] <span class="comment">#trg.size(30,9) ，在预测中，会提前输入起始符到ys中</span></span><br><span class="line">            <span class="string">"""</span></span><br><span class="line"><span class="string">              trg.size(30,9) 这里去掉的最后一个单词, 不是真正的单词, 而是标志 '&lt;eos&gt;' , 						输入与输出都还有一个 '&lt;sos&gt;' 在句子的开头,  是decoder的输入，</span></span><br><span class="line"><span class="string">            需要进行mask，使得Self-Attention不能访问未来的输入。最后一个词不需要用到trg</span></span><br><span class="line"><span class="string">            """</span></span><br><span class="line">    	    self.trg_y = trg[:, <span class="number">1</span>:] <span class="comment"># trg_y.size(30,9) </span></span><br><span class="line">            <span class="comment">#trg_y: 最后的结果。用于loss中的比较。 去掉开头的'&lt;sos&gt;'，是decoder的输出</span></span><br><span class="line">            self.trg_mask = \</span><br><span class="line">                self.make_std_mask(self.trg, pad)</span><br><span class="line">            self.ntokens = (self.trg_y != pad).data.sum() <span class="comment">#不为0的总数 30*9 = 270</span></span><br><span class="line">    </span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_std_mask</span><span class="params">(tgt, pad)</span>:</span> <span class="comment">#tgt_mask.size(30,9,9)，每一个序列都是一个9*9的矩阵</span></span><br><span class="line">        <span class="string">"Create a mask to hide padding and future words."</span></span><br><span class="line">        <span class="comment">#"创建Mask，使得我们不能attend to未来的词"</span></span><br><span class="line">        tgt_mask = (tgt != pad).unsqueeze(<span class="number">-2</span>)</span><br><span class="line">        tgt_mask = tgt_mask &amp; Variable(</span><br><span class="line">            subsequent_mask(tgt.size(<span class="number">-1</span>)).type_as(tgt_mask.data))</span><br><span class="line">        <span class="keyword">return</span> tgt_mask</span><br></pre></td></tr></tbody></table></figure>
<p>Batch构造函数的输入是src和trg，后者可以为None，因为再预测的时候是没有tgt的。</p>
<p>我们用一个例子来说明Batch的代码，这是训练阶段的一个Batch，<strong>src是(48, 20)</strong>，48是batch大小，而20是最长的句子长度，其它的不够长的都padding成20了。而<strong>trg是(48, 25)</strong>，表示翻译后的最长句子是25个词，不足的也padding过了。</p>
<p>我们首先看src_mask怎么得到，(src != pad)把src中大于0的时刻置为1，这样表示它可以attend to的范围。然后unsqueeze(-2)把src_mask变成(48/batch, 1, 20/time)。它的用法参考前面的attention函数。</p>
<p>对于训练来说(Teaching Forcing模式)，Decoder有一个输入和一个输出。<strong>比如句子”<sos> it is a good day <eos>”，输入会变成”<sos> it is a good day”，而输出为”it is a good day <eos>”。对应到代码里，self.trg就是输入，而self.trg_y就是输出。</eos></sos></eos></sos></strong>接着对输入self.trg进行mask，使得Self-Attention不能访问未来的输入。这是通过make_std_mask函数实现的，这个函数会调用我们之前详细介绍过的subsequent_mask函数。最终得到的<strong>trg_mask的shape是(48/batch, 24, 24)</strong>，表示24个时刻的Mask矩阵，这是一个对角线以及之下都是1的矩阵，前面已经介绍过了。</p>
<p>注意<strong>src_mask的shape是(batch, 1, time)</strong>，而<strong>trg_mask是(batch, time, time)</strong>。因为src_mask的每一个时刻都能attend to所有时刻(padding的除外)，一次只需要一个向量就行了，而trg_mask需要一个矩阵。</p>
<h4 id="training-loop">Training Loop</h4>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_epoch</span><span class="params">(data_iter, model, loss_compute)</span>:</span> <span class="comment">#返回total_loss / total_tokens 。是一个数值，损失计算</span></span><br><span class="line">    <span class="comment">#遍历一个epoch的数据</span></span><br><span class="line">    <span class="string">"Standard Training and Logging Function"</span></span><br><span class="line">    start = time.time() <span class="comment">#开始时间，计算用时</span></span><br><span class="line">    total_tokens = <span class="number">0</span> </span><br><span class="line">    total_loss = <span class="number">0</span> </span><br><span class="line">    tokens = <span class="number">0</span> </span><br><span class="line">    <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(data_iter): <span class="comment">#每一步data_iter（gen_data），实例化batch数据用于学习.进行20次</span></span><br><span class="line">        <span class="comment">#gen_data返回的是20个Batch，通过enumerate实例化20个batch </span></span><br><span class="line">        out = model.forward(batch.src, batch.trg, </span><br><span class="line">                            batch.src_mask, batch.trg_mask) <span class="comment">#调用EncoderDecoder的实例化model，解码器作为输出</span></span><br><span class="line">        loss = loss_compute(out, batch.trg_y, batch.ntokens) <span class="comment">#计算出一个batch中的loss。 trg_y是标准值。ntokens作为norm</span></span><br><span class="line">        total_loss += loss <span class="comment">#loss叠加。进行20次</span></span><br><span class="line">        total_tokens += batch.ntokens </span><br><span class="line">        tokens += batch.ntokens</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">50</span> == <span class="number">1</span>: <span class="comment">#i从0开始的，当i=1的时候，进行了一次batch，所以这里计算的就是一次batch所用的时间。而要进行20次。  50是随机设置</span></span><br><span class="line">            elapsed = time.time() - start <span class="comment">#计算一共用时</span></span><br><span class="line">            print(<span class="string">"Epoch Step: %d Loss: %f Tokens per Sec: %f"</span> % </span><br><span class="line">                    (i, loss / batch.ntokens, tokens / elapsed)) <span class="comment">#所有batch中的loss和ntoken,即一个epoch中</span></span><br><span class="line">            start = time.time() <span class="comment"># 重置时间</span></span><br><span class="line">            tokens = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> total_loss / total_tokens</span><br></pre></td></tr></tbody></table></figure>
<p>它遍历一个epoch的数据，然后调用forward，接着用loss_compute函数计算梯度，更新参数并且返回loss。这里的loss_compute是一个函数，它的输入是模型的预测out，真实的标签序列batch.trg_y和batch的词个数。实际的实现是MultiGPULossCompute类，这是一个callable。本来计算损失和更新参数比较简单，但是这里为了实现多GPU的训练，这个类就比较复杂了。</p>
<h4 id="training-data-和-batching">Training Data 和 Batching</h4>
<p>We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding, which has a shared source-target vocabulary of about 37000 tokens. For English- French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary.</p>
<p>Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.</p>
<blockquote>
<p>We will use torch text for batching. This is discussed in more detail below. Here we create batches in a torchtext function that ensures our batch size padded to the maximum batchsize does not surpass a threshold (25000 if we have 8 gpus).</p>
</blockquote>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">global</span> max_src_in_batch, max_tgt_in_batch</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_size_fn</span><span class="params">(new, count, sofar)</span>:</span></span><br><span class="line">    <span class="string">"Keep augmenting batch and calculate total number of tokens + padding."</span></span><br><span class="line">    <span class="keyword">global</span> max_src_in_batch, max_tgt_in_batch</span><br><span class="line">    <span class="keyword">if</span> count == <span class="number">1</span>:</span><br><span class="line">        max_src_in_batch = <span class="number">0</span></span><br><span class="line">        max_tgt_in_batch = <span class="number">0</span></span><br><span class="line">    max_src_in_batch = max(max_src_in_batch,  len(new.src))</span><br><span class="line">    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + <span class="number">2</span>)</span><br><span class="line">    src_elements = count * max_src_in_batch</span><br><span class="line">    tgt_elements = count * max_tgt_in_batch</span><br><span class="line">    <span class="keyword">return</span> max(src_elements, tgt_elements)</span><br></pre></td></tr></tbody></table></figure>
<h4 id="硬件-和-训练进度">硬件 和 训练进度</h4>
<p>We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models, step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).</p>
<h4 id="optimizer">Optimizer</h4>
<p>We used the <code>Adam optimizer</code> <a href="https://arxiv.org/abs/1412.6980" target="_blank" rel="noopener">(cite)</a> with β1=0.9β1=0.9, β2=0.98β2=0.98 and ϵ=10−9ϵ=10−9. We varied the learning rate over the course of training, according to the formula: lrate=d−0.5model⋅min(step_num−0.5,step_num⋅warmup_steps−1.5)lrate=dmodel−0.5⋅min(step_num−0.5,step_num⋅warmup_steps−1.5) This corresponds to increasing the learning rate linearly for the first warmupstepswarmupsteps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmupsteps=4000warmupsteps=4000.</p>
<blockquote>
<p>Note: This part is very important. Need to train with this setup of the model.</p>
</blockquote>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NoamOpt</span>:</span></span><br><span class="line">    <span class="string">"Optim wrapper that implements rate."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, model_size, factor, warmup, optimizer)</span>:</span></span><br><span class="line">        self.optimizer = optimizer</span><br><span class="line">        self._step = <span class="number">0</span></span><br><span class="line">        self.warmup = warmup</span><br><span class="line">        self.factor = factor</span><br><span class="line">        self.model_size = model_size</span><br><span class="line">        self._rate = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"Update parameters and rate"</span></span><br><span class="line">        self._step += <span class="number">1</span></span><br><span class="line">        rate = self.rate()</span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> self.optimizer.param_groups:</span><br><span class="line">            p[<span class="string">'lr'</span>] = rate</span><br><span class="line">        self._rate = rate</span><br><span class="line">        self.optimizer.step() <span class="comment">#更新参数</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">rate</span><span class="params">(self, step = None)</span>:</span></span><br><span class="line">        <span class="string">"Implement `lrate` above"</span></span><br><span class="line">        <span class="keyword">if</span> step <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            step = self._step</span><br><span class="line">        <span class="keyword">return</span> self.factor * \</span><br><span class="line">            (self.model_size ** (<span class="number">-0.5</span>) *</span><br><span class="line">            min(step ** (<span class="number">-0.5</span>), step * self.warmup ** (<span class="number">-1.5</span>)))</span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_std_opt</span><span class="params">(model)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> NoamOpt(model.src_embed[<span class="number">0</span>].d_model, <span class="number">2</span>, <span class="number">4000</span>,</span><br><span class="line">            torch.optim.Adam(model.parameters(), lr=<span class="number">0</span>, betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span>))</span><br></pre></td></tr></tbody></table></figure>
<blockquote>
<p>Example of the curves of this model for different model sizes and for optimization hyperparameters.</p>
</blockquote>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># Three settings of the lrate hyperparameters.</span></span><br><span class="line">opts = [NoamOpt(<span class="number">512</span>, <span class="number">1</span>, <span class="number">4000</span>, <span class="literal">None</span>), </span><br><span class="line">        NoamOpt(<span class="number">512</span>, <span class="number">1</span>, <span class="number">8000</span>, <span class="literal">None</span>),</span><br><span class="line">        NoamOpt(<span class="number">256</span>, <span class="number">1</span>, <span class="number">4000</span>, <span class="literal">None</span>)]</span><br><span class="line">plt.plot(np.arange(<span class="number">1</span>, <span class="number">20000</span>), [[opt.rate(i) <span class="keyword">for</span> opt <span class="keyword">in</span> opts] <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">20000</span>)])</span><br><span class="line">plt.legend([<span class="string">"512:4000"</span>, <span class="string">"512:8000"</span>, <span class="string">"256:4000"</span>])</span><br><span class="line"><span class="literal">None</span></span><br></pre></td></tr></tbody></table></figure>
<p><img src="http://nlp.seas.harvard.edu/images/the-annotated-transformer_69_0.png" alt="png"></p>
<h4 id="regularization">Regularization</h4>
<h5 id="label-smoothing">Label Smoothing</h5>
<p>During training, we employed label smoothing of value ϵls=0.1ϵls=0.1 <a href="https://arxiv.org/abs/1512.00567" target="_blank" rel="noopener">(cite)</a>. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.</p>
<blockquote>
<p>We implement label smoothing using the KL div loss. Instead of using a one-hot target distribution, we create a distribution that has <code>confidence</code> of the correct word and the rest of the <code>smoothing</code> mass distributed throughout the vocabulary.</p>
</blockquote>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LabelSmoothing</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Implement label smoothing."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, padding_idx, smoothing=<span class="number">0.0</span>)</span>:</span></span><br><span class="line">        super(LabelSmoothing, self).__init__()</span><br><span class="line">        self.criterion = nn.KLDivLoss(size_average=<span class="literal">False</span>)  <span class="comment">#KL散度</span></span><br><span class="line">        self.padding_idx = padding_idx</span><br><span class="line">        self.confidence = <span class="number">1.0</span> - smoothing</span><br><span class="line">        self.smoothing = smoothing</span><br><span class="line">        self.size = size</span><br><span class="line">        self.true_dist = <span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, target)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> x.size(<span class="number">1</span>) == self.size</span><br><span class="line">        true_dist = x.data.clone()</span><br><span class="line">        true_dist.fill_(self.smoothing / (self.size - <span class="number">2</span>))</span><br><span class="line">        true_dist.scatter_(<span class="number">1</span>, target.data.unsqueeze(<span class="number">1</span>), self.confidence)</span><br><span class="line">        true_dist[:, self.padding_idx] = <span class="number">0</span></span><br><span class="line">        mask = torch.nonzero(target.data == self.padding_idx)</span><br><span class="line">        <span class="keyword">if</span> mask.dim() &gt; <span class="number">0</span>:</span><br><span class="line">            true_dist.index_fill_(<span class="number">0</span>, mask.squeeze(), <span class="number">0.0</span>)</span><br><span class="line">        self.true_dist = true_dist</span><br><span class="line">        <span class="keyword">return</span> self.criterion(x, Variable(true_dist, requires_grad=<span class="literal">False</span>))</span><br></pre></td></tr></tbody></table></figure>
<blockquote>
<p>Here we can see an example of how the mass is distributed to the words based on confidence.</p>
</blockquote>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># Example of label smoothing.</span></span><br><span class="line">crit = LabelSmoothing(<span class="number">5</span>, <span class="number">0</span>, <span class="number">0.4</span>)</span><br><span class="line">predict = torch.FloatTensor([[<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>],</span><br><span class="line">                             [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>], </span><br><span class="line">                             [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>]])</span><br><span class="line">v = crit(Variable(predict.log()), </span><br><span class="line">         Variable(torch.LongTensor([<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>])))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show the target distributions expected by the system.</span></span><br><span class="line">plt.imshow(crit.true_dist)</span><br><span class="line"><span class="literal">None</span></span><br></pre></td></tr></tbody></table></figure>
<p><img src="http://nlp.seas.harvard.edu/images/the-annotated-transformer_74_0.png" alt="png"></p>
<blockquote>
<p>Label smoothing actually starts to penalize the model if it gets very confident about a given choice.</p>
</blockquote>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">crit = LabelSmoothing(<span class="number">5</span>, <span class="number">0</span>, <span class="number">0.1</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(x)</span>:</span></span><br><span class="line">    d = x + <span class="number">3</span> * <span class="number">1</span></span><br><span class="line">    predict = torch.FloatTensor([[<span class="number">0</span>, x / d, <span class="number">1</span> / d, <span class="number">1</span> / d, <span class="number">1</span> / d],</span><br><span class="line">                                 ])</span><br><span class="line">    <span class="comment">#print(predict)</span></span><br><span class="line">    <span class="keyword">return</span> crit(Variable(predict.log()),</span><br><span class="line">                 Variable(torch.LongTensor([<span class="number">1</span>]))).data[<span class="number">0</span>]</span><br><span class="line">plt.plot(np.arange(<span class="number">1</span>, <span class="number">100</span>), [loss(x) <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">100</span>)])</span><br><span class="line"><span class="literal">None</span></span><br></pre></td></tr></tbody></table></figure>
<p><img src="http://nlp.seas.harvard.edu/images/the-annotated-transformer_76_0.png" alt="png"></p>
<h3 id="总结"><strong>总结</strong></h3>
<p>transformer模型主要分为两大部分, 分别是编码器和解码器, 编码器负责把自然语言序列映射成为隐藏层(下图中第2步用九宫格比喻的部分), 含有自然语言序列的数学表达. 然后解码器把隐藏层再映射为自然语言序列, 从而使我们可以解决各种问题, 如情感分类, 命名实体识别, 语义关系抽取, 摘要生成, 机器翻译等等, 下面我们简单说一下下图的每一步都做了什么:</p>
<blockquote>
<p>1.输入自然语言序列到编码器: Why do we work?(为什么要工作);</p>
<p>2.编码器输出的隐藏层, 再输入到解码器;</p>
<p>3.输入&lt;𝑠𝑡𝑎𝑟𝑡&gt;<start>(起始)符号到解码器;</start></p>
<p>4.得到第一个字"为";</p>
<p>5.将得到的第一个字"为"落下来再输入到解码器;</p>
<p>6.得到第二个字"什";</p>
<p>7.将得到的第二字再落下来, 直到解码器输出&lt;𝑒𝑛𝑑&gt;<end>(终止符), 即序列生成完成.</end></p>
</blockquote>
<p><img src="https://i.loli.net/2020/08/06/1pea3WSThisHBql.png" alt="image-20200806233205808" style="zoom:67%;"></p>
<p><img src="https://i.loli.net/2020/08/07/ZGa1snNULJtjFWS.png" alt="transformer"></p>
<p>原始data数据是：(30,10)</p>
<p>src: (30,10) trg:(30,10)</p>
<p>在encoder中，</p>
<p>embedding： 参数x就是 src （30,10） 经过处理之后， x:（30,10,512）</p>
<p>-&gt; 即输入给encoder的x：(30,10,512)</p>
<p>经过encoder各个层处理之后，输出的（30，10,512） memory是encoder的输出，但是为什么memory：（1,10,512） ??? 因为在预测时 ，src是（1,10），不是（30,10）所以memory是（1,10,512）</p>
<p>decoder中：输入来自 memory 和 trg_emd</p>
<p>embedding ： 参数x是trg（30,9），经过处理之后，x：（30,9,512)</p>
<p>经过decoder各个层处理之后，输出的（30，9 , 512）</p>
<p>再经过generator层之后，x：（30,9,11）</p>
<p>在预测的时候是（1，1,512），不是（1,9,512），在预测完generator之后，（1,11），选一个最大的。</p>
<p>因为是一个数字一个数字预测输出的，所以是1，不是9</p>
<h4 id="具体流程">具体流程</h4>
<p>transformer的encoder部分，是并行计算各个token之间的关系，然后输出给decoder一个memory，decoder再产生预测值，计算loss之后，再反向传播。因为是一个计算图，所以encoder和decoder的参数都会更新。</p>
<p>参数更新完之后，下一个batch的数据再输进来，再计算loss，再更新，以此循环</p>
<h3 id="第一个例子">第一个例子</h3>
<blockquote>
<p>We can begin by trying out a simple copy-task. Given a random set of input symbols from a small vocabulary, the goal is to generate back those same symbols.</p>
</blockquote>
<h4 id="synthetic-data">Synthetic Data</h4>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_gen</span><span class="params">(V, batch, nbatches)</span>:</span> <span class="comment"># batch=30:一次输入多少， nbatch=20：输入多少次</span></span><br><span class="line">    <span class="string">"Generate random data for a src-tgt copy task."</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(nbatches): <span class="comment">#一共循环nbatches个，在每一个是一个batch</span></span><br><span class="line">		<span class="comment">#from_numpy ： 将numpy数据转换为tensor</span></span><br><span class="line">		<span class="comment">#注：生成返回的tensor会和ndarry共享数据，任何对tensor的操作都会影响到ndarry</span></span><br><span class="line">        data = torch.from_numpy(np.random.randint(<span class="number">1</span>, V, size=(batch, <span class="number">10</span>))) <span class="comment">#1是产生的最小值，V=11是最大值，size是形状（batch，10）。生成（batch，10）的矩阵，矩阵的每一个元素都是1~V-1之间  （取不到V）</span></span><br><span class="line">        data[:, <span class="number">0</span>] = <span class="number">1</span> <span class="comment">#将第0列的值赋值为1</span></span><br><span class="line">        <span class="comment"># Variable 就是一个存放值， 里面的值会不停的变化.  存放的是Torch 的 Tensor . 如果用一个 Variable 进行计算, 那返回的也是一个同类型的 Variable.  </span></span><br><span class="line">        <span class="comment">#requires_grad： 是否参与误差反向传播, 要不要计算梯度</span></span><br><span class="line">        src = Variable(data, requires_grad=<span class="literal">False</span>) <span class="comment">#size(batch,10) 和data的值完全一样</span></span><br><span class="line">        tgt = Variable(data, requires_grad=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">yield</span> Batch(src, tgt, <span class="number">0</span>)<span class="comment">#yield就是return一个值，并且记住这个返回的位置，下次迭代就从这个位置后(下一行)开始</span></span><br><span class="line">        <span class="comment">#batch返回的是trg_mask</span></span><br></pre></td></tr></tbody></table></figure>
<h4 id="loss-computation">Loss Computation</h4>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleLossCompute</span>:</span> <span class="comment">#loss计算以及更新。调用LabelSmoothing，使用KL散度</span></span><br><span class="line">    <span class="string">"A simple loss compute and train function."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, generator, criterion, opt=None)</span>:</span></span><br><span class="line">        self.generator = generator <span class="comment">#解码器后的生成函数</span></span><br><span class="line">        self.criterion = criterion <span class="comment"># LabelSmoothing（计算loss KLDivLoss KL散度）的实例化</span></span><br><span class="line">        self.opt = opt <span class="comment"># NoamOpt（优化）的实例化</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, x, y, norm)</span>:</span></span><br><span class="line">        x = self.generator(x) <span class="comment">#解码器的输出</span></span><br><span class="line">        loss = self.criterion(x.contiguous().view(<span class="number">-1</span>, x.size(<span class="number">-1</span>)), </span><br><span class="line">                              y.contiguous().view(<span class="number">-1</span>)) / norm  <span class="comment">#计算loss</span></span><br><span class="line">        loss.backward() <span class="comment">#将loss反向传播。loss是标量，根据链式法则自动计算出叶子节点的梯度值</span></span><br><span class="line">        <span class="keyword">if</span> self.opt <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: <span class="comment">#存在优化</span></span><br><span class="line">            self.opt.step() <span class="comment">#调用opt的step函数。 adam优化，更新参数</span></span><br><span class="line">            self.opt.optimizer.zero_grad() <span class="comment">#把梯度置零，也就是把loss关于weight的导数变成0.</span></span><br><span class="line">        <span class="keyword">return</span> loss.data[<span class="number">0</span>] * norm</span><br></pre></td></tr></tbody></table></figure>
<h4 id="greedy-decoding">Greedy Decoding</h4>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># Train the simple copy task.</span></span><br><span class="line">V = <span class="number">11</span></span><br><span class="line">criterion = LabelSmoothing(size=V, padding_idx=<span class="number">0</span>, smoothing=<span class="number">0.0</span>) <span class="comment">#LabelSmoothing是KL散度实现的</span></span><br><span class="line">model = make_model(V, V, N=<span class="number">2</span>) <span class="comment">#src_vocab=11, tgt_vocab=11，覆盖N=2</span></span><br><span class="line"><span class="comment"># 对模型参数进行更新优化，使用Adam优化</span></span><br><span class="line">model_opt = NoamOpt(model.src_embed[<span class="number">0</span>].d_model, <span class="number">1</span>, <span class="number">400</span>,</span><br><span class="line">        torch.optim.Adam(model.parameters(), lr=<span class="number">0</span>, betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#model.eval()，pytorch会自动把BN和DropOut固定住，不会取平均，而是用训练好的值。</span></span><br><span class="line"><span class="comment">#model.train() 让model变成训练模式，此时dropout和batch normalization的操作在训练起到防止网络过拟合的问题</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">10</span>): <span class="comment">#一共10大份， model.train()打印1行，model.eval()打印1行</span></span><br><span class="line">    model.train()</span><br><span class="line">    <span class="comment">#调用run_epoch(data_iter, model, loss_compute)函数</span></span><br><span class="line">    <span class="comment">#返回total_loss / total_tokens 。返回值可以没有接收，不会报错</span></span><br><span class="line">    run_epoch(data_gen(V, <span class="number">30</span>, <span class="number">20</span>), model, </span><br><span class="line">              SimpleLossCompute(model.generator, criterion, model_opt))</span><br><span class="line">    model.eval()</span><br><span class="line">    <span class="comment">#print接收run_epoch的返回值 在输出的第三行</span></span><br><span class="line">    print(run_epoch(data_gen(V, <span class="number">30</span>, <span class="number">5</span>), model, </span><br><span class="line">                    SimpleLossCompute(model.generator, criterion, <span class="literal">None</span>)))</span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">3.023465</span> Tokens per Sec: <span class="number">403.074173</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.920030</span> Tokens per Sec: <span class="number">641.689380</span></span><br><span class="line"><span class="number">1.9274832487106324</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.940011</span> Tokens per Sec: <span class="number">432.003378</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.699767</span> Tokens per Sec: <span class="number">641.979665</span></span><br><span class="line"><span class="number">1.657595729827881</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.860276</span> Tokens per Sec: <span class="number">433.320240</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.546011</span> Tokens per Sec: <span class="number">640.537198</span></span><br><span class="line"><span class="number">1.4888023376464843</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.682198</span> Tokens per Sec: <span class="number">432.092305</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.313169</span> Tokens per Sec: <span class="number">639.441857</span></span><br><span class="line"><span class="number">1.3485562801361084</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.278768</span> Tokens per Sec: <span class="number">433.568756</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.062384</span> Tokens per Sec: <span class="number">642.542067</span></span><br><span class="line"><span class="number">0.9853351473808288</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.269471</span> Tokens per Sec: <span class="number">433.388727</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">0.590709</span> Tokens per Sec: <span class="number">642.862135</span></span><br><span class="line"><span class="number">0.5686767101287842</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">0.997076</span> Tokens per Sec: <span class="number">433.009746</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">0.343118</span> Tokens per Sec: <span class="number">642.288427</span></span><br><span class="line"><span class="number">0.34273059368133546</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">0.459483</span> Tokens per Sec: <span class="number">434.594030</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">0.290385</span> Tokens per Sec: <span class="number">642.519464</span></span><br><span class="line"><span class="number">0.2612409472465515</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.031042</span> Tokens per Sec: <span class="number">434.557008</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">0.437069</span> Tokens per Sec: <span class="number">643.630322</span></span><br><span class="line"><span class="number">0.4323212027549744</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">0.617165</span> Tokens per Sec: <span class="number">436.652626</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">0.258793</span> Tokens per Sec: <span class="number">644.372296</span></span><br><span class="line"><span class="number">0.27331129014492034</span></span><br></pre></td></tr></tbody></table></figure>
<blockquote>
<p>This code predicts a translation using greedy decoding for simplicity.</p>
</blockquote>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">#预测过程</span></span><br><span class="line"><span class="comment">#预测的时候没有用tgt（标准值），而是每次解码器的输入都是ys，是预测的值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">greedy_decode</span><span class="params">(model, src, src_mask, max_len, start_symbol)</span>:</span></span><br><span class="line">    memory = model.encode(src, src_mask) <span class="comment">#memory是编码器的输出 。是一个矩阵</span></span><br><span class="line">    ys = torch.ones(<span class="number">1</span>, <span class="number">1</span>).fill_(start_symbol).type_as(src.data) <span class="comment">#填充输出开始符，和src的类型一样。对预测的句子进行初始化 ys =1 （1,1）</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(max_len<span class="number">-1</span>): <span class="comment">#0~8 对每一个词都进行预测</span></span><br><span class="line">        out = model.decode(memory, src_mask, </span><br><span class="line">                           Variable(ys), </span><br><span class="line">                           Variable(subsequent_mask(ys.size(<span class="number">1</span>))</span><br><span class="line">                                    .type_as(src.data)))</span><br><span class="line">         <span class="comment"># ys 的维度是 batch_size * times （固定的）,   所以target_mask 矩阵必须是ys.size(1),所以是 times * times</span></span><br><span class="line">        <span class="comment"># 根据 decoder 的训练步骤, 这里的 out 输出就应该是 batch_size * (times+1) 的矩阵</span></span><br><span class="line">        </span><br><span class="line">        prob = model.generator(out[:, <span class="number">-1</span>]) <span class="comment">#generator返回的是softmax</span></span><br><span class="line">          <span class="comment"># out[:, -1] 这里是最新的一个单词的 embedding 向量</span></span><br><span class="line">        <span class="comment"># generator 就是产生最后的 vocabulary 的概率, 是一个全连接层</span></span><br><span class="line">        </span><br><span class="line">        _, next_word = torch.max(prob, dim = <span class="number">1</span>) <span class="comment"># torch.max:按维度dim 返回最大值，并且会返回索引。next_data接收											#索引</span></span><br><span class="line">        next_word = next_word.data[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 将句子拼接起来  .type_as: 将tensor强制转换为src.data 格式的</span></span><br><span class="line">        ys = torch.cat([ys, </span><br><span class="line">                        torch.ones(<span class="number">1</span>, <span class="number">1</span>).type_as(src.data).fill_(next_word)], dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> ys</span><br><span class="line"></span><br><span class="line">model.eval()</span><br><span class="line">src = Variable(torch.LongTensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>]]) )</span><br><span class="line">src_mask = Variable(torch.ones(<span class="number">1</span>, <span class="number">1</span>, <span class="number">10</span>) )</span><br><span class="line">print(greedy_decode(model, src, src_mask, max_len=<span class="number">10</span>, start_symbol=<span class="number">1</span>))</span><br><span class="line">    <span class="number">1</span>     <span class="number">2</span>     <span class="number">3</span>     <span class="number">4</span>     <span class="number">5</span>     <span class="number">6</span>     <span class="number">7</span>     <span class="number">8</span>     <span class="number">9</span>    <span class="number">10</span></span><br><span class="line">[torch.LongTensor of size <span class="number">1</span>x10]</span><br></pre></td></tr></tbody></table></figure>
<h3 id="真实例子">真实例子</h3>
<blockquote>
<p>Now we consider a real-world example using the IWSLT German-English Translation task. This task is much smaller than the WMT task considered in the paper, but it illustrates the whole system. We also show how to use multi-gpu processing to make it really fast.</p>
</blockquote>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">#!pip install torchtext spacy</span></span><br><span class="line"><span class="comment">#!python -m spacy download en</span></span><br><span class="line"><span class="comment">#!python -m spacy download de</span></span><br></pre></td></tr></tbody></table></figure>
<h4 id="data-loading">Data Loading</h4>
<blockquote>
<p>We will load the dataset using torchtext and spacy for tokenization.</p>
<p>用torchtext来加载数据集 ， 用spacy来分词</p>
</blockquote>
<p><img src="https://i.loli.net/2020/08/07/teSG1hufEjF4Zkv.png" alt="image-20200807001353729" style="zoom: 67%;"></p>
<p>torchtext组件流程：</p>
<blockquote>
<ul>
<li>定义Field：声明如何处理数据，主要包含以下数据预处理的配置信息，比如指定分词方法，是否转成小写，起始字符，结束字符，补全字符以及词典等等</li>
<li>定义Dataset：用于得到数据集，继承自pytorch的Dataset。此时数据集里每一个样本是一个 经过 Field声明的预处理 预处理后的 wordlist</li>
<li>建立vocab：在这一步建立词汇表，词向量(word embeddings)</li>
<li>构造迭代器Iterator：: 主要是数据输出的模型的迭代器。构造迭代器，支持batch定制用来分批次训练模型。</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># For data loading.</span></span><br><span class="line"><span class="keyword">from</span> torchtext <span class="keyword">import</span> data, datasets</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="keyword">import</span> spacy</span><br><span class="line">    spacy_de = spacy.load(<span class="string">'de'</span>) <span class="comment">#加载德语语言模型</span></span><br><span class="line">    spacy_en = spacy.load(<span class="string">'en'</span>) <span class="comment">#加载英语语言模型</span></span><br><span class="line">	</span><br><span class="line">    </span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">   在文本处理的过程中，spaCy首先对文本分词，原始文本在空格处分割，类似于text.split(' ')，然后分词器（Tokenizer）从左向右依次处理token</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tokenize_de</span><span class="params">(text)</span>:</span> <span class="comment">#Tokenizer:分词器  进行德语分词  </span></span><br><span class="line">        <span class="comment">#text：输入的段落句子  tok.text：分后的token词</span></span><br><span class="line">        <span class="keyword">return</span> [tok.text <span class="keyword">for</span> tok <span class="keyword">in</span> spacy_de.tokenizer(text)]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tokenize_en</span><span class="params">(text)</span>:</span> <span class="comment"># 进行英语分词</span></span><br><span class="line">        <span class="keyword">return</span> [tok.text <span class="keyword">for</span> tok <span class="keyword">in</span> spacy_en.tokenizer(text)]</span><br><span class="line"></span><br><span class="line">    BOS_WORD = <span class="string">'&lt;s&gt;'</span>  <span class="comment">#开始符</span></span><br><span class="line">    EOS_WORD = <span class="string">'&lt;/s&gt;'</span> <span class="comment">#终止符</span></span><br><span class="line">    BLANK_WORD = <span class="string">"&lt;blank&gt;"</span> <span class="comment">#空格</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 构建Filed对象，声明如何处理数据。主要包含以下数据预处理的配置信息，比如指定分词方法，是否转成小写，		#起始字符，结束字符，补全字符以及词典等等</span></span><br><span class="line">    SRC = data.Field(tokenize=tokenize_de, pad_token=BLANK_WORD) <span class="comment">#得到源句子</span></span><br><span class="line">    TGT = data.Field(tokenize=tokenize_en, init_token = BOS_WORD,  </span><br><span class="line">                     eos_token = EOS_WORD, pad_token=BLANK_WORD)</span><br><span class="line"></span><br><span class="line">    MAX_LEN = <span class="number">100</span> <span class="comment">#最大长度</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># https://s3.amazonaws.com/opennmt-models/iwslt.pt 数据集</span></span><br><span class="line">    <span class="comment">#同时对训练集和验证集还有测试集的构建，此时数据集里每一个样本是一个 经过 Field声明的预处理 预处理后的 	#wordlist</span></span><br><span class="line">    train, val, test = datasets.IWSLT.splits(</span><br><span class="line">        exts=(<span class="string">'.de'</span>, <span class="string">'.en'</span>)   <span class="comment"># 构建数据集所需的数据集</span></span><br><span class="line">        , fields=(SRC, TGT),  <span class="comment">#如何赋值给train那三个的？？？？</span></span><br><span class="line">        filter_pred=<span class="keyword">lambda</span> x: len(vars(x)[<span class="string">'src'</span>]) &lt;= MAX_LEN <span class="keyword">and</span> </span><br><span class="line">            len(vars(x)[<span class="string">'trg'</span>]) &lt;= MAX_LEN)  <span class="comment">#源句子和目标句子长度小于100的筛选出来</span></span><br><span class="line">    </span><br><span class="line">    MIN_FREQ = <span class="number">2</span> <span class="comment">#定义最小频率</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#建立词汇表，词向量(word embeddings)。即需要给每个单词编码，然后输入模型</span></span><br><span class="line">    <span class="comment">#bulid_vocab()方法中传入用于构建词表的数据集</span></span><br><span class="line">    SRC.build_vocab(train.src, min_freq=MIN_FREQ) </span><br><span class="line">    TGT.build_vocab(train.trg, min_freq=MIN_FREQ)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#一旦运行了这些代码行，SRC.vocab.stoi将是一个词典，其词汇表中的标记作为键，而其对应的索引作为值； 	#SRC.vocab.itos将是相同的字典，其中的键和值被交换。</span></span><br></pre></td></tr></tbody></table></figure>
<blockquote>
<p>批训练对于速度来说很重要。希望批次分割非常均匀并且填充最少。 要做到这一点，我们<strong>必须修改torchtext默认的批处理函数</strong>。 这部分代码修补其默认批处理函数，以确保我们搜索足够多的句子以构建紧密批处理。 一般来说直接调用<code>BucketIterator</code> （训练用）和 <code>Iterator</code>（测试用） 即可</p>
<p><code>BucketIterator</code>和<code>Iterator</code>的区别是，BucketIterator尽可能的把长度相似的句子放在一个batch里面。</p>
</blockquote>
<h4 id="iterators">Iterators</h4>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">定义一个迭代器，该迭代器将相似长度的示例批处理在一起。 在为每个新纪元(epoch)生产新鲜改组的批次时，最大程度地减少所需的填充量。</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyIterator</span><span class="params">(data.Iterator)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_batches</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment">#在train的时候，要进行sort，尽量减少padding</span></span><br><span class="line">        <span class="comment">#目的是自动进行shuffle和padding，并且为了训练效率期间，尽量把句子长度相似的shuffle在一起。</span></span><br><span class="line">        <span class="keyword">if</span> self.train:</span><br><span class="line">            <span class="function"><span class="keyword">def</span> <span class="title">pool</span><span class="params">(d, random_shuffler)</span>:</span></span><br><span class="line">                <span class="keyword">for</span> p <span class="keyword">in</span> data.batch(d, self.batch_size * <span class="number">100</span>):</span><br><span class="line">                    p_batch = data.batch(</span><br><span class="line">                        sorted(p, key=self.sort_key), <span class="comment">#按照词的数大小排序</span></span><br><span class="line">                        self.batch_size, self.batch_size_fn)</span><br><span class="line">                    <span class="keyword">for</span> b <span class="keyword">in</span> random_shuffler(list(p_batch)):</span><br><span class="line">                        <span class="keyword">yield</span> b <span class="comment">#b就是batch， 类比上述的gen_data函数</span></span><br><span class="line">            self.batches = pool(self.data(), self.random_shuffler) <span class="comment">#调用pool</span></span><br><span class="line">            </span><br><span class="line">         <span class="comment">#在valid+test(验证集和测试集)的时候  和上面具体区别在哪？？？？</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.batches = []</span><br><span class="line">            <span class="keyword">for</span> b <span class="keyword">in</span> data.batch(self.data(), self.batch_size,</span><br><span class="line">                                          self.batch_size_fn):</span><br><span class="line">                self.batches.append(sorted(b, key=self.sort_key))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rebatch</span><span class="params">(pad_idx, batch)</span>:</span>  <span class="comment">#pad_idx：空格键</span></span><br><span class="line">    <span class="string">"Fix order in torchtext to match ours"</span></span><br><span class="line">    src, trg = batch.src.transpose(<span class="number">0</span>, <span class="number">1</span>), batch.trg.transpose(<span class="number">0</span>, <span class="number">1</span>)<span class="comment">#为什么要进行</span></span><br><span class="line">    <span class="keyword">return</span> Batch(src, trg, pad_idx) <span class="comment">#调用上述的Batch类   pad_idx就是pad</span></span><br></pre></td></tr></tbody></table></figure>
<h4 id="multi-gpu-training">Multi-GPU Training</h4>
<blockquote>
<p>最后为了真正地快速训练，将使用多个GPU。 这部分代码实现了多GPU字生成，它不是Transformer特有的。 其<strong>思想是将训练时的单词生成分成块，以便在许多不同的GPU上并行处理。</strong> 我们使用PyTorch并行原语来做到这一点：</p>
<ul>
<li>replicate -复制 - 将模块拆分到不同的GPU上</li>
<li>scatter -分散 - 将批次拆分到不同的GPU上</li>
<li>parallel_apply -并行应用 - 在不同GPU上将模块应用于批处理</li>
<li>gather - 聚集 - 将分散的数据聚集到一个GPU上</li>
<li>nn.DataParallel - 一个特殊的模块包装器，在评估之前调用它们。</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># Skip if not interested in multigpu.</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiGPULossCompute</span>:</span></span><br><span class="line">    <span class="string">"A multi-gpu loss compute and train function."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, generator, criterion, devices, opt=None, chunk_size=<span class="number">5</span>)</span>:</span></span><br><span class="line">        <span class="comment"># Send out to different gpus.</span></span><br><span class="line">        self.generator = generator</span><br><span class="line">        self.criterion = nn.parallel.replicate(criterion, </span><br><span class="line">                                               devices=devices)</span><br><span class="line">        self.opt = opt</span><br><span class="line">        self.devices = devices</span><br><span class="line">        self.chunk_size = chunk_size</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, out, targets, normalize)</span>:</span></span><br><span class="line">        total = <span class="number">0.0</span></span><br><span class="line">        generator = nn.parallel.replicate(self.generator, </span><br><span class="line">                                                devices=self.devices)</span><br><span class="line">        out_scatter = nn.parallel.scatter(out, </span><br><span class="line">                                          target_gpus=self.devices)</span><br><span class="line">        out_grad = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> out_scatter]</span><br><span class="line">        targets = nn.parallel.scatter(targets, </span><br><span class="line">                                      target_gpus=self.devices)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Divide generating into chunks.</span></span><br><span class="line">        chunk_size = self.chunk_size</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, out_scatter[<span class="number">0</span>].size(<span class="number">1</span>), chunk_size):</span><br><span class="line">            <span class="comment"># Predict distributions</span></span><br><span class="line">            out_column = [[Variable(o[:, i:i+chunk_size].data, </span><br><span class="line">                                    requires_grad=self.opt <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>)] </span><br><span class="line">                           <span class="keyword">for</span> o <span class="keyword">in</span> out_scatter]</span><br><span class="line">            gen = nn.parallel.parallel_apply(generator, out_column)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Compute loss. </span></span><br><span class="line">            y = [(g.contiguous().view(<span class="number">-1</span>, g.size(<span class="number">-1</span>)), </span><br><span class="line">                  t[:, i:i+chunk_size].contiguous().view(<span class="number">-1</span>)) </span><br><span class="line">                 <span class="keyword">for</span> g, t <span class="keyword">in</span> zip(gen, targets)]</span><br><span class="line">            loss = nn.parallel.parallel_apply(self.criterion, y)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Sum and normalize loss</span></span><br><span class="line">            l = nn.parallel.gather(loss, </span><br><span class="line">                                   target_device=self.devices[<span class="number">0</span>])</span><br><span class="line">            l = l.sum()[<span class="number">0</span>] / normalize</span><br><span class="line">            total += l.data[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Backprop loss to output of transformer</span></span><br><span class="line">            <span class="keyword">if</span> self.opt <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                l.backward()</span><br><span class="line">                <span class="keyword">for</span> j, l <span class="keyword">in</span> enumerate(loss):</span><br><span class="line">                    out_grad[j].append(out_column[j][<span class="number">0</span>].grad.data.clone())</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Backprop all loss through transformer.            </span></span><br><span class="line">        <span class="keyword">if</span> self.opt <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            out_grad = [Variable(torch.cat(og, dim=<span class="number">1</span>)) <span class="keyword">for</span> og <span class="keyword">in</span> out_grad]</span><br><span class="line">            o1 = out</span><br><span class="line">            o2 = nn.parallel.gather(out_grad, </span><br><span class="line">                                    target_device=self.devices[<span class="number">0</span>])</span><br><span class="line">            o1.backward(gradient=o2)</span><br><span class="line">            self.opt.step()</span><br><span class="line">            self.opt.optimizer.zero_grad()</span><br><span class="line">        <span class="keyword">return</span> total * normalize</span><br></pre></td></tr></tbody></table></figure>
<blockquote>
<p>Now we create our model, criterion, optimizer, data iterators, and paralelization</p>
</blockquote>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># GPUs to use</span></span><br><span class="line">devices = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line"><span class="keyword">if</span> <span class="literal">True</span>:</span><br><span class="line">    pad_idx = TGT.vocab.stoi[<span class="string">"&lt;blank&gt;"</span>]</span><br><span class="line">    model = make_model(len(SRC.vocab), len(TGT.vocab), N=<span class="number">6</span>)</span><br><span class="line">    model.cuda()</span><br><span class="line">    criterion = LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, smoothing=<span class="number">0.1</span>)</span><br><span class="line">    criterion.cuda()</span><br><span class="line">    BATCH_SIZE = <span class="number">12000</span></span><br><span class="line">    train_iter = MyIterator(train, batch_size=BATCH_SIZE, device=<span class="number">0</span>,</span><br><span class="line">                            repeat=<span class="literal">False</span>, sort_key=<span class="keyword">lambda</span> x: (len(x.src), len(x.trg)),</span><br><span class="line">                            batch_size_fn=batch_size_fn, train=<span class="literal">True</span>)</span><br><span class="line">    valid_iter = MyIterator(val, batch_size=BATCH_SIZE, device=<span class="number">0</span>,</span><br><span class="line">                            repeat=<span class="literal">False</span>, sort_key=<span class="keyword">lambda</span> x: (len(x.src), len(x.trg)),</span><br><span class="line">                            batch_size_fn=batch_size_fn, train=<span class="literal">False</span>)</span><br><span class="line">    model_par = nn.DataParallel(model, device_ids=devices)</span><br><span class="line"><span class="literal">None</span></span><br></pre></td></tr></tbody></table></figure>
<blockquote>
<p>Now we <strong>train the model</strong>. I will play with the warmup steps a bit, but everything else uses the default parameters. On an AWS p3.8xlarge with 4 Tesla V100s, this runs at ~27,000 tokens per second with a batch size of 12,000</p>
<p>在具有4个Tesla V100 GPU的AWS p3.8xlarge机器上，每秒运行约27,000个词，批训练大小为12,000。</p>
</blockquote>
<h4 id="training-the-system">Training the System</h4>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">#!wget https://s3.amazonaws.com/opennmt-models/iwslt.pt</span></span><br><span class="line"><span class="comment">#进行train和eval</span></span><br><span class="line"><span class="keyword">if</span> <span class="literal">False</span>: <span class="comment"># false存在的意义在哪？？？ 使用GPU？</span></span><br><span class="line">    model_opt = NoamOpt(model.src_embed[<span class="number">0</span>].d_model, <span class="number">1</span>, <span class="number">2000</span>,</span><br><span class="line">            torch.optim.Adam(model.parameters(), lr=<span class="number">0</span>, betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span>))</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        model_par.train()</span><br><span class="line">        run_epoch((rebatch(pad_idx, b) <span class="keyword">for</span> b <span class="keyword">in</span> train_iter), </span><br><span class="line">                  model_par, </span><br><span class="line">                  MultiGPULossCompute(model.generator, criterion, </span><br><span class="line">                                      devices=devices, opt=model_opt))</span><br><span class="line">        model_par.eval()</span><br><span class="line">        loss = run_epoch((rebatch(pad_idx, b) <span class="keyword">for</span> b <span class="keyword">in</span> valid_iter), </span><br><span class="line">                          model_par, </span><br><span class="line">                          MultiGPULossCompute(model.generator, criterion, </span><br><span class="line">                          devices=devices, opt=<span class="literal">None</span>))</span><br><span class="line">        print(loss)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    model = torch.load(<span class="string">"iwslt.pt"</span>) <span class="comment">#加载所有的tensor到CPU</span></span><br></pre></td></tr></tbody></table></figure>
<blockquote>
<p>Once trained we can decode the model to produce a set of translations. Here we simply translate the first sentence in the validation set. This dataset is pretty small so the translations with greedy search are reasonably accurate.</p>
</blockquote>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">#类比于run_epoch函数  </span></span><br><span class="line"><span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(valid_iter):</span><br><span class="line">    src = batch.src.transpose(<span class="number">0</span>, <span class="number">1</span>)[:<span class="number">1</span>]</span><br><span class="line">    src_mask = (src != SRC.vocab.stoi[<span class="string">"&lt;blank&gt;"</span>]).unsqueeze(<span class="number">-2</span>)</span><br><span class="line">    out = greedy_decode(model, src, src_mask, </span><br><span class="line">                        max_len=<span class="number">60</span>, start_symbol=TGT.vocab.stoi[<span class="string">"&lt;s&gt;"</span>])</span><br><span class="line">    print(<span class="string">"Translation:"</span>, end=<span class="string">"\t"</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, out.size(<span class="number">1</span>)):</span><br><span class="line">        sym = TGT.vocab.itos[out[<span class="number">0</span>, i]]</span><br><span class="line">        <span class="keyword">if</span> sym == <span class="string">"&lt;/s&gt;"</span>: <span class="keyword">break</span></span><br><span class="line">        print(sym, end =<span class="string">" "</span>)</span><br><span class="line">    print()</span><br><span class="line">    print(<span class="string">"Target:"</span>, end=<span class="string">"\t"</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, batch.trg.size(<span class="number">0</span>)):</span><br><span class="line">        sym = TGT.vocab.itos[batch.trg.data[i, <span class="number">0</span>]]</span><br><span class="line">        <span class="keyword">if</span> sym == <span class="string">"&lt;/s&gt;"</span>: <span class="keyword">break</span></span><br><span class="line">        print(sym, end =<span class="string">" "</span>)</span><br><span class="line">    print()</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line">Translation:	&lt;unk&gt; &lt;unk&gt; . In my language , that means , thank you very much . </span><br><span class="line">Gold:	&lt;unk&gt; &lt;unk&gt; . It means <span class="keyword">in</span> my language , thank you very much .</span><br></pre></td></tr></tbody></table></figure>
<h3 id="理解qkv">理解QKV</h3>
<p>作者：繁华里流浪 链接：https://www.zhihu.com/question/298810062/answer/1336554776 来源：知乎 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>
<p>我认为attention主要分为两个核心步骤：1. 计算注意力权重 2. 加权求和</p>
<p>其中Q(query)，K(key)用来计算对应的注意力权重atten_i，V(value)用来进行加权求和也就是求最后attention的结果。</p>
<p><img src="E:\myBlog\source_posts\image-20200904164236348.png" alt="image-20200904164236348"></p>
<p>在理解attention的时候我想了一个买水果的例子。今天你要去水果摊买水果，首先你脑袋里会想出一个买水果的标准（个大、成色好、价格美丽等）作为 query，然后你就去各个水果摊逛了，水果摊主上来给你拿出水果一顿介绍（我这香甜可口，新鲜美味，性价比高）这就是 key，然后你会通过水果的情况 key 和自己心中标准 query 权衡给这个水果摊子打个分，当你一条水果街都走完了，你就对整条街的水果摊都有了一个性价比分数（atten），然后根据这个性价比分数就开始买了，这个水果摊好分数高，我买多点，那个水果摊性价比分数低不能满足我的需求我就少买点，最后我就从不同的水果摊采购了不同数量的水果（value）放进了自己的推推车里（output）</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>transformer</category>
      </categories>
      <tags>
        <tag>transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-07-25-服务器心得</title>
    <url>/2020/07/25/2020-07-25-%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%BF%83%E5%BE%97/</url>
    <content><![CDATA[<p>目前使用的是Xshell + winSCP（连接+传输） ， MobaXterm也可用于ssh连接的备用软件。</p>
<p>MobaXterm（<a href="https://link.zhihu.com/?target=https%3A//mobaxterm.mobatek.net/">https://mobaxterm.mobatek.net/</a>）：功能很全，免费，有免安装版，支持多标签，同时自带文件传输系统，唯一的不足是对Z-moderm支持较差。</p>
<h3 id="linux后台执行命令和nohup">linux后台执行命令：&amp;和nohup</h3>
<p>在用本机Xshell连接服务器跑实验时，如果关闭本机电脑，Xshell不再运行时，那么linux终端会话就会关闭，跑的实验就会终止。有时候更希望它能够在每天的非负荷高峰时间段运行(例如凌晨)。为了使这些进程能够在后台运行，也就是说不在终端屏幕上运行，有几种选择方法可供使用。</p>
<h4 id="使用">&amp;使用</h4>
<p>在执行文件的时候，可以在命令后面加上<code>&amp;</code>，这样可以实现将进程挂到后台运行。例如： <code>python main.py &amp;</code></p>
<p>在使用&amp;之后，系统会返回一个进程号PID，需要记下此进程的PID</p>
<h4 id="nohup使用">nohup使用</h4>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">nohup python main.py &amp;</span><br></pre></td></tr></tbody></table></figure>
<p>这样执行的时候会将代码放在服务器后台执行，你的终端是看不到运行过程的，<strong>期间运行的结果</strong>（代码运行过程中打印出来的）会在一个生成的<code>nohup.out文件</code>中保存。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">nohup python main.py &gt;test.log  <span class="number">2</span>&gt;&amp;<span class="number">1</span> &amp;</span><br><span class="line"><span class="comment"># nohup和&amp; 一起使用。 &amp; 放后台</span></span><br><span class="line"><span class="comment"># &gt;表示将标准输出（STDOUT）重定向到test.log文件</span></span><br><span class="line"><span class="comment">#2&gt;&amp;1 ：把标准输出和标准错误一起重定向到一个文件中。1是标准输出的文件描述符，2是标准错误的文件描述符；</span></span><br></pre></td></tr></tbody></table></figure>
<p>可以实现运行main.py ，并将输出结果打印到<code>test.log文件</code>中（如果这个文件不存在, 那就创建, 否则就覆盖）</p>
<p>要想使ssh连接断掉也可以继续后台运行，需要用<code>exit命令</code>断开，否则其它关闭行为视为断开异常（如直接关掉xsheel软件），不会后台运行</p>
<h4 id="nohup与session的关系">nohup与session的关系</h4>
<p><code>如果我们在 session 中执行了 nohup 等类似的命令，当 session 消亡时，相关的进程并不会随着 session 结束，原因是这些进程不再受 SIGHUP 信号的影响。</code>比如我们执行下面的命令：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">$ nohup sleep <span class="number">1000</span> &gt;/dev/null <span class="number">2</span>&gt;&amp;<span class="number">1</span> &amp;</span><br></pre></td></tr></tbody></table></figure>
<p><img src="https://i.loli.net/2020/07/29/cAzPToGdLKmRZ2e.png" alt="img"></p>
<p>此时 sleep 进程的 sid 和其它进程是相同的，还可以通过 pstree 命令看到进程间的父子关系：</p>
<p><img src="https://i.loli.net/2020/07/29/8ZQDMlkmTHy1a2G.png" alt="img"></p>
<p><code>如果我们退出当前 session 的领头进程(bash)，sleep 进程并不会退出，这样我们就可以放心的等待该进程运行结果了。</code> nohup 并不改变进程的 sid，同时也说明在这种情况中，虽然 session 的领头进程退出了，但是 session 依然没有被销毁(至少 sid 还在被引用)。重新建立连接，通过下面的命令查看 sleep 进程的信息，发现进程的 sid 依然是 7837：</p>
<p><img src="https://i.loli.net/2020/07/29/5O83jJ2Hfsdoyil.png" alt="img"></p>
<p>但是<code>此时的 sleep 已经被系统的 1 号进程 systemd 收养了</code>：</p>
<p><img src="https://i.loli.net/2020/07/29/9quLvTHCNnES4F7.png" alt="img"></p>
<h5 id="参考">参考</h5>
<blockquote>
<p>https://www.cnblogs.com/sparkdev/p/12146305.html</p>
</blockquote>
<h4 id="忘记进程">忘记进程</h4>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">ps -ef | grep main.py  <span class="comment">#其中main.py是要查找的关键字</span></span><br><span class="line">ps -ef | grep main.py  | grep -v grep <span class="comment">#grep -v 排除进程。此时是排除grep自身的进程</span></span><br><span class="line">ps aux | grep main.py | less </span><br><span class="line"><span class="comment">#less进行显示， aux 是显示详细信息 </span></span><br><span class="line"><span class="comment">#R: 正在执行中</span></span><br><span class="line"><span class="comment">#S: 静止状态</span></span><br><span class="line"><span class="comment">#T: 暂停执行</span></span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">ps命令将某个进程显示出来</span><br><span class="line"></span><br><span class="line">grep命令是查找</span><br><span class="line"></span><br><span class="line">中间的|是管道命令 是指ps命令与grep同时执行</span><br><span class="line"></span><br><span class="line">字段含义如下：</span><br><span class="line">UID    PID    PPID    C   STIME   TTY    TIME     CMD</span><br><span class="line"></span><br><span class="line">zzw   <span class="number">14124</span>  <span class="number">13991</span>   <span class="number">0</span>   <span class="number">00</span>:<span class="number">38</span>   pts/<span class="number">0</span>   <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span>  grep --color=auto dae</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">UID   ：程序被该 UID 所拥有</span><br><span class="line"></span><br><span class="line">PID   ：就是这个程序的 ID </span><br><span class="line"></span><br><span class="line">PPID  ：则是其上级父程序的ID</span><br><span class="line"></span><br><span class="line">C     ：CPU使用的资源百分比</span><br><span class="line"></span><br><span class="line">STIME ：系统启动时间</span><br><span class="line"></span><br><span class="line">TTY   ：登入者的终端机位置</span><br><span class="line"></span><br><span class="line">TIME  ：使用掉的CPU时间。</span><br><span class="line"></span><br><span class="line">CMD  ：所下达的是什么指令</span><br></pre></td></tr></tbody></table></figure>
<p>这里是两个shell命令通过管道进行了结合，第一个ps能够列出当前系统所有活跃的进程，然后通过grep 关键字查找就能找到带有关键字的进程。<code>找到PID</code>（PID是输出的第二列那个数字）再杀掉。</p>
<h4 id="关闭进程">关闭进程</h4>
<p><code>kill -9 PID</code> . 用普通的ctrl+C是关不掉的</p>
<h4 id="疑问">疑问：</h4>
<p>在根指令行可以进行nohup，但是我用tmux建立会话之后，在tmux中是不可以运用notop，会弹出 <code>exit 1</code> 的错误指令？？？？</p>
<p>一种方法就是在后台运行之后，再进入tmux操作</p>
<h3 id="htop使用">htop使用</h3>
<h4 id="功能介绍">功能介绍</h4>
<p>监视内存，线程，CPU运行状态</p>
<p>htop是Linux系统下一个基本文本模式的、交互式的进程查看器，主要用于控制台或shell中，可以替代top，或者说是top的高级版。</p>
<h4 id="安装htop">安装htop</h4>
<p>Ubuntu <code>sudo apt-get install htop</code></p>
<h4 id="使用htop">使用htop</h4>
<h5 id="界面概述">界面概述</h5>
<p>安装完成后，命令行中直接敲击 htop 命令，即可进入 htop 的界面</p>
<p><img src="https://i.loli.net/2020/09/05/6URqObNc7Efkm5H.png" alt="image-20200905224348455"></p>
<p>各项从上至下分别说明如下：</p>
<p><a href="https://blog.xiewenlong.com/2018/12/htop/base.png" target="_blank" rel="noopener"><img src="https://blog.xiewenlong.com/2018/12/htop/base.png" alt="img"></a></p>
<p>左边部分从上至下，分别为，cpu、内存、交换分区的使用情况，右边部分为：Tasks 为进程总数，当前运行的进程数、Load average 为系统 1 分钟，5 分钟，10 分钟的平均负载情况、Uptime 为系统运行的时间。</p>
<p><a href="https://blog.xiewenlong.com/2018/12/htop/process.png" target="_blank" rel="noopener"><img src="https://blog.xiewenlong.com/2018/12/htop/process.png" alt="img"></a></p>
<p>以上各项分别为：</p>
<ul>
<li><strong>PID：</strong>进程的标识号</li>
<li><strong>USER：</strong>运行此进程的用户</li>
<li><strong>PRI：</strong>进程的优先级</li>
<li><strong>NI：</strong>进程的优先级别值，默认的为 0，可以进行调整</li>
<li><strong>VIRT：</strong>进程占用的虚拟内存值</li>
<li><strong>RES：</strong>进程占用的物理内存值</li>
<li><strong>SHR：</strong>进程占用的共享内存值</li>
<li><strong>S：</strong>进程的运行状况，R 表示正在运行、S 表示休眠，等待唤醒、Z 表示僵死状态</li>
<li><strong>%CPU：</strong>该进程占用的CPU使用率</li>
<li><strong>%MEM：</strong>该进程占用的物理内存和总内存的百分比</li>
<li><strong>TIME+：</strong>该进程启动后占用的总的 CPU 时间</li>
<li><strong>COMMAND：</strong>进程启动的启动命令名称</li>
</ul>
<h4 id="操作说明">操作说明</h4>
<p><code>htop</code> 界面底部给出了 F1 ~ F10 按键的简单说明。</p>
<p><a href="https://blog.xiewenlong.com/2018/12/htop/bottom.png" target="_blank" rel="noopener"><img src="https://blog.xiewenlong.com/2018/12/htop/bottom.png" alt="img"></a></p>
<h5 id="标注进程条目">标注进程条目</h5>
<p>在系统中运行着的实时进程视图中，要追踪某个进程是个大问题。因为整个列表在不停的刷新着，进程的排列顺序也在变动着。为了这个问题， <code>htop</code> 提供了一个很简单的解决方案：颜色标注。是的，你可以标注一个进程条目，它会以不同的颜色显示，因此要追踪它就变得容易了。</p>
<p>要标注某个进程条目，需要做的就是选中此条目，然后按下<code>空格</code>键。例如，在下面的截图示例中，我已经颜色标注了两个进程条目（黄色高亮显示的两行）:</p>
<p><img src="https://blog.xiewenlong.com/2018/12/htop/tag.png" alt="img"></p>
<h5 id="命令行选项">命令行选项</h5>
<p>除了上面介绍的一些热键，<code>htop</code> 还提供了很有用的命令行选项。下面是其中一部分:</p>
<ul>
<li><code>-s 选项</code> : 按指定的列排序。例如，<code>htop -s PID</code> 命令会按 PID 列的大小排序来显示。</li>
<li><code>-u 选项</code> : 显示指定的用户的进程信息列表。例如，<code>htop -u vagrant</code> 命令会只显示出用户名为 vagrant 的相关进程。</li>
<li><code>-d 选项</code> : 设置刷新的延迟时间。例如，<code>htop -d 100</code> 命令会使输出在 1 秒后才会刷新（参数 -d 的单位是 10 微秒）。</li>
<li><code>-p 选项</code>：只显示给定的PIDs。例如， <code>htop -p PID</code></li>
</ul>
<h5 id="常用命令">常用命令</h5>
<p><strong>shift + m</strong> ： 按照内存大小排序。 <strong>shift + h</strong> ： 收缩线程。 <strong>q</strong> ： 退出</p>
<p><strong>上下键</strong> 或 <strong>PgUP，PgDn</strong> : 选定想要的进程， <strong>左右键</strong> 或 <strong>Home，End</strong> : 移动字段，当然也可以直接用鼠标选定进程； <strong>Space</strong> 标记/取消标记一个进程（类似 windows 按着 Ctrl 多选一样 ）。命令可以作用于多个进程，例如 "kill"，将应用于所有已标记的进程</p>
<h4 id="参考-1">参考</h4>
<blockquote>
<p>https://blog.xiewenlong.com/2018/12/htop/</p>
</blockquote>
<h3 id="nvidia-smi使用">nvidia-smi使用</h3>
<p><code>nvidia-smi</code> 显示出当前GPU的所有基础信息，监控GPU状态和使用情况。命令判断哪几块GPU空闲</p>
<p><img src="https://i.loli.net/2020/07/22/u1CtU8Xgor45inh.png" alt="image-20200722153957282"></p>
<h4 id="解释相关参数含义">解释相关参数含义</h4>
<p>GPU：本机中的GPU编号</p>
<p>Name：GPU 类型</p>
<p>Persistence-M：</p>
<p>Fan：风扇转速</p>
<p>Temp：温度，单位摄氏度</p>
<p>Perf：表征性能状态，从P0到P12，P0表示最大性能，P12表示状态最小性能</p>
<p>Pwr:Usage/Cap：能耗表示</p>
<p>Bus-Id：涉及GPU总线的相关信息；</p>
<p>Disp.A：Display Active，表示GPU的显示是否初始化</p>
<p>Memory-Usage：显存使用率</p>
<p>Volatile GPU-Util：浮动的GPU利用率</p>
<p>Uncorr. ECC：关于ECC的东西</p>
<p>Compute M.：计算模式</p>
<p>Processes 显示每块GPU上每个进程所使用的显存情况</p>
<h3 id="tmux使用">Tmux使用</h3>
<h4 id="安装tmux--linux">安装tmux -linux</h4>
<p><code>sudo apt-get install tmux</code></p>
<h4 id="基本概念">基本概念</h4>
<p>tmux采用C/S模型构建，<code>输入tmux命令就相当于开启了一个服务器</code>，此时默认将新建一个会话，然后会话中默认新建一个窗口，窗口中默认新建一个面板。会话、窗口、面板之间的联系如下：</p>
<p><img src="https://i.loli.net/2020/07/27/l4YyAISG8d1Lcp9.png" alt="image-20200726233552371"></p>
<p>一个tmux <code>session</code>（会话）可以包含多个<code>window</code>（窗口），窗口默认充满会话界面，因此这些窗口中可以运行相关性不大的任务。</p>
<p>一个<code>window</code>又可以包含多个<code>pane</code>（面板），窗口下的面板，都处于同一界面下，这些面板适合运行相关性高的任务，以便同时观察到它们的运行情况。</p>
<p>一般在一个<code>session</code>里进行新建<code>windows</code>和<code>pane</code>操作即可</p>
<p>一个session显示如图</p>
<p><img src="https://i.loli.net/2020/07/27/3bWsR57zZTUODF6.png" alt="img"></p>
<h4 id="会话">会话</h4>
<h5 id="新建会话">新建会话</h5>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">tmux  <span class="comment"># 新建一个无名称的会话 </span></span><br><span class="line"></span><br><span class="line">tmux new -s s1 <span class="comment"># 新建一个名称为s1的会话</span></span><br></pre></td></tr></tbody></table></figure>
<h5 id="断开当前会话">断开当前会话</h5>
<p>暂时断开会话，可以进入到原始命令行界面进行操作。也可以<code>Ctrl+B  +d</code>进行断开</p>
<p>操作如 <code>tmux new -s demo</code></p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">tmux detach <span class="comment"># 断开当前会话，会话在后台运行</span></span><br></pre></td></tr></tbody></table></figure>
<h5 id="进入之前的会话">进入之前的会话</h5>
<p>断开会话后，想要接着上次留下的现场继续工作，就要使用到tmux的attach命令了，语法为<code>tmux attach-session -t session-name</code>，可简写为<code>tmux a -t session-name</code> 或 <code>tmux a</code>。通常我们使用如下两种方式之一即可：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">tmux a <span class="comment"># 默认进入第一个会话dd</span></span><br><span class="line">tmux a -t demo <span class="comment"># 进入到名称为demo的会话</span></span><br></pre></td></tr></tbody></table></figure>
<h5 id="关闭会话">关闭会话</h5>
<p>会话的使命完成后，一定是要关闭的。我们可以使用tmux的kill命令</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">tmux kill-session -t demo <span class="comment"># 关闭demo会话 </span></span><br><span class="line"></span><br><span class="line">tmux kill-server <span class="comment"># 关闭服务器，即关闭所有会话</span></span><br><span class="line"></span><br><span class="line">tmux kill-session -a -t s1　　<span class="comment">#关闭除s1外的所有会话</span></span><br></pre></td></tr></tbody></table></figure>
<h5 id="查看所有会话">查看所有会话</h5>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">tmux ls <span class="comment"># 查看所有会话，显示会话列表</span></span><br></pre></td></tr></tbody></table></figure>
<h5 id="重命名会话s1为s2">重命名会话S1为S2</h5>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">tmux rename -t s1 s2</span><br></pre></td></tr></tbody></table></figure>
<h4 id="tmux快捷指令">Tmux快捷指令</h4>
<p>tmux的所有指令，都包含同一个前缀，默认为<code>Ctrl+b</code>，输入完前缀过后，控制台激活，命令按键才能生效。</p>
<p><strong>表一：常用指令</strong></p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">前缀</th>
<th style="text-align: center;">指令</th>
<th style="text-align: center;">描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>s</code></td>
<td style="text-align: center;">显示会话列表用于选择并切换 （上下键选择+回车）</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>d</code></td>
<td style="text-align: center;">断开当前会话</td>
</tr>
<tr class="odd">
<td style="text-align: center;">===</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>c</code></td>
<td style="text-align: center;">新建窗口（windows）</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>&amp;</code></td>
<td style="text-align: center;">关闭当前窗口（关闭前需输入<code>y</code> or <code>n</code>确认）</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>w</code></td>
<td style="text-align: center;">打开窗口列表，用于且切换窗口</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>,</code></td>
<td style="text-align: center;">重命名当前窗口</td>
</tr>
<tr class="even">
<td style="text-align: center;">===</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>"</code></td>
<td style="text-align: center;">当前面板上下一分为二，下侧新建面板</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>%</code></td>
<td style="text-align: center;">当前面板左右一分为二，右侧新建面板</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>x</code></td>
<td style="text-align: center;">关闭当前面板（关闭前需输入<code>y</code> or <code>n</code>确认）</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>z</code></td>
<td style="text-align: center;">最大化当前面板，再重复一次按键后恢复正常（v1.8版本新增）</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>方向键</code></td>
<td style="text-align: center;">移动光标切换面板</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>Ctrl+o</code></td>
<td style="text-align: center;">顺时针旋转当前窗口中的所有面板</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>t</code></td>
<td style="text-align: center;">显示时钟</td>
</tr>
</tbody>
</table>
<p>表二：系统指令</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">前缀</th>
<th style="text-align: center;">指令</th>
<th style="text-align: center;">描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>?</code></td>
<td style="text-align: center;">显示快捷键帮助文档</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>d</code></td>
<td style="text-align: center;">断开当前会话</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>D</code></td>
<td style="text-align: center;">选择要断开的会话</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>Ctrl+z</code></td>
<td style="text-align: center;">挂起当前会话</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>r</code></td>
<td style="text-align: center;">强制重载当前会话</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>s</code></td>
<td style="text-align: center;">显示会话列表用于选择并切换</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>:</code></td>
<td style="text-align: center;">进入命令行模式，此时可直接输入<code>ls</code>等命令</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>[</code></td>
<td style="text-align: center;">进入复制模式，按<code>q</code>退出</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>]</code></td>
<td style="text-align: center;">粘贴复制模式中复制的文本</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>~</code></td>
<td style="text-align: center;">列出提示信息缓存</td>
</tr>
</tbody>
</table>
<p>表三：窗口（window）指令</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">前缀</th>
<th style="text-align: center;">指令</th>
<th style="text-align: center;">描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>c</code></td>
<td style="text-align: center;">新建窗口</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>&amp;</code></td>
<td style="text-align: center;">关闭当前窗口（关闭前需输入<code>y</code> or <code>n</code>确认）</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>0~9</code></td>
<td style="text-align: center;">切换到指定窗口</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>p</code></td>
<td style="text-align: center;">切换到上一窗口</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>n</code></td>
<td style="text-align: center;">切换到下一窗口</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>w</code></td>
<td style="text-align: center;">打开窗口列表，用于且切换窗口</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>,</code></td>
<td style="text-align: center;">重命名当前窗口</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>.</code></td>
<td style="text-align: center;">修改当前窗口编号（适用于窗口重新排序）</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>f</code></td>
<td style="text-align: center;">快速定位到窗口（输入关键字匹配窗口名称）</td>
</tr>
</tbody>
</table>
<p>表四：面板（pane）指令</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">前缀</th>
<th style="text-align: center;">指令</th>
<th style="text-align: left;">描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>"</code></td>
<td style="text-align: left;">当前面板上下一分为二，下侧新建面板</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>%</code></td>
<td style="text-align: left;">当前面板左右一分为二，右侧新建面板</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>x</code></td>
<td style="text-align: left;">关闭当前面板（关闭前需输入<code>y</code> or <code>n</code>确认）</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>z</code></td>
<td style="text-align: left;">最大化当前面板，再重复一次按键后恢复正常（v1.8版本新增）</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>!</code></td>
<td style="text-align: left;">将当前面板移动到新的窗口打开（原窗口中存在两个及以上面板有效）</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>;</code></td>
<td style="text-align: left;">切换到最后一次使用的面板</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>q</code></td>
<td style="text-align: left;">显示面板编号，在编号消失前输入对应的数字可切换到相应的面板</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>{</code></td>
<td style="text-align: left;">向前置换当前面板</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>}</code></td>
<td style="text-align: left;">向后置换当前面板</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>Ctrl+o</code></td>
<td style="text-align: left;">顺时针旋转当前窗口中的所有面板</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>方向键</code></td>
<td style="text-align: left;">移动光标切换面板</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>o</code></td>
<td style="text-align: left;">选择下一面板</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>空格键</code></td>
<td style="text-align: left;">在自带的面板布局中循环切换</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>Alt+方向键</code></td>
<td style="text-align: left;">以5个单元格为单位调整当前面板边缘</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>Ctrl+方向键</code></td>
<td style="text-align: left;">以1个单元格为单位调整当前面板边缘（Mac下被系统快捷键覆盖）</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>Ctrl+b</code></td>
<td style="text-align: center;"><code>t</code></td>
<td style="text-align: left;">显示时钟</td>
</tr>
</tbody>
</table>
<h4 id="tmux用于代码后台运行">tmux用于代码后台运行</h4>
<p>要想使代码后台运行，我用nohup训练模型时重定向到log文件发现日志显示不全，影响实验结果的呈现，而tmux可以解决这个问题。</p>
<p>在进入tmux，新建一个session的时候，实际上tmux在服务器创建了虚拟终端自己连自己。所以用ctrl+B +D退出tmux，并且断掉ssh连接时后台实验是一直在运行的。下次连接ssh后，再打开tmux的session即可，session不会断掉。并且不需要重定向，结果直接显示在屏幕上。</p>
<h4 id="tmux的个性化设置----待完成">tmux的个性化设置 -- 待完成</h4>
<p>比如通过写脚本，在新建session的时候就自动建多个pane，并运行命令</p>
<h4 id="参考-2">参考</h4>
<blockquote>
<p>http://louiszhai.github.io/2017/09/30/tmux/</p>
<p>https://harttle.land/2015/11/06/tmux-startup.html</p>
</blockquote>
<h3 id="查看cuda版本">查看CUDA版本</h3>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">nvcc  --version</span><br></pre></td></tr></tbody></table></figure>
<h3 id="zsh">zsh</h3>
<h4 id="安装">安装</h4>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">sudo apt install git-core zsh</span><br></pre></td></tr></tbody></table></figure>
<h3 id="杂">杂</h3>
<p><img src="https://i.loli.net/2020/09/09/Gl43ELFnjz5f6xQ.png" alt="image-20200909193607669"></p>
<p>在使用xshell 连接的时候，如果网络断开，那么xshell就会出现中断，出现上面的情况。</p>
<h4 id="屏蔽omp无用信息">屏蔽OMP无用信息</h4>
<p>在用tensorflow跑实验的时候，后台会输出OMP的输出，实际上是一些无用信息，如果要系统不显示这些信息，可以如下操作：</p>
<p><img src="E:\myBlog\source_posts\image-20201113151911779.png" alt="image-20201113151911779"></p>
<p>1.如果禁用OpenMP警告并将环境变量 KMP_WARNINGS 设置为 off ，这些消息应该消失了或 0 。在运行程序的时候可以选择禁用</p>
<figure class="highlight sh"><table><tbody><tr><td class="code"><pre><span class="line">KMP_WARNINGS = off python train.py</span><br></pre></td></tr></tbody></table></figure>
<p>2.或者从Python本身，在初始化OpenMP之前：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"> <span class="keyword">import</span> os </span><br><span class="line">os.environ [<span class="string">'KMP_WARNINGS'</span>] =<span class="string">'off'</span></span><br></pre></td></tr></tbody></table></figure>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>服务器</category>
      </categories>
      <tags>
        <tag>服务器</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-09-05-tf&amp;&amp;torch知识点杂</title>
    <url>/2020/09/05/2020-09-05-tf&amp;&amp;torch%E7%9F%A5%E8%AF%86%E7%82%B9%E6%9D%82%E7%9F%A5%E8%AF%86%E7%82%B9%E6%9D%82/</url>
    <content><![CDATA[<h2 id="others">🚀others</h2>
<h3 id="python的einops-rearrange函数">python的einops rearrange()函数</h3>
<p>例子：</p>
<p>假设我有一个3-D数组：</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">[[[0,1,2],</span><br><span class="line">  [0,1,2],</span><br><span class="line">  [0,1,2]],</span><br><span class="line"></span><br><span class="line"> [[3,4,5],</span><br><span class="line">  [3,4,5],</span><br><span class="line">  [3,4,5]]]</span><br></pre></td></tr></tbody></table></figure>
<p>我想按列重新排列：</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">[[0,1,2,3,4,5],</span><br><span class="line"> [0,1,2,3,4,5],</span><br><span class="line"> [0,1,2,3,4,5]]</span><br></pre></td></tr></tbody></table></figure>
<p>使用einops：</p>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line">einops.rearrange(a, <span class="string">'x y z -&gt; y (x z) '</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>并且我建议根据上下文（例如时间，高度等）为轴指定有意义的名称（而不是xyz）。 这将使您易于理解代码的作用</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">In : einops.rearrange(a, 'x y z -&gt; y (x z) ')</span><br><span class="line">Out:</span><br><span class="line">array([[0, 1, 2, 3, 4, 5],</span><br><span class="line">       [0, 1, 2, 3, 4, 5],</span><br><span class="line">       [0, 1, 2, 3, 4, 5]])</span><br></pre></td></tr></tbody></table></figure>
<h3 id="epochiterationbatch_size">Epoch、Iteration、Batch_size</h3>
<blockquote>
<p><a href="https://blog.csdn.net/program_developer/article/details/78597738" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/program_developer/article/details/78597738</a></p>
</blockquote>
<h2 id="tensorflow">🚀tensorflow</h2>
<h3 id="tf.tile用法">tf.tile()用法</h3>
<blockquote>
<p><a href="https://blog.csdn.net/tsyccnh/article/details/82459859" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/tsyccnh/article/details/82459859</a></p>
</blockquote>
<h3 id="dataset-api-和-iterator">Dataset API 和 Iterator</h3>
<blockquote>
<p>Dataset API 和 Iterator</p>
<p><a href="https://blog.csdn.net/briblue/article/details/80962728" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/briblue/article/details/80962728</a></p>
<p>TensorFlow中的Dataset API</p>
<p><a href="https://blog.csdn.net/dqcfkyqdxym3f8rb0/article/details/79342369" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/dqcfkyqdxym3f8rb0/article/details/79342369</a></p>
<p>TensorFlow data模块详解</p>
<p><a href="https://www.weaf.top/posts/cd5ba0c4/" target="_blank" rel="noopener" class="uri">https://www.weaf.top/posts/cd5ba0c4/</a></p>
<p>使用Tensorflow的DataSet和Iterator读取数据</p>
<p><a href="https://www.jianshu.com/p/bcff8a99b15b" target="_blank" rel="noopener" class="uri">https://www.jianshu.com/p/bcff8a99b15b</a></p>
<p>tensorflow数据读取机制（附代码）</p>
<p><a href="https://zhuanlan.zhihu.com/p/27238630" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/27238630</a></p>
<p>Dataset API入门教程</p>
<p><a href="https://zhuanlan.zhihu.com/p/30751039" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/30751039</a></p>
<p>Dataset.from_generator</p>
<p><a href="https://blog.csdn.net/foreseerwang/article/details/80572182" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/foreseerwang/article/details/80572182</a></p>
</blockquote>
<p>看个简单的示例：</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">#创建一个Dataset对象</span><br><span class="line">dataset = tf.data.Dataset.from_tensor_slices([1,2,3,4,5,6,7,8,9])</span><br><span class="line"></span><br><span class="line">#创建一个迭代器</span><br><span class="line">iterator = dataset.make_one_shot_iterator()</span><br><span class="line"></span><br><span class="line">#get_next()函数可以帮助我们从迭代器中获取元素</span><br><span class="line">element = iterator.get_next()</span><br><span class="line"></span><br><span class="line">#遍历迭代器，获取所有元素</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">   for i in range(9):</span><br><span class="line">       print(sess.run(element))</span><br></pre></td></tr></tbody></table></figure>
<p>以上打印结果为：1 2 3 4 5 6 7 8 9</p>
<p>from_generator</p>
<p>创建Dataset由其生成元素的元素generator。</p>
<p>函数形式：from_generator(generator,output_types,output_shapes=None,args=None)</p>
<p>参数generator:一个可调用对象，它返回支持该iter()协议的对象 。如果args未指定，generator则不得参数; 否则它必须采取与有值一样多的参数args。 参数output_types：tf.DType对应于由元素生成的元素的每个组件的对象的嵌套结构generator。 参数output_shapes:tf.TensorShape 对应于由元素生成的元素的每个组件的对象 的嵌套结构generator 参数args:tf.Tensor将被计算并将generator作为NumPy数组参数传递的对象元组。</p>
<p>具体例子</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">#定义一个生成器</span><br><span class="line">def data_generator():</span><br><span class="line">    dataset = np.array(range(9))</span><br><span class="line">    for i in dataset:</span><br><span class="line">        yield i</span><br><span class="line"></span><br><span class="line">#接收生成器，并生产dataset数据结构</span><br><span class="line">dataset = tf.data.Dataset.from_generator(data_generator, (tf.int32))</span><br><span class="line"></span><br><span class="line">iterator = concat_dataset.make_one_shot_iterator()</span><br><span class="line"></span><br><span class="line">element = iterator.get_next()</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">   for i in range(3):</span><br><span class="line">       print(sess.run(element))</span><br></pre></td></tr></tbody></table></figure>
<p>以上代码运行结果：0 1 2</p>
<h3 id="tf-strip-和-split">tf strip() 和 split()</h3>
<blockquote>
<p><a href="https://blog.csdn.net/hjxu2016/article/details/78676859" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/hjxu2016/article/details/78676859</a></p>
</blockquote>
<h3 id="summary用法--tensorborad可视化">Summary用法 -tensorborad可视化</h3>
<blockquote>
<p><a href="https://www.cnblogs.com/lyc-seu/p/8647792.html" target="_blank" rel="noopener" class="uri">https://www.cnblogs.com/lyc-seu/p/8647792.html</a></p>
</blockquote>
<h3 id="math.ceil">math.ceil()</h3>
<blockquote>
<p><a href="https://www.runoob.com/python/func-number-ceil.html" target="_blank" rel="noopener" class="uri">https://www.runoob.com/python/func-number-ceil.html</a></p>
</blockquote>
<h3 id="format-格式化函数">.format() 格式化函数</h3>
<blockquote>
<p><a href="https://www.runoob.com/python/att-string-format.html" target="_blank" rel="noopener" class="uri">https://www.runoob.com/python/att-string-format.html</a></p>
</blockquote>
<h3 id="tf.shapea-和-a.get_shape.as_list-和-tf.split">tf.shape(A) 和 A.get_shape().as_list() 和 tf.split()</h3>
<blockquote>
<p><a href="https://www.itread01.com/content/1544436557.html" target="_blank" rel="noopener" class="uri">https://www.itread01.com/content/1544436557.html</a></p>
<p><a href="https://blog.csdn.net/xc_zhou/article/details/85632109" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/xc_zhou/article/details/85632109</a></p>
</blockquote>
<ul>
<li>tf.shape(A) # 獲取張量A（陣列，list, tensor張量）的大小，返回的是一個list</li>
<li>x.get_shape()，只有<strong>tensor</strong>才可以使用這種方法，返回的是一個元組</li>
<li>tf.split(dimension, num_split, input)：dimension的意思就是輸入張量的哪一個維度，如果是0就表示對第0維度進行切割。num_split就是切割的數量，如果是2就表示輸入張量被切成2份，每一份是一個列表。</li>
</ul>
<h3 id="tf.range">tf.range()</h3>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">w=tf.range(<span class="number">3</span>)</span><br><span class="line"><span class="keyword">print</span> (sess.run(w))<span class="comment">#输出[0 1 2]</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="os.path">os.path（）</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">方法</th>
<th style="text-align: left;">说明</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">os.path.abspath(path)</td>
<td style="text-align: left;">返回绝对路径</td>
</tr>
<tr class="even">
<td style="text-align: left;">os.path.basename(path)</td>
<td style="text-align: left;">返回文件名</td>
</tr>
<tr class="odd">
<td style="text-align: left;">os.path.join(path1[, path2[, ...]])</td>
<td style="text-align: left;">把目录和文件名合成一个路径</td>
</tr>
<tr class="even">
<td style="text-align: left;">os.path.dirname(path)</td>
<td style="text-align: left;">返回文件路径</td>
</tr>
<tr class="odd">
<td style="text-align: left;">os.path.exists(path)</td>
<td style="text-align: left;">如果路径 path 存在，返回 True；如果路径 path 不存在，返回 False。</td>
</tr>
<tr class="even">
<td style="text-align: left;">os.path.split(path)</td>
<td style="text-align: left;">把路径分割成 dirname 和 basename，返回一个元组</td>
</tr>
</tbody>
</table>
<blockquote>
<p><a href="https://www.runoob.com/python/python-os-path.html" target="_blank" rel="noopener" class="uri">https://www.runoob.com/python/python-os-path.html</a></p>
</blockquote>
<h3 id="embedding_lookup">embedding_lookup()</h3>
<p>tf.nn.embedding_lookup()就是根据input_ids中的id，寻找embeddings中的第id行。比如input_ids=[1,3,5]，则找出embeddings中第1，3，5行，组成一个tensor返回。</p>
<blockquote>
<p><a href="https://www.jianshu.com/p/7bb87873f89e" target="_blank" rel="noopener" class="uri">https://www.jianshu.com/p/7bb87873f89e</a></p>
<p><a href="https://www.zhihu.com/question/52250059" target="_blank" rel="noopener" class="uri">https://www.zhihu.com/question/52250059</a></p>
</blockquote>
<h3 id="模型保存和加载">模型保存和加载</h3>
<p>Saver的作用是将我们训练好的模型的参数保存下来，以便下一次继续用于训练或测试；Restore的用法是将训练好的参数提取出来。</p>
<p>1.Saver类训练完后，是以<strong>checkpoints文件形式</strong>保存。提取的时候也是从checkpoints文件中恢复变量。 Checkpoints文件是一个二进制文件，它把变量名映射到对应的tensor值 。</p>
<p>2.通过for循环，Saver类可以自动的生成checkpoint文件。这样我们就可以<strong>保存多个训练结果</strong>。例如，我们可以保存每一步训练的结果。但是为了避免填满整个磁盘，<strong>Saver可以自动的管理Checkpoints文件</strong>。例如，我们可以指定保存最近的N个Checkpoints文件。</p>
<h3 id="tensorflow模型保存和读取tf.train.saver">Tensorflow模型保存和读取tf.train.Saver</h3>
<p>目标：训练网络后想保存训练好的模型，以及在程序中读取以保存的训练好的模型。</p>
<p>首先，保存和恢复都需要实例化一个 tf.train.Saver。</p>
<blockquote>
<p>saver = tf.train.Saver()</p>
</blockquote>
<p>然后，在训练循环中，定期调用 saver.save() 方法，向文件夹中写入包含了当前模型中所有可训练变量的 checkpoint 文件。</p>
<blockquote>
<p>saver.save(sess, save_path, global_step=step)</p>
</blockquote>
<p>之后，就可以使用 saver.restore() 方法，重载模型的参数，继续训练或用于测试数据。</p>
<blockquote>
<p>saver.restore(sess, save_path)</p>
</blockquote>
<p>模型的恢复用的是restore()函数，它需要两个参数restore(sess, save_path)，save_path指的是保存的模型路径。我们可以使用<code>tf.train.latest_checkpoint（）</code>来自动获取最后一次保存的模型。如：</p>
<figure class="highlight javascript"><table><tbody><tr><td class="code"><pre><span class="line">model_file=tf.train.latest_checkpoint(<span class="string">'ckpt/'</span>)</span><br><span class="line">saver.restore(sess,model_file)</span><br></pre></td></tr></tbody></table></figure>
<p>一次 saver.save() 后可以在文件夹中看到新增的四个文件，</p>
<p><img src="https://i.loli.net/2020/09/29/kRYmSZn8BbwJ4NK.png" alt="image-20200929102459806"></p>
<p>实际上每调用一次保存操作会创建后3个数据文件并创建一个检查点（checkpoint）文件，简单理解就是权重等参数被保存到 .ckpt.data 文件中，以字典的形式；图和元数据被保存到 .ckpt.meta 文件中，可以被 tf.train.import_meta_graph 加载到当前默认的图。</p>
<p>saver.restore()时填的文件名，因为在saver.save的时候，每个checkpoint会保存三个文件，如 <code>my-model-10000.meta</code>, <code>my-model-10000.index</code>, <code>my-model-10000.data-00000-of-00001</code></p>
<p>在<code>import_meta_graph</code>时填的就是<code>meta</code>文件名，我们知道权值都保存在my-model-10000.data-00000-of-00001这个文件中，但是如果在restore方法中填这个文件名，就会报错，应该填的是前缀，这个前缀可以使用<code>tf.train.latest_checkpoint(checkpoint_dir)</code>这个方法获取。</p>
<p>下面代码是简单的保存和读取模型：（不包括加载图数据）</p>
<figure class="highlight javascript"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"> </span><br><span class="line">#用numpy产生数据</span><br><span class="line">x_data = np.linspace(-1,1,300)[:, np.newaxis] #转置</span><br><span class="line">noise = np.random.normal(<span class="number">0</span>,<span class="number">0.05</span>, x_data.shape)</span><br><span class="line">y_data = np.square(x_data)<span class="number">-0.5</span>+noise</span><br><span class="line"> </span><br><span class="line">#输入层</span><br><span class="line">x_ph = tf.placeholder(tf.float32, [None, <span class="number">1</span>])</span><br><span class="line">y_ph = tf.placeholder(tf.float32, [None, <span class="number">1</span>])</span><br><span class="line"> </span><br><span class="line">#隐藏层</span><br><span class="line">w1 = tf.Variable(tf.random_normal([<span class="number">1</span>,<span class="number">10</span>]))</span><br><span class="line">b1 = tf.Variable(tf.zeros([<span class="number">1</span>,<span class="number">10</span>])+<span class="number">0.1</span>)</span><br><span class="line">wx_plus_b1 = tf.matmul(x_ph, w1) + b1</span><br><span class="line">hidden = tf.nn.relu(wx_plus_b1)</span><br><span class="line"> </span><br><span class="line">#输出层</span><br><span class="line">w2 = tf.Variable(tf.random_normal([<span class="number">10</span>,<span class="number">1</span>]))</span><br><span class="line">b2 = tf.Variable(tf.zeros([<span class="number">1</span>,<span class="number">1</span>])+<span class="number">0.1</span>)</span><br><span class="line">wx_plus_b2 = tf.matmul(hidden, w2) + b2</span><br><span class="line">y = wx_plus_b2</span><br><span class="line"> </span><br><span class="line">#损失</span><br><span class="line">loss = tf.reduce_mean(tf.reduce_sum(tf.square(y_ph-y),reduction_indices=[<span class="number">1</span>]))</span><br><span class="line">train_op = tf.train.GradientDescentOptimizer(<span class="number">0.1</span>).minimize(loss)</span><br><span class="line"> </span><br><span class="line">#保存模型对象saver</span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line"> </span><br><span class="line">#判断模型保存路径是否存在，不存在就创建</span><br><span class="line"><span class="keyword">if</span> not os.path.exists(<span class="string">'tmp/'</span>):</span><br><span class="line">    os.mkdir(<span class="string">'tmp/'</span>)</span><br><span class="line"> </span><br><span class="line">#初始化</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    if os.path.exists('tmp/checkpoint'):         #判断模型是否存在</span><br><span class="line">        saver.restore(sess, 'tmp/model.ckpt')    #存在就从模型中恢复变量</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        init = tf.global_variables_initializer() #不存在就初始化变量</span><br><span class="line">        sess.run(init)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">        _,loss_value = sess.run([train_op,loss], feed_dict={<span class="attr">x_ph</span>:x_data, <span class="attr">y_ph</span>:y_data})</span><br><span class="line">        <span class="keyword">if</span>(i%<span class="number">50</span>==<span class="number">0</span>):</span><br><span class="line">            save_path = saver.save(sess, <span class="string">'tmp/model.ckpt'</span>)</span><br><span class="line">            print(<span class="string">"迭代次数：%d , 训练损失：%s"</span>%(i, loss_value))</span><br></pre></td></tr></tbody></table></figure>
<p>注：</p>
<ul>
<li>saver 的操作必须在 sess 建立后进行。</li>
<li>model.ckpt 必须存在给定文件夹中，‘tmp/model.ckpt’ 这里至少要有一层文件夹，否则无法保存。</li>
<li>恢复模型时同保存时一样，是 ‘tmp/model.ckpt’，和那3个文件名都不一样。</li>
</ul>
<p>如果不用<code>tf.train.latest_checkpoint（）</code>来自动获取最后一次保存的模型，则怎么做呢？</p>
<blockquote>
<p><a href="https://www.jianshu.com/p/7ebee4d10e49" target="_blank" rel="noopener" class="uri">https://www.jianshu.com/p/7ebee4d10e49</a></p>
<p><a href="https://blog.csdn.net/mylove0414/article/details/55097486" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/mylove0414/article/details/55097486</a></p>
</blockquote>
<h3 id="saver中的max_to_keep-参数">Saver中的max_to_keep 参数</h3>
<h3 id="keras中的timedistributed函数">keras中的TimeDistributed函数</h3>
<blockquote>
<p><a href="https://blog.csdn.net/u012193416/article/details/79477220" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/u012193416/article/details/79477220</a></p>
<p><a href="https://keras.io/zh/layers/wrappers/" target="_blank" rel="noopener" class="uri">https://keras.io/zh/layers/wrappers/</a></p>
<p><a href="https://blog.csdn.net/zh_JNU/article/details/85160379" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/zh_JNU/article/details/85160379</a></p>
<p><a href="https://www.cnblogs.com/CheeseZH/p/13408658.html" target="_blank" rel="noopener" class="uri">https://www.cnblogs.com/CheeseZH/p/13408658.html</a></p>
</blockquote>
<h3 id="tf.concat详解">tf.concat()详解</h3>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">tf.concat([tensor1, tensor2, tensor3,...], axis)</span><br><span class="line"><span class="comment"># axis=0     代表在第0个维度拼接</span></span><br><span class="line"><span class="comment"># axis=1     代表在第1个维度拼接 </span></span><br><span class="line"><span class="comment">#axis=-1	 代表倒数第一个维度</span></span><br></pre></td></tr></tbody></table></figure>
<blockquote>
<p><a href="https://blog.csdn.net/leviopku/article/details/82380118" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/leviopku/article/details/82380118</a></p>
</blockquote>
<h3 id="shape">shape</h3>
<p>numpy数据的形状：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">x.shape()</span><br></pre></td></tr></tbody></table></figure>
<p>list 数据的形状：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">np.shape(x)</span><br></pre></td></tr></tbody></table></figure>
<p><strong>注：</strong>如果写<code>x.shape()</code> , 则会报错<code>ValueError: invalid literal for int() with base 10</code></p>
<p>torsor形状：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">x.get_shape()</span><br></pre></td></tr></tbody></table></figure>
<h3 id="keras-的-fit函数">keras 的 fit函数</h3>
<p>fit中以call()方法的形式来run session</p>
<blockquote>
<p><a href="https://blog.csdn.net/u012526436/article/details/102488164" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/u012526436/article/details/102488164</a></p>
</blockquote>
<h3 id="model-类继承">Model 类继承</h3>
<p><strong>可以通过继承 <code>Model</code> 类并在 <code>call</code> 方法中实现你自己的前向传播，以创建你自己的完全定制化的模型，</strong>（<code>Model</code> 类继承 API 引入于 Keras 2.2.0）。</p>
<p>这里是一个用 <code>Model</code> 类继承写的简单的多层感知器的例子：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleMLP</span><span class="params">(keras.Model)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, use_bn=False, use_dp=False, num_classes=<span class="number">10</span>)</span>:</span></span><br><span class="line">        super(SimpleMLP, self).__init__(name=<span class="string">'mlp'</span>)</span><br><span class="line">        self.use_bn = use_bn</span><br><span class="line">        self.use_dp = use_dp</span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line"></span><br><span class="line">        self.dense1 = keras.layers.Dense(<span class="number">32</span>, activation=<span class="string">'relu'</span>)</span><br><span class="line">        self.dense2 = keras.layers.Dense(num_classes, activation=<span class="string">'softmax'</span>)</span><br><span class="line">        <span class="keyword">if</span> self.use_dp:</span><br><span class="line">            self.dp = keras.layers.Dropout(<span class="number">0.5</span>)</span><br><span class="line">        <span class="keyword">if</span> self.use_bn:</span><br><span class="line">            self.bn = keras.layers.BatchNormalization(axis=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        x = self.dense1(inputs)</span><br><span class="line">        <span class="keyword">if</span> self.use_dp:</span><br><span class="line">            x = self.dp(x)</span><br><span class="line">        <span class="keyword">if</span> self.use_bn:</span><br><span class="line">            x = self.bn(x)</span><br><span class="line">        <span class="keyword">return</span> self.dense2(x)</span><br><span class="line"></span><br><span class="line">model = SimpleMLP()</span><br><span class="line">model.compile(...)</span><br><span class="line">model.fit(...)</span><br></pre></td></tr></tbody></table></figure>
<p>网络层定义在 <code>__init__(self, ...)</code> 中，前向传播在 <code>call(self, inputs)</code> 中指定。在 <code>call</code> 中，你可以指定自定义的损失函数，通过调用 <code>self.add_loss(loss_tensor)</code> （就像你在自定义层中一样）。</p>
<p>在类继承模型中，模型的拓扑结构是由 Python 代码定义的（而不是网络层的静态图）。这意味着该模型的拓扑结构不能被检查或序列化。因此，以下方法和属性<strong>不适用于类继承模型</strong>：</p>
<ul>
<li><code>model.inputs</code> 和 <code>model.outputs</code>。</li>
<li><code>model.to_yaml()</code> 和 <code>model.to_json()</code>。</li>
<li><code>model.get_config()</code> 和 <code>model.save()</code>。</li>
</ul>
<p><strong>关键点</strong>：为每个任务使用正确的 API。<code>Model</code> 类继承 API 可以为实现复杂模型提供更大的灵活性，但它需要付出代价（比如缺失的特性）：它更冗长，更复杂，并且有更多的用户错误机会。如果可能的话，尽可能使用函数式 API，这对用户更友好。</p>
<blockquote>
<p><a href="https://blog.csdn.net/qq_27825451/article/details/90517036" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/qq_27825451/article/details/90517036</a></p>
</blockquote>
<h3 id="关于tensorflow的sessiontensorshape等基础知识整理">关于tensorflow的session、tensor、shape等基础知识（整理）</h3>
<p>在tensorflow程序中，tensor只是占位符，在会话层没有run出tensor的值之前，我们是无法获知tensor的值的</p>
<blockquote>
<p><a href="https://blog.csdn.net/jiongnima/article/details/78524551" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/jiongnima/article/details/78524551</a></p>
<p><a href="https://www.tensorflow.org/guide/tensor?hl=zh-cn" target="_blank" rel="noopener" class="uri">https://www.tensorflow.org/guide/tensor?hl=zh-cn</a></p>
<p><a href="https://www.jianshu.com/p/75a903a44cf2" target="_blank" rel="noopener" class="uri">https://www.jianshu.com/p/75a903a44cf2</a></p>
</blockquote>
<h3 id="tf.layers.flatten">tf.layers.flatten</h3>
<p>在保留第0轴的情况下对输入的张量进行Flatten(扁平化)</p>
<p>代码示例：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">x=tf.placeholder(shape=(<span class="literal">None</span>,<span class="number">4</span>,<span class="number">4</span>),dtype=<span class="string">'float32'</span>)</span><br><span class="line"></span><br><span class="line">y=tf.layers.flatten(x)</span><br><span class="line"></span><br><span class="line">print(y)</span><br></pre></td></tr></tbody></table></figure>
<p>输出： 将后两维进行合并</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">Tensor("flatten/Reshape:0", shape=(?, 16), dtype=float32)</span><br></pre></td></tr></tbody></table></figure>
<h3 id="tf.layers.dense">tf.layers.dense</h3>
<p>全连接层 ，相当于添加一个层。只<strong>改变输入的最后一维</strong></p>
<h3 id="python---tensorflow中-none-1和之间的区别">python - Tensorflow中 None，-1和？之间的区别</h3>
<p><code>None</code>表示未指定的维度。因此，如果您定义了一个占位符，您可以使用<code>None</code>来表示“这个维度可以有任何大小”。 占位符可以有多个<code>None</code>维度这仅仅意味着多个维度可以是不同的大小甚至整个形状都可以<code>None</code>来指定未知的维数。 <code>-1</code>是TensorFlow的一条指令，用于自行推断维度的大小。在<code>tf.reshape(input, [-1, input_size])</code>中，这意味着“重塑它，使第二个维度<code>input_size</code>，第一个维度是匹配元素总数所需的任何内容”。 这并不一定意味着维数是未知的，因为对于<code>None</code>如果输入张量的已知大小为10个元素，并且将其重塑为<code>[-1, 2]</code>，则张量流能够推断出完整的形状<code>[5, 2]</code>。 <code>-1</code>纯粹是为了方便。你可以把形状写下来，而不是让Tensorflow推断出来<code>None</code>另一方面，对于接受可变大小张量是必要的。 一个形状中只能有一个<code>-1</code>。多个是没有意义的，因为不可能推断出形状。例如，如果一个张量中有12个元素，则未定义将其重塑为<code>[-1, -1, 2]</code>——我们是否应该这样做？<code>[3, 2, 2]</code>？<code>[2, 3, 2]</code>？… 最后，问号正是tensorflow在打印张量和/或其形状时用来标记“未知”维度的内容。您发布的示例实际上会产生语法错误——您不能自己使用问号。未知维度的原因当然可以是具有<code>[6, 1, 2]</code>维度的占位符，并且通常根据占位符定义的张量（即应用于它们的某些运算的结果）也将具有未知维度。此外，有些操作可能没有指定（部分）它们的输出形状，这也可能导致未知。 这里可能还有一些我遗漏的技术细节，但根据经验：使用<code>None</code>作为占位符，使用<code>None</code>进行整形。这应该涵盖大多数用例。</p>
<blockquote>
<p><code>？</code>== <code>None</code> ，维度是未知的</p>
<p><code>-1</code>代表根据推断之后的维度</p>
<p><code>(3,)</code> 表明张量是一个一维数组，这个数组的长度为3</p>
</blockquote>
<blockquote>
<p><a href="https://www.coder.work/article/2032326" target="_blank" rel="noopener" class="uri">https://www.coder.work/article/2032326</a></p>
</blockquote>
<h3 id="keras的-call-函数build-函数">keras的 call 函数、build 函数</h3>
<p>build() 用来初始化定义weights, 这里可以用父类的self.add_weight() 函数来初始化数据, 该函数必须将 self.built 设置为True, 以保证该 Layer 已经成功 build , 通常如上所示, 使用 super(MyLayer, self).build(input_shape) 来完成。</p>
<p>call() 用来执行 Layer 的职能, x就是该层的输入，x与权重kernel做点积，生成新的节点层，即当前 Layer 所有的计算过程均在该函数中完成。</p>
<p><code>__init__()</code>和<code>build()</code>函数都在对Layer进行初始化，都初始化了一些成员函数</p>
<p><code>__init__()</code>：保存成员变量的设置</p>
<p><code>build()</code>：在<code>call()</code>函数第一次执行时会被调用一次，这时候可以知道输入数据的<code>shape</code>。返回去看一看，果然是<code>__init__()</code>函数中只初始化了输出数据的<code>shape</code>，而输入数据的<code>shape</code>需要在<code>build()</code>函数中动态获取，这也解释了为什么在有<code>__init__()</code>函数时还需要使用<code>build()</code>函数</p>
<p><code>call()</code>函数则是在该layer被调用时执行。</p>
<blockquote>
<p><a href="https://blog.csdn.net/qq_32623363/article/details/104128497" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/qq_32623363/article/details/104128497</a></p>
<p><a href="https://blog.csdn.net/qq_27825451/article/details/90517036" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/qq_27825451/article/details/90517036</a></p>
</blockquote>
<h3 id="tf.expand_dims">tf.expand_dims（）</h3>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">tf.expand_dims(input, dim, name=<span class="literal">None</span>) <span class="comment">#在指定位置增加维度</span></span><br></pre></td></tr></tbody></table></figure>
<blockquote>
<p><a href="https://blog.csdn.net/jasonzzj/article/details/60811035" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/jasonzzj/article/details/60811035</a></p>
</blockquote>
<h3 id="tf.boolean_mask">tf.boolean_mask（）</h3>
<p>选择张量的特定维度的值</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">tf.boolean_mask(tensor,mask,name=<span class="string">'boolean_mask'</span>,axis=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#1-D example</span></span><br><span class="line">tensor = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">mask = np.array([<span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">False</span>])</span><br><span class="line">boolean_mask(tensor, mask)  <span class="comment"># [0, 2]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#2-D example</span></span><br><span class="line">tensor = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>]]</span><br><span class="line">mask = np.array([<span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">True</span>])</span><br><span class="line">boolean_mask(tensor, mask)  <span class="comment"># [[1, 2], [5, 6]]</span></span><br></pre></td></tr></tbody></table></figure>
<blockquote>
<p><a href="https://blog.csdn.net/wuguangbin1230/article/details/81334544" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/wuguangbin1230/article/details/81334544</a></p>
</blockquote>
<h2 id="pytorch">🚀pytorch</h2>
<h3 id="pytorch-torch.nn.parameter">PyTorch torch.nn.Parameter()</h3>
<p><strong>作用</strong>：对于<code>self.v = torch.nn.Parameter(torch.FloatTensor(hidden_size))</code>，也就是将一个不可训练的类型<code>Tensor</code>转换成可以训练的类型<code>parameter</code>并将这个<code>parameter</code>绑定到这个<code>module</code>里面(<code>net.parameter()</code>中就有这个绑定的<code>parameter</code>，所以在参数优化的时候可以进行优化的)，所以经过类型转换这个<code>self.v</code>变成了模型的一部分，成为了模型中根据训练可以改动的参数了。</p>
<p>使用这个函数的目的也是想让某些变量在学习的过程中不断的修改其值以达到最优化。</p>
<blockquote>
<p><a href="https://www.jianshu.com/p/d8b77cc02410" target="_blank" rel="noopener" class="uri">https://www.jianshu.com/p/d8b77cc02410</a></p>
</blockquote>
<h3 id="pytorch-nn.linear">PyTorch nn.Linear（）</h3>
<p>用于设置网络中的<strong>全连接层的</strong></p>
<blockquote>
<p><a href="https://blog.csdn.net/qq_42079689/article/details/102873766" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/qq_42079689/article/details/102873766</a></p>
</blockquote>
<h3 id="pytorch-nn.embedding-词向量">pytorch nn.embedding() 词向量</h3>
<p>词嵌入在 pytorch 中非常简单，只需要调用 <code>torch.nn.Embedding(m, n)</code> 就可以了，m 表示单词的总数目，n 表示词嵌入的维度，其实词嵌入就相当于是一个大矩阵，矩阵的每一行表示一个单词。</p>
<p><strong>随机初始化</strong></p>
<blockquote>
<p><a href="https://blog.csdn.net/david0611/article/details/81090371" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/david0611/article/details/81090371</a></p>
</blockquote>
<h3 id="pytorch-torch.mean">pytorch torch.mean()</h3>
<p><strong>torch.mean(input, dim, keepdim=False, out=None)</strong></p>
<p>返回新的张量，其中包含输入张量input指定维度dim中每行的平均值。</p>
<p>若keepdim值为True，则在输出张量中，除了被操作的dim维度值降为1，其它维度与输入张量input相同。否则，dim维度相当于被执行torch.squeeze()维度压缩操作，导致此维度消失，最终输出张量会比输入张量少一个维度。</p>
<p><strong>参数：</strong></p>
<ul>
<li>input (Tensor) - 输入张量</li>
<li>dim (int) - 指定进行均值计算的维度</li>
<li>keepdim (bool, optional) - 输出张量是否保持与输入张量有相同数量的维度</li>
<li>out (Tensor) - 结果张量</li>
</ul>
<p><strong>例子：</strong></p>
<blockquote>
<p>a = torch.randn(4, 5) a 0.3168 0.4953 -0.6758 -0.5559 -0.6906 0.2241 2.2450 1.5735 -1.3815 -1.5199 0.0033 0.5236 -0.9070 -0.5961 -2.1281 0.9605 1.5314 -0.6555 -1.2584 -0.4160 [torch.FloatTensor of size 4x5] torch.mean(a, 1, True) -0.2220 0.2283 -0.6209 0.0324 [torch.FloatTensor of size 4x1]</p>
</blockquote>
<h3 id="np.triu-np.tril">np.triu() &amp; np.tril()</h3>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">triu</span>（<span class="title">m</span>， <span class="title">k</span>）：</span></span><br><span class="line"><span class="function">#取上三角阵  </span></span><br><span class="line"><span class="function">#<span class="title">m</span>：表示一个矩阵</span></span><br><span class="line"><span class="function">#<span class="title">K</span>：表示对角线的起始位置（<span class="title">k</span>取值默认为0）</span></span><br><span class="line"><span class="function"></span></span><br><span class="line">#k=0表示正常的上三角矩阵</span><br><span class="line"><span class="comment">#k=-1表示对角线的位置下移1个对角线</span></span><br><span class="line"><span class="comment">#k=1表示对角线的位置上移1个对角线</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#同理，np.tril取下三角阵</span></span><br></pre></td></tr></tbody></table></figure>
<blockquote>
<p><a href="https://blog.csdn.net/weixin_37724529/article/details/102881776" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/weixin_37724529/article/details/102881776</a></p>
</blockquote>
<h3 id="pytorch-forward的使用以及原理---pytorch使用">pytorch forward的使用以及原理 --pytorch使用</h3>
<blockquote>
<p><a href="https://blog.csdn.net/u011501388/article/details/84062483" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/u011501388/article/details/84062483</a></p>
</blockquote>
<h3 id="pytorch-torch.nn.parameter详解">PyTorch torch.nn.Parameter()详解</h3>
<blockquote>
<p><a href="https://cloud.tencent.com/developer/article/1608348" target="_blank" rel="noopener" class="uri">https://cloud.tencent.com/developer/article/1608348</a></p>
</blockquote>
<h3 id="pytorch-view">pytorch view()</h3>
<p>PyTorch中<strong>view</strong>函数作用为重构张量的维度</p>
<blockquote>
<p>torch.view(参数a,参数b,.....)，其中参数a=3,参数b=2决定了将一维的tt1重构成3*2维的张量。 有时候会出现torch.view(-1)或者torch.view(参数a,-1)这种情况。则-1参数是需要估算的。</p>
</blockquote>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">tt3=torch.tensor([[<span class="number">-0.3623</span>,<span class="number">-0.6115</span>],[<span class="number">0.7283</span>,<span class="number">0.4699</span>],[<span class="number">2.3261</span>,<span class="number">0.1599</span>]])</span><br><span class="line">result2=tt3.view(<span class="number">2</span>,<span class="number">-1</span>).contiguous()</span><br></pre></td></tr></tbody></table></figure>
<p>则<code>result2</code>为</p>
<figure class="highlight css"><table><tbody><tr><td class="code"><pre><span class="line"><span class="selector-tag">tensor</span>(<span class="selector-attr">[[-0.3623, -0.6115,  0.7283]</span>,</span><br><span class="line">        <span class="selector-attr">[ 0.4699,  2.3261,  0.1599]</span>])</span><br></pre></td></tr></tbody></table></figure>
<h3 id="pytorch-model.parameters">pytorch model.parameters()</h3>
<p>这个方法会获得模型的参数信息 。</p>
<p>model.parameters()方法<strong>返回的是一个生成器generator，每一个元素是从开头到结尾的参数</strong>，parameters没有对应的key名称，是一个由纯参数组成的generator，查看Module的参数信息，<strong>用于更新参数，或者用于模型的保存。</strong></p>
<blockquote>
<p><a href="https://blog.csdn.net/qq_27825451/article/details/95888267" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/qq_27825451/article/details/95888267</a></p>
</blockquote>
<h3 id="pytorch-torch.optim.lr_scheduler">pytorch torch.optim.lr_scheduler</h3>
<p>用于设置学习率的衰减</p>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/69411064" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/69411064</a></p>
</blockquote>
<h3 id="pytorch-torch.gather">pytorch torch.gather（）</h3>
<p>例子</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">b = torch.Tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line"><span class="keyword">print</span> b</span><br><span class="line">index_1 = torch.LongTensor([[<span class="number">0</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">0</span>]])</span><br><span class="line">index_2 = torch.LongTensor([[<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]])</span><br><span class="line"><span class="keyword">print</span> torch.gather(b, dim=<span class="number">1</span>, index=index_1)</span><br><span class="line"><span class="keyword">print</span> torch.gather(b, dim=<span class="number">0</span>, index=index_2)</span><br></pre></td></tr></tbody></table></figure>
<p>观察它的输出结果：</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line"> 1  2  3</span><br><span class="line"> 4  5  6</span><br><span class="line">[torch.FloatTensor of size 2x3]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> 1  2</span><br><span class="line"> 6  4</span><br><span class="line">[torch.FloatTensor of size 2x2]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> 1  5  6</span><br><span class="line"> 1  2  3</span><br><span class="line">[torch.FloatTensor of size 2x3]</span><br></pre></td></tr></tbody></table></figure>
<p>这里是官方文档的解释</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">torch.gather(input, dim, index, out=None) → Tensor</span><br><span class="line"></span><br><span class="line">    Gathers values along an axis specified by dim.</span><br><span class="line"></span><br><span class="line">    For a 3-D tensor the output is specified by:</span><br><span class="line"></span><br><span class="line">    out[i][j][k] = input[index[i][j][k]][j][k]  # dim=0</span><br><span class="line">    out[i][j][k] = input[i][index[i][j][k]][k]  # dim=1</span><br><span class="line">    out[i][j][k] = input[i][j][index[i][j][k]]  # dim=2</span><br><span class="line"></span><br><span class="line">    Parameters:	</span><br><span class="line"></span><br><span class="line">        input (Tensor) – The source tensor</span><br><span class="line">        dim (int) – The axis along which to index</span><br><span class="line">        index (LongTensor) – The indices of elements to gather</span><br><span class="line">        out (Tensor, optional) – Destination tensor</span><br><span class="line"></span><br><span class="line">    Example:</span><br><span class="line"></span><br><span class="line">    &gt;&gt;&gt; t = torch.Tensor([[1,2],[3,4]])</span><br><span class="line">    &gt;&gt;&gt; torch.gather(t, 1, torch.LongTensor([[0,0],[1,0]]))</span><br><span class="line">     1  1</span><br><span class="line">     4  3</span><br><span class="line">    [torch.FloatTensor of size 2x2]</span><br></pre></td></tr></tbody></table></figure>
<p><span class="github-emoji" style="display:inline;vertical-align:middle;color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8">⭐</span> 可以看出，gather的作用是这样的，index实际上是索引，具体是行还是列的索引要看前面dim 的指定，比如对于我们的栗子，【1,2,3;4,5,6,】，指定dim=1，也就是横向，那么索引就是列号。index的大小就是输出的大小，所以比如index是【1,0;0,0】，那么看index第一行，1列指的是2， 0列指的是1，同理，第二行为4，4 。这样就输入为【2,1;4,4】，参考这样的解释看上面的输出结果，即可理解gather的含义。</p>
<p>gather在one-hot为输出的多分类问题中，可以把最大值坐标作为index传进去，然后提取到每一行的正确预测结果，这也是gather可能的一个作用。</p>
<blockquote>
<p><a href="https://blog.csdn.net/edogawachia/article/details/80515038" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/edogawachia/article/details/80515038</a></p>
<p><a href="https://www.cnblogs.com/HongjianChen/p/9451526.html" target="_blank" rel="noopener" class="uri">https://www.cnblogs.com/HongjianChen/p/9451526.html</a></p>
</blockquote>
<h3 id="pytorch-损失函数nllloss和crossentropyloss">pytorch 损失函数NLLLoss和CrossEntropyLoss ()</h3>
<blockquote>
<p><a href="https://blog.csdn.net/qq_22210253/article/details/85229988" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/qq_22210253/article/details/85229988</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/98785902" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/98785902</a></p>
</blockquote>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
  </entry>
  <entry>
    <title>2020-11-12-从attention到BERT-family</title>
    <url>/2020/11/12/2020-11-12-%E4%BB%8Eattention%E5%88%B0BERT-family/</url>
    <content><![CDATA[<h3 id="前言">前言</h3>
<p>做NLP领域的应该没人不知道BERT，之前开组会的时候讲到过BERT的一篇改进论文，导师建议对transformer、BERT以及预训练模型发展做个总结综述，作为下次汇报的内容。于是这几天在网上参考了一些博客，还有大佬邱锡鹏老师和刘群老师关于预训练模型的报告，以及结合自己读的一些相关论文，对整个模型的发展演变梳理了一下，汇总如下</p>
<h3 id="attention以及self-attention">attention以及self-attention</h3>
<h4 id="提出">提出</h4>
<p>借鉴了人类的注意力机制</p>
<p><img src="E:\myBlog\source_posts\516P2JBYrgOToyU.jpg" alt="人类的视觉注意力"></p>
<p>人类视觉通过快速扫描全局图像，获得注意力焦点，而后对这一区域投入更多注意力资源，以获取更多所需要关注目标的细节信息，而抑制其他无用信息。这是人类利用有限的注意力资源从大量信息中快速筛选出高价值信息的手段</p>
<p>深度学习中的注意力机制从本质上讲和人类的选择性视觉注意力机制类似，核心目标也是从众多信息中选择出对当前任务目标更关键的信息。</p>
<h4 id="encoder-decoder框架">Encoder-Decoder框架</h4>
<p>一般的seq2seq模型</p>
<p><img src="E:\myBlog\source_posts\ohBVdz1KDn4ZcOb.png" alt="抽象的文本处理领域的Encoder-Decoder框架"></p>
<p>对于句子对&lt;Source,Target&gt;，我们的目标是给定输入句子Source，期待通过Encoder-Decoder框架来生成目标句子Target。Source和Target分别由各自的单词序列构成：</p>
<p><img src="E:\myBlog\source_posts\qrCwtas6iRVKYbA.png" alt="img"></p>
<p>Encoder对输入句子Source进行编码，将输入句子通过非线性变换转化为中间语义表示C：</p>
<p><img src="E:\myBlog\source_posts\M7EXx2KPgeHFQhL.png" alt="img"></p>
<p>对于解码器Decoder来说，其任务是根据句子Source的中间语义表示C和之前已经生成的历史信息 y1,y2,y3...yi-1来生成i时刻要生成的单词yi：</p>
<p><img src="E:\myBlog\source_posts\6uHkShXDNKOlBQ3.png" alt="img"></p>
<p>每个yi都依次这么产生，那么看起来就是整个系统根据输入句子Source生成了目标句子Target。机器翻译、文本摘要、问答系统都是encoder-decoder框架。</p>
<h4 id="问题">问题</h4>
<p><img src="E:\myBlog\source_posts\KNWS6Ax1uI285cH.png" alt="img"></p>
<p>在生成目标句子的单词时，不论生成哪个单词，它们使用的输入句子Source的语义编码C都是一样的，没有任何区别。</p>
<p>而语义编码C是由句子Source的每个单词经过Encoder编码产生的，这意味着不论是生成哪个单词，y1,y2还是y3，其实句子Source中任意单词对生成某个目标单词yi来说影响力都是相同的，这是为何说这个模型没有体现出注意力的缘由。这类似于人类看到眼前的画面，但是眼中却没有注意焦点</p>
<p>没有引入注意力的模型在输入句子比较短的时候问题不大，但是如果输入句子比较长，此时所有语义完全通过一个中间语义向量来表示，单词自身的信息已经消失，可想而知会丢失很多细节信息，这也是为何要引入注意力模型的重要原因。</p>
<h4 id="attention">attention</h4>
<p>目标句子中的每个单词都应该学会其对应的源语句子中单词的注意力分配概率信息。这意味着在生成每个单词yi的时候，原先都是相同的中间语义表示C会被替换成根据当前生成单词而不断变化的Ci（C是动态的，根据生成次词的不同，C也是不同的）。</p>
<p><strong>即由固定的中间语义表示C换成了根据当前输出单词来调整成加入注意力模型的变化的Ci</strong>。</p>
<p><img src="E:\myBlog\source_posts\oe6E7C9sUHrVuNZ.png" alt="引入注意力模型的Encoder-Decoder框架"></p>
<p>即生成目标句子单词的过程成了下面的形式：</p>
<p><img src="E:\myBlog\source_posts\q8ZFEu4BSI1lo9z.png" alt="img"></p>
<p>而每个Ci可能对应着不同的源语句子单词的注意力分配概率分布</p>
<p><img src="E:\myBlog\source_posts\TBg8KZhoyiOlUst.png" alt="img"></p>
<p>实际中，Tom、chase、jerry都是被编码成512维的向量，所以权重相加之后Ctom应该也是一个向量</p>
<p>其中，f2函数代表Encoder对输入英文单词的某种变换函数，比如如果Encoder是用的RNN模型的话，这个f2函数的结果往往是某个时刻输入xi后隐层节点的状态值；g代表Encoder根据单词的中间表示合成整个句子中间语义表示的变换函数，一般的做法中，g函数就是对构成元素加权求和，即下列公式：</p>
<p><img src="E:\myBlog\source_posts\W9SnjkNw3BVasdc.png" alt="img"></p>
<p>其中，Lx代表输入句子Source的长度，aij代表在Target输出第i个单词时Source输入句子中第j个单词的注意力分配系数，而hj则是Source输入句子中第j个单词的语义编码。假设下标i就是上面例子所说的“ 汤姆” ，那么Lx就是3，h1=f(“Tom”)，h2=f(“Chase”),h3=f(“Jerry”)分别是输入句子每个单词的语义编码，对应的注意力模型权值则分别是0.6,0.2,0.2，所以g函数本质上就是个加权求和函数。如果形象表示的话，翻译中文单词“汤姆”的时候，数学公式对应的中间语义表示Ci的形成过程类似图4。</p>
<p><img src="E:\myBlog\source_posts\LRJZMaUbN9VuxGr.png" alt="Attention的形成过程"></p>
<p>如何得到单词注意力分配概率Ｃ呢？</p>
<p>为了便于说明，我们假设对图2的非Attention模型的Encoder-Decoder框架进行细化，Encoder采用RNN模型，Decoder也采用RNN模型，这是比较常见的一种模型配置，则图2的框架转换为图5。</p>
<p><img src="E:\myBlog\source_posts\ebTh6NzIdlVG1L2.png" alt="RNN作为具体模型的Encoder-Decoder框架"></p>
<p>那么用下图可以较为便捷地说明注意力分配概率分布值的通用计算过程。</p>
<p><img src="E:\myBlog\source_posts\TEXoxdzgrHOuiMQ.jpg" alt="注意力分配概率计算"></p>
<p><span class="github-emoji" style="display:inline;vertical-align:middle;color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8">⭐</span> 利用的是i-1时刻的隐状态（作为query）去和h1，h2，h3求相似性</p>
<p>对于采用RNN的Decoder来说，在时刻i，如果要生成yi单词，我们是可以知道Target在生成Yi之前的时刻i-1时，隐层节点i-1时刻的输出值Hi-1的，而我们的目的是要计算<strong>生成Yi时</strong>输入句子中的单词对Yi来说的注意力分配概率分布，那么<strong>可以用Target输出句子i-1时刻的隐层节点状态Hi-1去一一和输入句子Source中每个单词对应的RNN隐层节点状态hj进行对比</strong>，然后函数F的输出经过Softmax进行归一化就得到了符合概率分布取值区间的注意力分配概率分布数值。</p>
<h4 id="attention机制的本质思想">Attention机制的本质思想</h4>
<p><img src="E:\myBlog\source_posts\y3BToK96rsgUZfF.png" alt="Attention机制的本质思想"></p>
<p>我们可以这样来看待Attention机制：将Source中的构成元素想象成是由一系列的&lt;Key,Value&gt;数据对构成，此时给定Target中的某个元素Query，通过计算Query和各个Key的相似性或者相关性，得到每个Key对应Value的权重系数，然后对Value进行加权求和，即得到了最终的Attention数值。</p>
<p>所以本质上Attention机制是对Source中元素的Value值进行加权求和，而Query和Key用来计算对应Value的权重系数。即可以将其本质思想改写为如下公式：</p>
<p><img src="E:\myBlog\source_posts\ipGlzuFcmS8n2VR.png" alt="img"></p>
<p>其中，Lx=||Source||代表Source的长度，公式含义即如上所述。上文所举的机器翻译的例子里，因为在计算Attention的过程中，Source中的Key和Value合二为一，指向的是同一个东西，也即输入句子中每个单词对应的语义编码，所以可能不容易看出这种能够体现本质思想的结构。</p>
<p>当然，从概念上理解，把Attention仍然理解为从大量信息中有选择地筛选出少量重要信息并聚焦到这些重要信息上，忽略大多不重要的信息，这种思路仍然成立。聚焦的过程体现在权重系数的计算上，权重越大越聚焦于其对应的Value值上，即权重代表了信息的重要性，而Value是其对应的信息。</p>
<h4 id="计算过程">计算过程</h4>
<p>至于Attention机制的具体计算过程，如果对目前大多数方法进行抽象的话，可以将其归纳为两个过程：第一个过程是根据Query和Key计算权重系数，第二个过程根据权重系数对Value进行加权求和。而第一个过程又可以细分为两个阶段：第一个阶段根据Query和Key计算两者的相似性或者相关性；第二个阶段对第一阶段的原始分值进行归一化处理；这样，可以将Attention的计算过程抽象为如图10展示的三个阶段。</p>
<p><img src="E:\myBlog\source_posts\tzO9fhXUeBblmVG.png" alt="三阶段计算Attention过程"></p>
<p>在第一个阶段，可以引入不同的函数和计算机制，根据Query和某个Key_i，计算两者的相似性或者相关性，最常见的方法包括：求两者的向量点积、求两者的向量Cosine相似性或者通过再引入额外的神经网络来求值，即如下方式：</p>
<p><img src="E:\myBlog\source_posts\9xpPOa7ohFf3u1d.png" alt="img"></p>
<p>第一阶段产生的分值根据具体产生的方法不同其数值取值范围也不一样，第二阶段引入类似SoftMax的计算方式对第一阶段的得分进行数值转换，一方面可以进行归一化，将原始计算分值整理成所有元素权重之和为1的概率分布；另一方面也可以通过SoftMax的内在机制更加突出重要元素的权重。即一般采用如下公式计算：</p>
<p><img src="E:\myBlog\source_posts\XFW5tcSGjqBnIyN.png" alt="img"></p>
<p>第二阶段的计算结果a_i即为value_i对应的权重系数，然后进行加权求和即可得到Attention数值：</p>
<p><img src="E:\myBlog\source_posts\soa1M9LIPGi3krC.png" alt="img"></p>
<p>通过如上三个阶段的计算，即可求出针对Query的Attention数值，目前绝大多数具体的注意力机制计算方法都符合上述的三阶段抽象计算过程。</p>
<h4 id="self-attention模型">Self Attention模型</h4>
<p>在attention机制中，query是来自外在的张量，而Self Attention的query则是自己本身</p>
<p>但是Self Attention在计算过程中会直接将句子中任意两个单词的联系通过一个计算步骤直接联系起来，所以远距离依赖特征之间的距离被极大缩短，有利于有效地利用这些特征。除此外，Self Attention对于增加计算的并行性也有直接帮助作用。这是为何Self Attention逐渐被广泛使用的主要原因。</p>
<h3 id="transformer">transformer</h3>
<h3 id="transformer的改进模型">transformer的改进模型</h3>
<h3 id="预训练模型">预训练模型</h3>
<h3 id="bert">BERT</h3>
<h3 id="bert的改进模型">BERT的改进模型</h3>
<h3 id="总结">总结</h3>
<hr>
<h4 id="总结-1">总结</h4>
<p>Transformer相比LSTM的优点</p>
<ol type="1">
<li><strong>完全的并行计算</strong>，Transformer的attention和feed-forward，均可以并行计算。而LSTM则依赖上一时刻，必须串行</li>
<li><strong>减少长程依赖</strong>，利用self-attention将每个字之间距离缩短为1，大大缓解了长距离依赖问题</li>
<li><strong>提高网络深度</strong>。由于大大缓解了长程依赖梯度衰减问题，Transformer网络可以很深，基于Transformer的BERT甚至可以做到24层。而LSTM一般只有2层或者4层。网络越深，高阶特征捕获能力越好，模型performance也可以越高。</li>
<li><strong>真正的双向网络</strong>。Transformer可以同时融合前后位置的信息，而双向LSTM只是简单的将两个方向的结果相加，严格来说仍然是单向的。</li>
<li><strong>可解释性强</strong>。完全基于attention的Transformer，可以表达字与字之间的相关关系，可解释性更强。</li>
</ol>
<p>Transformer也不是一定就比LSTM好，它的缺点如下</p>
<ol type="1">
<li>文本长度很长时，比如篇章级别，<strong>计算量爆炸</strong>。self-attention的计算量为O(n^2), n为文本长度。Transformer-xl利用层级方式，将计算速度提升了1800倍</li>
<li>Transformer位置信息只靠<strong>position encoding</strong>，效果比较一般。当语句较短时，比如小于10个字，Transformer效果不一定比LSTM好</li>
<li>Transformer参数量较大，在大规模数据集上，效果远好于LSTM。但在<strong>小规模数据集</strong>上，如果不是利用pretrain models，效果不一定有LSTM好。</li>
</ol>
<h3 id="transformer家族2----编码长度优化transformer-xllongformer">🚀Transformer家族2 -- 编码长度优化（Transformer-XL、Longformer）</h3>
<h4 id="背景">1 背景</h4>
<p>NLP中经常出现长程依赖问题，比如一个词语可能和它距离上千位置的另一个词语有关系。长程关系的建立十分困难。常见序列结构模型都有一些难点，如下。</p>
<ol type="1">
<li>在RNN中，由于反向传播梯度衰减和梯度爆炸问题，使得模型只能捕获较短距离。</li>
<li>LSTM利用门限机制，将连乘转变了为连加，提升了模型长程捕获能力，但梯度弥散问题没有从根本上得到解决，故其最大程度只能在400左右。</li>
<li>Transformer利用self-attention机制进行建模，使得任何两个位置token距离都为1。如果没有内存和算力的限制，Transformer理论上可以编码无限长的文本。但由于attention计算量十分大，而且计算复杂度和序列长度为O(n^2)关系，导致序列长度增加，内存和计算量消耗飞快增加。实际中由于内存和算力有限，一般只能编码一定长度，例如512。</li>
</ol>
<p>为了提升模型的长程编码能力，从而提升模型在长文本，特别是document-level语料上的效果，我们必须对Transformer编码长度进行优化。本文带来了Transformer-XL、Longformer，详细分析他们如何实现编码长度优化。</p>
<p>LongFormer通过降低attention计算所需内存和算力，来实现长文本编码。我们也可以把它归入到算力优化中。但鉴于其名字就重点体现了它的长距离能力，故还是放在了编码长度优化中，和Transformer-XL一起来分析</p>
<h4 id="transformer-xl">2 Transformer-XL</h4>
<p><img src="E:\myBlog\source_posts\ctFMhR8IEdPVSZ1.png" alt="在这里插入图片描述"> 论文信息：2019年01月，谷歌 &amp; CMU，ACL 2019 论文地址 https://arxiv.org/abs/1901.02860 代码和模型地址 https://github.com/kimiyoung/transformer-xl</p>
<h5 id="为什么需要transformer-xl">2.1 为什么需要Transformer-XL</h5>
<p>为了解决长文本编码问题，原版Transformer采用了固定编码长度的方案，例如512个token。将长文本按照固定长度，切分为多个segment。每个segment内部单独编码，segment之间不产生交互信息。这种方式的问题如下</p>
<ol type="1">
<li>模型无法建模超过固定编码长度的文本</li>
<li>segment之间没有交互信息，导致了文本碎片化。长语句的编码效果有待提升。</li>
<li>predict阶段，decoder每生成一个字，就往后挪一个，没有重复利用之前信息，导致计算量爆炸</li>
</ol>
<p>train和evaluate过程如下<img src="E:\myBlog\source_posts\8YW7R4Q5J6zbmD1.png" alt="在这里插入图片描述"></p>
<h5 id="实现方法">2.2 实现方法</h5>
<h6 id="segment-level-recurrence-with-state-reuse-片段级递归和信息复用">2.2.1 Segment-Level Recurrence with State Reuse 片段级递归和信息复用</h6>
<p>Transformer-XL在编码后一个segment时，将前一个segment的隐层缓存下来。后一个segment的self-attention计算，会使用到前一个segment的隐层。后一个segment的第n+1层，对前一个segment的第n层隐层进行融合。故最大编码长度理论上为O(N × L)。在预测阶段，由于对segment隐层使用了缓存，故每预测一个词，不需要重新对之前的文本进行计算。大大提升了预测速度，最大可达到原始Transformer的1800倍。如下图所示 <img src="E:\myBlog\source_posts\KmazXviordxyZuw.png" alt="在这里插入图片描述"></p>
<h6 id="relative-positional-encodings-相对位置编码">2.2.2 Relative Positional Encodings 相对位置编码</h6>
<p>segment递归中有个比较大的问题，就是如何区分不同segment中的相同位置。如果采用原版Transformer中的绝对编码方案，两者是无法区分的。如下 <img src="E:\myBlog\source_posts\38l7XhVjZnqyuBe.png" alt="gh">不同segment中的相同位置，其position encoding会相同。这显然是有问题的。Transformer-XL将绝对位置编码改为了q和k之间的相对位置编码，代表了两个token之间的相对位置。从语义上讲，是make sense的。我们来看看具体实现方式。</p>
<p>绝对位置编码的attention计算如下 <img src="E:\myBlog\source_posts\IKCVE7riwaQ9Zl1.png" alt="在这里插入图片描述">分为四部分</p>
<ol type="1">
<li>query的token encoding和 key的token encoding，之间的关联信息</li>
<li>query的token encoding和 key的position encoding，之间的关联信息。Uj为绝对位置j的编码向量</li>
<li>query的position encoding和 key的token encoding，之间的关联信息。Ui为绝对位置i的编码向量</li>
<li>query的position encoding和 key的position encoding，之间的关联信息</li>
</ol>
<p>而采用相对位置编码后，attention计算如下 <img src="E:\myBlog\source_posts\Id9fV3bxSnmHyu2.png" alt="在这里插入图片描述">同样包含四部分，仍然为二者token encoding和position encoding之间的关联关系。区别在于</p>
<ol type="1">
<li>Ri-j为i和j之间相对位置编码，其中R为相对位置编码矩阵</li>
<li>u和v为query的位置编码，采用一个固定向量。因为采用相对位置编码后，无法对单个绝对位置进行编码了。文中称为global content bias，和global positional bias</li>
</ol>
<p>也有其他文章，采用了不同的相对位置编码方案。比如"Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.2018. Self-attention with relative position representations. arXiv preprint arXiv:1803.02155." 中只有a和b两部分，丢掉了c和d。Transformer-XL对两种方案进行了对比实验，证明前一种好。</p>
<h5 id="实验结果">2.3 实验结果</h5>
<h6 id="长文本编码效果">长文本编码效果</h6>
<p><img src="E:\myBlog\source_posts\IyoR924J3BQWrOu.png" alt="在这里插入图片描述"> 在WikiText-103上的实验结果。WikiText-103包含词级别的超长文本，平均每篇文章长度为3.6K token。利用它可以验证模型的长文本编码能力。实验结果表明Transformer-XL large的PPL最低，效果最好。同时作者在One Billion Word、enwik8、text8上均进行了实验，都表明Transformer-XL效果最好。</p>
<h6 id="有效编码长度">有效编码长度</h6>
<p><img src="E:\myBlog\source_posts\oKQv3akDcXWOZ7B.png" alt="在这里插入图片描述"> 模型可编码的有效长度如上，r为top-r难度样本上的表现。Transformer-XL比RNN长80%，比Transformer长450%。证明Transformer-XL可编码长度最长，长程捕获能力最强。</p>
<h6 id="预测速度">预测速度</h6>
<p><img src="E:\myBlog\source_posts\g2TyZhWtRSEA7pB.png" alt="在这里插入图片描述"> 在不同segment长度下，模型预测速度的对比。和原始Transformer相比，预测速度最大可以提升1874倍。</p>
<h6 id="消融分析">消融分析</h6>
<p>文章对片段级递归和相对位置编码两个Method进行了消融分析，如下 <img src="E:\myBlog\source_posts\4EeY78JcQojSuhD.png" alt="在这里插入图片描述"> 两个改进点均有作用，其中片段级递归作用更大。</p>
<h4 id="longformer">3 Longformer</h4>
<p><img src="E:\myBlog\source_posts\KhcS2VIyvNEAwrg.png" alt="在这里插入图片描述"> 论文信息：2020年04月，allenai 论文地址 https://arxiv.org/abs/2004.05150 代码和模型地址 https://github.com/allenai/longformer</p>
<h5 id="改进方法">3.1 改进方法</h5>
<h6 id="attention稀疏化">3.1.1 attention稀疏化</h6>
<p>Transformer不能捕获长距离信息，本质原因还是因为计算量过大导致的。那我们通过降低attention计算量，是不是就可以提升长距离编码能力呢。答案是肯定的，LongFormer提出了三种attention稀疏化的方法，来降低计算量。 <img src="E:\myBlog\source_posts\hDmpknyqTSIUEW7.png" alt="在这里插入图片描述"> a是原始的全连接方式的attention。后面三种为文章使用的稀疏attention。</p>
<ol type="1">
<li>Sliding Window attention。滑窗方式的attention。假设序列长度为n，滑窗大小w，则每个位置的attention只和滑窗范围内的token进行计算，复杂度从O(n^2)变为了O(n * w)。当w &lt;&lt; n时，可以大大降低计算量。</li>
<li>Dilated Sliding Window attention。受到空洞卷积的启发，提出了空洞滑窗attention。看下面这张图就明白了。</li>
</ol>
<p><img src="E:\myBlog\source_posts\ZwAy1H3kB7SjIP5.png" alt="在这里插入图片描述"></p>
<ol type="1">
<li>Global Attention + sliding window。某些关键位置采用全局attention，这些位置的attention和所有token进行计算。而其他相对不关键的位置则采用滑窗attention。那什么叫关键位置呢？作者举例，分类问题中[CLS]为关键位置，需要计算全局attention。QA中question每个位置均为关键位置，同样计算全局attention。</li>
</ol>
<h6 id="tensor-virtual-machine-tvm">3.1.2 Tensor Virtual Machine (TVM)</h6>
<p>作者使用了TVM构建CUDA kernel，加快了longformer的速度，并降低了显存需求。这个又是另一个模型加速方面的话题，我们就先不展开了。</p>
<h5 id="实验结果-1">3.2 实验结果</h5>
<h6 id="大小模型效果">大小模型效果</h6>
<p><img src="E:\myBlog\source_posts\ZThJDNXByQp2Lod.png" alt="在这里插入图片描述">作者在大小模型上均实验了LongFormer的效果。小模型为12 layers，512 hidden。大模型为30 layers，512 hidden。在text8和enwik8数据集上。小模型达到了SOTA。大模型比18层的Transformer-XL好，虽然不如Adaptive-span-Transformer和Compressive，但胜在可以pretrain-finetune</p>
<h6 id="消融分析-1">消融分析</h6>
<p><img src="E:\myBlog\source_posts\pMCoW2ZLv6gq3j4.png" alt="在这里插入图片描述">消融分析中，可以发现</p>
<ol type="1">
<li>Dilation空洞，有一定的收益</li>
<li>top layer滑窗大小比bottom layer大时，效果好一些。这个也是make sense的。因为top layer捕获高维语义，关联信息距离相对较远，窗口应该尽量大一些。</li>
</ol>
<h6 id="语料长度">语料长度</h6>
<p><img src="E:\myBlog\source_posts\IjGfH7Mi1LK2hsC.png" alt="在这里插入图片描述"> 从上表中我们发现，语料都是特别长的长文本。LongFormer真的是document级别的Transformer。</p>
<h6 id="下游任务finetune效果">下游任务finetune效果</h6>
<p><img src="E:\myBlog\source_posts\JdZUrh6L3QFBHIn.png" alt="在这里插入图片描述"> <img src="E:\myBlog\source_posts\WlOZR3iaJSA2nIU.png" alt="在这里插入图片描述"> 第一个table为RoBERTa和LongFormer在问答、指代消解、分类任务中的对比。第二个table为这几个任务数据集平均文本长度。每项任务都是超出RoBERTa，当文本长度大于512时，performance提升特别明显。更加说明了长程捕获能力在NLP中的重要性。</p>
<h3 id="transformer家族3----计算效率优化adaptive-spanreformerlite-transformer">🚀Transformer家族3 -- 计算效率优化（Adaptive-Span、Reformer、Lite-Transformer）</h3>
<h4 id="背景-1">1 背景</h4>
<p>上文我们从编码长度优化的角度，分析了如何对Transformer进行优化。Transformer-XL、LongFormer等模型，通过片段递归和attention稀疏化等方法，将长文本编码能力提升到了很高的高度。基本已经克服了Transformer长文本捕获能力偏弱的问题，使得下游任务模型performance得到了较大提升，特别是文本较长（大于512）的任务上。</p>
<p>但Transformer计算量和内存消耗过大的问题，还亟待解决。事实上，Transformer-XL、LongFormer已经大大降低了内存和算力消耗。毕竟Transformer之所以长距离编码能力偏弱，就是因为其计算量是序列长度的平方关系，对算力需求过大，导致当前GPU/TPU不能满足需求。编码长度优化和计算量优化，二者是相辅相成的。但着眼于论文的出发点，我们还是分为两个不同的章节进行分析。毕竟总不能所有模型都放在一个章节吧（_）。</p>
<p>本文我们带来Adaptive-Span Transformer、Reformer、Lite-Transformer等几篇文章</p>
<h4 id="adaptive-span-transformer">2 Adaptive-Span Transformer</h4>
<p><img src="E:\myBlog\source_posts\SZIDO7KcoGHqvhF.png" alt="在这里插入图片描述"> 论文信息：2019年5月，FaceBook，ACL2019 论文地址 https://arxiv.org/pdf/1905.07799.pdf 代码和模型地址 https://github.com/facebookresearch/adaptive-span</p>
<h5 id="为什么需要adaptive-span">2.1 为什么需要Adaptive-Span</h5>
<p>之前Transformer-XL将长文本编码能力提升到了较高高度，但是否每个layer的每个head，都需要这么长的attention呢？尽管使用了多种优化手段，长距离attention毕竟还是需要较大的内存和算力。研究发现，大部分head只需要50左右的attention长度，只有少部分head需要较长的attention。这个是make sense的，大部分token只和它附近的token有关联。如下图 <img src="E:\myBlog\source_posts\nOstVdScKG68CDz.png" alt="在这里插入图片描述">我们是否可以实现attention span长度的自适应呢？让不同的layer的不同的head，自己去学习自己的attention span长度呢？Adaptive-Span Transformer给出了肯定答案。</p>
<h5 id="实现方案">2.2 实现方案</h5>
<p>文章设定每个attention head内的token计算，都使用同一个span长度。我们就可以利用attention mask来实现自适应span。对每个head都添加一个attention mask，mask为0的位置不进行attention计算。文章设计的mask函数如下 <img src="E:\myBlog\source_posts\yRVqrjfJPgBKlIk.png" alt="在这里插入图片描述"> R为超参，控制曲线平滑度。其为单调递减函数，如下图。 <img src="E:\myBlog\source_posts\hvoFmDIp1BjgV5f.png" alt="在这里插入图片描述"></p>
<h5 id="实验结果-2">2.3 实验结果</h5>
<p><img src="E:\myBlog\source_posts\EDsBxfipL4HcOwF.png" alt="在这里插入图片描述"> 和Transformer家族其他很多模型一样，Adaptive-span也在字符级别的语言模型上进行了验证，数据集为text8。如上，Transformer注意力长度固定为512，结论如下</p>
<ol type="1">
<li>Transformer-XL长程编码能力确实很强，平均span可达3800。</li>
<li>注意力长度确实不需要总那么长，Adaptive-Span大模型上，平均长度只有245</li>
<li>Adaptive-Span在算力需求很小（只有XL的1/3）的情况下，效果可以达到SOTA。</li>
</ol>
<p><img src="E:\myBlog\source_posts\IDjAR2ucfELsegw.png" alt="在这里插入图片描述">上面是在enwik8上的结果。Adaptive-Span又一次在算力很小的情况下，达到了最优效果。值得注意的是，64层的Transformer居然需要120G的计算量，又一次证明了原版Transformer是多么的吃计算资源。另外Transformer-XL在节省计算资源上，其实也算可圈可点。</p>
<h4 id="reformer">3 Reformer</h4>
<p><img src="E:\myBlog\source_posts\G8AVxR7fuCZdPHn.png" alt="在这里插入图片描述"> 论文信息：2020年1月，谷歌，ICLR2020 论文地址 https://arxiv.org/abs/2001.04451 代码和模型地址 https://github.com/google/trax/tree/master/trax/models/reformer</p>
<h5 id="为什么需要reformer">3.1 为什么需要Reformer</h5>
<p>Transformer内存和计算量消耗大的问题，一直以来广为诟病，并导致其一直不能在长文本上进行应用。（BERT、RoBERTa均设置最大长度为512）。Reformer认为Transformer有三大问题</p>
<ol type="1">
<li>attention层计算量和序列长度为平方关系，导致无法进行长距离编码</li>
<li>内存占用和模型层数呈N倍关系，导致加深Transformer层数，消耗的内存特别大</li>
<li>feed-forward的dff比隐层dmodel一般大很多，导致FF层占用的内存特别大</li>
</ol>
<p>针对这几个问题，Reformer创新性的提出了三点改进方案</p>
<ol type="1">
<li>LOCALITY-SENSITIVE HASHING 局部敏感hash，使得计算量从 O(L^2)降低为O(L log L) ,L为序列长度</li>
<li>Reversible Transformer 可逆Transformer，使得N层layers内存消耗变为只需要一层，从而使得模型加深不会受内存限制。</li>
<li>Feed-forward Chunking 分块全连接，大大降低了feed-forward层的内存消耗。</li>
</ol>
<p>Reformer是Transformer家族中最为关键的几个模型之一（去掉之一貌似都可以，顶多Transformer-XL不答应），其创新新也特别新颖，很多思想值得我们深入思考和借鉴。其效果也是特别明显，大大提高了内存和计算资源效率，编码长度可达64k。下面针对它的三点改进方案进行分析，有点难懂哦。</p>
<h5 id="实现方案-1">3.2 实现方案</h5>
<h6 id="locality-sensitive-hashing-局部敏感hash">3.2.1 LOCALITY-SENSITIVE HASHING 局部敏感hash</h6>
<p>局部敏感hash有点难懂，Reformer针对Transformer结构进行了深度灵魂拷问</p>
<h6 id="query和key必须用两套吗">Query和Key必须用两套吗</h6>
<p>Transformer主体结构为attention，原版attention计算方法如下 <img src="E:\myBlog\source_posts\Z1fYDpO6BnLyHj4.png" alt="在这里插入图片描述"> 每个token，利用其query向量，和其他token的key向量进行点乘，从而代表两个token之间的相关性。归一化后，利用得到的相关性权重，对每个token的value向量进行加权求和。首先一个问题就是，query和key向量可以是同一套吗？我们可否利用key向量去和其他token的key计算相关性呢？</p>
<p>为此文章进行实验分析，证明是可行的。个人认为这一点也是make sense的。 <img src="E:\myBlog\source_posts\YyTZDG1Bohkswit.png" alt="在这里插入图片描述">在文本和图像上，Q=K的attention，和普通attention，效果差别不大。</p>
<h6 id="必须和每个token计算相关性吗">必须和每个token计算相关性吗</h6>
<p>原版attention中，一个token必须和序列中其他所有token计算相关性，导致计算量随序列长度呈平方关系增长，大大制约了可编码最大长度。那必须和每个token计算相关性吗？其实之前Adaptive-Span Transformer也深度拷问过这个话题。它得出的结论是，对于大部分layer的multi-head，长度50范围内进行attention就已经足够了。不过Adaptive-Span采取的方法还是简单粗暴了一点，它约定每个head的attention span长度是固定的，并且attention span为当前token附近的其他token。</p>
<p>Adaptive-Span Transformer的这种方法显然还是没有抓住Attention计算冗余的痛点。Attention本质是加权求和，权重为两个token间的相关性。最终结果取决于较大的topk权重，其他权值较小的基本就是炮灰。并且softmax归一化更是加剧了这一点。小者更小，大者更大。为了减少计算冗余，我们可以只对相关性大的其他token的key向量计算Attention。</p>
<h6 id="怎么找到相关性大的向量呢">怎么找到相关性大的向量呢</h6>
<p>我们现在要从序列中找到与本token相关性最大的token，也就是当前key向量与哪些key向量相关性大。极端例子，如果两个向量完全相同，他们的相关性是最高的。确定两个高维向量的相关性确实比较困难，好在我们可以利用向量Hash来计算。</p>
<p>Reformer采用了局部敏感hash。我们让两个key向量在随机向量上投影，将它们划分到投影区间内。 <img src="E:\myBlog\source_posts\gSHlhWRaoqfk9pN.png" alt="在这里插入图片描述"> 如图所示，划分了四个区间（4个桶bucket），进行了三次Hash。第一次Hash，使得上面两个向量分别归入bucket0和bucket3中，下面两个向量都归入bucket0。第二次Hash，上面两个向量和下面两个，均归入到bucket2中了。我们可以发现</p>
<ol type="1">
<li>相似的向量，也就是相关性大的，容易归入到一个bucket中</li>
<li>局部敏感Hash还是有一定的错误率的，我们可以利用多轮Hash来缓解。这也是Reformer的做法，它采取了4轮和8轮的Hash。</li>
</ol>
<h6 id="整个流程">整个流程</h6>
<p>经过局部敏感Hash后，我们可以将相关性大的key归入同一个bucket中。这样只用在bucket内进行普通Attention即可，大大降低了计算冗余度。为了实现并行计算，考虑到每个bucket包含的向量数目可能不同，实际处理中需要多看一个bucket。整个流程如下 <img src="E:\myBlog\source_posts\ZS5dCT8nVcaiob4.png" alt="在这里插入图片描述"></p>
<ol type="1">
<li>让query等于key</li>
<li>局部敏感Hash（LSH）分桶。上图同一颜色的为同一个桶，共4个桶</li>
<li>桶排序，将相同的桶放在一起</li>
<li>为了实现并行计算，将所有桶分块（chunk），每个chunk大小相同</li>
<li>桶内计算Attention，由于之前做了分块操作，所以需要多看一个块。</li>
</ol>
<h6 id="多轮lsh">多轮LSH</h6>
<p>为了减少分桶错误率，文章采用了多次分桶，计算LSH Attention，Multi-round LSH attention。可以提升整体准确率。如下表。<img src="E:\myBlog\source_posts\L4BDoiwTfaRjInv.png" alt="在这里插入图片描述"></p>
<h6 id="reversible-transformer-可逆transformer">3.2.2 REVERSIBLE TRANSFORMER 可逆Transformer</h6>
<p>LSH局部敏感Hash确实比较难理解，可逆Transformer相对好懂一些。这个方案是为了解决Transformer内存占用量，随layers层数线性增长的问题。为什么会线性增长呢？原因是反向传播中，梯度会从top layer向bottom layer传播，所以必须保存住每一层的Q K V向量，也就导致N层就需要N套Q K V。</p>
<p>那有没有办法不保存每一层的Q K V呢？可逆Transformer正是这个思路。它利用时间换空间的思想，只保留一层的向量，反向传播时，实时计算出之前层的向量。所以叫做Reversible。Reformer每一层分为两部分，x1和x2。输出也两部分，y1和y2。计算如下</p>
<p><img src="E:\myBlog\source_posts\gKZT1mRFhEj48nU.png" alt="image-20201028214552153"></p>
<p>采用可逆残差连接后，模型效果基本没有下降。这也是make sense的，毕竟可逆是从计算角度来解决问题的，对模型本身没有改变。 <img src="E:\myBlog\source_posts\hElWr4uHtUomVYG.png" alt="在这里插入图片描述"></p>
<h6 id="feed-forward-chunking-ff层分块">3.2.3 Feed-Forward chunking FF层分块</h6>
<p>针对fead-forward层内存消耗过大的问题，Reformer也给出了解决方案，就是FF层分块。如下 <img src="E:\myBlog\source_posts\rpzwVXOSUavEs18.png" alt="在这里插入图片描述"></p>
<h5 id="实验结果-3">3.3 实验结果</h5>
<h6 id="内存和时间复杂度">内存和时间复杂度</h6>
<p>Reformer三个创新点，大大降低了内存和时间复杂度，消融分析如下 <img src="E:\myBlog\source_posts\5ixhnpvKeQHrBw1.png" alt="在这里插入图片描述"></p>
<h6 id="模型效果">模型效果</h6>
<p>如下为在机器翻译上的效果。Reformer减少了算力消耗，同时也大大增加了长文本编码能力，故模型效果也得到了提升。如下。 <img src="E:\myBlog\source_posts\8EtHVoBsfYxnmhO.png" alt="在这里插入图片描述"></p>
<h4 id="lite-transformer">4 Lite Transformer</h4>
<p><img src="E:\myBlog\source_posts\9UfkunTVEpL5XBG.png" alt="在这里插入图片描述"> 论文信息：2020年4月，MIT &amp; 上海交大，ICLR2020 论文地址 https://arxiv.org/abs/2004.11886 代码和模型地址 https://github.com/mit-han-lab/lite-transformer</p>
<h5 id="为什么要做lite-transformer">4.1 为什么要做Lite Transformer</h5>
<p>主要出发点仍然是Transformer计算量太大，计算冗余过多的问题。跟Adaptive-Span Transformer和Reformer想法一样，Lite Transformer也觉得没必要做Full Attention，很多Attention连接是冗余的。不一样的是，它通过压缩Attention通道的方式实现，将多头减少了一半。与Base Transformer相比，计算量减少了2.5倍。并且文章使用了量化和剪枝技术，使得模型体积减小了18.2倍。</p>
<h5 id="实现方案-2">4.2 实现方案</h5>
<p>实现方案很简单，仍然采用了原版Transformer的seq2seq结构，创新点为</p>
<ol type="1">
<li>multiHead self-attention变为了两路并行，分别为一半的通道数（多头）。如下图a所示。其中左半部分为正常的fully attention，它用来捕获全局信息。右半部分为CNN卷积，用来捕获布局信息。最终二者通过FFN层融合。这个架构称为LONG-SHORT RANGE ATTENTION (LSRA)，长短期Attention。</li>
<li>为了进一步降低计算量，作者将CNN转变为了一个depth wise卷积和一个线性全连接。dw卷积在mobileNet中有讲过，不清楚可自行谷歌。</li>
</ol>
<p><img src="E:\myBlog\source_posts\WQtJvpbP9rUczhs.png" alt="在这里插入图片描述"></p>
<h5 id="实验结果-4">4.3 实验结果</h5>
<h6 id="计算复杂度">计算复杂度</h6>
<p><img src="E:\myBlog\source_posts\w6b1LlqIpZgt8fX.png" alt="在这里插入图片描述"> 如上图，在文本摘要任务上，Lite Transformer计算量相比Base Transformer，减少了2.5倍。同时Rouge指标基本没变。</p>
<h6 id="模型体积">模型体积</h6>
<p><img src="E:\myBlog\source_posts\JYzeocGBUr6jy4W.png" alt="在这里插入图片描述"> Lite Transformer模型体积只有Transformer的2.5分之一，通过8bit量化和剪枝，最终模型体积下降了18.2倍。</p>
<h4 id="其他">5 其他</h4>
<p>其他几篇文章，也建议拜读下</p>
<ol type="1">
<li><a href="https://arxiv.org/abs/1904.10509" target="_blank" rel="noopener">Generating Long Sequences with Sparse Transformers</a> (OpenAI, 2019.04)</li>
<li><a href="https://arxiv.org/abs/1909.00015" target="_blank" rel="noopener">Adaptively Sparse Transformers</a> (EMNLP2019, 2019.09)</li>
<li><a href="https://arxiv.org/abs/1911.05507" target="_blank" rel="noopener">Compressive Transformers for Long-Range Sequence Modelling</a> (2019.11)</li>
<li><a href="https://arxiv.org/abs/2002.06170" target="_blank" rel="noopener">Transformer on a Diet</a> (2020.02)</li>
</ol>
<h3 id="邱锡鹏教授nlp预训练模型综述">🚀邱锡鹏教授：NLP预训练模型综述</h3>
<h4 id="引言"><strong>1.引言</strong></h4>
<p>随深度学习的发展，多种神经网络都被应用在 NLP 任务中，比如 CNN、RNN、GNN 和 attention 机制等，但由于现有的数据集对于大部分有监督 NLP 任务来说都很小，因此，早期的模型对 NLP 任务来说都很“浅”，往往只包含 1-3 层。</p>
<p>而预训练模型（Pre-trained Models, PTMs）的出现将NLP带入一个新的时代，更“深”的模型和训练技巧的增强也使得 PTMs 由“浅”变“深”，在多项任务都达到了 SOTA 性能。</p>
<p>近日，复旦大学的邱锡鹏老师等人发布了预训练模型综述 *<strong>Pre-trained Models for Natural Language Processing: A Survey*</strong>，从背景、分类到应用与前景对 PTMs 做了详细而全面的调研。</p>
<p><img src="E:\myBlog\source_posts\lLPuxzv8IVRTAQO.png" alt="img"></p>
<p><strong>论文标题：</strong>Pre-trained Models for Natural Language Processing: A Survey</p>
<p><strong>论文链接：</strong> https://arxiv.org/abs/2003.08271</p>
<h4 id="背景-2"><strong>2.背景</strong></h4>
<h5 id="语言表示学习"><strong>2.1 语言表示学习</strong></h5>
<p>对于语言来说，一个好的表示应当描绘语言的内在规则比如词语含义、句法结构、语义角色甚至语用。</p>
<p><img src="E:\myBlog\source_posts\ZmF3yXaiHPME1LQ.png" alt="img"></p>
<p>而分布式表示的核心思想就是通过低维实值向量来描述一段文本的意义，而向量的每一个维度都没有对于意义，整体则代表一个具体的概念。图 1 是 NLP 的通用神经体系架构。</p>
<p>有两种 embedding（词嵌入）方式：上下文嵌入和非上下文嵌入，两者的区别在于词的 embedding 是否根据词出现的上下文动态地改变。</p>
<p><strong>非上下文嵌入：</strong>表示语言的第一步就是将分离的语言符号映射到分布式嵌入空间中。也就是对于词汇表中的每个单词（词根），通过 lookup table 映射到一个向量。</p>
<p>这种嵌入方式有两个局限：一是一个词通过这种方法获得的词嵌入总是静态且与上下文无关的，无法处理多义词；二是难以解决不在词汇表中的词（针对这个问题，很多 NLP 任务提出了字符级或词根级的词表示，如 CharCNN、FastText、Byte-Pair Encoding (BPE)）。</p>
<p><strong>上下文嵌入：</strong>为解决多义性和上下文相关的问题，将词在不同上下文的语义做区分。通过对词（词根）的 token 加一层 Neural Contextual Encoder（神经上下文编码器）得到词的上下文嵌入。</p>
<h3 id="section"></h3>
<h5 id="神经上下文编码器"><strong>2.2 神经上下文编码器</strong></h5>
<p><img src="E:\myBlog\source_posts\DeQYroznLq4cCkt.png" alt="img"></p>
<p>如图 2 中所示，大部分的神经上下文编码器都可以被分为三类：卷积模型、序列模型、基于图的模型。</p>
<p><strong>卷积模型 ：</strong>卷积模型通过卷积操作将输入句子中的 embeddings 与其相邻的局部信息集成。</p>
<p><strong>序列模型 ：</strong>序列模型通常使用 RNN（如 LSTM 和 GRU）来描述词的上下文表示。实践中，双向 RNN 常用于收集词的两边信息，但表现往往会受到长程依赖问题的影响。</p>
<p><strong>基于图的模型 ：</strong>基于图的模型将词视做节点，通过预先定义的语言结构（如句法结构和语义联系）来学习上下文表示。但如何构造一个好的图结构往往严重依赖于专家知识和外部 NLP 工具，如依存分析器。</p>
<p>实际操作中往往直接通过一个全连接图来建模并让模型自己学习结构（一般通过自注意力机制）。一个典型的成功运用就是 Transformer。</p>
<p><strong>分析：</strong>卷积模型和序列模型都很难解决词之间的长程依赖问题，而 Transformer 虽然能更好地描述词之间的深层联系，却往往需要非常大的语料来训练，且容易在中等规模的数据集上过拟合。</p>
<h5 id="为什么要预训练"><strong>2.3 为什么要预训练？</strong></h5>
<p>正如上文提到的，模型参数的数量增长迅速，而为了训练这些参数，就需要更大的数据集来避免过拟合，而大规模的标注数据集成本又非常高。而相比之下，大规模未标注的语料却很容易构建。</p>
<p>为了利用大量的未标注文本数据，我们可以先从其中学习一个好的表示，再将这些表示用在别的任务中。这一通过 PTMs 从未标注大规模数据集中提取表示的预训练过程在很多 NLP 任务中都取得了很好的表现。</p>
<p>预训练的优点可以总结为以下三点：1 在大规模语料上通过预训练学习通用语言表示对下游任务很有帮助；2) 预训练提供了更好的模型初始化参数，使得在目标任务上有更好的泛化性能和更快的收敛速度；3) 预训练是一种有效的正则化方法，能够避免在小数据集上过拟合。</p>
<h4 id="ptms概述"><strong>3.PTMs概述</strong></h4>
<p>PTMs 的主要区别在于上下文编码器的使用、预训练任务和目标。上下文编码器已在 2.2 中做了叙述，接下来对预训练任务进行分析，并提出一种 PTMs 分类方法。</p>
<p><img src="E:\myBlog\source_posts\LhoxNVG5rF3McDK.png" alt="img"></p>
<p>如图 3，这一部分内容作者在文中有一张非常详细的分类图可供参考。</p>
<p>表 1 从多个角度区分了文中提到的一些 PTMs。</p>
<p><img src="E:\myBlog\source_posts\ZI176zJekFciyGr.png" alt="img"></p>
<h5 id="预训练任务"><strong>3.1 预训练任务</strong></h5>
<p>PTMs 按照预训练任务类型可以被分为两类：有监督学习、无监督学习/自监督学习。</p>
<p>有监督学习的预训练任务主要有机器翻译 (MT)，典型的模型是 CoVe。而下文进一步根据实现思路将自监督/无监督任务分为两类，一是基于上下文的 (LM, DAE, PLM)，二是基于对比的 (CTL)。</p>
<h4 id="section-1"></h4>
<h6 id="语言模型-lm"><strong>3.1.1 语言模型 (LM)</strong></h6>
<p>作为 NLP 中最常见的无监督任务，LM 一般指自回归 LM (auto-regressive LM) 或者单向 LM (unidirectional LM)。具体训练过程是基于一个大的语料，通过最大似然估计 (MLE) 训练计算一个句子出现的概率。</p>
<p>然而单向 LM 的缺点则是只能编码一个词左侧的文本和其自身，而更好的上下文应该编码左右两侧的文本。针对这一缺点，解决方案是双向 LM (BiLM)，即一个从左到右和一个从右到左的模型的组合。</p>
<h6 id="去噪声自编码器-denoising-autoencoder-dae"><strong>3.1.2 去噪声自编码器 (Denoising Autoencoder, DAE)</strong></h6>
<blockquote>
<p>这里将原文中 Masked Language Modeling (MLM) 与 DAE 合并为一个部分，因为一般将 BERT 中提出的 MLM 看作是基于 DAE 的思路实现的。</p>
</blockquote>
<p>DAE 的目的是通过向输入文本中添加噪声，利用含噪声的样本去重构不含噪声的输入。主要有五个实现方式：挡住 (MASK) token、删除 token、填充 token、句子排列、文本轮换。</p>
<p>MLM 随机选出一些词用 [MASK] 标记，然后去预测被 MASK 的词。但由于被 MASK 的词并不出现在 fine-tuning 的过程中，会导致预训练和微调的过程出现不一致性。针对这种情况，BERT 通过 80% [MASK]，10% 随机 token,10% 原 token 的方式来进行 mask。</p>
<p>而 MLM 的一种变体，<strong>Seq2SeqMLM</strong>，则是通过将 encoder-decoder (Seq2Seq) 应用到 MLM 上，这种变体有利于 Seq2Seq 类型的下游任务，比如 QA，总结和机器翻译。这一结构主要用在 MASS 和 T5 中。</p>
<p>而在 BERT 之后的很多论文都对 MLM 做了一些改进以增强性能，作者将其总结为 E-MLM (Enhanced Masked Language Modeling)。</p>
<p>其中 RoBERTa 使用动态 masking，UniLM 将对 mask 的预测扩展到三种任务：单向、双向和 Seq2Seq。XLM 通过一种串联并行双语句对叫做 TLM (translation language modeling) 的模型实现 MLM。</p>
<p>而 SpanBERT 和 StructBERT 则是引入了结构化信息。而 ERINE (Baidu) 则是选择 MASK 实体和短语，E-BERT 和 ERINE (THU) 则是利用了实体 embedding 方法，这三者都是借助了外部知识来丰富 MLM。</p>
<h6 id="排列语言模型plm"><strong>3.1.3 排列语言模型（PLM）</strong></h6>
<p>针对 MLM 中使用 MASK 导致的预训练与微调过程的不一致，Permuted Language Modeling (PLM) 对于一个给定序列，生成其所有可能排列进行采样作为训练的目标。值得注意的是，PLM 并不改变原始文本的位置，而是重新定义 token 预测的顺序。</p>
<h6 id="对比学习ctl"><strong>3.1.4 对比学习（CTL）</strong></h6>
<p>CTL (Contrastive Learning) 基于一种“learning by comparison”的思路，假设某些观测文本对比随机采样文本在语义上更相似，通过构建正样本和负样本并度量距离来实现学习。CTL 通常比 LM 具有更少的计算复杂度，也因此成为一个值得选择的 PTMs 训练标准。</p>
<h6 id="deep-infomax-dim"><strong>3.1.5 Deep InfoMax (DIM)</strong></h6>
<p>DIM 最初是在 CV 领域提出的用于最大化图像全局特征与局部特征之间的互信息（Mutual Information）的方法。</p>
<p>InfoWord 将 DIM 引入到语义表达学习中，提出用 DIM objective 以最大化句子的全局表示和一个 N-gram 的具备表示之间的互信息。</p>
<p>噪声对比估计（Noise-Contrastive Estimation，NCE）通过训练一个二元分类器来区分真实样本和假样本，训练词嵌入。NCE 的思想也被用在 word2vec 中。</p>
<h6 id="replaced-token-detection-rtd"><strong>3.1.6 Replaced Token Detection (RTD)</strong></h6>
<p>RTD 和 NCE 大体相同，根据上下文来预测 token 是否替换。</p>
<p>CBOW 的 negetive sampling 就可以看作是一个 RTD 的简单版本，其中采样是根据词汇表中的分布进行采样。</p>
<p>ELECTRA 基于 RTD 提出了一种新的 generator-discriminator 框架。首先用 MLM 任务训练 generator，再用 generator 的权重初始化 discriminator，再用判别任务（判别哪些 token 被 generator 替换过）训练 discriminator。</p>
<p>最终在下游任务只需要对 discriminator 进行 fine-tuning。TRD 也是一种很好的解决 MLM 导致的不一致问题的方法。</p>
<p>WKLM 则是通过在实体层面（entity-level）进行词替换，替换为同一个实体类型的实体名。</p>
<h5 id="section-2"></h5>
<h6 id="next-sentence-prediction-nsp"><strong>3.1.7 Next Sentence Prediction (NSP)</strong></h6>
<p>NSP 训练模型区分两个输入语句是否为训练语料中连续的片段，在选择预训练句对时，第二个句子 50% 是第一个句子实际的连续片段，50% 是语料中的随机段落。NSP 能够教会模型理解两个输入句子之间的联系，从而使得如 QA 和 NLI 这种对此类信息敏感的下游任务受益。</p>
<p>然而，近来 NSP 的必要性也遭到了质疑，XLNet 的作者发现不用 NSP loss 的单句训练优于使用 NSP 的句对训练。RoBERTa 的作者进一步分析表明：在对单个文本中的文本块训练时，去除 NSP 会在下游任务稍微提高性能。</p>
<h6 id="sentence-order-prediction-sop"><strong>3.1.8 Sentence Order Prediction (SOP)</strong></h6>
<p>NSP 结合了主题预测相关性预测，而因为主题预测更容易，模型将更依赖于主题预测。为更好建模句子之间的相关性，ALBERT 提出使用 SOP loss 替换 NSP loss，SOP 使用一个文档中的两个连续片段作为正样本，将这两个片段交换顺序作为负样本。</p>
<p>采用了 SOP 的 ALBERT 在多项下游任务中结果都优于 BERT。StructBERT 和 BERTje 也使用 SOP 作为自监督学习任务。</p>
<h3 id="section-3"></h3>
<h5 id="ptms的拓展"><strong>3.2 PTMs的拓展</strong></h5>
<h6 id="引入知识的ptms"><strong>3.2.1 引入知识的PTMs</strong></h6>
<p>通常 PTMs 都是用大量语料训练通用的语言表示，而将外部的领域知识引入到 PTMs 被证明式有效的。自 BERT 以来，就有很多预训练任务用以将外部知识纳入 PTMs，如：</p>
<p><strong>LIBERT：</strong>linguistically-informed BERT ，通过附加语言约束任务纳入了语言知识。</p>
<p><strong>SentiLR：</strong>通过对每个单词添加情感极性，将 MLM 拓展至 Label-Aware MLM (LA-MLM)，在多个情感分类任务达到 SOTA。</p>
<p><strong>SenseBERT：</strong>不仅能预测被 mask 的 token，还能预测 WordNet 中的 supersense。</p>
<p><strong>ERINE (THU)：</strong>将知识图谱中预训练的实体嵌入与文本中相应的实体提及相结合，以增强文本表示。</p>
<p><strong>KnowBERT：</strong>端到端将带实体连接模型与实体表示集成。</p>
<p><strong>KEPLER：</strong>将知识嵌入和语言模型对象联合。</p>
<p><strong>K-BERT：</strong>不同于以上几个模型通过实体嵌入引入知识图谱中的结构化信息，K-BERT 通过直接将知识图谱中相关三元组引入句子，获得一个 BERT 的拓展的树形输入。</p>
<p><strong>K-Adapter：</strong>针对不同预训练任务独立训练不同的适配器以引入多种知识，以解决上述模型在注入多种知识出现的遗忘问题。</p>
<h6 id="多模态ptms"><strong>3.2.2 多模态PTMs</strong></h6>
<p>随 PTMs 在 NLP 领域的广泛应用，一些多模态 PTMs 也被设计出来，在一些语音、视频、图像数据集上进行了预训练，比如：</p>
<ul>
<li><strong>视频-语言：</strong>VideoBERT、CBT</li>
<li><strong>图像-语言：</strong>用于 visual question answering (VQA) and visual commonsense reasoning (VCR)，如 ViLBERT、LXMERT、VisualBERT、B2T2、VLBERT、 Unicoder-VL、UNITER</li>
<li><strong>音频-文本：</strong>用于端到端 Speech Question Answering (SQA) 任务，如 SpeechBERT</li>
</ul>
<h4 id="section-4"></h4>
<h6 id="领域预训练ptms"><strong>3.2.3 领域预训练PTMs </strong></h6>
<p>大多数 PTMs 都是在 Wikipedia 这样的通用领域语料库上训练的，这就限制了他们在特定领域内的表现。</p>
<p>近期有一些用专业领域语料训练的 PTMs，比如：生物医学领域的 BioBERT，科学领域的 SciBERT，临床医学领域的 ClinicalBERT。还有一些工作尝试将预训练模型更好地使用目标应用，比如生物医学实体归一化、专利分类等。</p>
<h6 id="多语言与特定语言ptms"><strong>3.2.4 多语言与特定语言PTMs </strong></h6>
<p>学习多语言文本表示对于跨语言 NLP 任务是很重要的。早期工作着力于学习来自同一语义环境下的多语言词嵌入，这一方法往往缺乏语言间的校准。近期有如下几个多语言 PTMs：</p>
<p><strong>Multilingual-BERT：</strong>M-BERT，在 Wikipedia 上 104 种种语言的文本上进行 MLM 训练，每个训练样本都是单语言的，也没有专门设计跨语言目标，但即便如此，M-BERT 在跨语言任务上表现还是非常好。</p>
<p><strong>XLM：</strong>通过结合跨语言任务 TLM (translation language modeling)，提升了 M-BERT 的性能。</p>
<p><strong>Unicoder：</strong>提出三个跨语言预训练任务：1) cross-lingual word recovery; 2) cross-lingual paraphrase classification; 3) cross-lingual masked language model。</p>
<p>除此之外还有一些单语言的 PTMs：BERT-wwm，ZEN，NEZHA，ERNIE (Baidu)，BERTje，CamemBERT， FlauBERT ，RobBERT 。</p>
<h5 id="如何压缩ptms"><strong>3.3 如何压缩PTMs</strong></h5>
<p>预训练模型往往包含至少几千万个参数，这也使得模型难以部署到生活中的线上服务以及资源有限的设备上，这就使得模型压缩成为一条可能能够压缩模型尺寸并提高计算效率的方法。表 2 展示了一些压缩的 PTMs 的对比。</p>
<p><img src="E:\myBlog\source_posts\6cBzpUQ3J7e1Xd2.png" alt="img"></p>
<p>压缩 PTMs 一般有四个方法：</p>
<ul>
<li><strong>剪枝（pruning）：</strong>去除不那么重要的参数（e.g. 权重、层数、通道数、attention heads）</li>
<li><strong>量化（weight quantization）：</strong>使用占位更少（低精度）的参数</li>
<li><strong>参数共享（parameter sharing）：</strong>相似模型单元间共享参数</li>
<li><strong>知识蒸馏（knowledge diistillation）：</strong>用一些优化目标从大型 teacher 模型学习一个小的 student 模型，一些利用知识蒸馏的 PTMs 见表 3。</li>
</ul>
<p><img src="E:\myBlog\source_posts\5tdJqBHve3XYuCo.png" alt="img"></p>
<h4 id="如何将ptms应用至下游任务"><strong>4.如何将PTMs应用至下游任务</strong></h4>
<h5 id="迁移学习"><strong>4.1 迁移学习</strong></h5>
<p>迁移学习就是将源任务中的知识适应到目标任务，将 PTMs 适应到下游任务是一种顺序迁移学习任务。那么，如何迁移呢？我们需要考虑以下几个问题：</p>
<ul>
<li><strong>选择合适的预训练任务</strong>：近期，LM 是最流行的预训练任务，也有效解决了很多 NLP 问题。但不同的预训练任务在不同的下游任务上有不同的效果，比如 NSP 任务能帮助 PTM 理解句子之间的关系，因此 PTM 对于 QA 和 NLI 这样的下游任务很有帮助。</li>
<li><strong>选择合适的模型架构</strong>：比如 BERT 使用的 MLM 和 Transformer 结构使其擅长 NLU 任务，却很难生成语言。</li>
<li><strong>选择合适的语料</strong>：下游任务的数据应该接近 PTMs 的预训练任务。</li>
<li><strong>选择合适的layers</strong>：在“深”的预训练模型中，不同的 layer 往往描绘不同种类的信息。有三种选择 layers 的方式：1) 只用 Embedding，如 word2vec 和 Glove；2) Top Layer，如 BERT；3) All Layers，如 ELMo。</li>
<li><strong>是否进行fine-tune</strong>：模型迁移一般有两种方法：特征提取和 fine-tuning。特征提取的参数是冻结的，且往往需要特定任务的体系结构。fine-tunig 的参数是非冻结的，比特征提取方法更为通用且方便。</li>
</ul>
<h5 id="fine-tuning的策略"><strong>4.2 fine-tuning的策略</strong></h5>
<p>自 ULMFit 和 BERT 起，fine-tuning 已经成为 PTMs 主要的适配方法。这里有一些实用的 fine-tunig 策略：</p>
<ul>
<li>两阶段 fine-tuning：两阶段迁移的方法在预训练和 fine-tuning 阶段引入了一个中间阶段。在第一阶段，通过中间任务或语料来微调模型。在第二阶段，通过目标任务微调模型。</li>
<li>多任务 fine-tuning：liu等人在多任务学习框架下对 BERT 进行了微调，结果显示多任务学习和预训练是互补的方法。</li>
<li>采用额外的适配器 fine-tuning：fine-tuning 的主要缺点是参数效率低，在每一个下游任务上都有各自的 dine-tuning 参数。对此的解决方案是在固定原始参数时引入一些可以 fine-tuning 的适配器。</li>
<li>其他：逐层解冻而非连续 fine-tune 所有层；self-ensemble 和 self-distillation</li>
</ul>
<h4 id="一些ptms的资源"><strong>5.一些PTMs的资源</strong></h4>
<h5 id="一些开源的应用"><strong>一些开源的应用：</strong></h5>
<p><img src="E:\myBlog\source_posts\jTUZBNqcrlm9hR2.png" alt="img"></p>
<p><strong>word2vec:</strong></p>
<p>https://github.com/tmikolov/word2vec</p>
<p><strong>GloVe:</strong></p>
<p>https://nlp.stanford.edu/projects/glove</p>
<p><strong>FastText:</strong></p>
<p>https://github.com/facebookresearch/fastText</p>
<p><strong>Transformers:</strong></p>
<p>https://github.com/huggingface/transformers</p>
<p><strong>Fairseq:</strong></p>
<p>https://github.com/pytorch/fairseq</p>
<p><strong>Flair:</strong></p>
<p>https://github.com/flairNLP/flair</p>
<p><strong>AllenNLP:</strong></p>
<p>https://github.com/allenai/allennlp</p>
<p><strong>FastNLP:</strong></p>
<p>https://github.com/fastnlp/fastNLP</p>
<p><strong>Chinese-BERT:</strong></p>
<p>https://github.com/ymcui/Chinese-BERT-wwm</p>
<p><strong>BERT:</strong></p>
<p>https://github.com/google-research/bert</p>
<p><strong>RoBERTa:</strong></p>
<p>https://github.com/pytorch/fairseq/tree/master/examples/roberta</p>
<p><strong>XLNet:</strong></p>
<p>https://github.com/zihangdai/xlnet/</p>
<p><strong>ALBERT:</strong></p>
<p>https://github.com/google-research/ALBERT</p>
<p><strong>T5:</strong></p>
<p>https://github.com/google-research/text-to-text-transfer-transformer</p>
<p><strong>ERNIE (Baidu):</strong></p>
<p>https://github.com/PaddlePaddle/ERNIE</p>
<p><strong>相关资源：</strong></p>
<p><strong>论文列表：</strong></p>
<p>https://github.com/thunlp/PLMpapers</p>
<p>https://github.com/tomohideshibata/BERT-related-papers</p>
<p>https://github.com/cedrickchee/awesome-bert-nlp</p>
<p><strong>BERT Lang Street（收集 BERT 在不同数据集和任务上的表现）：</strong></p>
<p>https://bertlang.unibocconi.it/</p>
<p><strong>BERTViz（应用 transformer 的模型的注意力可视化）：</strong></p>
<p>https://github.com/jessevig/bertviz</p>
<h4 id="应用"><strong>6.应用</strong></h4>
<h5 id="通用评估标准"><strong>6.1 通用评估标准</strong></h5>
<p>GLUE (The General Language Understanding Evaluation) 标准是一个集合了 9 个自然语言理解任务的标准。</p>
<p>其中包括：单个句子分类任务（CoLA和SST-2）、文本对分类任务（MNLI, RTE, WNLI, QQP, MRPC）、文本相似度任务（STSB）、相关性排行任务（QNLI）。GLUE 标准能够能够很好地评估模型的鲁棒性和通用性。</p>
<p>而近期 NLP 的快速发展促使了新的标准 SuperGLUE 的提出，相比 GLUE，SuperGLUE 有更多富有挑战性且多种多样的任务，如指代消解和 QA。</p>
<h3 id="section-5"></h3>
<h5 id="机器翻译"><strong>6.2 机器翻译</strong></h5>
<p>机器翻译（Machine Translation, MT）也是 NLP 的一项重要任务。几乎所有 MT 模型都使用了 encoder-decoder 框架。而近期随预训练模型的发展，也有不少尝试将 BERT 之类的预训练模型用于初始化 encoder，取得了一定成效。</p>
<h3 id="section-6"></h3>
<h5 id="问答系统"><strong>6.3 问答系统</strong></h5>
<p>问答系统（Question answering, QA）或是狭义概念的机器阅读理解（machine reading comprehension, MRC）也是 NLP 的重要任务。</p>
<p>从易到难，有三种类型的 QA 任务：单回合提取 QA (single-round extractive QA, SQuAD)、多回合生成QA (multi-round generative QA, CoQA)、多跳问答 (multi-hop QA, HotpotQA)。</p>
<p>针对提取 QA，有通过 PTM 初始化 encoder 的回溯阅读架构（retrospective reader architecture）；针对多回合生成 QA，有“PTM+Adversarial Training+Rationale Tagging+Knowledge Distillation”架构；针对多跳 QA，有“Select, Answer, and Explain” (SAE) 系统。</p>
<h5 id="情感分析"><strong>6.4 情感分析</strong></h5>
<p>BERT 通过在广泛使用的情感分析数据集 SST-2 上进行微调后，表现超过了先前的 SOTA 模型。而后又有很多将 BERT 进行调整以应用在 aspect 级的情感分析（ABSA）任务上。</p>
<h5 id="总结-2"><strong>6.5 总结</strong></h5>
<p>从长文本中总结出短文本也是近期 NLP 的热点。也有很多尝试将 PTM 应用在总结文本任务上，如将 BERT 通过插入 [CLS] token 来学习句子表示的模型 BERTSUM。</p>
<h5 id="命名实体识别"><strong>6.6 命名实体识别</strong></h5>
<p>命名实体识别（Named Entity Recognition, NER）也是知识提取的一个基础任务，在很多 NLP 任务上都有重要作用。TagLM 和 ELMo 利用预训练语言模型的最后一层的输入和各层的加权总和作为词嵌入的一部分。</p>
<h4 id="未来方向"><strong>7.未来方向</strong></h4>
<h5 id="ptms的上界"><strong>7.1 PTMs的上界</strong></h5>
<p>随 BERT 的出现，我们可以发现，很多模型都可以通过更长的训练步长不在和更大的语料来提升性能，比如去年的 T5 使用的 C4 数据集。而我们也可以通过加深模型来提升性能，比如 Turing-NLG 使用了 72 个 transformer 层。</p>
<p>PTMs 的共同目标都是学习语言的本质通用知识(或者说是世界的知识)，然而，随着模型的不断加深，语料的不断增大，训练模型的花销也越来越大。一种更可行的解决方案是设计更有效的模型架构、自监督预训练任务、优化器和软硬件方面的技巧等。ELECTRA 就是这个方向上一个很好的尝试。</p>
<h5 id="面向任务的预训练与模型压缩"><strong>7.2 面向任务的预训练与模型压缩</strong></h5>
<p>在实践中，不同的下游任务要求 PTMs 拥有不同的功能。而 PTMs 与下游目标任务间的差异通常表现在两方面：模型架构与数据分布。较大的 PTMs 通常情况下会有更好的性能，但实际问题是如何在低容量设备和低时延应用上使用如此庞大的 PTM。</p>
<p>除此之外，我们可以通过模型压缩来将通用 PTMs 教给面向对象的 PTM。尽管 CV 中对 CNNs 的压缩已经非常成熟，但 Tansformer 的全连接结构使得模型压缩非常具有挑战性。</p>
<h5 id="ptms架构"><strong>7.3 PTMs架构</strong></h5>
<p>Transformer 是 PTMs 的一个高效的框架，但 Transformer 的局限在于计算复杂度。由于 GPU 显存大小的限制，目前大多数 PTM 无法处理序列长度超过 512 个 token 的序列。搭配这一限制需要改进 Transformer 的结构，如 Transformer-XL。因此，寻求更有效的模型架构对于解决长程文本信息也是很重要的。</p>
<h5 id="fine-tunig中的知识迁移"><strong>7.4 Fine-tunig中的知识迁移 </strong></h5>
<p>Fine-tuning 是目前将 PTM 的知识迁移至下游任务的主要方法，但参数效率却很低，每个下游任务都有特定的 fine-tuned 参数。</p>
<p>一个可以改进的解决方案是固定 PTMs 的原始参数，并为特定任务添加小型的可微调的适配器，这样就可以在不同的下游任务使用共享的 PTMs。从 PTM‘s 中挖掘知识也可以更灵活，比如：知识提取、知识蒸馏、数据增加、将 PTMs 作为外部知识等等。</p>
<h5 id="ptms的可解释性与可靠性"><strong>7.5 PTMs的可解释性与可靠性 </strong></h5>
<p>PTMs 的深且非线性的架构使得决策制定的过程非常不透明。近期，可解释人工智能（explainable artificial intelligence, XAI）成为热点。通过对模型词嵌入的研究我们可以分析 PTMs 中的语言和世界知识，但更多有关注意力机制的可解释性的问题还值得探讨。</p>
<p>PTMs 这种深模型很容易受到对抗样本的扰动而产生错误的预测。在 CV 领域，对抗攻击与防御已经被广泛学习，而由于语言的特性，文本的对抗还非常具有挑战性。PTMs 的对抗防御也对于提升 PTMs 的鲁棒性很重要。</p>
<h4 id="总结-3"><strong>8.总结</strong></h4>
<p>邱锡鹏老师的这篇综述很全面地概括了预训练模型，也非常适合初学者当作一个 roadmap 来阅读。我们可以看到 NLP 的发展过程是非常令人感动的，从最开始的“要表示语言”的目标，使用词袋模型和 N-gram。</p>
<p>再想到“词语具有多义性”，所以需要有上下文，使用 LSTM。LSTM 只有单向，那就使用双向 LSTM。“想要更大范围的上下文”，就产生了 transformer。</p>
<p>“再大一些”，有了 transformer-XL。还是不够好，怎么办？“更多知识”，于是不断加大语料库，不断堆 GPU，直到 T5 探索了“Limits of Transfer Learning with a Uniﬁed Text-to-Text Transformer”。</p>
<p>模型太大，成本太高，那就压缩模型，改进框架，于是有了 ELECTRA。预训练模型缺乏尝试推理能力，那就知识提取，于是有了 COMET。每一步尝试都是在靠近语言的本质与世界的知识。</p>
<p><em>“The whole of science is nothing more than a refinement of everyday thinking.”</em></p>
<p>Bert最近很火，应该是最近最火爆的AI进展，网上的评价很高，那么Bert值得这么高的评价吗？我个人判断是值得。那为什么会有这么高的评价呢？是因为它有重大的理论或者模型创新吗？其实并没有，从模型创新角度看一般，创新不算大。但是架不住效果太好了，基本刷新了很多NLP的任务的最好性能，有些任务还被刷爆了，这个才是关键。另外一点是Bert具备广泛的通用性，就是说绝大部分NLP任务都可以采用类似的两阶段模式直接去提升效果，这个第二关键。客观的说，把Bert当做最近两年NLP重大进展的集大成者更符合事实。</p>
<p>本文的主题是自然语言处理中的预训练过程，会大致说下NLP中的预训练技术是一步一步如何发展到Bert模型的，从中可以很自然地看到Bert的思路是如何逐渐形成的，Bert的历史沿革是什么，继承了什么，创新了什么，为什么效果那么好，主要原因是什么，以及为何说模型创新不算太大，为何说Bert是近年来NLP重大进展的集大成者。我们一步一步来讲，而串起来这个故事的脉络就是自然语言的预训练过程，但是落脚点还是在Bert身上。要讲自然语言的预训练，得先从图像领域的预训练说起。</p>
<h3 id="图像领域的预训练">图像领域的预训练</h3>
<p>自从深度学习火起来后，预训练过程就是做图像或者视频领域的一种比较常规的做法，有比较长的历史了，而且这种做法很有效，能明显促进应用的效果。</p>
<p><img src="E:\myBlog\source_posts\HSZgfVhv5cAO2Qt.jpg" alt="img"></p>
<p>那么图像领域怎么做预训练呢，上图展示了这个过程，我们设计好网络结构以后，对于图像来说一般是CNN的多层叠加网络结构，可以先用某个训练集合比如训练集合A或者训练集合B对这个网络进行预先训练，在A任务上或者B任务上学会网络参数，然后存起来以备后用。假设我们面临第三个任务C，网络结构采取相同的网络结构，在比较浅的几层CNN结构，网络参数初始化的时候可以加载A任务或者B任务学习好的参数，其它CNN高层参数仍然随机初始化。之后我们用C任务的训练数据来训练网络，此时有两种做法，一种是浅层加载的参数在训练C任务过程中不动，这种方法被称为“Frozen”;另外一种是底层网络参数尽管被初始化了，在C任务训练过程中仍然随着训练的进程不断改变，这种一般叫“Fine-Tuning”，顾名思义，就是更好地把参数进行调整使得更适应当前的C任务。一般图像或者视频领域要做预训练一般都这么做。</p>
<p>这么做有几个好处，首先，如果手头任务C的训练集合数据量较少的话，现阶段的好用的CNN比如Resnet/Densenet/Inception等网络结构层数很深，几百万上千万参数量算起步价，上亿参数的也很常见，训练数据少很难很好地训练这么复杂的网络，但是如果其中大量参数通过大的训练集合比如ImageNet预先训练好直接拿来初始化大部分网络结构参数，然后再用C任务手头比较可怜的数据量上Fine-tuning过程去调整参数让它们更适合解决C任务，那事情就好办多了。这样原先训练不了的任务就能解决了，即使手头任务训练数据也不少，加个预训练过程也能极大加快任务训练的收敛速度，所以这种预训练方式是老少皆宜的解决方案，另外疗效又好，所以在做图像处理领域很快就流行开来。</p>
<p>那么新的问题来了，为什么这种预训练的思路是可行的？</p>
<p><img src="E:\myBlog\source_posts\hnsrCeotdP4jLSq.jpg" alt="img"></p>
<p>目前我们已经知道，对于层级的CNN结构来说，不同层级的神经元学习到了不同类型的图像特征，由底向上特征形成层级结构，如上图所示，如果我们手头是个人脸识别任务，训练好网络后，把每层神经元学习到的特征可视化肉眼看一看每层学到了啥特征，你会看到最底层的神经元学到的是线段等特征，图示的第二个隐层学到的是人脸五官的轮廓，第三层学到的是人脸的轮廓，通过三步形成了特征的层级结构，越是底层的特征越是所有不论什么领域的图像都会具备的比如边角线弧线等底层基础特征，越往上抽取出的特征越与手头任务相关。正因为此，所以预训练好的网络参数，尤其是底层的网络参数抽取出特征跟具体任务越无关，越具备任务的通用性，所以这是为何一般用底层预训练好的参数初始化新任务网络参数的原因。而高层特征跟任务关联较大，实际可以不用使用，或者采用Fine-tuning用新数据集合清洗掉高层无关的特征抽取器。</p>
<p><img src="E:\myBlog\source_posts\RrSMsuboYdOzvKl.jpg" alt="img"></p>
<p>一般我们喜欢用ImageNet来做网络的预训练，主要有两点，一方面ImageNet是图像领域里有超多事先标注好训练数据的数据集合，分量足是个很大的优势，量越大训练出的参数越靠谱；另外一方面因为ImageNet有1000类，类别多，算是通用的图像数据，跟领域没太大关系，所以通用性好，预训练完后哪哪都能用，是个万金油。分量足的万金油当然老少通吃，人人喜爱。</p>
<p>听完上述话，如果你是具备研究素质的人，也就是说具备好奇心，你一定会问下面这个问题：”既然图像领域预训练这么好用，那干嘛自然语言处理不做这个事情呢？是不是搞NLP的人比搞CV的傻啊？就算你傻，你看见人家这么做，有样学样不就行了吗？这不就是创新吗，也许能成，万一成了，你看，你的成功来得就是这么突然!”</p>
<p>嗯，好问题，其实搞NLP的人一点都不比你傻，早就有人尝试过了，不过总体而言不太成功而已。听说过word embedding吗？2003年出品，陈年技术，馥郁芳香。word embedding其实就是NLP里的早期预训练技术。当然也不能说word embedding不成功，一般加到下游任务里，都能有1到2个点的性能提升，只是没有那么耀眼的成功而已。</p>
<p>没听过？那下面就把这段陈年老账讲给你听听。</p>
<h3 id="word-embedding考古史">Word Embedding考古史</h3>
<p>这块大致讲讲Word Embedding的故事，很粗略，因为网上关于这个技术讲的文章太多了，汗牛冲动，我不属牛，此刻更没有流汗，所以其实丝毫没有想讲Word Embedding的冲动和激情，但是要说预训练又得从这开始，那就粗略地讲讲，主要是引出后面更精彩的部分。在说Word Embedding之前，先更粗略地说下语言模型，因为一般NLP里面做预训练一般的选择是用语言模型任务来做。</p>
<p><img src="E:\myBlog\source_posts\4aFBLDCEQmgXp9Z.jpg" alt="img"></p>
<p>什么是语言模型？其实看上面这张PPT上扣下来的图就明白了，为了能够量化地衡量哪个句子更像一句人话，可以设计如上图所示函数，核心函数P的思想是根据句子里面前面的一系列前导单词预测后面跟哪个单词的概率大小（理论上除了上文之外，也可以引入单词的下文联合起来预测单词出现概率）。句子里面每个单词都有个根据上文预测自己的过程，把所有这些单词的产生概率乘起来，数值越大代表这越像一句人话。语言模型压下暂且不表，我隐约预感到我这么讲你可能还是不太会明白，但是大概这个意思，不懂的可以去网上找，资料多得一样地汗牛冲动。</p>
<p>假设现在让你设计一个神经网络结构，去做这个语言模型的任务，就是说给你很多语料做这个事情，训练好一个神经网络，训练好之后，以后输入一句话的前面几个单词，要求这个网络输出后面紧跟的单词应该是哪个，你会怎么做？</p>
<p><img src="E:\myBlog\source_posts\SUI2jF7xEsaz9Z4.jpg" alt="img"></p>
<p>你可以像上图这么设计这个网络结构，这其实就是大名鼎鼎的中文人称“神经网络语言模型”，英文小名NNLM的网络结构，用来做语言模型。这个工作有年头了，是个陈年老工作，是Bengio 在2003年发表在JMLR上的论文。它生于2003，火于2013，以后是否会不朽暂且不知，但是不幸的是出生后应该没有引起太大反响，沉寂十年终于时来运转沉冤得雪，在2013年又被NLP考古工作者从海底湿淋淋地捞出来了祭入神殿。为什么会发生这种技术奇遇记？你要想想2013年是什么年头，是深度学习开始渗透NLP领域的光辉时刻，万里长征第一步，而NNLM可以算是南昌起义第一枪。在深度学习火起来之前，极少有人用神经网络做NLP问题，如果你10年前坚持用神经网络做NLP，估计别人会认为你这人神经有问题。所谓红尘滚滚，谁也挡不住历史发展趋势的车轮，这就是个很好的例子。</p>
<p>上面是闲话，闲言碎语不要讲，我们回来讲一讲NNLM的思路。先说训练过程，现在看其实很简单，见过RNN、LSTM、CNN后的你们回头再看这个网络甚至显得有些简陋。学习任务是输入某个句中单词Wt=“Bert”前面句子的t-1个单词，要求网络正确预测单词Bert，即最大化：</p>
<p><img src="https://i.loli.net/2020/10/30/k8XpuUwT47WOo5Q.png" alt="image-20201030202319906" style="zoom:50%;"></p>
<p>前面任意单词Wi用Onehot编码（比如：0001000）作为原始单词输入，之后乘以矩阵Q后获得向量C（Wi），每个单词的 C（Wi）拼接，上接隐层，然后接softmax去预测后面应该后续接哪个单词。这个 C（Wi）是什么？这其实就是单词对应的Word Embedding值，那个矩阵Q包含V行，V代表词典大小，每一行内容代表对应单词的Word embedding值。只不过Q的内容也是网络参数，需要学习获得，训练刚开始用随机值初始化矩阵Q，当这个网络训练好之后，矩阵Q的内容被正确赋值，每一行代表一个单词对应的Word embedding值。所以你看，通过这个网络学习语言模型任务，这个网络不仅自己能够根据上文预测后接单词是什么，同时获得一个副产品，就是那个矩阵Q，这就是单词的Word Embedding是被如何学会的。</p>
<p>2013年最火的用语言模型做Word Embedding的工具是Word2Vec，后来又出了Glove，Word2Vec是怎么工作的呢？看下图。</p>
<p><img src="E:\myBlog\source_posts\bXq5RvhS3niaTxB.jpg" alt="img"></p>
<p>Word2Vec的网络结构其实和NNLM是基本类似的，只是这个图长得清晰度差了点，看上去不像，其实它们是亲兄弟。不过这里需要指出：尽管网络结构相近，而且也是做语言模型任务，但是其训练方法不太一样。Word2Vec有两种训练方法，一种叫CBOW，核心思想是从一个句子里面把一个词抠掉，用这个词的上文和下文去预测被抠掉的这个词；第二种叫做Skip-gram，和CBOW正好反过来，输入某个单词，要求网络预测它的上下文单词。而你回头看看，NNLM是怎么训练的？是输入一个单词的上文，去预测这个单词。这是有显著差异的。为什么Word2Vec这么处理？原因很简单，因为Word2Vec和NNLM不一样，NNLM的主要任务是要学习一个解决语言模型任务的网络结构，语言模型就是要看到上文预测下文，而word embedding只是无心插柳的一个副产品。但是Word2Vec目标不一样，它单纯就是要word embedding的，这是主产品，所以它完全可以随性地这么去训练网络。</p>
<p>为什么要讲Word2Vec呢？这里主要是要引出CBOW的训练方法，BERT其实跟它有关系，后面会讲它们之间是如何的关系，当然它们的关系BERT作者没说，是我猜的，至于我猜的对不对，后面你看后自己判断。</p>
<p><img src="E:\myBlog\source_posts\aM9lpsNOS7vrmnq.jpg" alt="img"></p>
<p>使用Word2Vec或者Glove，通过做语言模型任务，就可以获得每个单词的Word Embedding，那么这种方法的效果如何呢？上图给了网上找的几个例子，可以看出有些例子效果还是很不错的，一个单词表达成Word Embedding后，很容易找出语义相近的其它词汇。</p>
<p>我们的主题是预训练，那么问题是Word Embedding这种做法能算是预训练吗？这其实就是标准的预训练过程。要理解这一点要看看学会Word Embedding后下游任务是怎么用它的。</p>
<p><img src="E:\myBlog\source_posts\YEf95lnIW28OGRa.jpg" alt="img"></p>
<p>假设如上图所示，我们有个NLP的下游任务，比如QA，就是问答问题，所谓问答问题，指的是给定一个问题X，给定另外一个句子Y,要判断句子Y是否是问题X的正确答案。问答问题假设设计的网络结构如上图所示，这里不展开讲了，懂得自然懂，不懂的也没关系，因为这点对于本文主旨来说不关键，关键是网络如何使用训练好的Word Embedding的。它的使用方法其实和前面讲的NNLM是一样的，句子中每个单词以Onehot形式作为输入，然后乘以学好的Word Embedding矩阵Q，就直接取出单词对应的Word Embedding了。这乍看上去好像是个查表操作，不像是预训练的做法是吧？其实不然，那个Word Embedding矩阵Q其实就是网络Onehot层到embedding层映射的网络参数矩阵。所以你看到了，使用Word Embedding等价于什么？等价于把Onehot层到embedding层的网络用预训练好的参数矩阵Q初始化了。这跟前面讲的图像领域的低层预训练过程其实是一样的，区别无非Word Embedding只能初始化第一层网络参数，再高层的参数就无能为力了。下游NLP任务在使用Word Embedding的时候也类似图像有两种做法，一种是Frozen，就是Word Embedding那层网络参数固定不动；另外一种是Fine-Tuning，就是Word Embedding这层参数使用新的训练集合训练也需要跟着训练过程更新掉。</p>
<p>上面这种做法就是18年之前NLP领域里面采用预训练的典型做法，之前说过，Word Embedding其实对于很多下游NLP任务是有帮助的，只是帮助没有大到闪瞎忘记戴墨镜的围观群众的双眼而已。那么新问题来了，为什么这样训练及使用Word Embedding的效果没有期待中那么好呢？答案很简单，因为Word Embedding有问题呗。这貌似是个比较弱智的答案，关键是Word Embedding存在什么问题？这其实是个好问题。</p>
<p><img src="E:\myBlog\source_posts\U3YwNJ1RmydDukM.jpg" alt="img"></p>
<p>这片在Word Embedding头上笼罩了好几年的乌云是什么？是多义词问题。我们知道，多义词是自然语言中经常出现的现象，也是语言灵活性和高效性的一种体现。多义词对Word Embedding来说有什么负面影响？如上图所示，比如多义词Bank，有两个常用含义，但是Word Embedding在对bank这个单词进行编码的时候，是区分不开这两个含义的，因为它们尽管上下文环境中出现的单词不同，但是在用语言模型训练的时候，不论什么上下文的句子经过word2vec，都是预测相同的单词bank，而同一个单词占的是同一行的参数空间，这导致两种不同的上下文信息都会编码到相同的word embedding空间里去。所以word embedding无法区分多义词的不同语义，这就是它的一个比较严重的问题。</p>
<p>你可能觉得自己很聪明，说这可以解决啊，确实也有很多研究人员提出很多方法试图解决这个问题，但是从今天往回看，这些方法看上去都成本太高或者太繁琐了，有没有简单优美的解决方案呢？</p>
<p>ELMO提供了一种简洁优雅的解决方案。</p>
<h3 id="从word-embedding到elmo">从Word Embedding到ELMO</h3>
<p>ELMO是“Embedding from Language Models”的简称，其实这个名字并没有反应它的本质思想，提出ELMO的论文题目：“Deep contextualized word representation”更能体现其精髓，而精髓在哪里？在deep contextualized这个短语，一个是deep，一个是context，其中context更关键。在此之前的Word Embedding本质上是个静态的方式，所谓静态指的是训练好之后每个单词的表达就固定住了，以后使用的时候，不论新句子上下文单词是什么，这个单词的Word Embedding不会跟着上下文场景的变化而改变，所以对于比如Bank这个词，它事先学好的Word Embedding中混合了几种语义 ，在应用中来了个新句子，即使从上下文中（比如句子包含money等词）明显可以看出它代表的是“银行”的含义，但是对应的Word Embedding内容也不会变，它还是混合了多种语义。这是为何说它是静态的，这也是问题所在。ELMO的本质思想是：我事先用语言模型学好一个单词的Word Embedding，此时多义词无法区分，不过这没关系。在我实际使用Word Embedding的时候，单词已经具备了特定的上下文了，这个时候我可以根据上下文单词的语义去调整单词的Word Embedding表示，这样经过调整后的Word Embedding更能表达在这个上下文中的具体含义，自然也就解决了多义词的问题了。所以ELMO本身是个根据当前上下文对Word Embedding动态调整的思路。</p>
<p><img src="E:\myBlog\source_posts\m6NvFoRGhWbji83.jpg" alt="img"></p>
<p>ELMO采用了典型的两阶段过程，第一个阶段是利用语言模型进行预训练；第二个阶段是在做下游任务时，从预训练网络中提取对应单词的网络各层的Word Embedding作为新特征补充到下游任务中。上图展示的是其预训练过程，它的网络结构采用了双层双向LSTM，目前语言模型训练的任务目标是根据单词 Wi 的上下文去正确预测单词 Wi ， Wi 之前的单词序列Context-before称为上文，之后的单词序列Context-after称为下文。图中左端的前向双层LSTM代表正方向编码器，输入的是从左到右顺序的除了预测单词外 Wi 的上文Context-before；右端的逆向双层LSTM代表反方向编码器，输入的是从右到左的逆序的句子下文Context-after；每个编码器的深度都是两层LSTM叠加。这个网络结构其实在NLP中是很常用的。使用这个网络结构利用大量语料做语言模型任务就能预先训练好这个网络，如果训练好这个网络后，输入一个新句子 Snew ，句子中每个单词都能得到对应的三个Embedding:最底层是单词的Word Embedding，往上走是第一层双向LSTM中对应单词位置的Embedding，这层编码单词的句法信息更多一些；再往上走是第二层LSTM中对应单词位置的Embedding，这层编码单词的语义信息更多一些。也就是说，ELMO的预训练过程不仅仅学会单词的Word Embedding，还学会了一个双层双向的LSTM网络结构，而这两者后面都有用。</p>
<p><img src="E:\myBlog\source_posts\ymSXKwFh8WBcEd5.jpg" alt="img"></p>
<p>上面介绍的是ELMO的第一阶段：预训练阶段。那么预训练好网络结构后，如何给下游任务使用呢？上图展示了下游任务的使用过程，比如我们的下游任务仍然是QA问题，此时对于问句X，我们可以先将句子X作为预训练好的ELMO网络的输入，这样句子X中每个单词在ELMO网络中都能获得对应的三个Embedding，之后给予这三个Embedding中的每一个Embedding一个权重a，这个权重可以学习得来，根据各自权重累加求和，将三个Embedding整合成一个。然后将整合后的这个Embedding作为X句在自己任务的那个网络结构中对应单词的输入，以此作为补充的新特征给下游任务使用。对于上图所示下游任务QA中的回答句子Y来说也是如此处理。因为ELMO给下游提供的是每个单词的特征形式，所以这一类预训练的方法被称为“Feature-based Pre-Training”。至于为何这么做能够达到区分多义词的效果，你可以想一想，其实比较容易想明白原因。</p>
<p><img src="E:\myBlog\source_posts\b8ToQxv5BPgELI1.jpg" alt="img"></p>
<p>上面这个图是TagLM采用类似ELMO的思路做命名实体识别任务的过程，其步骤基本如上述ELMO的思路，所以此处不展开说了。TagLM的论文发表在2017年的ACL会议上，作者就是AllenAI里做ELMO的那些人，所以可以将TagLM看做ELMO的一个前导工作。前几天这个PPT发出去后有人质疑说FastAI的在18年4月提出的ULMFiT才是抛弃传统Word Embedding引入新模式的开山之作，我深不以为然。首先TagLM出现的更早而且模式基本就是ELMO的思路；另外ULMFiT使用的是三阶段模式，在通用语言模型训练之后，加入了一个领域语言模型预训练过程，而且论文重点工作在这块，方法还相对比较繁杂，这并不是一个特别好的主意，因为领域语言模型的限制是它的规模往往不可能特别大，精力放在这里不太合适，放在通用语言模型上感觉更合理；再者，尽管ULFMiT实验做了6个任务，但是都集中在分类问题相对比较窄，不如ELMO验证的问题领域广，我觉得这就是因为第二步那个领域语言模型带来的限制。所以综合看，尽管ULFMiT也是个不错的工作，但是重要性跟ELMO比至少还是要差一档，当然这是我个人看法。每个人的学术审美口味不同，我个人一直比较赞赏要么简洁有效体现问题本质要么思想特别游离现有框架脑洞开得异常大的工作，所以ULFMiT我看论文的时候就感觉看着有点难受，觉得这工作没抓住重点而且特别麻烦，但是看ELMO论文感觉就赏心悦目，觉得思路特别清晰顺畅，看完暗暗点赞，心里说这样的文章获得NAACL2018最佳论文当之无愧，比ACL很多最佳论文也好得不是一点半点，这就是好工作带给一个有经验人士的一种在读论文时候就能产生的本能的感觉，也就是所谓的这道菜对上了食客的审美口味。</p>
<p><img src="E:\myBlog\source_posts\6y7VvCDm9NRpHJx.jpg" alt="img"></p>
<p>前面我们提到静态Word Embedding无法解决多义词的问题，那么ELMO引入上下文动态调整单词的embedding后多义词问题解决了吗？解决了，而且比我们期待的解决得还要好。上图给了个例子，对于Glove训练出的Word Embedding来说，多义词比如play，根据它的embedding找出的最接近的其它单词大多数集中在体育领域，这很明显是因为训练数据中包含play的句子中体育领域的数量明显占优导致；而使用ELMO，根据上下文动态调整后的embedding不仅能够找出对应的“演出”的相同语义的句子，而且还可以保证找出的句子中的play对应的词性也是相同的，这是超出期待之处。之所以会这样，是因为我们上面提到过，第一层LSTM编码了很多句法信息，这在这里起到了重要作用。</p>
<p><img src="E:\myBlog\source_posts\YFAVkuIxemlaHPN.jpg" alt="img"></p>
<p>ELMO经过这般操作，效果如何呢？实验效果见上图，6个NLP任务中性能都有幅度不同的提升，最高的提升达到25%左右，而且这6个任务的覆盖范围比较广，包含句子语义关系判断，分类任务，阅读理解等多个领域，这说明其适用范围是非常广的，普适性强，这是一个非常好的优点。</p>
<p><img src="E:\myBlog\source_posts\LpMSFe5kX7Qxroh.jpg" alt="img"></p>
<p>那么站在现在这个时间节点看，ELMO有什么值得改进的缺点呢？首先，一个非常明显的缺点在特征抽取器选择方面，ELMO使用了LSTM而不是新贵Transformer，Transformer是谷歌在17年做机器翻译任务的“Attention is all you need”的论文中提出的，引起了相当大的反响，很多研究已经证明了Transformer提取特征的能力是要远强于LSTM的。如果ELMO采取Transformer作为特征提取器，那么估计Bert的反响远不如现在的这种火爆场面。另外一点，ELMO采取双向拼接这种融合特征的能力可能比Bert一体化的融合特征方式弱，但是，这只是一种从道理推断产生的怀疑，目前并没有具体实验说明这一点。</p>
<p>我们如果把ELMO这种预训练方法和图像领域的预训练方法对比，发现两者模式看上去还是有很大差异的。除了以ELMO为代表的这种基于特征融合的预训练方法外，NLP里还有一种典型做法，这种做法和图像领域的方式就是看上去一致的了，一般将这种方法称为“基于Fine-tuning的模式”，而GPT就是这一模式的典型开创者。</p>
<h3 id="从word-embedding到gpt">从Word Embedding到GPT</h3>
<p><img src="E:\myBlog\source_posts\IDl2hH8j3JVdx6F.jpg" alt="img"></p>
<p>GPT是“Generative Pre-Training”的简称，从名字看其含义是指的生成式的预训练。GPT也采用两阶段过程，第一个阶段是利用语言模型进行预训练，第二阶段通过Fine-tuning的模式解决下游任务。上图展示了GPT的预训练过程，其实和ELMO是类似的，主要不同在于两点：首先，特征抽取器不是用的RNN，而是用的Transformer，上面提到过它的特征抽取能力要强于RNN，这个选择很明显是很明智的；其次，GPT的预训练虽然仍然是以语言模型作为目标任务，但是采用的是单向的语言模型，所谓“单向”的含义是指：语言模型训练的任务目标是根据 Wi 单词的上下文去正确预测单词 Wi ， Wi 之前的单词序列Context-before称为上文，之后的单词序列Context-after称为下文。ELMO在做语言模型预训练的时候，预测单词 Wi 同时使用了上文和下文，而GPT则只采用Context-before这个单词的上文来进行预测，而抛开了下文。这个选择现在看不是个太好的选择，原因很简单，它没有把单词的下文融合进来，这限制了其在更多应用场景的效果，比如阅读理解这种任务，在做任务的时候是可以允许同时看到上文和下文一起做决策的。如果预训练时候不把单词的下文嵌入到Word Embedding中，是很吃亏的，白白丢掉了很多信息。</p>
<p>这里强行插入一段简单提下Transformer，尽管上面提到了，但是说的还不完整，补充两句。首先，Transformer是个叠加的“自注意力机制（Self Attention）”构成的深度网络，是目前NLP里最强的特征提取器，注意力这个机制在此被发扬光大，从任务的配角不断抢戏，直到Transformer一跃成为踢开RNN和CNN传统特征提取器，荣升头牌，大红大紫。注意力机制可以参考“<a href="https://zhuanlan.zhihu.com/p/37601161" target="_blank" rel="noopener">深度学习中的注意力模型</a>”，补充下相关基础知识，如果不了解注意力机制你肯定会落后时代的发展。而介绍Transformer比较好的文章可以参考以下两篇文章：一个是Jay Alammar可视化地介绍Transformer的博客文章<a href="https://link.zhihu.com/?target=https%3A//jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a> ，非常容易理解整个机制，建议先从这篇看起；然后可以参考哈佛大学NLP研究组写的“<a href="https://link.zhihu.com/?target=http%3A//nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer.</a> ”，代码原理双管齐下，讲得非常清楚。我相信上面两个文章足以让你了解Transformer了，所以这里不展开介绍。</p>
<p>其次，我的判断是Transformer在未来会逐渐替代掉RNN成为主流的NLP工具，RNN一直受困于其并行计算能力，这是因为它本身结构的序列性依赖导致的，尽管很多人在试图通过修正RNN结构来修正这一点，但是我不看好这种模式，因为给马车换轮胎不如把它升级到汽车，这个道理很好懂，更何况目前汽车的雏形已经出现了，干嘛还要执着在换轮胎这个事情呢？是吧？再说CNN，CNN在NLP里一直没有形成主流，CNN的最大优点是易于做并行计算，所以速度快，但是在捕获NLP的序列关系尤其是长距离特征方面天然有缺陷，不是做不到而是做不好，目前也有很多改进模型，但是特别成功的不多。综合各方面情况，很明显Transformer同时具备并行性好，又适合捕获长距离特征，没有理由不在赛跑比赛中跑不过RNN和CNN。</p>
<p>好了，题外话结束，我们再回到主题，接着说GPT。上面讲的是GPT如何进行第一阶段的预训练，那么假设预训练好了网络模型，后面下游任务怎么用？它有自己的个性，和ELMO的方式大有不同。</p>
<p><img src="E:\myBlog\source_posts\3Gr9vqoPHkSfcAg.jpg" alt="img"></p>
<p>上图展示了GPT在第二阶段如何使用。首先，对于不同的下游任务来说，本来你可以任意设计自己的网络结构，现在不行了，你要向GPT的网络结构看齐，把任务的网络结构改造成和GPT的网络结构是一样的。然后，在做下游任务的时候，利用第一步预训练好的参数初始化GPT的网络结构，这样通过预训练学到的语言学知识就被引入到你手头的任务里来了，这是个非常好的事情。再次，你可以用手头的任务去训练这个网络，对网络参数进行Fine-tuning，使得这个网络更适合解决手头的问题。就是这样。看到了么？这有没有让你想起最开始提到的图像领域如何做预训练的过程（请参考上图那句非常容易暴露年龄的歌词）？对，这跟那个模式是一模一样的。</p>
<p>这里引入了一个新问题：对于NLP各种花样的不同任务，怎么改造才能靠近GPT的网络结构呢？</p>
<p><img src="E:\myBlog\source_posts\iJqb8TYLwCvdSVk.jpg" alt="img"></p>
<p>GPT论文给了一个改造施工图如上，其实也很简单：对于分类问题，不用怎么动，加上一个起始和终结符号即可；对于句子关系判断问题，比如Entailment，两个句子中间再加个分隔符即可；对文本相似性判断问题，把两个句子顺序颠倒下做出两个输入即可，这是为了告诉模型句子顺序不重要；对于多项选择问题，则多路输入，每一路把文章和答案选项拼接作为输入即可。从上图可看出，这种改造还是很方便的，不同任务只需要在输入部分施工即可。</p>
<p><img src="E:\myBlog\source_posts\qnLcVGo5IK6riYh.jpg" alt="img"></p>
<p>GPT的效果是非常令人惊艳的，在12个任务里，9个达到了最好的效果，有些任务性能提升非常明显。</p>
<p><img src="E:\myBlog\source_posts\96zdAXvcOmJTuP2.jpg" alt="img"></p>
<p>那么站在现在的时间节点看，GPT有什么值得改进的地方呢？其实最主要的就是那个单向语言模型，如果改造成双向的语言模型任务估计也没有Bert太多事了。当然，即使如此GPT也是非常非常好的一个工作，跟Bert比，其作者炒作能力亟待提升。</p>
<h3 id="bert的诞生">Bert的诞生</h3>
<p><img src="E:\myBlog\source_posts\rSJAqOMB4sathDg.jpg" alt="img"></p>
<p>我们经过跋山涉水，终于到了目的地Bert模型了。</p>
<p>Bert采用和GPT完全相同的两阶段模型，首先是语言模型预训练；其次是使用Fine-Tuning模式解决下游任务。和GPT的最主要不同在于在预训练阶段采用了类似ELMO的双向语言模型，当然另外一点是语言模型的数据规模要比GPT大。所以这里Bert的预训练过程不必多讲了。</p>
<p><img src="E:\myBlog\source_posts\61JpWKSZ5fF3tNk.jpg" alt="img"></p>
<p>第二阶段，Fine-Tuning阶段，这个阶段的做法和GPT是一样的。当然，它也面临着下游任务网络结构改造的问题，在改造任务方面Bert和GPT有些不同，下面简单介绍一下。</p>
<p><img src="E:\myBlog\source_posts\UTQdhtVA7PzcIlF.jpg" alt="img"></p>
<p>在介绍Bert如何改造下游任务之前，先大致说下NLP的几类问题，说这个是为了强调Bert的普适性有多强。通常而言，绝大部分NLP问题可以归入上图所示的四类任务中：一类是序列标注，这是最典型的NLP任务，比如中文分词，词性标注，命名实体识别，语义角色标注等都可以归入这一类问题，它的特点是句子中每个单词要求模型根据上下文都要给出一个分类类别。第二类是分类任务，比如我们常见的文本分类，情感计算等都可以归入这一类。它的特点是不管文章有多长，总体给出一个分类类别即可。第三类任务是句子关系判断，比如Entailment，QA，语义改写，自然语言推理等任务都是这个模式，它的特点是给定两个句子，模型判断出两个句子是否具备某种语义关系；第四类是生成式任务，比如机器翻译，文本摘要，写诗造句，看图说话等都属于这一类。它的特点是输入文本内容后，需要自主生成另外一段文字。</p>
<p><img src="E:\myBlog\source_posts\mxJybVWl2OatfUc.jpg" alt="img"></p>
<p>对于种类如此繁多而且各具特点的下游NLP任务，Bert如何改造输入输出部分使得大部分NLP任务都可以使用Bert预训练好的模型参数呢？上图给出示例，对于句子关系类任务，很简单，和GPT类似，加上一个起始和终结符号，句子之间加个分隔符即可。对于输出来说，把第一个起始符号对应的Transformer最后一层位置上面串接一个softmax分类层即可。对于分类问题，与GPT一样，只需要增加起始和终结符号，输出部分和句子关系判断任务类似改造；对于序列标注问题，输入部分和单句分类是一样的，只需要输出部分Transformer最后一层每个单词对应位置都进行分类即可。从这里可以看出，上面列出的NLP四大任务里面，除了生成类任务外，Bert其它都覆盖到了，而且改造起来很简单直观。尽管Bert论文没有提，但是稍微动动脑子就可以想到，其实对于机器翻译或者文本摘要，聊天机器人这种生成式任务，同样可以稍作改造即可引入Bert的预训练成果。只需要附着在S2S结构上，encoder部分是个深度Transformer结构，decoder部分也是个深度Transformer结构。根据任务选择不同的预训练数据初始化encoder和decoder即可。这是相当直观的一种改造方法。当然，也可以更简单一点，比如直接在单个Transformer结构上加装隐层产生输出也是可以的。不论如何，从这里可以看出，NLP四大类任务都可以比较方便地改造成Bert能够接受的方式。这其实是Bert的非常大的优点，这意味着它几乎可以做任何NLP的下游任务，具备普适性，这是很强的。</p>
<p><img src="E:\myBlog\source_posts\e7tSMGZjDHmY1Ck.jpg" alt="img"></p>
<p>Bert采用这种两阶段方式解决各种NLP任务效果如何？在11个各种类型的NLP任务中达到目前最好的效果，某些任务性能有极大的提升。一个新模型好不好，效果才是王道。</p>
<p><img src="E:\myBlog\source_posts\RniS8uQhpDmcHN6.jpg" alt="img"></p>
<p>到这里我们可以再梳理下几个模型之间的演进关系。从上图可见，Bert其实和ELMO及GPT存在千丝万缕的关系，比如如果我们把GPT预训练阶段换成双向语言模型，那么就得到了Bert；而如果我们把ELMO的特征抽取器换成Transformer，那么我们也会得到Bert。所以你可以看出：Bert最关键两点，一点是特征抽取器采用Transformer；第二点是预训练的时候采用双向语言模型。</p>
<p>那么新问题来了：对于Transformer来说，怎么才能在这个结构上做双向语言模型任务呢？乍一看上去好像不太好搞。我觉得吧，其实有一种很直观的思路，怎么办？看看ELMO的网络结构图，只需要把两个LSTM替换成两个Transformer，一个负责正向，一个负责反向特征提取，其实应该就可以。当然这是我自己的改造，Bert没这么做。那么Bert是怎么做的呢？我们前面不是提过Word2Vec吗？我前面肯定不是漫无目的地提到它，提它是为了在这里引出那个CBOW训练方法，所谓写作时候埋伏笔的“草蛇灰线，伏脉千里”，大概就是这个意思吧？前面提到了CBOW方法，它的核心思想是：在做语言模型任务的时候，我把要预测的单词抠掉，然后根据它的上文Context-Before和下文Context-after去预测单词。其实Bert怎么做的？Bert就是这么做的。从这里可以看到方法间的继承关系。当然Bert作者没提Word2Vec及CBOW方法，这是我的判断，Bert作者说是受到完形填空任务的启发，这也很可能，但是我觉得他们要是没想到过CBOW估计是不太可能的。</p>
<p>从这里可以看出，在文章开始我说过Bert在模型方面其实没有太大创新，更像一个最近几年NLP重要技术的集大成者，原因在于此，当然我不确定你怎么看，是否认同这种看法，而且我也不关心你怎么看。其实Bert本身的效果好和普适性强才是最大的亮点。</p>
<p><img src="E:\myBlog\source_posts\LO9j7cIxEJCe2Ay.jpg" alt="img"></p>
<p>那么Bert本身在模型和方法角度有什么创新呢？就是论文中指出的Masked 语言模型和Next Sentence Prediction。而Masked语言模型上面讲了，本质思想其实是CBOW，但是细节方面有改进。</p>
<p><img src="E:\myBlog\source_posts\75DVNACdHgtRbS9.jpg" alt="img"></p>
<p>Masked双向语言模型向上图展示这么做：随机选择语料中15%的单词，把它抠掉，也就是用[Mask]掩码代替原始单词，然后要求模型去正确预测被抠掉的单词。但是这里有个问题：训练过程大量看到[mask]标记，但是真正后面用的时候是不会有这个标记的，这会引导模型认为输出是针对[mask]这个标记的，但是实际使用又见不到这个标记，这自然会有问题。为了避免这个问题，Bert改造了一下，15%的被上天选中要执行[mask]替身这项光荣任务的单词中，只有80%真正被替换成[mask]标记，10%被狸猫换太子随机替换成另外一个单词，10%情况这个单词还待在原地不做改动。这就是Masked双向语音模型的具体做法。</p>
<p><img src="E:\myBlog\source_posts\MTCajrZPKuF51Ne.jpg" alt="img"></p>
<p>至于说“Next Sentence Prediction”，指的是做语言模型预训练的时候，分两种情况选择两个句子，一种是选择语料中真正顺序相连的两个句子；另外一种是第二个句子从语料库中抛色子，随机选择一个拼到第一个句子后面。我们要求模型除了做上述的Masked语言模型任务外，附带再做个句子关系预测，判断第二个句子是不是真的是第一个句子的后续句子。之所以这么做，是考虑到很多NLP任务是句子关系判断任务，单词预测粒度的训练到不了句子关系这个层级，增加这个任务有助于下游句子关系判断任务。所以可以看到，它的预训练是个多任务过程。这也是Bert的一个创新。</p>
<p><img src="E:\myBlog\source_posts\XQ17TqJcYpPoA3i.jpg" alt="img"></p>
<p>上面这个图给出了一个我们此前利用微博数据和开源的Bert做预训练时随机抽出的一个中文训练实例，从中可以体会下上面讲的masked语言模型和下句预测任务。训练数据就长这种样子。</p>
<p><img src="E:\myBlog\source_posts\E1vcQhzTsgbLqkF.jpg" alt="img"></p>
<p>顺带讲解下Bert的输入部分，也算是有些特色。它的输入部分是个线性序列，两个句子通过分隔符分割，最前面和最后增加两个标识符号。每个单词有三个embedding:位置信息embedding，这是因为NLP中单词顺序是很重要的特征，需要在这里对位置信息进行编码；单词embedding,这个就是我们之前一直提到的单词embedding；第三个是句子embedding，因为前面提到训练数据都是由两个句子构成的，那么每个句子有个句子整体的embedding项对应给每个单词。把单词对应的三个embedding叠加，就形成了Bert的输入。</p>
<p><img src="E:\myBlog\source_posts\wHfCcyhaiXPMx1d.jpg" alt="img"></p>
<p>至于Bert在预训练的输出部分如何组织，可以参考上图的注释。</p>
<p><img src="E:\myBlog\source_posts\B5wItXbYpPr619Z.jpg" alt="img"></p>
<p>我们说过Bert效果特别好，那么到底是什么因素起作用呢？如上图所示，对比试验可以证明，跟GPT相比，双向语言模型起到了最主要的作用，对于那些需要看到下文的任务来说尤其如此。而预测下个句子来说对整体性能来说影响不算太大，跟具体任务关联度比较高。</p>
<p><img src="E:\myBlog\source_posts\u9mKAEGjp4fa7ys.jpg" alt="img"></p>
<p>最后，我讲讲我对Bert的评价和看法，我觉得Bert是NLP里里程碑式的工作，对于后面NLP的研究和工业应用会产生长久的影响，这点毫无疑问。但是从上文介绍也可以看出，从模型或者方法角度看，Bert借鉴了ELMO，GPT及CBOW，主要提出了Masked 语言模型及Next Sentence Prediction，但是这里Next Sentence Prediction基本不影响大局，而Masked LM明显借鉴了CBOW的思想。所以说Bert的模型没什么大的创新，更像最近几年NLP重要进展的集大成者，这点如果你看懂了上文估计也没有太大异议，如果你有大的异议，杠精这个大帽子我随时准备戴给你。如果归纳一下这些进展就是：首先是两阶段模型，第一阶段双向语言模型预训练，这里注意要用双向而不是单向，第二阶段采用具体任务Fine-tuning或者做特征集成；第二是特征抽取要用Transformer作为特征提取器而不是RNN或者CNN；第三，双向语言模型可以采取CBOW的方法去做（当然我觉得这个是个细节问题，不算太关键，前两个因素比较关键）。Bert最大的亮点在于效果好及普适性强，几乎所有NLP任务都可以套用Bert这种两阶段解决思路，而且效果应该会有明显提升。可以预见的是，未来一段时间在NLP应用领域，Transformer将占据主导地位，而且这种两阶段预训练方法也会主导各种应用。</p>
<p>另外，我们应该弄清楚预训练这个过程本质上是在做什么事情，本质上预训练是通过设计好一个网络结构来做语言模型任务，然后把大量甚至是无穷尽的无标注的自然语言文本利用起来，预训练任务把大量语言学知识抽取出来编码到网络结构中，当手头任务带有标注信息的数据有限时，这些先验的语言学特征当然会对手头任务有极大的特征补充作用，因为当数据有限的时候，很多语言学现象是覆盖不到的，泛化能力就弱，集成尽量通用的语言学知识自然会加强模型的泛化能力。如何引入先验的语言学知识其实一直是NLP尤其是深度学习场景下的NLP的主要目标之一，不过一直没有太好的解决办法，而ELMO/GPT/Bert的这种两阶段模式看起来无疑是解决这个问题自然又简洁的方法，这也是这些方法的主要价值所在。</p>
<p>对于当前NLP的发展方向，我个人觉得有两点非常重要，一个是需要更强的特征抽取器，目前看Transformer会逐渐担当大任，但是肯定还是不够强的，需要发展更强的特征抽取器；第二个就是如何优雅地引入大量无监督数据中包含的语言学知识，注意我这里强调地是优雅，而不是引入，此前相当多的工作试图做各种语言学知识的嫁接或者引入，但是很多方法看着让人牙疼，就是我说的不优雅。目前看预训练这种两阶段方法还是很有效的，也非常简洁，当然后面肯定还会有更好的模型出现。</p>
<p>完了，这就是自然语言模型预训练的发展史。</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
  </entry>
  <entry>
    <title>2020-11-12-论文分享</title>
    <url>/2020/11/11/2020-11-12-%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/</url>
    <content><![CDATA[<blockquote>
<p>IJCAI 2019</p>
<p>code url (official torch) : <a href="https://github.com/sodawater/T-CVAE" target="_blank" rel="noopener" class="uri">https://github.com/sodawater/T-CVAE</a></p>
<p>被引用次数：12</p>
</blockquote>
<h3 id="背景">背景</h3>
<h3 id="问题">问题</h3>
<h3 id="解决">解决</h3>
<h3 id="贡献">贡献</h3>
<h3 id="模型">模型</h3>
<h3 id="实验">实验</h3>
<h3 id="总结">总结</h3>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>论文分享</category>
      </categories>
  </entry>
  <entry>
    <title>2020-11-11-论文分享</title>
    <url>/2020/11/11/2020-11-11-%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/</url>
    <content><![CDATA[<p><img src="E:\myBlog\source_posts\image-20201111153229465.png" alt="image-20201111153229465"></p>
<blockquote>
<p>ICLR 2020</p>
<p>code url (official tf) : <a href="https://github.com/google-research/ALBERT" target="_blank" rel="noopener" class="uri">https://github.com/google-research/ALBERT</a></p>
<p>code url (unofficial torch): <a href="https://github.com/huggingface/transformers" target="_blank" rel="noopener" class="uri">https://github.com/huggingface/transformers</a></p>
<p>cited ：646</p>
</blockquote>
<h3 id="背景">背景</h3>
<h3 id="问题">问题</h3>
<h3 id="解决">解决</h3>
<h3 id="贡献">贡献</h3>
<h3 id="模型">模型</h3>
<h3 id="实验">实验</h3>
<h3 id="总结">总结</h3>
<hr>
<h4 id="三roberta">三、RoBERTa</h4>
<h3 id="动机">3.1 动机</h3>
<ul>
<li>Bert 序列模型的问题</li>
</ul>
<blockquote>
<p>确定方法的哪些方面贡献最大可能是具有挑战性的;</p>
<p>训练在计算上是昂贵的的，限制了可能完成的调整量</p>
</blockquote>
<h3 id="工作">3.2 工作</h3>
<ul>
<li>更大的模型参数量（论文提供的训练时间来看，模型使用 1024 块 V100 GPU 训练了 1 天的时间）</li>
<li>更大bacth size。RoBERTa 在训练过程中使用了更大的bacth size。尝试过从 256 到 8000 不等的bacth size。</li>
<li>更多的训练数据（包括：CC-NEWS 等在内的 160GB 纯文本。而最初的BERT使用16GB BookCorpus数据集和英语维基百科进行训练） 另外，RoBERTa在训练方法上有以下改进：</li>
</ul>
<h3 id="预训练">3.3 预训练</h3>
<h4 id="动态掩码">3.3.1 动态掩码</h4>
<ul>
<li>动机：NSP 作用不大。 去掉下一句预测(NSP)任务</li>
<li>动态掩码。BERT 依赖随机掩码和预测 token。原版的 BERT 实现在数据预处理期间执行一次掩码，得到一个静态掩码。 而 RoBERTa 使用了动态掩码：每次向模型输入一个序列时都会生成新的掩码模式。这样，在大量数据不断输入的过程中，模型会逐渐适应不同的掩码策略，学习不同的语言表征。</li>
<li>文本编码。Byte-Pair Encoding（BPE）是字符级和词级别表征的混合，支持处理自然语言语料库中的众多常见词汇。原版的 BERT 实现使用字符级别的 BPE 词汇，大小为 30K，是在利用启发式分词规则对输入进行预处理之后学得的。Facebook 研究者没有采用这种方式，而是考虑用更大的 byte 级别 BPE 词汇表来训练 BERT，这一词汇表包含 50K 的 subword 单元，且没有对输入作任何额外的预处理或分词。</li>
</ul>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>论文分享</category>
      </categories>
  </entry>
  <entry>
    <title>2020-11-09-pytorch反向传播以及参数更新理解</title>
    <url>/2020/11/09/2020-11-09-pytorch%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%BB%A5%E5%8F%8A%E5%8F%82%E6%95%B0%E6%9B%B4%E6%96%B0%E7%90%86%E8%A7%A3/</url>
    <content><![CDATA[<h3 id="反向传播以及更新">反向传播以及更新</h3>
<h4 id="方法一手动计算变量">方法一：手动计算变量</h4>
<p>这种方法不常用，因为一般的模型参数太多了</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="comment"># 定义参数</span></span><br><span class="line">w1 = Variable(torch.FloatTensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]),requires_grad = <span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 定义输出</span></span><br><span class="line">d = torch.mean(w1)</span><br><span class="line"><span class="comment"># 反向求导</span></span><br><span class="line">d.backward()</span><br><span class="line"><span class="comment"># 定义学习率等参数</span></span><br><span class="line">lr = <span class="number">0.001</span></span><br><span class="line"><span class="comment"># 手动更新参数</span></span><br><span class="line">w1.data.zero_() <span class="comment"># BP求导更新参数之前,需先对导数置0</span></span><br><span class="line">w1.data.sub_(lr*w1.grad.data)<span class="number">12345678910111213</span></span><br></pre></td></tr></tbody></table></figure>
<blockquote>
<p><em>一个网络中通常有很多变量,如果按照上述的方法手动求导,然后更新参数,是很麻烦的,这个时候可以调用torch.optim</em></p>
</blockquote>
<h4 id="方法二使用torch.optim">方法二:使用torch.optim</h4>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="comment"># 这里假设我们定义了一个网络,为net</span></span><br><span class="line">steps = <span class="number">10000</span></span><br><span class="line"><span class="comment"># 定义一个optim对象</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr = <span class="number">0.01</span>)</span><br><span class="line"><span class="comment"># 在for循环中更新参数</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(steps):</span><br><span class="line">  optimizer.zero_grad() <span class="comment"># 对网络中参数当前的导数置0，清零梯度缓存</span></span><br><span class="line">  output = net(input) <span class="comment"># 网络前向计算</span></span><br><span class="line">  loss = criterion(output, target) <span class="comment"># 通过定义损失函数：criterion，计算误差，得到网络的损失值：loss；</span></span><br><span class="line">  loss.backward() <span class="comment">#　通过loss.backward()完成误差的反向传播，通过pytorch的内在机制完成自动求导得到每个参数的梯度。</span></span><br><span class="line">  optimizer.step() <span class="comment"># 更新参数123456789101112131415</span></span><br></pre></td></tr></tbody></table></figure>
<blockquote>
<p>torch.optim只用于参数更新和对参数的梯度置０，不能计算参数的梯度，在使用torch.optim进行参数更新之前，需要写前向与反向传播求导的代码</p>
</blockquote>
<h4 id="注">注</h4>
<p>loss是反向传播整个计算图/模型（有一条传播路径）的节点参数，其中一个模型可以认为是一个连通图，是由数据传播的，比如encoder和decoder之间会有隐藏向量Z进行连接，那么就是一个计算图，那么loss反向传播就会更新所有的参数。参数在定义时默认就是可动态更新的。</p>
<h3 id="variable-parameter的区别">Variable &amp; Parameter的区别</h3>
<p>之所以有Variable这个数据结构，是为了引入计算图（自动求导），方便构建神经网络。也就是一般模型网络（计算图）的输入是Variable类型的，是要外部给值的，返回的是tensor类型。</p>
<p>不同于Parameter，Parameter一般是随机初始化，然后根据loss反向传播被动更新值</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">x = Variable(torch.Tensor(array), requires_grad = True) #可以自求导更新，若一个节点requires_grad被设置为True，那么计图中所有依赖它求得的节点的requires_grad都为True</span><br></pre></td></tr></tbody></table></figure>
<p>Pytorch主要通过引入<code>nn.Parameter</code>类型的变量和<code>optimizer机制</code>来解决自动更新多个参数的问题。</p>
<p>Parameter是Variable的子类，本质上和后者一样，只不过<strong>parameter默认是求梯度的</strong>，同时一个网络net中的parameter变量是可以通过 <code>net.parameters()</code> 来很方便地访问到的，只需将网络中所有需要训练更新的参数定义为Parameter类型，再佐以optimizer，就能够完成所有参数的更新了</p>
<p>Parameter是torch.autograd.Variable的一个字类，常被用于Module的参数。例如权重和偏置。自动加入参数列表，可以进行保存恢复。和Variable具有相同的运算。</p>
<p>Parameter的require_grad默认设置为true。Varaible默认设置为False.</p>
<p>Parameters类是<a href="https://pytorch.apachecn.org/docs/1.0/tensors.html#torch.Tensor" target="_blank" rel="noopener"><code>Tensor</code></a> 的子类, 不过相对于它的父类，Parameters类有一个很重要的特性就是当其在 <a href="https://pytorch.apachecn.org/docs/1.0/nn.html#torch.nn.Module" target="_blank" rel="noopener"><code>Module</code></a>类中被使用并被当做这个<a href="https://pytorch.apachecn.org/docs/1.0/nn.html#torch.nn.Module" target="_blank" rel="noopener"><code>Module</code></a>类的模块属性的时候，那么这个Parameters对象会被自动地添加到这个<a href="https://pytorch.apachecn.org/docs/1.0/nn.html#torch.nn.Module" target="_blank" rel="noopener"><code>Module</code></a>类的参数列表(list of parameters)之中，同时也就会被添加入此<a href="https://pytorch.apachecn.org/docs/1.0/nn.html#torch.nn.Module" target="_blank" rel="noopener"><code>Module</code></a>类的 <a href="https://pytorch.apachecn.org/docs/1.0/nn.html#torch.nn.Module.parameters" target="_blank" rel="noopener"><code>parameters()</code></a>方法所返回的参数迭代器中。而Parameters类的父类Tensor类也可以被用为构建模块的属性，但不会被加入参数列表。这样主要是因为，有时可能需要在模型中存储一些非模型参数的临时状态，比如RNN中的最后一个隐状态。而通过使用非<a href="https://pytorch.apachecn.org/docs/1.0/nn.html#torch.nn.Parameter" target="_blank" rel="noopener"><code>Parameter</code></a>的Tensor类，可以将这些临时变量注册(register)为模型的属性的同时使其不被加入参数列表。</p>
<p><strong>我们可以这样简单区分，在计算图中，数据（包括输入数据和计算过程中产生的feature map等）是 variable 类型，该类型不会被保存到模型中。 </strong></p>
<p><strong>网络的权重是 parameter 类型，在计算过程中会被更新，将会被保存到模型中。</strong></p>
<blockquote>
<p><a href="https://www.jianshu.com/p/cb739922ce88" target="_blank" rel="noopener" class="uri">https://www.jianshu.com/p/cb739922ce88</a></p>
<p><a href="https://zhoef.com/2019/08/12/16_Pytorch_Basic/" target="_blank" rel="noopener" class="uri">https://zhoef.com/2019/08/12/16_Pytorch_Basic/</a></p>
</blockquote>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
  </entry>
  <entry>
    <title>2020-11-09-安装jupyter远程访问服务器</title>
    <url>/2020/11/09/2020-11-09-%E5%AE%89%E8%A3%85jupyter%E8%BF%9C%E7%A8%8B%E8%AE%BF%E9%97%AE%E6%9C%8D%E5%8A%A1%E5%99%A8/</url>
    <content><![CDATA[<h3 id="前言">前言</h3>
<p>想要去用jupyter远程访问我们实验室的服务器，于是参考网上教程如下：</p>
<h3 id="具体操作">具体操作</h3>
<h4 id="一.-ubuntu下安装jupyter-notebook">一. Ubuntu下安装jupyter notebook</h4>
<h5 id="使用anaconda安装">1. 使用Anaconda安装</h5>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">conda install jupyter notebook</span><br></pre></td></tr></tbody></table></figure>
<h5 id="使用pip安装">2. 使用pip安装</h5>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">pip install jupyter notebook</span><br></pre></td></tr></tbody></table></figure>
<h4 id="二.-jupyter-notebook-配置">二. Jupyter notebook 配置</h4>
<h5 id="生成配置文件">1. 生成配置文件</h5>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">jupyter notebook --generate-config</span><br></pre></td></tr></tbody></table></figure>
<h5 id="创建密码">2. 创建密码</h5>
<p>使用python中的<code>passwd()</code>创建密码，终端输入<code>ipython</code>打开ipython并输入:</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">from</span> notebook.auth <span class="keyword">import</span> passwd</span><br><span class="line">In [<span class="number">2</span>]: passwd()</span><br><span class="line">Enter password: ******</span><br><span class="line">Verify password: ******</span><br><span class="line">Out [<span class="number">2</span>]: <span class="string">'sha1:...'</span>  <span class="comment">#应该是密钥</span></span><br></pre></td></tr></tbody></table></figure>
<p>复制Out [2] 显示的密码（'sha1:...' 包括引号）。</p>
<h5 id="修改jupyter-notebook的配置文件">3. 修改jupyter notebook的配置文件</h5>
<ul>
<li>打开配置文件</li>
</ul>
<figure class="highlight vim"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">vim</span> ~/.jupyter/jupyter_notebook_config.<span class="keyword">py</span></span><br></pre></td></tr></tbody></table></figure>
<ul>
<li>在该文件中做如下修改或直接在文件尾端添加：</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">c.NotebookApp.allow_remote_access = <span class="literal">True</span> <span class="comment">#允许远程连接</span></span><br><span class="line">c.NotebookApp.ip=<span class="string">'*'</span> <span class="comment"># 设置所有ip皆可访问</span></span><br><span class="line">c.NotebookApp.password = <span class="string">u'sha:..'</span> <span class="comment">#之前复制的密码</span></span><br><span class="line">c.NotebookApp.open_browser = <span class="literal">False</span> <span class="comment"># 禁止自动打开浏览器</span></span><br><span class="line">c.NotebookApp.port =<span class="number">23333</span> <span class="comment">#任意指定一个端口 ，我们指定的是23333</span></span><br></pre></td></tr></tbody></table></figure>
<h4 id="启动jupyter-notebook">4. 启动jupyter notebook</h4>
<p>终端输入：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">jupyter notebook</span><br></pre></td></tr></tbody></table></figure>
<p>或使用<code>nohup</code>后台运行 jupyter notebook:</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">nohup jupyter notebook &gt;~/jupyter.log <span class="number">2</span>&gt;&amp;<span class="number">1</span> &amp;  <span class="comment">#后台挂起，并且将输出重定向到jupyter.log文件中</span></span><br></pre></td></tr></tbody></table></figure>
<h4 id="远程访问jupyter-notebook">5. 远程访问jupyter notebook</h4>
<p>本地浏览器输入<code>http://(服务器地址):(配置文件中设定的端口)</code>； 假设服务器地址为210.30.97.69，配置的端口为23333，这里的浏览器输入地址应为<code>http://210.30.97.69:23333</code>； 即可访问jupyter notebook。</p>
<h3 id="注">注</h3>
<p>只能通过和服务器所在的局域网来访问，也就是校园网可以正常访问服务器，外网不可。试过用电脑连接我手机热点，无法访问</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
  </entry>
  <entry>
    <title>2020-11-07-论文分享</title>
    <url>/2020/11/07/2020-11-07-%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/</url>
    <content><![CDATA[<p><img src="https://i.loli.net/2020/11/09/7bSkvNlUTsB6VoC.png" alt="image-20201107220502247"></p>
<blockquote>
<p>ACL 2020</p>
<p>code url (official torch) : <a href="https://github.com/namisan/mt-dnn" target="_blank" rel="noopener" class="uri">https://github.com/namisan/mt-dnn</a></p>
<p>被引用次数：11</p>
</blockquote>
<h3 id="背景">背景</h3>
<p>泛化性和鲁棒性对于机器学习来说是很重要的，对抗训练可以增强鲁棒性，但是同时也会使泛化性受到损失；</p>
<p>BERT等大型自然语言模型已经在泛化性方面取得了巨大的进步，然而这种预训练模型容易受到对抗攻击</p>
<h3 id="问题">问题</h3>
<p>如何使得大型NLP模型兼得泛化性和鲁棒性？</p>
<h3 id="解决">解决</h3>
<p>提出了一种通用算法ALUM (Adversarial training for large neural Language Models), 把对抗训练用到了预训练和微调两个阶段，通过对抗训练来提高模型的泛化性和鲁棒性。</p>
<p>对抗训练的方法是针对embedding space，通过最大化对抗损失、最小化模型损失的方式进行对抗，在下游任务上取得了一致的效果提升。</p>
<p>这种对抗训练方法不仅能够在BERT上有提高，而且在RoBERTa这种已经预训练好的模型上也能有所提高，说明对抗训练的确可以帮助模型纠正易错点。</p>
<p>算法可以应用在任何基于transformer的语言模型中</p>
<h3 id="贡献">贡献</h3>
<p><img src="https://i.loli.net/2020/11/09/sTFuUcA7MvNkre5.png" alt="image-20201108095152820"></p>
<h3 id="模型">模型</h3>
<h4 id="准备">准备</h4>
<p>tokenization使用的是BPE（Byte-PairEncoding）</p>
<p>模型基于BERT和 RoBERTa模型，但是在训练策略上与前两者有所改动如下：</p>
<p>在一个epoch中，掩码率以每经过20%的epoch，增加5%掩码率的增速使得掩码率从5%增加到25%</p>
<p><img src="https://i.loli.net/2020/11/09/gd1sfkUao5wqhrc.png" alt="image-20201108100621048"></p>
<h4 id="标准训练目标函数">标准训练目标函数</h4>
<p>标准的预训练和微调函数都可以认为是在训练数据上进行最小化标准差</p>
<p><img src="https://i.loli.net/2020/11/09/2C9JdbBunrHYxTl.png" alt="image-20201108101520342"></p>
<h4 id="对抗训练">对抗训练</h4>
<p><img src="https://i.loli.net/2020/11/09/iLw28ZgrYTVWbnU.png" alt="image-20201108133249939"></p>
<h4 id="alum算法">ALUM算法</h4>
<p>基于几个关键想法：</p>
<ol type="1">
<li><p>扰动embedding空间，优于直接对输入文本应用扰动。</p></li>
<li><p>通过虚拟对抗训练为标准目标添加正则化项。比传统的对抗训练有效果，尤其是在标签有噪声时。</p></li>
</ol>
<p><img src="https://i.loli.net/2020/11/09/O5fCEFvolWSPiDZ.png" alt="img"></p>
<p>其中超参α用于调节标准差和鲁棒差的平衡</p>
<p>（预训练α = 10，微调α = 1）</p>
<ul>
<li>因为有最大化操作，所以训练昂贵。</li>
<li>有利于embedding邻域的标签平滑。</li>
</ul>
<h4 id="算法流程">算法流程</h4>
<p>首先使用标准目标（1）训练模型；然后使用虚拟对抗训练（3）继续训练。</p>
<p><img src="https://i.loli.net/2020/11/09/mYZrkBKpTVelFsa.png" alt="image-20201108103125543"></p>
<p><img src="https://i.loli.net/2020/11/09/mdbNu7MXOc5WZy6.png" alt="image-20201108133930095"></p>
<h3 id="总结">总结</h3>
<p>本文提出了一种通用的对抗性训练算法ALUM：</p>
<ul>
<li><p>对抗预训练可以显著提高泛化能力和鲁棒性。</p></li>
<li><p>ALUM大大提高了BERT和RoBERTa在各种NLP任务中的准确性，并且可以与对抗微调相结合以获得进一步的收益。</p></li>
<li><p>未来的发展方向：</p>
<ul>
<li>进一步研究对抗性预训练在提高泛化和鲁棒性方面的作用；</li>
<li>对抗性训练加速；</li>
<li>将ALUM应用于其他领域。</li>
</ul></li>
</ul>
<p>论文提出了一个通用的模型无关的对抗训练算法架构，可以应用在任何基于transformer的语言模型中。可以尝试去结合模型</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>论文分享</category>
      </categories>
  </entry>
  <entry>
    <title>2020-11-06-论文分享</title>
    <url>/2020/11/06/2020-11-06-%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/</url>
    <content><![CDATA[<p><img src="https://i.loli.net/2020/11/06/dEebUnIiFygwDWk.png" alt="image-20201106095828031"></p>
<blockquote>
<p>EMNLP 2020</p>
<p>code url (official torch) : <a href="https://github.com/ChunyuanLI/Optimus" target="_blank" rel="noopener" class="uri">https://github.com/ChunyuanLI/Optimus</a></p>
<p>被引用次数：9</p>
</blockquote>
<h3 id="背景">背景</h3>
<p>在NLP领域中，VAE是一个很有效的生成模型和表示学习框架</p>
<p>PLM（Pre-trained language models）一般可以分为两种。（1）基于transformer的encoder的BERT， 用于自然语言理解任务。它能够输出一个上下文的表示信息，用于下游任务；（2）基于transformer的decoder的GPT-2，用于自然语言生成任务。它能够以自回归的方式产生文本序列（机器翻译）</p>
<h3 id="问题">问题</h3>
<p>前人想着去结合语言理解任务和语言生成任务，如UniLM、T5模型，效果有提升，但是这些模型缺少一种在紧密（compact）空间（低维空间）中显式的建模，导致很难在一个abstract level去控制语言的生成和表示</p>
<p><img src="https://i.loli.net/2020/11/06/3hC7D1c2vMRGubZ.png" alt="image-20201106102922590"></p>
<p>VAE可以克服这种局限性，可以生成higher-level 的句子表示，从而控制low-level 的word-by-word generation。</p>
<p>但是目前VAE都是应用在浅层的模型中，例如two-layer LSTMs ，这限制了模型的表现</p>
<h3 id="解决">解决</h3>
<p>提出了OPTIMUS， the first large-scale pre-trained deep latent variable models for natural language. ，一个统一的潜在编码空间在大型文本库（large text corpus）训练完之后，在多个下游任务（自然语言理解、自然语言生成）中进行微调</p>
<p>以下是OPTIMUS的优点，它结合了BERT和GPT-2的优势，用于处理自然语言任务，同时相比于BERT和GPT-2，克服了它们的局限性</p>
<p><img src="https://i.loli.net/2020/11/06/1pkceDbh7qXGxzK.png" alt="image-20201106103607562"></p>
<p><img src="https://i.loli.net/2020/11/06/BbtwAayhgp8frzD.png" alt="image-20201106103657363"></p>
<h3 id="贡献">贡献</h3>
<ul>
<li><p>提出首个大规模预训练隐变量生成模型OPTIMUS；</p></li>
<li><p>高效地将隐变量和预训练的GPT-2相结合（ Latent vector injection），提出两种结合方法；</p></li>
<li><p>发现大规模预训练可以减缓KL Vanishing的问题；</p></li>
<li><p>在多个任务上取得显著的效果。</p></li>
</ul>
<h3 id="模型">模型</h3>
<h4 id="目标函数">目标函数</h4>
<p>一般的自然语言模型（如GPT-2）的生成目标，依靠前面的输出的token来预测后面的token，通常训练是通过maximum likelihood estimate (MLE). 但是这样也有局限性，前面已经提到了</p>
<p><img src="https://i.loli.net/2020/11/06/EN3LTgnCxiowcJ8.png" alt="image-20201106105452675"></p>
<p>在生成阶段（训练阶段），模型的生成基于隐变量z，对于给定的文本x，VAE的生成目标，相比于公式（1）多了一个条件z，即显式地依赖z。</p>
<p><strong>z是高层次的语义特征，来指导生成低层次的x，即句法和词汇</strong></p>
<p><img src="https://i.loli.net/2020/11/06/piNRVdgaPS3yxLI.png" alt="image-20201106105946819"></p>
<p>这里θ表示的是用于文本生成的<strong>解码器</strong>。而隐变量是通过一个<strong>编码器</strong>得到的，可以形式化为<img src="https://i.loli.net/2020/11/06/NMvZcI8k6R4KxUV.png" alt="image-20201106115812296"></p>
<p>此时的证据下界（ELBO）就是</p>
<p><img src="https://i.loli.net/2020/11/06/6rVTSBlHvmYnqLi.png" alt="image-20201106115847275"></p>
<p>在本文中，添加了一个超参β， 用于控制训练过程。</p>
<p>所以目标函数可以转化为如下形式，<code>Lβ</code></p>
<p><img src="https://i.loli.net/2020/11/06/fWcLjZQ1iEOty5F.png" alt="image-20201106120223123"></p>
<p><code>LE</code>是重构损失（ (or negative log-likelihood(NLL)）， <code>LR</code>是KL散度（正则项），用于让生成的z逼近先验<code>p(z)</code></p>
<h4 id="模型架构">模型架构</h4>
<p><img src="https://i.loli.net/2020/11/06/eQYDwAVuWnOs25d.png" alt="image-20201106120651825"></p>
<p>可以看出来，模型架构比较简单，但是一些细节也要考虑一下</p>
<p>基本流程如下：</p>
<p>1.使用预训练好的BERT和GPT-2参数，用于OPTIMUS模型encoder和decoder参数的初始化；</p>
<p>此时BERT(L=12,H=768,A=12,Total Parameters=110M) and GPT-2 (L=12,H=768, A=12，,Total Parameters=117M），其中L表示transformer block的层数；H表示中间隐藏层的维度；A表示自注意力头的个数</p>
<blockquote>
<p>可以发现，BERT和GPT-2的超参L、H、A都是一样的</p>
</blockquote>
<ol start="2" type="1">
<li>对于初始化后的OPTIMUS，在大型文本库（large text corpus）的训练集下进行预训练</li>
<li>预训练完OPTIMUS，再在具体的下游任务上进行微调</li>
</ol>
<h4 id="connecting-bert-gpt-2">Connecting BERT &amp; GPT-2</h4>
<p>同时在连接BERT和GPT-2中，存在一些问题</p>
<h5 id="问题1-tokenization如何分词">问题1 Tokenization（如何分词）</h5>
<p>BERT和GPT-2采取了不同的分词方法，如何去表示句子？</p>
<h5 id="解决-1">解决</h5>
<p>在BERT中用的是Word Piece Embeddings (WPE)分词方法，在GPT-2中, 用的是 Byte Pair Encoding (BPE)，</p>
<p>本文中同时采取了两种方法</p>
<p><img src="https://i.loli.net/2020/11/06/rXBTJk6bOqMG5Iy.png" alt="image-20201106122651781"></p>
<p>等于没说。。。</p>
<p>是否可以统一分词方法 ？</p>
<h5 id="问题2-融合隐变量和gpt-2">问题2 融合隐变量和GPT-2</h5>
<p>如何高效地将Z融合进GPT-2中？</p>
<h5 id="解决-2">解决</h5>
<h5 id="section"></h5>
<p><img src="https://i.loli.net/2020/11/06/SPL4c7rhjRyznHx.png" alt="image-20201106104604195"></p>
<p>该如何把隐变量<code>z</code>提供给解码器呢？</p>
<p>本文提供两种方法，分别为记忆（Memory）和嵌入（Embedding）：</p>
<p><img src="https://i.loli.net/2020/11/06/2cu9DGXztrTRFLd.png" alt="image-20201106114441841"></p>
<p>经过实验验证，使用Memory比Embedding方法更有效，作者给出理由如下，就是Memory能够使得decoder在每一层都能直接获得潜在信息，而Embedding只能在输入输出才能获得信息。 Memory能从潜在信息中获得更多的信息用于生成任务</p>
<p><img src="https://i.loli.net/2020/11/06/qebzUvKSh1AOC2N.png" alt="image-20201106114209547"></p>
<blockquote>
<p><strong>在本文中，默认Memory和Embedding方法一起使用</strong></p>
</blockquote>
<h4 id="optimus的预训练">OPTIMUS的预训练</h4>
<p>存在一个问题，就是当VAE和auto regressive models在一起训练时，会出现“KL-vanishing problem”，或者说是“posterior collapse”</p>
<p><img src="https://i.loli.net/2020/11/06/pi7UOIJXy5dQqFg.png" alt="image-20201106130104016"></p>
<p>ELBO 包含reconstruction loss和KL loss两部分。我们的目标是最大化ELBO，等价于最小化KL项并最大化reconstruction项。存在如下问题：</p>
<p>问题1.对于reconstruction部分，当扮演p(x|z)角色的decoder足够强大，仅凭自己就可以model q(x)分布，那么也没有必要去依赖z。</p>
<p>问题2.对于KL项，如果简单的将data x和latent variable z无关并让q(z|x) = q(z) = p(z)，即posterior退化为和prior一样的高斯，<strong>KL就可以取得最小值0</strong>。</p>
<p>所以针对上述问题，本文在预训练的时候做了如下优化：</p>
<p><img src="https://i.loli.net/2020/11/06/fWcLjZQ1iEOty5F.png" alt="image-20201106120223123"></p>
<ul>
<li><p>对β使用循环调度（cyclical schedule），一共10个 periods 来加强β的作用；</p></li>
<li><p>每一个 period中，在训练的前一半，设置β=0 只训练encoder，避免了上述问题1；对后一半的前一半，将其增长到1，对最后四分之一，固定为1；</p></li>
<li><p>当β≠0的时候，加入KL thresholding scheme （KL 阈值），保证KL项始终大于一个常数 λ，这样可以避免了上述问题2，避免LR项取得最小值0。此时LR被替换为 hinge loss</p>
<p><img src="https://i.loli.net/2020/11/06/n3ZKDu7pEOSWaiN.png" alt="image-20201106131315827"></p></li>
</ul>
<p><img src="https://i.loli.net/2020/11/06/1VFDdL2fcsTexPj.png" alt="image-20201106130006529"></p>
<h3 id="实验">实验</h3>
<p>预训练OPTIMUS之后，剩下的就是对不同的任务进行微调了。本文在三类任务上实验：语言模型、受限文本生成和自然语言理解</p>
<p><img src="https://i.loli.net/2020/11/06/2YQ7lmnkhc618pV.png" alt="image-20201106110800369"></p>
<h3 id="总结">总结</h3>
<p>VAE和PLM在自然语言处理中都是很重要的部分，自然会想到将二者结合起来</p>
<p>总体来说，论文没太多创新点，而且在正文部分有些故弄玄虚，明明一个很简单的概念，说的让人无法理解，非要绕个弯子，可能这样会让人觉得更高深些吧，但是对于后来的研究者来说很痛苦</p>
<p>代码还没有看，准备再读读代码，加深模型的理解</p>
<h4 id="更改方向">更改方向</h4>
<p>1.用更好的方法去解决KL vanishing 问题</p>
<p>2.统一分词方法是否有更好的结果</p>
<p>3.使用最新的VAE方法/PLM方法去解决问题</p>
<p>4.结合T-CVAE框架</p>
<p>5.因为NLP模型普遍对于对抗攻击很敏感，所以增加对抗思想，提高模型的鲁棒性。结合论文《Adversarial Training for Large Neural Language Models》（ACL 2020）中的通用算法ALUM去解决对抗攻击的问题</p>
<h3 id="参考">参考</h3>
<blockquote>
<p>OPTIMUS <a href="https://zhuanlan.zhihu.com/p/143517152" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/143517152</a></p>
<p>KL vanishing <a href="https://zhuanlan.zhihu.com/p/64071467" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/64071467</a></p>
</blockquote>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>论文分享</category>
      </categories>
  </entry>
  <entry>
    <title>2020-11-04-论文分享</title>
    <url>/2020/11/04/2020-11-04-%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/</url>
    <content><![CDATA[<p><img src="https://i.loli.net/2020/11/05/d4IuoeCYcaxO8v9.png" alt="image-20201104220110364"></p>
<blockquote>
<p>IJCAI 2019</p>
<p>code url (official tf ，无torch版本) : <a href="https://github.com/sodawater/T-CVAE" target="_blank" rel="noopener" class="uri">https://github.com/sodawater/T-CVAE</a></p>
<p>被引用次数：12</p>
</blockquote>
<h3 id="背景">背景</h3>
<p>故事补全是一个非常具有挑战性的任务，即为一个不完整的故事生成缺失的情节。</p>
<p>它涉及两个方面：<strong>理解和生成</strong>。故事理解包括识别人物角色等。生成是基于理解的下一步，即根据给定故事中的线索进行推理。一个好的故事情节应该是有意义的和连贯的上下文。此外，输入文本的不连续性使得理解和生成更加困难。</p>
<p>本模型的任务：给定一个故事的任何四个句子，目标是生成缺失的句子，即缺失的情节，来完成这个故事。 （基于ROCStories的常识故事语料库）</p>
<p><img src="https://i.loli.net/2020/11/05/iSonHTZ7I5g9Atz.png" alt="根据缺失的故事，不同的补全句子"></p>
<h3 id="问题">问题</h3>
<p>前人的研究都关注于为不完整的故事选择或产生一个合理的结局。这些任务是我们故事完成任务的特殊化，因此先前的方法不适合生成故事的开始或中间情节。并且倾向于生成泛型和非连贯性的情节。</p>
<p>1.如何去补全缺失中间部分的故事 ？</p>
<p>2.怎样使补全的句子是<strong>有意义的、连贯的、多样的</strong> ？</p>
<h3 id="解决">解决</h3>
<p>我们提出了一种新的<strong>基于transformer的条件变量自动编码模型</strong>（T-CVAE）</p>
<p>1.Transformer：作为模型基础，并采用了一个改进的<strong>具有共享自我注意层</strong>的Transformer，这种共享自我注意层能够使解码器同时关注到编码器和解码器的状态，以此能够使模型获取更多的上下文线索。</p>
<p>2.条件变分自编码模型：提高生成的多样性和一致性</p>
<h3 id="贡献">贡献</h3>
<p><img src="https://i.loli.net/2020/11/05/aH6sB72CvqQTPoj.png" alt="贡献"></p>
<p>可以看出，并没有太多的贡献。</p>
<h3 id="模型">模型</h3>
<h4 id="模型流程">模型流程</h4>
<p><img src="https://i.loli.net/2020/11/05/3HsRKVLjtPhfcx2.png" alt="image-20201105104914786"></p>
<p>数据在encoder和decoder的流动</p>
<p><img src="https://i.loli.net/2020/11/05/iJFKlqYbGe3CnjO.png" alt="image-20201105105933096"></p>
<h4 id="后验网络-pzxy">后验网络 P(z|x,y)</h4>
<p><img src="https://i.loli.net/2020/11/05/b7exp5tuoq4W6V3.png" alt="image-20201105105357954"></p>
<h4 id="先验网络pzx">先验网络P(z|x)</h4>
<p><img src="https://i.loli.net/2020/11/05/TxVdC3bBXFpjnUY.png" alt="image-20201105105419533"></p>
<h4 id="组合层combination-layer">组合层（combination layer）</h4>
<p>并不是利用z来直接初始化decoder的状态，而是利用了组合层。之后再经过Linear层和softmax层输出最终的预测</p>
<p><img src="https://i.loli.net/2020/11/05/owdjV8YQah6eqpB.png" alt="image-20201105105628307"></p>
<h4 id="目标函数">目标函数</h4>
<p>由于z上的积分是难以解决的，因此我们应用变分推理并优化相应的证据下限（ELBO）：</p>
<p><img src="https://i.loli.net/2020/11/05/I3iZ2EebhXOzyTk.png" alt="image-20201105104715312"></p>
<p><img src="https://i.loli.net/2020/11/05/ACP2tKvEyraOoJh.png" alt="image-20201105104837678"></p>
<p>训练目标：</p>
<p>1.最大化重构y的概率，这样可以使得后验网络和情节生成器 ( p(y|x,z) ) 做出的预测更加接近于标准值 ；</p>
<p>2.最小化z的先验分布和后验分布的KL散度，这样当标准值不存在的时候（推理），可以促使先验网络去产生合理的概率分布</p>
<h3 id="实验">实验</h3>
<p><img src="https://i.loli.net/2020/11/05/2KNSnML9rlE6dvT.png" alt="image-20201104222356336"></p>
<p><img src="https://i.loli.net/2020/11/05/9z3pS2onE6vlBgy.png" alt="消融实验"></p>
<p><img src="https://i.loli.net/2020/11/05/ahGK4kXudPfMU25.png" alt="image-20201104222411554"></p>
<h3 id="总结">总结</h3>
<p>总体来说，论文最大的贡献就是将CVAE和transformer结合了起来，处理故事补全的问题，transformer模型上并没有太大的创新。但是VAE和transformer系列模型结合的思路值得我去借鉴。</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>论文分享</category>
      </categories>
  </entry>
  <entry>
    <title>2020-10-31-注意力机制总结</title>
    <url>/2020/10/31/2020-10-31-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h3 id="前言">前言</h3>
<p>注意力模型最近几年在深度学习各个领域被广泛使用，无论是图像处理、语音识别还是自然语言处理的各种不同类型的任务中，都很容易遇到注意力模型的身影。所以，了解注意力机制的工作原理对于关注深度学习技术发展的技术人员来说有很大的必要。</p>
<blockquote>
<p>RNN做机器翻译有它自身的弱点，Attention正是为了克服这个弱点而出现的。所以，要理解Attention，就要搞明白两件事： - RNN在做机器翻译时有什么弱点 - Attention是如何克服这个弱点的</p>
</blockquote>
<h3 id="rnn做机器翻译的经典思路-encoder-decoder">RNN做机器翻译的经典思路 encoder-decoder</h3>
<p>用RNN做机器翻译时，通常需要两个RNN网络，一个用来将接收待翻译语句，对其进行编码，最后输出一个vector，这个网络叫encoder。然后，该vector会作为输入，传给另一个RNN网络，该网络用来根据vector产生目标语言的翻译语句，这个网络叫做decoder。如下图所示：</p>
<p><img src="https://i.loli.net/2020/10/31/UOF2qxEoRB7ftsP.png" alt="image-20201031104117636"></p>
<p>上图中间的Context就是我们这里说的第一个RNN产生的vector</p>
<h3 id="encoder-decoder的缺点在哪里">encoder-decoder的缺点在哪里？</h3>
<p>encoder-decoder最大的缺点是，encoder接收了不管多长的语句，最后输出的只是最后一个vector，当语句很长时，这个vector能否有效地表示该语句是很值得怀疑的。 如何解决这个问题呢？我们很自然会想到，第一个RNN其实在中间会产生很多输出，这些输出都被我们抛弃了，我们只用了最后的一个。如果能利用上中间的输出，兴许可以解决问题。Attention正是利用上了这些中间的输出。</p>
<h3 id="attention是如何利用中间的输出的">Attention是如何利用中间的输出的</h3>
<p>先上图，再来解释：</p>
<p><img src="https://i.loli.net/2020/10/31/ixs6baohzDmjfPn.png" alt="image-20201031104152681"></p>
<p>上图中的A是我们的encoder， B是我们的decoder。 可以想象，A网络接收了一个四个字的句子，对每个字都产生了一个输出（这些输出都是一个vector），我们称其为s1，s2，s3，s4。</p>
<p>我们看上图的B网络，在第一个B产生的hidden state（称其为h1）除了传给下一个cell外，还传到了A网络，这里就是Attention发挥作用的地方，我们来看看发生了什么。</p>
<p><strong>第一步</strong>： h1 分别与s1，s2，s3，s4做点积，产生了四个数，称其为m1，m2，m3，m4（这些都是标量，不是向量了！）</p>
<p><strong>第二步</strong>： m1，m2，m3，m4 传到一个softmax层，产生一个概率分布a1，a2，a3， a4。</p>
<p><strong>第三步</strong>： 将a1，a2，a3， a4 与s1，s2，s3，s4分别相乘，再相加，得到得到一个vector，称其为Attention vector。</p>
<p><strong>第四步</strong>：</p>
<p>Attention vector 将作为输入传到B网络的第二个cell中，参与预测。</p>
<p>以上就是Attention机制的基本思想了。我们看到，Attention vector 实际上融合了s1，s2，s3，s4的信息，具体的融合是用一个概率分布来达到的，而这个概率分布又是通过B网络上一个cell的hidden state与s1，s2，s3，s4进行点乘得到的。 Attention vector实际上达到了让B网络聚焦于A网络输出的某一部分的作用。</p>
<h3 id="attention中产生概率分布的两种方法">Attention中产生概率分布的两种方法</h3>
<p>在第3部分中，我们的概率分布来自于h与s的点积再做softmax，这只是最基本的方式。在实际中，我们可以有不同的方法来产生这个概率分布，每一种方法都代表了一种具体的Attention机制。</p>
<ul>
<li><h4 id="加法attention">1 加法Attention</h4>
<p>在加法Attention中，我们不再让h与s做点积，而是做如下的运算：</p>
<p><img src="https://i.loli.net/2020/10/31/dOAr8BHlK25vQuU.png" alt="image-20201031104221351"></p></li>
</ul>
<p>va和Wa都是可以训练的参数。h与s之间的分号表示将二者接到一起产生一个更长的vector。这样产生的数再送往softmax层，进而产生一个概率分布。</p>
<p>当然，我们还可以这么做：</p>
<p><img src="https://i.loli.net/2020/10/31/Hu4eWOfmQLk9DrK.png" alt="image-20201031104229311"></p>
<p>这里只是不再把h与s接到一起而已，本质上没有什么区别的。</p>
<ul>
<li><h4 id="乘法attention">2 乘法Attention</h4>
<p>乘法Attention将h与s做如下的运算：</p>
<p><img src="https://i.loli.net/2020/10/31/V87BZWl3sMUeGvb.png" alt="image-20201031104239592"></p></li>
</ul>
<p>显然，乘法Attention的参数更少，效率自然也会更高一些。</p>
<h3 id="attention机制的扩展">Attention机制的扩展</h3>
<p>Attention机制的核心在于对一个序列数据进行聚焦，这个聚焦是通过一个概率分布来实现的。这种机制其实有很强的普适性，可以用在各个方面。</p>
<p>比如，根据图片产生描述该图片的文字， 首先，图片会经过CNN进行特征的提取，提取的数据会输入到产生描述文字的RNN中，这里，我们可以引入Attention机制，让我们在产生下一个文字时，聚焦于我们正在描述的图片部位。</p>
<p>其次，在句子表示中，self Attention机制是成功扩展的Attention的范例。其基本原理如下：</p>
<p>假如我们用一个RNN读入了一个句子，产生了h1， h2，h3，h4四个hidden state。 为了得到该句子的摘要，我们可以这样做： 对每一个h计算一个分数：</p>
<p><img src="https://i.loli.net/2020/10/31/7ltIKSuAZFiBzQn.png" alt="image-20201031104249976"></p>
<p>四个h共产生了4个分数，将这四个分数送入一个softmax层，产生一个概率分布，根据这个概率分布对四个h进行加和，得到句子摘要的第一个vector。如下图所示：</p>
<p><img src="https://i.loli.net/2020/10/31/x2DGOb9jBoNJweq.png" alt="image-20201031104257776"></p>
<p>为了得到更多的vector，我们可以把上面图中的小写va换成一个矩阵，然后，我们的a也就变成了多个概率分布组成的矩阵，每个概率分布都可以用来与h进行加和产生一个vector，这样我们就产生了摘要的多个vector，如下图所示：</p>
<p><img src="https://i.loli.net/2020/10/31/9esynl6cDKfmEiS.png" alt="image-20201031104320465"></p>
<h3 id="人类的视觉注意力">人类的视觉注意力</h3>
<p>从注意力模型的命名方式看，很明显其借鉴了人类的注意力机制，因此，我们首先简单介绍人类视觉的选择性注意力机制。</p>
<p><img src="https://i.loli.net/2020/10/31/516P2JBYrgOToyU.jpg" alt="人类的视觉注意力"></p>
<p>视觉注意力机制是人类视觉所特有的大脑信号处理机制。人类视觉通过快速扫描全局图像，获得需要重点关注的目标区域，也就是一般所说的注意力焦点，而后对这一区域投入更多注意力资源，以获取更多所需要关注目标的细节信息，而抑制其他无用信息。</p>
<p>这是人类利用有限的注意力资源从大量信息中快速筛选出高价值信息的手段，是人类在长期进化中形成的一种生存机制，人类视觉注意力机制极大地提高了视觉信息处理的效率与准确性。</p>
<p>图1形象化展示了人类在看到一副图像时是如何高效分配有限的注意力资源的，其中红色区域表明视觉系统更关注的目标，很明显对于图1所示的场景，人们会把注意力更多投入到人的脸部，文本的标题以及文章首句等位置。</p>
<p>深度学习中的注意力机制从本质上讲和人类的选择性视觉注意力机制类似，核心目标也是从众多信息中选择出对当前任务目标更关键的信息。</p>
<h3 id="encoder-decoder框架">Encoder-Decoder框架</h3>
<p>要了解深度学习中的注意力模型，就不得不先谈Encoder-Decoder框架，因为目前大多数注意力模型附着在Encoder-Decoder框架下，当然，其实注意力模型可以看作一种通用的思想，本身并不依赖于特定框架，这点需要注意。</p>
<p>Encoder-Decoder框架可以看作是一种深度学习领域的研究模式，应用场景异常广泛。图2是文本处理领域里常用的Encoder-Decoder框架最抽象的一种表示。</p>
<p><img src="https://i.loli.net/2020/10/31/ohBVdz1KDn4ZcOb.png" alt="抽象的文本处理领域的Encoder-Decoder框架"></p>
<p>文本处理领域的Encoder-Decoder框架可以这么直观地去理解：可以把它看作适合处理由一个句子（或篇章）生成另外一个句子（或篇章）的通用处理模型。对于句子对&lt;Source,Target&gt;，我们的目标是给定输入句子Source，期待通过Encoder-Decoder框架来生成目标句子Target。Source和Target可以是同一种语言，也可以是两种不同的语言。而Source和Target分别由各自的单词序列构成：</p>
<p><img src="https://i.loli.net/2020/10/31/qrCwtas6iRVKYbA.png" alt="img"></p>
<p>Encoder顾名思义就是对输入句子Source进行编码，将输入句子通过非线性变换转化为中间语义表示C：</p>
<p><img src="https://i.loli.net/2020/10/31/M7EXx2KPgeHFQhL.png" alt="img"></p>
<p>对于解码器Decoder来说，其任务是根据句子Source的中间语义表示C和之前已经生成的历史信息 y1,y2,y3...yi-1来生成i时刻要生成的单词yi：</p>
<p><img src="https://i.loli.net/2020/10/31/6uHkShXDNKOlBQ3.png" alt="img"></p>
<p>每个yi都依次这么产生，那么看起来就是整个系统根据输入句子Source生成了目标句子Target。如果Source是中文句子，Target是英文句子，那么这就是解决机器翻译问题的Encoder-Decoder框架；如果Source是一篇文章，Target是概括性的几句描述语句，那么这是文本摘要的Encoder-Decoder框架；如果Source是一句问句，Target是一句回答，那么这是问答系统或者对话机器人的Encoder-Decoder框架。由此可见，在文本处理领域，Encoder-Decoder的应用领域相当广泛。</p>
<p>Encoder-Decoder框架不仅仅在文本领域广泛使用，在语音识别、图像处理等领域也经常使用。比如对于语音识别来说，图2所示的框架完全适用，区别无非是Encoder部分的输入是语音流，输出是对应的文本信息；而对于“图像描述”任务来说，Encoder部分的输入是一副图片，Decoder的输出是能够描述图片语义内容的一句描述语。一般而言，文本处理和语音识别的Encoder部分通常采用RNN模型，图像处理的Encoder一般采用CNN模型。</p>
<h3 id="attention模型">Attention模型</h3>
<p>本节先以机器翻译作为例子讲解最常见的Soft Attention模型的基本原理，之后抛离Encoder-Decoder框架抽象出了注意力机制的本质思想，然后简单介绍最近广为使用的Self Attention的基本思路。</p>
<h4 id="soft-attention模型">Soft Attention模型</h4>
<p>图2中展示的Encoder-Decoder框架是没有体现出“注意力模型”的，所以可以把它看作是注意力不集中的分心模型。为什么说它注意力不集中呢？请观察下目标句子Target中每个单词的生成过程如下：</p>
<p><img src="https://i.loli.net/2020/10/31/KNWS6Ax1uI285cH.png" alt="img"></p>
<p>其中f是Decoder的非线性变换函数。从这里可以看出，在生成目标句子的单词时，不论生成哪个单词，它们使用的输入句子Source的语义编码C都是一样的，没有任何区别。</p>
<p>而语义编码C是由句子Source的每个单词经过Encoder</p>
<p>编码产生的，这意味着不论是生成哪个单词，y1,y2还是y3，其实句子Source中任意单词对生成某个目标单词yi来说影响力都是相同的，这是为何说这个模型没有体现出注意力的缘由。这类似于人类看到眼前的画面，但是眼中却没有注意焦点一样。</p>
<p>如果拿机器翻译来解释这个分心模型的Encoder-Decoder框架更好理解，比如输入的是英文句子：Tom chase Jerry，Encoder-Decoder框架逐步生成中文单词：“汤姆”，“追逐”，“杰瑞”。</p>
<p>在翻译“杰瑞”这个中文单词的时候，分心模型里面的每个英文单词对于翻译目标单词“杰瑞”贡献是相同的，很明显这里不太合理，显然“Jerry”对于翻译成“杰瑞”更重要，但是分心模型是无法体现这一点的，这就是为何说它没有引入注意力的原因。</p>
<p>没有引入注意力的模型在输入句子比较短的时候问题不大，但是如果输入句子比较长，此时所有语义完全通过一个中间语义向量来表示，单词自身的信息已经消失，可想而知会丢失很多细节信息，这也是为何要引入注意力模型的重要原因。</p>
<p>上面的例子中，如果引入Attention模型的话，应该在翻译“杰瑞”的时候，体现出英文单词对于翻译当前中文单词不同的影响程度，比如给出类似下面一个概率分布值：</p>
<p>（Tom,0.3）(Chase,0.2) (Jerry,0.5)</p>
<p>每个英文单词的概率代表了翻译当前单词“杰瑞”时，注意力分配模型分配给不同英文单词的注意力大小。这对于正确翻译目标语单词肯定是有帮助的，因为引入了新的信息。</p>
<p>同理，目标句子中的每个单词都应该学会其对应的源语句子中单词的注意力分配概率信息。这意味着在生成每个单词yi的时候，原先都是相同的中间语义表示C会被替换成根据当前生成单词而不断变化的Ci。理解Attention模型的关键就是这里，即由固定的中间语义表示C换成了根据当前输出单词来调整成加入注意力模型的变化的Ci。增加了注意力模型的Encoder-Decoder框架理解起来如图3所示。</p>
<p><img src="https://i.loli.net/2020/10/31/oe6E7C9sUHrVuNZ.png" alt="引入注意力模型的Encoder-Decoder框架"></p>
<p>即生成目标句子单词的过程成了下面的形式：</p>
<p><img src="https://i.loli.net/2020/10/31/q8ZFEu4BSI1lo9z.png" alt="img"></p>
<p>而每个Ci可能对应着不同的源语句子单词的注意力分配概率分布，比如对于上面的英汉翻译来说，其对应的信息可能如下：</p>
<p><img src="https://i.loli.net/2020/10/31/TBg8KZhoyiOlUst.png" alt="img"></p>
<p>其中，f2函数代表Encoder对输入英文单词的某种变换函数，比如如果Encoder是用的RNN模型的话，这个f2函数的结果往往是某个时刻输入xi后隐层节点的状态值；g代表Encoder根据单词的中间表示合成整个句子中间语义表示的变换函数，一般的做法中，g函数就是对构成元素加权求和，即下列公式：</p>
<p><img src="https://i.loli.net/2020/10/31/W9SnjkNw3BVasdc.png" alt="img"></p>
<p>其中，Lx代表输入句子Source的长度，aij代表在Target输出第i个单词时Source输入句子中第j个单词的注意力分配系数，而hj则是Source输入句子中第j个单词的语义编码。假设下标i就是上面例子所说的“ 汤姆” ，那么Lx就是3，h1=f(“Tom”)，h2=f(“Chase”),h3=f(“Jerry”)分别是输入句子每个单词的语义编码，对应的注意力模型权值则分别是0.6,0.2,0.2，所以g函数本质上就是个加权求和函数。如果形象表示的话，翻译中文单词“汤姆”的时候，数学公式对应的中间语义表示Ci的形成过程类似图4。</p>
<p><img src="https://i.loli.net/2020/10/31/LRJZMaUbN9VuxGr.png" alt="Attention的形成过程"></p>
<p>这里还有一个问题：生成目标句子某个单词，比如“汤姆”的时候，如何知道Attention模型所需要的输入句子单词注意力分配概率分布值呢？就是说“汤姆”对应的输入句子Source中各个单词的概率分布：(Tom,0.6)(Chase,0.2) (Jerry,0.2) 是如何得到的呢？</p>
<p>为了便于说明，我们假设对图2的非Attention模型的Encoder-Decoder框架进行细化，Encoder采用RNN模型，Decoder也采用RNN模型，这是比较常见的一种模型配置，则图2的框架转换为图5。</p>
<p><img src="https://i.loli.net/2020/10/31/ebTh6NzIdlVG1L2.png" alt="RNN作为具体模型的Encoder-Decoder框架"></p>
<p>那么用下图可以较为便捷地说明注意力分配概率分布值的通用计算过程。</p>
<p><img src="https://i.loli.net/2020/10/31/TEXoxdzgrHOuiMQ.jpg" alt="注意力分配概率计算"></p>
<p>对于采用RNN的Decoder来说，在时刻i，如果要生成yi单词，我们是可以知道Target在生成Yi之前的时刻i-1时，隐层节点i-1时刻的输出值Hi-1的，而我们的目的是要计算生成Yi时输入句子中的单词“Tom”、“Chase”、“Jerry”对Yi来说的注意力分配概率分布，那么可以用Target输出句子i-1时刻的隐层节点状态Hi-1去一一和输入句子Source中每个单词对应的RNN隐层节点状态hj进行对比，即通过函数F(hj,Hi-1)来获得目标单词yi和每个输入单词对应的对齐可能性，这个F函数在不同论文里可能会采取不同的方法，然后函数F的输出经过Softmax进行归一化就得到了符合概率分布取值区间的注意力分配概率分布数值。</p>
<p>绝大多数Attention模型都是采取上述的计算框架来计算注意力分配概率分布信息，区别只是在F的定义上可能有所不同。图7可视化地展示了在英语-德语翻译系统中加入Attention机制后，Source和Target两个句子每个单词对应的注意力分配概率分布。</p>
<p><img src="https://i.loli.net/2020/10/31/T1tNMbc6DE3HkGQ.png" alt="英语-德语翻译的注意力概率分布"></p>
<p>上述内容就是经典的Soft Attention模型的基本思想，那么怎么理解Attention模型的物理含义呢？一般在自然语言处理应用里会把Attention模型看作是输出Target句子中某个单词和输入Source句子每个单词的对齐模型，这是非常有道理的。</p>
<p>目标句子生成的每个单词对应输入句子单词的概率分布可以理解为输入句子单词和这个目标生成单词的对齐概率，这在机器翻译语境下是非常直观的：传统的统计机器翻译一般在做的过程中会专门有一个短语对齐的步骤，而注意力模型其实起的是相同的作用。</p>
<p><img src="https://i.loli.net/2020/10/31/zYmvXW8pHiO2VUo.jpg" alt="Google 神经网络机器翻译系统结构图"></p>
<p>图8所示即为Google于2016年部署到线上的基于神经网络的机器翻译系统，相对传统模型翻译效果有大幅提升，翻译错误率降低了60%，其架构就是上文所述的加上Attention机制的Encoder-Decoder框架，主要区别无非是其Encoder和Decoder使用了8层叠加的LSTM模型。</p>
<h4 id="attention机制的本质思想">Attention机制的本质思想</h4>
<p>如果把Attention机制从上文讲述例子中的Encoder-Decoder框架中剥离，并进一步做抽象，可以更容易看懂Attention机制的本质思想。</p>
<p><img src="https://i.loli.net/2020/10/31/y3BToK96rsgUZfF.png" alt="Attention机制的本质思想"></p>
<p>我们可以这样来看待Attention机制（参考图9）：将Source中的构成元素想象成是由一系列的&lt;Key,Value&gt;数据对构成，此时给定Target中的某个元素Query，通过计算Query和各个Key的相似性或者相关性，得到每个Key对应Value的权重系数，然后对Value进行加权求和，即得到了最终的Attention数值。所以本质上Attention机制是对Source中元素的Value值进行加权求和，而Query和Key用来计算对应Value的权重系数。即可以将其本质思想改写为如下公式：</p>
<p><img src="https://i.loli.net/2020/10/31/ipGlzuFcmS8n2VR.png" alt="img"></p>
<p>其中，Lx=||Source||代表Source的长度，公式含义即如上所述。上文所举的机器翻译的例子里，因为在计算Attention的过程中，Source中的Key和Value合二为一，指向的是同一个东西，也即输入句子中每个单词对应的语义编码，所以可能不容易看出这种能够体现本质思想的结构。</p>
<p>当然，从概念上理解，把Attention仍然理解为从大量信息中有选择地筛选出少量重要信息并聚焦到这些重要信息上，忽略大多不重要的信息，这种思路仍然成立。聚焦的过程体现在权重系数的计算上，权重越大越聚焦于其对应的Value值上，即权重代表了信息的重要性，而Value是其对应的信息。</p>
<p>从上图可以引出另外一种理解，也可以将Attention机制看作一种软寻址（Soft Addressing）:Source可以看作存储器内存储的内容，元素由地址Key和值Value组成，当前有个Key=Query的查询，目的是取出存储器中对应的Value值，即Attention数值。通过Query和存储器内元素Key的地址进行相似性比较来寻址，之所以说是软寻址，指的不像一般寻址只从存储内容里面找出一条内容，而是可能从每个Key地址都会取出内容，取出内容的重要性根据Query和Key的相似性来决定，之后对Value进行加权求和，这样就可以取出最终的Value值，也即Attention值。所以不少研究人员将Attention机制看作软寻址的一种特例，这也是非常有道理的。</p>
<p>至于Attention机制的具体计算过程，如果对目前大多数方法进行抽象的话，可以将其归纳为两个过程：第一个过程是根据Query和Key计算权重系数，第二个过程根据权重系数对Value进行加权求和。而第一个过程又可以细分为两个阶段：第一个阶段根据Query和Key计算两者的相似性或者相关性；第二个阶段对第一阶段的原始分值进行归一化处理；这样，可以将Attention的计算过程抽象为如图10展示的三个阶段。</p>
<p><img src="https://i.loli.net/2020/10/31/tzO9fhXUeBblmVG.png" alt="三阶段计算Attention过程"></p>
<p>在第一个阶段，可以引入不同的函数和计算机制，根据Query和某个Key_i，计算两者的相似性或者相关性，最常见的方法包括：求两者的向量点积、求两者的向量Cosine相似性或者通过再引入额外的神经网络来求值，即如下方式：</p>
<p><img src="https://i.loli.net/2020/10/31/9xpPOa7ohFf3u1d.png" alt="img"></p>
<p>第一阶段产生的分值根据具体产生的方法不同其数值取值范围也不一样，第二阶段引入类似SoftMax的计算方式对第一阶段的得分进行数值转换，一方面可以进行归一化，将原始计算分值整理成所有元素权重之和为1的概率分布；另一方面也可以通过SoftMax的内在机制更加突出重要元素的权重。即一般采用如下公式计算：</p>
<p><img src="https://i.loli.net/2020/10/31/XFW5tcSGjqBnIyN.png" alt="img"></p>
<p>第二阶段的计算结果a_i即为value_i对应的权重系数，然后进行加权求和即可得到Attention数值：</p>
<p><img src="https://i.loli.net/2020/10/31/soa1M9LIPGi3krC.png" alt="img"></p>
<p>通过如上三个阶段的计算，即可求出针对Query的Attention数值，目前绝大多数具体的注意力机制计算方法都符合上述的三阶段抽象计算过程。</p>
<h4 id="self-attention模型">Self Attention模型</h4>
<p>通过上述对Attention本质思想的梳理，我们可以更容易理解本节介绍的Self Attention模型。Self Attention也经常被称为intra Attention（内部Attention），最近一年也获得了比较广泛的使用，比如Google最新的机器翻译模型内部大量采用了Self Attention模型。</p>
<p>在一般任务的Encoder-Decoder框架中，输入Source和输出Target内容是不一样的，比如对于英-中机器翻译来说，Source是英文句子，Target是对应的翻译出的中文句子，Attention机制发生在Target的元素Query和Source中的所有元素之间。而Self</p>
<p>Attention顾名思义，指的不是Target和Source之间的Attention机制，而是Source内部元素之间或者Target内部元素之间发生的Attention机制，也可以理解为Target=Source这种特殊情况下的注意力计算机制。其具体计算过程是一样的，只是计算对象发生了变化而已，所以此处不再赘述其计算过程细节。</p>
<p>如果是常规的Target不等于Source情形下的注意力计算，其物理含义正如上文所讲，比如对于机器翻译来说，本质上是目标语单词和源语单词之间的一种单词对齐机制。那么如果是Self Attention机制，一个很自然的问题是：通过Self Attention到底学到了哪些规律或者抽取出了哪些特征呢？或者说引入Self Attention有什么增益或者好处呢？我们仍然以机器翻译中的Self Attention来说明，图11和图12是可视化地表示Self Attention在同一个英语句子内单词间产生的联系。</p>
<p><img src="https://i.loli.net/2020/10/31/BqbvNSUWyrnTjt4.jpg" alt="可视化Self Attention实例"></p>
<p><img src="https://i.loli.net/2020/10/31/q2iCXflnh5oH1WE.jpg" alt="可视化Self Attention实例"></p>
<p>从两张图（图11、图12）可以看出，Self Attention可以捕获同一个句子中单词之间的一些句法特征（比如图11展示的有一定距离的短语结构）或者语义特征（比如图12展示的its的指代对象Law）。</p>
<p>很明显，引入Self Attention后会更容易捕获句子中长距离的相互依赖的特征，因为如果是RNN或者LSTM，需要依次序序列计算，对于远距离的相互依赖的特征，要经过若干时间步步骤的信息累积才能将两者联系起来，而距离越远，有效捕获的可能性越小。</p>
<p>但是Self Attention在计算过程中会直接将句子中任意两个单词的联系通过一个计算步骤直接联系起来，所以远距离依赖特征之间的距离被极大缩短，有利于有效地利用这些特征。除此外，Self Attention对于增加计算的并行性也有直接帮助作用。这是为何Self Attention逐渐被广泛使用的主要原因。</p>
<h4 id="attention机制的应用">Attention机制的应用</h4>
<p>前文有述，Attention机制在深度学习的各种应用领域都有广泛的使用场景。上文在介绍过程中我们主要以自然语言处理中的机器翻译任务作为例子，下面分别再从图像处理领域和语音识别选择典型应用实例来对其应用做简单说明。</p>
<p><img src="https://i.loli.net/2020/10/31/Yog7WcVFCXbRv3U.jpg" alt="图片-描述任务的Encoder-Decoder框架"></p>
<p>图片描述（Image-Caption）是一种典型的图文结合的深度学习应用，输入一张图片，人工智能系统输出一句描述句子，语义等价地描述图片所示内容。很明显这种应用场景也可以使用Encoder-Decoder框架来解决任务目标，此时Encoder输入部分是一张图片，一般会用CNN来对图片进行特征抽取，Decoder部分使用RNN或者LSTM来输出自然语言句子（参考上图）。</p>
<p>此时如果加入Attention机制能够明显改善系统输出效果，Attention模型在这里起到了类似人类视觉选择性注意的机制，在输出某个实体单词的时候会将注意力焦点聚焦在图片中相应的区域上。图14给出了根据给定图片生成句子“A person is standing on a beach with a surfboard.”过程时每个单词对应图片中的注意力聚焦区域。</p>
<p><img src="https://i.loli.net/2020/10/31/RGVXYj8W2yQsqtz.jpg" alt="图片生成句子中每个单词时的注意力聚焦区域"></p>
<p>下图给出了另外四个例子形象地展示了这种过程，每个例子上方左侧是输入的原图，下方句子是人工智能系统自动产生的描述语句，上方右侧图展示了当AI系统产生语句中划横线单词的时候，对应图片中聚焦的位置区域。比如当输出单词dog的时候，AI系统会将注意力更多地分配给图片中小狗对应的位置。</p>
<p><img src="https://i.loli.net/2020/10/31/v7Eubye6A4dSLZp.jpg" alt="图像描述任务中Attention机制的聚焦作用"></p>
<p><img src="https://i.loli.net/2020/10/31/Yt5fPBJWRAaUHNv.jpg" alt="语音识别中音频序列和输出字符之间的Attention"></p>
<p>语音识别的任务目标是将语音流信号转换成文字，所以也是Encoder-Decoder的典型应用场景。Encoder部分的Source输入是语音流信号，Decoder部分输出语音对应的字符串流。</p>
<p>上图可视化地展示了在Encoder-Decoder框架中加入Attention机制后，当用户用语音说句子 how much would a woodchuck chuck 时，输入部分的声音特征信号和输出字符之间的注意力分配概率分布情况，颜色越深代表分配到的注意力概率越高。从图中可以看出，在这个场景下，Attention机制起到了将输出字符和输入语音信号进行对齐的功能。</p>
<p>上述内容仅仅选取了不同AI领域的几个典型Attention机制应用实例，Encoder-Decoder加Attention架构由于其卓越的实际效果，目前在深度学习领域里得到了广泛的使用，了解并熟练使用这一架构对于解决实际问题会有极大帮助。</p>
<h3 id="总结">总结</h3>
<p>通过以上的内容，我们了解到，Attention机制最初用来克服RNN做机器翻译时的缺点，然后，人们发现，Attention机制具有广泛的适用性，于是它又被扩展到了产生图片描述，做句子摘要等任务上。</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
  </entry>
  <entry>
    <title>2020-10-30-从embedding到BERT预训练模型</title>
    <url>/2020/10/30/2020-10-30-%E4%BB%8Eembedding%E5%88%B0BERT%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<p>Bert最近很火，应该是最近最火爆的AI进展，网上的评价很高，那么Bert值得这么高的评价吗？我个人判断是值得。那为什么会有这么高的评价呢？是因为它有重大的理论或者模型创新吗？其实并没有，从模型创新角度看一般，创新不算大。但是架不住效果太好了，基本刷新了很多NLP的任务的最好性能，有些任务还被刷爆了，这个才是关键。另外一点是Bert具备广泛的通用性，就是说绝大部分NLP任务都可以采用类似的两阶段模式直接去提升效果，这个第二关键。客观的说，把Bert当做最近两年NLP重大进展的集大成者更符合事实。</p>
<p>本文的主题是自然语言处理中的预训练过程，会大致说下NLP中的预训练技术是一步一步如何发展到Bert模型的，从中可以很自然地看到Bert的思路是如何逐渐形成的，Bert的历史沿革是什么，继承了什么，创新了什么，为什么效果那么好，主要原因是什么，以及为何说模型创新不算太大，为何说Bert是近年来NLP重大进展的集大成者。我们一步一步来讲，而串起来这个故事的脉络就是自然语言的预训练过程，但是落脚点还是在Bert身上。要讲自然语言的预训练，得先从图像领域的预训练说起。</p>
<h3 id="图像领域的预训练">图像领域的预训练</h3>
<p>自从深度学习火起来后，预训练过程就是做图像或者视频领域的一种比较常规的做法，有比较长的历史了，而且这种做法很有效，能明显促进应用的效果。</p>
<p><img src="https://i.loli.net/2020/10/30/HSZgfVhv5cAO2Qt.jpg" alt="img"></p>
<p>那么图像领域怎么做预训练呢，上图展示了这个过程，我们设计好网络结构以后，对于图像来说一般是CNN的多层叠加网络结构，可以先用某个训练集合比如训练集合A或者训练集合B对这个网络进行预先训练，在A任务上或者B任务上学会网络参数，然后存起来以备后用。假设我们面临第三个任务C，网络结构采取相同的网络结构，在比较浅的几层CNN结构，网络参数初始化的时候可以加载A任务或者B任务学习好的参数，其它CNN高层参数仍然随机初始化。之后我们用C任务的训练数据来训练网络，此时有两种做法，一种是浅层加载的参数在训练C任务过程中不动，这种方法被称为“Frozen”;另外一种是底层网络参数尽管被初始化了，在C任务训练过程中仍然随着训练的进程不断改变，这种一般叫“Fine-Tuning”，顾名思义，就是更好地把参数进行调整使得更适应当前的C任务。一般图像或者视频领域要做预训练一般都这么做。</p>
<p>这么做有几个好处，首先，如果手头任务C的训练集合数据量较少的话，现阶段的好用的CNN比如Resnet/Densenet/Inception等网络结构层数很深，几百万上千万参数量算起步价，上亿参数的也很常见，训练数据少很难很好地训练这么复杂的网络，但是如果其中大量参数通过大的训练集合比如ImageNet预先训练好直接拿来初始化大部分网络结构参数，然后再用C任务手头比较可怜的数据量上Fine-tuning过程去调整参数让它们更适合解决C任务，那事情就好办多了。这样原先训练不了的任务就能解决了，即使手头任务训练数据也不少，加个预训练过程也能极大加快任务训练的收敛速度，所以这种预训练方式是老少皆宜的解决方案，另外疗效又好，所以在做图像处理领域很快就流行开来。</p>
<p>那么新的问题来了，为什么这种预训练的思路是可行的？</p>
<p><img src="https://i.loli.net/2020/10/30/hnsrCeotdP4jLSq.jpg" alt="img"></p>
<p>目前我们已经知道，对于层级的CNN结构来说，不同层级的神经元学习到了不同类型的图像特征，由底向上特征形成层级结构，如上图所示，如果我们手头是个人脸识别任务，训练好网络后，把每层神经元学习到的特征可视化肉眼看一看每层学到了啥特征，你会看到最底层的神经元学到的是线段等特征，图示的第二个隐层学到的是人脸五官的轮廓，第三层学到的是人脸的轮廓，通过三步形成了特征的层级结构，越是底层的特征越是所有不论什么领域的图像都会具备的比如边角线弧线等底层基础特征，越往上抽取出的特征越与手头任务相关。正因为此，所以预训练好的网络参数，尤其是底层的网络参数抽取出特征跟具体任务越无关，越具备任务的通用性，所以这是为何一般用底层预训练好的参数初始化新任务网络参数的原因。而高层特征跟任务关联较大，实际可以不用使用，或者采用Fine-tuning用新数据集合清洗掉高层无关的特征抽取器。</p>
<p><img src="https://i.loli.net/2020/10/30/RrSMsuboYdOzvKl.jpg" alt="img"></p>
<p>一般我们喜欢用ImageNet来做网络的预训练，主要有两点，一方面ImageNet是图像领域里有超多事先标注好训练数据的数据集合，分量足是个很大的优势，量越大训练出的参数越靠谱；另外一方面因为ImageNet有1000类，类别多，算是通用的图像数据，跟领域没太大关系，所以通用性好，预训练完后哪哪都能用，是个万金油。分量足的万金油当然老少通吃，人人喜爱。</p>
<p>听完上述话，如果你是具备研究素质的人，也就是说具备好奇心，你一定会问下面这个问题：”既然图像领域预训练这么好用，那干嘛自然语言处理不做这个事情呢？是不是搞NLP的人比搞CV的傻啊？就算你傻，你看见人家这么做，有样学样不就行了吗？这不就是创新吗，也许能成，万一成了，你看，你的成功来得就是这么突然!”</p>
<p>嗯，好问题，其实搞NLP的人一点都不比你傻，早就有人尝试过了，不过总体而言不太成功而已。听说过word embedding吗？2003年出品，陈年技术，馥郁芳香。word embedding其实就是NLP里的早期预训练技术。当然也不能说word embedding不成功，一般加到下游任务里，都能有1到2个点的性能提升，只是没有那么耀眼的成功而已。</p>
<p>没听过？那下面就把这段陈年老账讲给你听听。</p>
<h3 id="word-embedding考古史">Word Embedding考古史</h3>
<p>这块大致讲讲Word Embedding的故事，很粗略，因为网上关于这个技术讲的文章太多了，汗牛冲动，我不属牛，此刻更没有流汗，所以其实丝毫没有想讲Word Embedding的冲动和激情，但是要说预训练又得从这开始，那就粗略地讲讲，主要是引出后面更精彩的部分。在说Word Embedding之前，先更粗略地说下语言模型，因为一般NLP里面做预训练一般的选择是用语言模型任务来做。</p>
<p><img src="https://i.loli.net/2020/10/30/4aFBLDCEQmgXp9Z.jpg" alt="img"></p>
<p>什么是语言模型？其实看上面这张PPT上扣下来的图就明白了，为了能够量化地衡量哪个句子更像一句人话，可以设计如上图所示函数，核心函数P的思想是根据句子里面前面的一系列前导单词预测后面跟哪个单词的概率大小（理论上除了上文之外，也可以引入单词的下文联合起来预测单词出现概率）。句子里面每个单词都有个根据上文预测自己的过程，把所有这些单词的产生概率乘起来，数值越大代表这越像一句人话。语言模型压下暂且不表，我隐约预感到我这么讲你可能还是不太会明白，但是大概这个意思，不懂的可以去网上找，资料多得一样地汗牛冲动。</p>
<p>假设现在让你设计一个神经网络结构，去做这个语言模型的任务，就是说给你很多语料做这个事情，训练好一个神经网络，训练好之后，以后输入一句话的前面几个单词，要求这个网络输出后面紧跟的单词应该是哪个，你会怎么做？</p>
<p><img src="https://i.loli.net/2020/10/30/SUI2jF7xEsaz9Z4.jpg" alt="img"></p>
<p>你可以像上图这么设计这个网络结构，这其实就是大名鼎鼎的中文人称“神经网络语言模型”，英文小名NNLM的网络结构，用来做语言模型。这个工作有年头了，是个陈年老工作，是Bengio 在2003年发表在JMLR上的论文。它生于2003，火于2013，以后是否会不朽暂且不知，但是不幸的是出生后应该没有引起太大反响，沉寂十年终于时来运转沉冤得雪，在2013年又被NLP考古工作者从海底湿淋淋地捞出来了祭入神殿。为什么会发生这种技术奇遇记？你要想想2013年是什么年头，是深度学习开始渗透NLP领域的光辉时刻，万里长征第一步，而NNLM可以算是南昌起义第一枪。在深度学习火起来之前，极少有人用神经网络做NLP问题，如果你10年前坚持用神经网络做NLP，估计别人会认为你这人神经有问题。所谓红尘滚滚，谁也挡不住历史发展趋势的车轮，这就是个很好的例子。</p>
<p>上面是闲话，闲言碎语不要讲，我们回来讲一讲NNLM的思路。先说训练过程，现在看其实很简单，见过RNN、LSTM、CNN后的你们回头再看这个网络甚至显得有些简陋。学习任务是输入某个句中单词Wt=“Bert”前面句子的t-1个单词，要求网络正确预测单词Bert，即最大化：</p>
<p><img src="https://i.loli.net/2020/10/30/k8XpuUwT47WOo5Q.png" alt="image-20201030202319906" style="zoom:50%;"></p>
<p>前面任意单词Wi用Onehot编码（比如：0001000）作为原始单词输入，之后乘以矩阵Q后获得向量C（Wi），每个单词的 C（Wi）拼接，上接隐层，然后接softmax去预测后面应该后续接哪个单词。这个 C（Wi）是什么？这其实就是单词对应的Word Embedding值，那个矩阵Q包含V行，V代表词典大小，每一行内容代表对应单词的Word embedding值。只不过Q的内容也是网络参数，需要学习获得，训练刚开始用随机值初始化矩阵Q，当这个网络训练好之后，矩阵Q的内容被正确赋值，每一行代表一个单词对应的Word embedding值。所以你看，通过这个网络学习语言模型任务，这个网络不仅自己能够根据上文预测后接单词是什么，同时获得一个副产品，就是那个矩阵Q，这就是单词的Word Embedding是被如何学会的。</p>
<p>2013年最火的用语言模型做Word Embedding的工具是Word2Vec，后来又出了Glove，Word2Vec是怎么工作的呢？看下图。</p>
<p><img src="https://i.loli.net/2020/10/30/bXq5RvhS3niaTxB.jpg" alt="img"></p>
<p>Word2Vec的网络结构其实和NNLM是基本类似的，只是这个图长得清晰度差了点，看上去不像，其实它们是亲兄弟。不过这里需要指出：尽管网络结构相近，而且也是做语言模型任务，但是其训练方法不太一样。Word2Vec有两种训练方法，一种叫CBOW，核心思想是从一个句子里面把一个词抠掉，用这个词的上文和下文去预测被抠掉的这个词；第二种叫做Skip-gram，和CBOW正好反过来，输入某个单词，要求网络预测它的上下文单词。而你回头看看，NNLM是怎么训练的？是输入一个单词的上文，去预测这个单词。这是有显著差异的。为什么Word2Vec这么处理？原因很简单，因为Word2Vec和NNLM不一样，NNLM的主要任务是要学习一个解决语言模型任务的网络结构，语言模型就是要看到上文预测下文，而word embedding只是无心插柳的一个副产品。但是Word2Vec目标不一样，它单纯就是要word embedding的，这是主产品，所以它完全可以随性地这么去训练网络。</p>
<p>为什么要讲Word2Vec呢？这里主要是要引出CBOW的训练方法，BERT其实跟它有关系，后面会讲它们之间是如何的关系，当然它们的关系BERT作者没说，是我猜的，至于我猜的对不对，后面你看后自己判断。</p>
<p><img src="https://i.loli.net/2020/10/30/aM9lpsNOS7vrmnq.jpg" alt="img"></p>
<p>使用Word2Vec或者Glove，通过做语言模型任务，就可以获得每个单词的Word Embedding，那么这种方法的效果如何呢？上图给了网上找的几个例子，可以看出有些例子效果还是很不错的，一个单词表达成Word Embedding后，很容易找出语义相近的其它词汇。</p>
<p>我们的主题是预训练，那么问题是Word Embedding这种做法能算是预训练吗？这其实就是标准的预训练过程。要理解这一点要看看学会Word Embedding后下游任务是怎么用它的。</p>
<p><img src="https://i.loli.net/2020/10/30/YEf95lnIW28OGRa.jpg" alt="img"></p>
<p>假设如上图所示，我们有个NLP的下游任务，比如QA，就是问答问题，所谓问答问题，指的是给定一个问题X，给定另外一个句子Y,要判断句子Y是否是问题X的正确答案。问答问题假设设计的网络结构如上图所示，这里不展开讲了，懂得自然懂，不懂的也没关系，因为这点对于本文主旨来说不关键，关键是网络如何使用训练好的Word Embedding的。它的使用方法其实和前面讲的NNLM是一样的，句子中每个单词以Onehot形式作为输入，然后乘以学好的Word Embedding矩阵Q，就直接取出单词对应的Word Embedding了。这乍看上去好像是个查表操作，不像是预训练的做法是吧？其实不然，那个Word Embedding矩阵Q其实就是网络Onehot层到embedding层映射的网络参数矩阵。所以你看到了，使用Word Embedding等价于什么？等价于把Onehot层到embedding层的网络用预训练好的参数矩阵Q初始化了。这跟前面讲的图像领域的低层预训练过程其实是一样的，区别无非Word Embedding只能初始化第一层网络参数，再高层的参数就无能为力了。下游NLP任务在使用Word Embedding的时候也类似图像有两种做法，一种是Frozen，就是Word Embedding那层网络参数固定不动；另外一种是Fine-Tuning，就是Word Embedding这层参数使用新的训练集合训练也需要跟着训练过程更新掉。</p>
<p>上面这种做法就是18年之前NLP领域里面采用预训练的典型做法，之前说过，Word Embedding其实对于很多下游NLP任务是有帮助的，只是帮助没有大到闪瞎忘记戴墨镜的围观群众的双眼而已。那么新问题来了，为什么这样训练及使用Word Embedding的效果没有期待中那么好呢？答案很简单，因为Word Embedding有问题呗。这貌似是个比较弱智的答案，关键是Word Embedding存在什么问题？这其实是个好问题。</p>
<p><img src="https://i.loli.net/2020/10/30/U3YwNJ1RmydDukM.jpg" alt="img"></p>
<p>这片在Word Embedding头上笼罩了好几年的乌云是什么？是多义词问题。我们知道，多义词是自然语言中经常出现的现象，也是语言灵活性和高效性的一种体现。多义词对Word Embedding来说有什么负面影响？如上图所示，比如多义词Bank，有两个常用含义，但是Word Embedding在对bank这个单词进行编码的时候，是区分不开这两个含义的，因为它们尽管上下文环境中出现的单词不同，但是在用语言模型训练的时候，不论什么上下文的句子经过word2vec，都是预测相同的单词bank，而同一个单词占的是同一行的参数空间，这导致两种不同的上下文信息都会编码到相同的word embedding空间里去。所以word embedding无法区分多义词的不同语义，这就是它的一个比较严重的问题。</p>
<p>你可能觉得自己很聪明，说这可以解决啊，确实也有很多研究人员提出很多方法试图解决这个问题，但是从今天往回看，这些方法看上去都成本太高或者太繁琐了，有没有简单优美的解决方案呢？</p>
<p>ELMO提供了一种简洁优雅的解决方案。</p>
<h3 id="从word-embedding到elmo">从Word Embedding到ELMO</h3>
<p>ELMO是“Embedding from Language Models”的简称，其实这个名字并没有反应它的本质思想，提出ELMO的论文题目：“Deep contextualized word representation”更能体现其精髓，而精髓在哪里？在deep contextualized这个短语，一个是deep，一个是context，其中context更关键。在此之前的Word Embedding本质上是个静态的方式，所谓静态指的是训练好之后每个单词的表达就固定住了，以后使用的时候，不论新句子上下文单词是什么，这个单词的Word Embedding不会跟着上下文场景的变化而改变，所以对于比如Bank这个词，它事先学好的Word Embedding中混合了几种语义 ，在应用中来了个新句子，即使从上下文中（比如句子包含money等词）明显可以看出它代表的是“银行”的含义，但是对应的Word Embedding内容也不会变，它还是混合了多种语义。这是为何说它是静态的，这也是问题所在。ELMO的本质思想是：我事先用语言模型学好一个单词的Word Embedding，此时多义词无法区分，不过这没关系。在我实际使用Word Embedding的时候，单词已经具备了特定的上下文了，这个时候我可以根据上下文单词的语义去调整单词的Word Embedding表示，这样经过调整后的Word Embedding更能表达在这个上下文中的具体含义，自然也就解决了多义词的问题了。所以ELMO本身是个根据当前上下文对Word Embedding动态调整的思路。</p>
<p><img src="https://i.loli.net/2020/10/30/m6NvFoRGhWbji83.jpg" alt="img"></p>
<p>ELMO采用了典型的两阶段过程，第一个阶段是利用语言模型进行预训练；第二个阶段是在做下游任务时，从预训练网络中提取对应单词的网络各层的Word Embedding作为新特征补充到下游任务中。上图展示的是其预训练过程，它的网络结构采用了双层双向LSTM，目前语言模型训练的任务目标是根据单词 Wi 的上下文去正确预测单词 Wi ， Wi 之前的单词序列Context-before称为上文，之后的单词序列Context-after称为下文。图中左端的前向双层LSTM代表正方向编码器，输入的是从左到右顺序的除了预测单词外 Wi 的上文Context-before；右端的逆向双层LSTM代表反方向编码器，输入的是从右到左的逆序的句子下文Context-after；每个编码器的深度都是两层LSTM叠加。这个网络结构其实在NLP中是很常用的。使用这个网络结构利用大量语料做语言模型任务就能预先训练好这个网络，如果训练好这个网络后，输入一个新句子 Snew ，句子中每个单词都能得到对应的三个Embedding:最底层是单词的Word Embedding，往上走是第一层双向LSTM中对应单词位置的Embedding，这层编码单词的句法信息更多一些；再往上走是第二层LSTM中对应单词位置的Embedding，这层编码单词的语义信息更多一些。也就是说，ELMO的预训练过程不仅仅学会单词的Word Embedding，还学会了一个双层双向的LSTM网络结构，而这两者后面都有用。</p>
<p><img src="https://i.loli.net/2020/10/30/ymSXKwFh8WBcEd5.jpg" alt="img"></p>
<p>上面介绍的是ELMO的第一阶段：预训练阶段。那么预训练好网络结构后，如何给下游任务使用呢？上图展示了下游任务的使用过程，比如我们的下游任务仍然是QA问题，此时对于问句X，我们可以先将句子X作为预训练好的ELMO网络的输入，这样句子X中每个单词在ELMO网络中都能获得对应的三个Embedding，之后给予这三个Embedding中的每一个Embedding一个权重a，这个权重可以学习得来，根据各自权重累加求和，将三个Embedding整合成一个。然后将整合后的这个Embedding作为X句在自己任务的那个网络结构中对应单词的输入，以此作为补充的新特征给下游任务使用。对于上图所示下游任务QA中的回答句子Y来说也是如此处理。因为ELMO给下游提供的是每个单词的特征形式，所以这一类预训练的方法被称为“Feature-based Pre-Training”。至于为何这么做能够达到区分多义词的效果，你可以想一想，其实比较容易想明白原因。</p>
<p><img src="https://i.loli.net/2020/10/30/b8ToQxv5BPgELI1.jpg" alt="img"></p>
<p>上面这个图是TagLM采用类似ELMO的思路做命名实体识别任务的过程，其步骤基本如上述ELMO的思路，所以此处不展开说了。TagLM的论文发表在2017年的ACL会议上，作者就是AllenAI里做ELMO的那些人，所以可以将TagLM看做ELMO的一个前导工作。前几天这个PPT发出去后有人质疑说FastAI的在18年4月提出的ULMFiT才是抛弃传统Word Embedding引入新模式的开山之作，我深不以为然。首先TagLM出现的更早而且模式基本就是ELMO的思路；另外ULMFiT使用的是三阶段模式，在通用语言模型训练之后，加入了一个领域语言模型预训练过程，而且论文重点工作在这块，方法还相对比较繁杂，这并不是一个特别好的主意，因为领域语言模型的限制是它的规模往往不可能特别大，精力放在这里不太合适，放在通用语言模型上感觉更合理；再者，尽管ULFMiT实验做了6个任务，但是都集中在分类问题相对比较窄，不如ELMO验证的问题领域广，我觉得这就是因为第二步那个领域语言模型带来的限制。所以综合看，尽管ULFMiT也是个不错的工作，但是重要性跟ELMO比至少还是要差一档，当然这是我个人看法。每个人的学术审美口味不同，我个人一直比较赞赏要么简洁有效体现问题本质要么思想特别游离现有框架脑洞开得异常大的工作，所以ULFMiT我看论文的时候就感觉看着有点难受，觉得这工作没抓住重点而且特别麻烦，但是看ELMO论文感觉就赏心悦目，觉得思路特别清晰顺畅，看完暗暗点赞，心里说这样的文章获得NAACL2018最佳论文当之无愧，比ACL很多最佳论文也好得不是一点半点，这就是好工作带给一个有经验人士的一种在读论文时候就能产生的本能的感觉，也就是所谓的这道菜对上了食客的审美口味。</p>
<p><img src="https://i.loli.net/2020/10/30/6y7VvCDm9NRpHJx.jpg" alt="img"></p>
<p>前面我们提到静态Word Embedding无法解决多义词的问题，那么ELMO引入上下文动态调整单词的embedding后多义词问题解决了吗？解决了，而且比我们期待的解决得还要好。上图给了个例子，对于Glove训练出的Word Embedding来说，多义词比如play，根据它的embedding找出的最接近的其它单词大多数集中在体育领域，这很明显是因为训练数据中包含play的句子中体育领域的数量明显占优导致；而使用ELMO，根据上下文动态调整后的embedding不仅能够找出对应的“演出”的相同语义的句子，而且还可以保证找出的句子中的play对应的词性也是相同的，这是超出期待之处。之所以会这样，是因为我们上面提到过，第一层LSTM编码了很多句法信息，这在这里起到了重要作用。</p>
<p><img src="https://i.loli.net/2020/10/30/YFAVkuIxemlaHPN.jpg" alt="img"></p>
<p>ELMO经过这般操作，效果如何呢？实验效果见上图，6个NLP任务中性能都有幅度不同的提升，最高的提升达到25%左右，而且这6个任务的覆盖范围比较广，包含句子语义关系判断，分类任务，阅读理解等多个领域，这说明其适用范围是非常广的，普适性强，这是一个非常好的优点。</p>
<p><img src="https://i.loli.net/2020/10/30/LpMSFe5kX7Qxroh.jpg" alt="img"></p>
<p>那么站在现在这个时间节点看，ELMO有什么值得改进的缺点呢？首先，一个非常明显的缺点在特征抽取器选择方面，ELMO使用了LSTM而不是新贵Transformer，Transformer是谷歌在17年做机器翻译任务的“Attention is all you need”的论文中提出的，引起了相当大的反响，很多研究已经证明了Transformer提取特征的能力是要远强于LSTM的。如果ELMO采取Transformer作为特征提取器，那么估计Bert的反响远不如现在的这种火爆场面。另外一点，ELMO采取双向拼接这种融合特征的能力可能比Bert一体化的融合特征方式弱，但是，这只是一种从道理推断产生的怀疑，目前并没有具体实验说明这一点。</p>
<p>我们如果把ELMO这种预训练方法和图像领域的预训练方法对比，发现两者模式看上去还是有很大差异的。除了以ELMO为代表的这种基于特征融合的预训练方法外，NLP里还有一种典型做法，这种做法和图像领域的方式就是看上去一致的了，一般将这种方法称为“基于Fine-tuning的模式”，而GPT就是这一模式的典型开创者。</p>
<h3 id="从word-embedding到gpt">从Word Embedding到GPT</h3>
<p><img src="https://i.loli.net/2020/10/30/IDl2hH8j3JVdx6F.jpg" alt="img"></p>
<p>GPT是“Generative Pre-Training”的简称，从名字看其含义是指的生成式的预训练。GPT也采用两阶段过程，第一个阶段是利用语言模型进行预训练，第二阶段通过Fine-tuning的模式解决下游任务。上图展示了GPT的预训练过程，其实和ELMO是类似的，主要不同在于两点：首先，特征抽取器不是用的RNN，而是用的Transformer，上面提到过它的特征抽取能力要强于RNN，这个选择很明显是很明智的；其次，GPT的预训练虽然仍然是以语言模型作为目标任务，但是采用的是单向的语言模型，所谓“单向”的含义是指：语言模型训练的任务目标是根据 Wi 单词的上下文去正确预测单词 Wi ， Wi 之前的单词序列Context-before称为上文，之后的单词序列Context-after称为下文。ELMO在做语言模型预训练的时候，预测单词 Wi 同时使用了上文和下文，而GPT则只采用Context-before这个单词的上文来进行预测，而抛开了下文。这个选择现在看不是个太好的选择，原因很简单，它没有把单词的下文融合进来，这限制了其在更多应用场景的效果，比如阅读理解这种任务，在做任务的时候是可以允许同时看到上文和下文一起做决策的。如果预训练时候不把单词的下文嵌入到Word Embedding中，是很吃亏的，白白丢掉了很多信息。</p>
<p>这里强行插入一段简单提下Transformer，尽管上面提到了，但是说的还不完整，补充两句。首先，Transformer是个叠加的“自注意力机制（Self Attention）”构成的深度网络，是目前NLP里最强的特征提取器，注意力这个机制在此被发扬光大，从任务的配角不断抢戏，直到Transformer一跃成为踢开RNN和CNN传统特征提取器，荣升头牌，大红大紫。注意力机制可以参考“<a href="https://zhuanlan.zhihu.com/p/37601161" target="_blank" rel="noopener">深度学习中的注意力模型</a>”，补充下相关基础知识，如果不了解注意力机制你肯定会落后时代的发展。而介绍Transformer比较好的文章可以参考以下两篇文章：一个是Jay Alammar可视化地介绍Transformer的博客文章<a href="https://link.zhihu.com/?target=https%3A//jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a> ，非常容易理解整个机制，建议先从这篇看起；然后可以参考哈佛大学NLP研究组写的“<a href="https://link.zhihu.com/?target=http%3A//nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer.</a> ”，代码原理双管齐下，讲得非常清楚。我相信上面两个文章足以让你了解Transformer了，所以这里不展开介绍。</p>
<p>其次，我的判断是Transformer在未来会逐渐替代掉RNN成为主流的NLP工具，RNN一直受困于其并行计算能力，这是因为它本身结构的序列性依赖导致的，尽管很多人在试图通过修正RNN结构来修正这一点，但是我不看好这种模式，因为给马车换轮胎不如把它升级到汽车，这个道理很好懂，更何况目前汽车的雏形已经出现了，干嘛还要执着在换轮胎这个事情呢？是吧？再说CNN，CNN在NLP里一直没有形成主流，CNN的最大优点是易于做并行计算，所以速度快，但是在捕获NLP的序列关系尤其是长距离特征方面天然有缺陷，不是做不到而是做不好，目前也有很多改进模型，但是特别成功的不多。综合各方面情况，很明显Transformer同时具备并行性好，又适合捕获长距离特征，没有理由不在赛跑比赛中跑不过RNN和CNN。</p>
<p>好了，题外话结束，我们再回到主题，接着说GPT。上面讲的是GPT如何进行第一阶段的预训练，那么假设预训练好了网络模型，后面下游任务怎么用？它有自己的个性，和ELMO的方式大有不同。</p>
<p><img src="https://i.loli.net/2020/10/30/3Gr9vqoPHkSfcAg.jpg" alt="img"></p>
<p>上图展示了GPT在第二阶段如何使用。首先，对于不同的下游任务来说，本来你可以任意设计自己的网络结构，现在不行了，你要向GPT的网络结构看齐，把任务的网络结构改造成和GPT的网络结构是一样的。然后，在做下游任务的时候，利用第一步预训练好的参数初始化GPT的网络结构，这样通过预训练学到的语言学知识就被引入到你手头的任务里来了，这是个非常好的事情。再次，你可以用手头的任务去训练这个网络，对网络参数进行Fine-tuning，使得这个网络更适合解决手头的问题。就是这样。看到了么？这有没有让你想起最开始提到的图像领域如何做预训练的过程（请参考上图那句非常容易暴露年龄的歌词）？对，这跟那个模式是一模一样的。</p>
<p>这里引入了一个新问题：对于NLP各种花样的不同任务，怎么改造才能靠近GPT的网络结构呢？</p>
<p><img src="https://i.loli.net/2020/10/30/iJqb8TYLwCvdSVk.jpg" alt="img"></p>
<p>GPT论文给了一个改造施工图如上，其实也很简单：对于分类问题，不用怎么动，加上一个起始和终结符号即可；对于句子关系判断问题，比如Entailment，两个句子中间再加个分隔符即可；对文本相似性判断问题，把两个句子顺序颠倒下做出两个输入即可，这是为了告诉模型句子顺序不重要；对于多项选择问题，则多路输入，每一路把文章和答案选项拼接作为输入即可。从上图可看出，这种改造还是很方便的，不同任务只需要在输入部分施工即可。</p>
<p><img src="https://i.loli.net/2020/10/30/qnLcVGo5IK6riYh.jpg" alt="img"></p>
<p>GPT的效果是非常令人惊艳的，在12个任务里，9个达到了最好的效果，有些任务性能提升非常明显。</p>
<p><img src="https://i.loli.net/2020/10/30/96zdAXvcOmJTuP2.jpg" alt="img"></p>
<p>那么站在现在的时间节点看，GPT有什么值得改进的地方呢？其实最主要的就是那个单向语言模型，如果改造成双向的语言模型任务估计也没有Bert太多事了。当然，即使如此GPT也是非常非常好的一个工作，跟Bert比，其作者炒作能力亟待提升。</p>
<h3 id="bert的诞生">Bert的诞生</h3>
<p><img src="https://i.loli.net/2020/10/30/rSJAqOMB4sathDg.jpg" alt="img"></p>
<p>我们经过跋山涉水，终于到了目的地Bert模型了。</p>
<p>Bert采用和GPT完全相同的两阶段模型，首先是语言模型预训练；其次是使用Fine-Tuning模式解决下游任务。和GPT的最主要不同在于在预训练阶段采用了类似ELMO的双向语言模型，当然另外一点是语言模型的数据规模要比GPT大。所以这里Bert的预训练过程不必多讲了。</p>
<p><img src="https://i.loli.net/2020/10/30/61JpWKSZ5fF3tNk.jpg" alt="img"></p>
<p>第二阶段，Fine-Tuning阶段，这个阶段的做法和GPT是一样的。当然，它也面临着下游任务网络结构改造的问题，在改造任务方面Bert和GPT有些不同，下面简单介绍一下。</p>
<p><img src="https://i.loli.net/2020/10/30/UTQdhtVA7PzcIlF.jpg" alt="img"></p>
<p>在介绍Bert如何改造下游任务之前，先大致说下NLP的几类问题，说这个是为了强调Bert的普适性有多强。通常而言，绝大部分NLP问题可以归入上图所示的四类任务中：一类是序列标注，这是最典型的NLP任务，比如中文分词，词性标注，命名实体识别，语义角色标注等都可以归入这一类问题，它的特点是句子中每个单词要求模型根据上下文都要给出一个分类类别。第二类是分类任务，比如我们常见的文本分类，情感计算等都可以归入这一类。它的特点是不管文章有多长，总体给出一个分类类别即可。第三类任务是句子关系判断，比如Entailment，QA，语义改写，自然语言推理等任务都是这个模式，它的特点是给定两个句子，模型判断出两个句子是否具备某种语义关系；第四类是生成式任务，比如机器翻译，文本摘要，写诗造句，看图说话等都属于这一类。它的特点是输入文本内容后，需要自主生成另外一段文字。</p>
<p><img src="https://i.loli.net/2020/10/30/mxJybVWl2OatfUc.jpg" alt="img"></p>
<p>对于种类如此繁多而且各具特点的下游NLP任务，Bert如何改造输入输出部分使得大部分NLP任务都可以使用Bert预训练好的模型参数呢？上图给出示例，对于句子关系类任务，很简单，和GPT类似，加上一个起始和终结符号，句子之间加个分隔符即可。对于输出来说，把第一个起始符号对应的Transformer最后一层位置上面串接一个softmax分类层即可。对于分类问题，与GPT一样，只需要增加起始和终结符号，输出部分和句子关系判断任务类似改造；对于序列标注问题，输入部分和单句分类是一样的，只需要输出部分Transformer最后一层每个单词对应位置都进行分类即可。从这里可以看出，上面列出的NLP四大任务里面，除了生成类任务外，Bert其它都覆盖到了，而且改造起来很简单直观。尽管Bert论文没有提，但是稍微动动脑子就可以想到，其实对于机器翻译或者文本摘要，聊天机器人这种生成式任务，同样可以稍作改造即可引入Bert的预训练成果。只需要附着在S2S结构上，encoder部分是个深度Transformer结构，decoder部分也是个深度Transformer结构。根据任务选择不同的预训练数据初始化encoder和decoder即可。这是相当直观的一种改造方法。当然，也可以更简单一点，比如直接在单个Transformer结构上加装隐层产生输出也是可以的。不论如何，从这里可以看出，NLP四大类任务都可以比较方便地改造成Bert能够接受的方式。这其实是Bert的非常大的优点，这意味着它几乎可以做任何NLP的下游任务，具备普适性，这是很强的。</p>
<p><img src="https://i.loli.net/2020/10/30/e7tSMGZjDHmY1Ck.jpg" alt="img"></p>
<p>Bert采用这种两阶段方式解决各种NLP任务效果如何？在11个各种类型的NLP任务中达到目前最好的效果，某些任务性能有极大的提升。一个新模型好不好，效果才是王道。</p>
<p><img src="https://i.loli.net/2020/10/30/RniS8uQhpDmcHN6.jpg" alt="img"></p>
<p>到这里我们可以再梳理下几个模型之间的演进关系。从上图可见，Bert其实和ELMO及GPT存在千丝万缕的关系，比如如果我们把GPT预训练阶段换成双向语言模型，那么就得到了Bert；而如果我们把ELMO的特征抽取器换成Transformer，那么我们也会得到Bert。所以你可以看出：Bert最关键两点，一点是特征抽取器采用Transformer；第二点是预训练的时候采用双向语言模型。</p>
<p>那么新问题来了：对于Transformer来说，怎么才能在这个结构上做双向语言模型任务呢？乍一看上去好像不太好搞。我觉得吧，其实有一种很直观的思路，怎么办？看看ELMO的网络结构图，只需要把两个LSTM替换成两个Transformer，一个负责正向，一个负责反向特征提取，其实应该就可以。当然这是我自己的改造，Bert没这么做。那么Bert是怎么做的呢？我们前面不是提过Word2Vec吗？我前面肯定不是漫无目的地提到它，提它是为了在这里引出那个CBOW训练方法，所谓写作时候埋伏笔的“草蛇灰线，伏脉千里”，大概就是这个意思吧？前面提到了CBOW方法，它的核心思想是：在做语言模型任务的时候，我把要预测的单词抠掉，然后根据它的上文Context-Before和下文Context-after去预测单词。其实Bert怎么做的？Bert就是这么做的。从这里可以看到方法间的继承关系。当然Bert作者没提Word2Vec及CBOW方法，这是我的判断，Bert作者说是受到完形填空任务的启发，这也很可能，但是我觉得他们要是没想到过CBOW估计是不太可能的。</p>
<p>从这里可以看出，在文章开始我说过Bert在模型方面其实没有太大创新，更像一个最近几年NLP重要技术的集大成者，原因在于此，当然我不确定你怎么看，是否认同这种看法，而且我也不关心你怎么看。其实Bert本身的效果好和普适性强才是最大的亮点。</p>
<p><img src="https://i.loli.net/2020/10/30/LO9j7cIxEJCe2Ay.jpg" alt="img"></p>
<p>那么Bert本身在模型和方法角度有什么创新呢？就是论文中指出的Masked 语言模型和Next Sentence Prediction。而Masked语言模型上面讲了，本质思想其实是CBOW，但是细节方面有改进。</p>
<p><img src="https://i.loli.net/2020/10/30/75DVNACdHgtRbS9.jpg" alt="img"></p>
<p>Masked双向语言模型向上图展示这么做：随机选择语料中15%的单词，把它抠掉，也就是用[Mask]掩码代替原始单词，然后要求模型去正确预测被抠掉的单词。但是这里有个问题：训练过程大量看到[mask]标记，但是真正后面用的时候是不会有这个标记的，这会引导模型认为输出是针对[mask]这个标记的，但是实际使用又见不到这个标记，这自然会有问题。为了避免这个问题，Bert改造了一下，15%的被上天选中要执行[mask]替身这项光荣任务的单词中，只有80%真正被替换成[mask]标记，10%被狸猫换太子随机替换成另外一个单词，10%情况这个单词还待在原地不做改动。这就是Masked双向语音模型的具体做法。</p>
<p><img src="https://i.loli.net/2020/10/30/MTCajrZPKuF51Ne.jpg" alt="img"></p>
<p>至于说“Next Sentence Prediction”，指的是做语言模型预训练的时候，分两种情况选择两个句子，一种是选择语料中真正顺序相连的两个句子；另外一种是第二个句子从语料库中抛色子，随机选择一个拼到第一个句子后面。我们要求模型除了做上述的Masked语言模型任务外，附带再做个句子关系预测，判断第二个句子是不是真的是第一个句子的后续句子。之所以这么做，是考虑到很多NLP任务是句子关系判断任务，单词预测粒度的训练到不了句子关系这个层级，增加这个任务有助于下游句子关系判断任务。所以可以看到，它的预训练是个多任务过程。这也是Bert的一个创新。</p>
<p><img src="https://i.loli.net/2020/10/30/XQ17TqJcYpPoA3i.jpg" alt="img"></p>
<p>上面这个图给出了一个我们此前利用微博数据和开源的Bert做预训练时随机抽出的一个中文训练实例，从中可以体会下上面讲的masked语言模型和下句预测任务。训练数据就长这种样子。</p>
<p><img src="https://i.loli.net/2020/10/30/E1vcQhzTsgbLqkF.jpg" alt="img"></p>
<p>顺带讲解下Bert的输入部分，也算是有些特色。它的输入部分是个线性序列，两个句子通过分隔符分割，最前面和最后增加两个标识符号。每个单词有三个embedding:位置信息embedding，这是因为NLP中单词顺序是很重要的特征，需要在这里对位置信息进行编码；单词embedding,这个就是我们之前一直提到的单词embedding；第三个是句子embedding，因为前面提到训练数据都是由两个句子构成的，那么每个句子有个句子整体的embedding项对应给每个单词。把单词对应的三个embedding叠加，就形成了Bert的输入。</p>
<p><img src="https://i.loli.net/2020/10/30/wHfCcyhaiXPMx1d.jpg" alt="img"></p>
<p>至于Bert在预训练的输出部分如何组织，可以参考上图的注释。</p>
<p><img src="https://i.loli.net/2020/10/30/B5wItXbYpPr619Z.jpg" alt="img"></p>
<p>我们说过Bert效果特别好，那么到底是什么因素起作用呢？如上图所示，对比试验可以证明，跟GPT相比，双向语言模型起到了最主要的作用，对于那些需要看到下文的任务来说尤其如此。而预测下个句子来说对整体性能来说影响不算太大，跟具体任务关联度比较高。</p>
<p><img src="https://i.loli.net/2020/10/30/u9mKAEGjp4fa7ys.jpg" alt="img"></p>
<p>最后，我讲讲我对Bert的评价和看法，我觉得Bert是NLP里里程碑式的工作，对于后面NLP的研究和工业应用会产生长久的影响，这点毫无疑问。但是从上文介绍也可以看出，从模型或者方法角度看，Bert借鉴了ELMO，GPT及CBOW，主要提出了Masked 语言模型及Next Sentence Prediction，但是这里Next Sentence Prediction基本不影响大局，而Masked LM明显借鉴了CBOW的思想。所以说Bert的模型没什么大的创新，更像最近几年NLP重要进展的集大成者，这点如果你看懂了上文估计也没有太大异议，如果你有大的异议，杠精这个大帽子我随时准备戴给你。如果归纳一下这些进展就是：首先是两阶段模型，第一阶段双向语言模型预训练，这里注意要用双向而不是单向，第二阶段采用具体任务Fine-tuning或者做特征集成；第二是特征抽取要用Transformer作为特征提取器而不是RNN或者CNN；第三，双向语言模型可以采取CBOW的方法去做（当然我觉得这个是个细节问题，不算太关键，前两个因素比较关键）。Bert最大的亮点在于效果好及普适性强，几乎所有NLP任务都可以套用Bert这种两阶段解决思路，而且效果应该会有明显提升。可以预见的是，未来一段时间在NLP应用领域，Transformer将占据主导地位，而且这种两阶段预训练方法也会主导各种应用。</p>
<p>另外，我们应该弄清楚预训练这个过程本质上是在做什么事情，本质上预训练是通过设计好一个网络结构来做语言模型任务，然后把大量甚至是无穷尽的无标注的自然语言文本利用起来，预训练任务把大量语言学知识抽取出来编码到网络结构中，当手头任务带有标注信息的数据有限时，这些先验的语言学特征当然会对手头任务有极大的特征补充作用，因为当数据有限的时候，很多语言学现象是覆盖不到的，泛化能力就弱，集成尽量通用的语言学知识自然会加强模型的泛化能力。如何引入先验的语言学知识其实一直是NLP尤其是深度学习场景下的NLP的主要目标之一，不过一直没有太好的解决办法，而ELMO/GPT/Bert的这种两阶段模式看起来无疑是解决这个问题自然又简洁的方法，这也是这些方法的主要价值所在。</p>
<p>对于当前NLP的发展方向，我个人觉得有两点非常重要，一个是需要更强的特征抽取器，目前看Transformer会逐渐担当大任，但是肯定还是不够强的，需要发展更强的特征抽取器；第二个就是如何优雅地引入大量无监督数据中包含的语言学知识，注意我这里强调地是优雅，而不是引入，此前相当多的工作试图做各种语言学知识的嫁接或者引入，但是很多方法看着让人牙疼，就是我说的不优雅。目前看预训练这种两阶段方法还是很有效的，也非常简洁，当然后面肯定还会有更好的模型出现。</p>
<p>完了，这就是自然语言模型预训练的发展史。</p>
<h3 id="参考">参考</h3>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/49271699" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/49271699</a></p>
</blockquote>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
  </entry>
  <entry>
    <title>2020-10-30-论文分享</title>
    <url>/2020/10/30/2020-10-30-%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/</url>
    <content><![CDATA[<h3 id="xlnet-generalized-autoregressive-pretraining-for-language-understanding">XLNet: Generalized Autoregressive Pretraining for Language Understanding</h3>
<p><del>BERT和GPT都是只使用了transformer的encoder和decoder部分，原本transformer层也是可以attend to 双向的，但是GPT为了要基于前面的序列预测下一个word，所以只有上文信息，所以像decoder一样mask掩码掉了，只能利用上文的信息；而BERT没有进行掩码，为了更加利用好双向的关系，BERT在transformer的基础上使用了MLM的策略，主要处理的是自然语言理解的任务。</del></p>
<blockquote>
<p>NIPS 2019</p>
<p>authors ： ZhilinYang , ZihangDai （Carnegie Mellon University, Google AI BrainTeam ）</p>
<p>code url (official tf) : <a href="https://github.com/zihangdai/xlnet" target="_blank" rel="noopener" class="uri">https://github.com/zihangdai/xlnet</a></p>
<p>code url (unofficial torch): <a href="https://github.com/huggingface/transformers" target="_blank" rel="noopener" class="uri">https://github.com/huggingface/transformers</a></p>
</blockquote>
<p><del>论文名字的含义：一般的AR模型更适合处理自然语言生成的任务，比如transformer、transformer-XL； 而AE更适合处理自然语言理解的任务。本文通过XLNet模型，能够是AR预训练（结合了transformer-XL的思想）能够泛化到处理多个自然语言理解的问题上（与BERT功能类似）</del></p>
<h4 id="背景">背景</h4>
<p><img src="https://i.loli.net/2020/11/01/MmsCokbayGLrXdW.png" alt="image-20201031210916531"></p>
<p>AR语言模型（transformer-XL）只是训练编码一个单向的上下文，然而这限制了其在更多应用场景的效果，比如阅读理解这种任务，在做任务的时候是可以允许同时看到上文和下文一起做决策的。 所以AR语言模型适合自然语言生成的任务 （机器翻译等）</p>
<p><img src="https://i.loli.net/2020/11/01/8WRguEihUAc1P6G.png" alt="image-20201031211538083"></p>
<p>因为没有将密度估计作为目标函数的一部分，所以AE语言模型（BERT）就可以获取双向信息，利用上下文信息进行重建masked token。适合自然语言理解任务（阅读理解，问答等）</p>
<p>借助对双向上下文进行建模的功能，像BERT这种的基于denoising autoencoding （AE）比基于autoregressive language modeling（AR）的方法具有更好的性能。</p>
<h4 id="问题">问题</h4>
<p><img src="https://i.loli.net/2020/11/01/LzoWnBEO7GcY5IZ.png" alt="image-20201101103324693"></p>
<p>BERT的MLM策略的缺点</p>
<ol type="1">
<li><p>mask掉的词之间的联系忽略了，即BERT假设被mask掉的词之间是独立无依赖的</p></li>
<li><p>pretrain （有mask）和fine-tune（无mask）直接有区别 （pretrain-ﬁnetune discrepancy）</p></li>
</ol>
<h4 id="解决">解决</h4>
<p><img src="https://i.loli.net/2020/11/01/XfrVJ79nZQezpKw.png" alt="image-20201031212429943"></p>
<p>本文结合AR LM和AE LM，在Transformer-XL的基础上提出generalized autoregressive method，也就是XLNet。</p>
<p>（1）没有使用BERT的MLM，而是用的是PLM策略。即通过最大化因式分解的所有排列的预期似然性来实现双向上下文学习，克服MLM的缺点。</p>
<p>所有的分解序列作为一个集合，对所有采样序列，XLNet按照AR LM的计算方式求对数似然期望的极大值。</p>
<p>（2）XLNet将来自最先进的自回归模型Transformer-XL的思想（segment recurrence mechanism和relative encoding scheme）整合到预训练中，能够提升涉及长文本序列时的效果</p>
<p>（3）引入Masked Two-Stream Self-Attention 策略来解决PLM出现的目标预测歧义（the ambiguity in target prediction）问题</p>
<p>每一步在随机排列之后的token，进行的都是一个AR语言模型的过程（在排列顺序中，根据前面的token来预测当前的token ），这样进行T次（具体次数是超参），原则上就可以克服AR中只能看到原始序列顺序之前token的缺点，可以关注到双向信息。</p>
<h4 id="模型">模型</h4>
<h5 id="背景知识">背景知识</h5>
<p>给定文本序列x=[x1,…,xT]，语言模型(AR)的目标是调整参数使得训练数据上的似然函数最大：</p>
<p><img src="https://i.loli.net/2020/11/01/Zcnh7PNyGksS2JV.png" alt="image-20201031221410608"></p>
<p>记号x&lt;t表示t时刻之前的所有x，也就是x1:xt−1。hθ(x1:t−1)是RNN或者Transformer。e(x)是词x的embedding。</p>
<p>BERT是去噪(denoising)自编码的方法。对于序列x，BERT会随机挑选15%的Token变成[MASK]得到带噪声版本的x^。假设被Mask部分的原始值为x¯，那么BERT希望尽量根据上下文恢复(猜测)出原始值了：</p>
<p><img src="https://i.loli.net/2020/11/01/OXGfrqR6PZKTiEC.png" alt="image-20201031221357201"></p>
<p>上面的公式中，mt=1表示xt被Mask掉，Hθ是一个Transformer，它把长度为T的序列x映射为隐状态的向量序列。</p>
<p>不同点：</p>
<ol type="1">
<li>BERT是“≈” ，因为BERT假设被mask掉的词之间是独立无依赖的，没考虑之间的关系，而AR是“=”</li>
<li>BERT的输入是BERT会随机挑选15%的Token变成[MASK]得到带噪声版本的x^，而AR是原始的x序列</li>
<li>BERT可以获取上下文的双向信息，而AR只能获得上文token信息</li>
</ol>
<h5 id="permutation-language-modeling-plm">Permutation Language Modeling （ PLM ）</h5>
<h6 id="思想">思想</h6>
<p>提出了一种序列语言建模目标，它不仅可以保留 AR 模型的优点，同时也允许模型捕获双向语境。</p>
<p>具体来说，一个长度为 T 的序列 x 拥有 T! 种不同的排序方式，可以执行有效的自回归因式分解。在本文中选择了T种</p>
<p>如果模型参数在所有因式分解顺序中共享，那么预计模型将学习从两边的所有位置上收集信息。</p>
<p>以下图为例，对于序列[1,2,3,4]有24种排列方式，那么下图中四种排列方式的，该序列的期望函数分别是：</p>
<p><img src="https://i.loli.net/2020/11/01/zNDVbehHL9SRtYf.png" alt="image-20201101004521295"></p>
<p><img src="https://i.loli.net/2020/11/01/Hnt4SWwmQPdbloB.png" alt="image-20201031223904029"></p>
<p>在给定相同输入序列 x（但因式分解顺序不同）时预测 token x3的示例</p>
<p>相比于普通的语言模型只能学习一种方向的依赖关系，排列语言模型会学习各种顺序的预测方法</p>
<p>问题：遍历T!种路径，然后学习语言模型的参数。计算量非常大</p>
<p>解决：随机采样T！中的部分排列</p>
<p>PLM的目标是调整模型参数使得下面的似然概率最大：</p>
<p><img src="https://i.loli.net/2020/11/01/cBRaNlDnrYuoF37.png" alt="image-20201031224727513"></p>
<p>其中ZT表示长度为T的序列的所有排列组成的集合，则z∈ZT是一种排列方法。</p>
<p>用Xzt表示排列的第t个元素，zt表示第t个位置，而z&lt;t表示z的第1到第t-1个元素。</p>
<p>这样pretrain和finetune阶段就一样了，输入都是原始序列，通过attention mask实现随机产生的排列。例如排列是2-4-3-1，那么在预测X3的时候就只有2、4作为先验，并且2、4的位置信息是通过Zt来体现的，这样也保留了排列的时序信息。</p>
<p>注意：上面的模型只会遍历概率的分解顺序，并不会改变原始词的顺序。</p>
<p>实现：</p>
<p>通过Attention Mask来对应不同的分解方法。比如p(x1|x3)p(x2|x1x3)p(x3)，我们可以在用Transformer编码x1时候让它可以Attend to x3，而把x2Mask掉；编码x3的时候把x1,x2都Mask掉。</p>
<p><img src="https://i.loli.net/2020/11/01/qzQkLmKdMcPrUy7.png" alt="image-20201031235156078"></p>
<p>将上述的策略结合AR语言模型，那么就可以避免BERT的问题</p>
<h5 id="基于目标感知的双流注意力模型">基于目标感知的双流注意力模型</h5>
<p>问题：</p>
<p><img src="https://i.loli.net/2020/11/01/SUGq9x5MBKeTpsZ.png" alt="image-20201101005004082"></p>
<p><img src="https://i.loli.net/2020/11/01/UothZ8Lk6S5AWqG.png" alt="image-20201101005315581"></p>
<p>这两个概率不应该相等的，但是对比这两个公式会发现，这两个公式的概率是相等的。为什么会出现这样的情况呢？上面问题的关键是<strong>模型无法知道当前mask掉的文本在原始序列中的位置。在Transformer中输入的embedding会加入position embedding，输入已经带入了位置信息，但是我们重新排列之后模型无法预测当前位置在原始序列中的位置，因此我们需要让模型来预测当前文本的位置。</strong> 那么在模型中当前位置的文本的概率计算方式则如下所示，其中g（θ）不仅需要输入当前位置之前的文本，还需要输入他们在原始文本中的位置。</p>
<p><img src="https://i.loli.net/2020/11/01/RWPnZ5dwYAIbU2c.png" alt="image-20201101005459040"></p>
<p>XLNet 打乱了句子的顺序，这时在预测的时候 token 的位置信息会非常重要，同时在预测的时候也必须将 token 的内容信息遮掩起来 (否则输入包含了要预测的内容信息，模型就无法学到知识)。<strong>也就是说 XLNet 需要看到 token 的位置信息，但是又不能看到 token 的内容信息</strong></p>
<h6 id="双流self-attention">双流self-attention</h6>
<p><strong>1.Query Stream</strong>，对于每一个 token，其对应的 Query Stream 只包含了该 token 的位置信息，注意是 token 在原始句子的位置信息，不是重新排列的位置信息。</p>
<p><strong>2.Content Stream</strong>，对于每一个 token，其对应的 Content Stream 包含了该 token 的内容信息。</p>
<p>查询表征单元(Query Representation)：查询表征单元和我们上述需要注意的点相同，<strong>可以看到上下文的信息和当前位置，不可以看到当前的Token</strong>，例如[1,2,3,4]在第4个位置只能看到[1,2,3]。查询表征单元中矩阵Q由于计算了各个位置的信息，保留了当前位置，但是KV矩阵分别表示各个context的重要性，没有计算当前位置。</p>
<p><img src="https://i.loli.net/2020/11/01/CU1PnWlaArwDN8E.png" alt="img"></p>
<p>内容表征单元(Context Representation):内容表征单元和我们上文中说的Transformer一致，<strong>可以看到上下文的信息和当前的Token</strong>，例如文本序列[1,2,3,4]，在第4个位置，内容表征单元可以看到[1,2,3,4]，在第3个位置内容表征单元可以看到[1,2,3]。如下图所示QKV矩阵的计算都包含了当前位置。</p>
<p><img src="https://i.loli.net/2020/11/01/xD53wJVQsdIoC7i.png" alt="img"></p>
<p><strong>Query Stream 和 Content Stream 组合</strong></p>
<p>XLNet 将 Query Stream 和 Content Stream 组合在一起，整体架构如下图所示。</p>
<p><img src="https://i.loli.net/2020/11/01/wyni1NAtfYsGrV4.png" alt="image-20201101011712221"></p>
<p>图中最下面的一层是输入层，其中 e(x) 是单词的词向量，表示输入的 Content Stream，而 w 表示输入的位置信息，即 Query Stream。</p>
<p>图中的掩码矩阵，红色表示不遮掩，白色表示遮掩。第 1 行表示 token 1 的掩码，可以看到，1 是句子的最后一个 token，因此可以看到之前的所有 token (3,2,4)。3 是句子的第一个 token，看不到句子的任何信息，因此第 3 行都是白色的 (表示遮掩)。</p>
<h6 id="partial-prediction">Partial Prediction</h6>
<p>XLNet 将句子重新排列，然后根据排列后的顺序使用 AR 方式预测，但是由于句子是随机排列的，会导致优化比较困难且收敛速度慢。因此 XLNet 采用了 Partial Prediction (部分预测) 的方式进行训练，对于排列后的句子，只预测句子末尾的 1/K 个 token。</p>
<p>例如 K=4，就是只预测最后 1/4 的 token。给定句子 [1,2,3,4,5,6,7,8] 和一种随机排列 [2,8,3,4,5,1,7,6]，则只预测 7 和 6。论文中训练 XLNet-Large 时使用的 K 为 6，大约是预测末尾 <strong>14.3%</strong>的 token。</p>
<h4 id="实验">实验</h4>
<p><img src="https://i.loli.net/2020/11/01/i7qlvJjTtxAgw4a.png" alt="image-20201101012324865"></p>
<p><img src="https://i.loli.net/2020/11/01/pIOMyiU9V2Y6wSz.png" alt="image-20201101012421507"></p>
<p>消融实验</p>
<p>排列语言模型和transfomer-xl对效果的影响很大。而且NSP任务对效果的影响倒是几乎没有，这也是上文中我们没有用NSP任务的原因。</p>
<h4 id="总结">总结</h4>
<p>XLNet 的核心思想是 PLM，排列原来的句子，然后预测末尾的单词。这样可以学习到单词之间的依赖关系，而且可以利用 token 前后向的信息。</p>
<blockquote>
<p><a href="http://aiblog.top/2019/07/12/XLNET%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E5%8F%8A%E7%90%86%E8%A7%A3/" target="_blank" rel="noopener">http://aiblog.top/2019/07/12/XLNET%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E5%8F%8A%E7%90%86%E8%A7%A3/</a></p>
<p><a href="https://my.oschina.net/u/4373067/blog/4476706" target="_blank" rel="noopener" class="uri">https://my.oschina.net/u/4373067/blog/4476706</a></p>
<p><a href="https://baijiahao.baidu.com/s?id=1654814515140351919&amp;wfr=spider&amp;for=pc" target="_blank" rel="noopener" class="uri">https://baijiahao.baidu.com/s?id=1654814515140351919&amp;wfr=spider&amp;for=pc</a></p>
</blockquote>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
  </entry>
  <entry>
    <title>2020-10-30-预训练模型总结</title>
    <url>/2020/10/30/2020-10-30-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h3 id="前言">前言</h3>
<p>Bert模型自18年10月推出，到目前为止快两年了。它卜一问世即引起轰动，之后，各种改进版本的预训练模型（Pre-Training Model, PTM）与应用如过江之鲫，层出不穷。Bert及它的继任者们，确实也不负众望，在NLP各个领域攻城略地，所向披靡，多种NLP数据集竞赛榜单，连续多年被各种新出现的预训练模型霸榜，有些榜单，个别模型已经把指标刷到超过人类。</p>
<p>那么，在近两年的时间里，诸多改进模型中，有哪些令人印象深刻的新模型？在那些表现突出的新模型中，是哪些因素导致它们的良好表现？预训练模型技术本身有重大的改动或创新么？或者，关于预训练模型，目前有哪些相对明确的结论？根据目前的技术发展水准，如何根据现有结论，来打造最强的预训练模型？本文通过梳理现有技术文献，试图来回答上述一系列问题。本文的数据都客观有出处，但是对数据的解读，带有严重的个人色彩，偏颇难免，还请谨慎参考。</p>
<p>我们知道，在预训练模型框架下，解决NLP问题，会划分为序列进行的两阶段：第一阶段是预训练阶段，然后是Fine-tuning阶段，本文集中在预训练阶段。</p>
<p><img src="https://i.loli.net/2020/10/30/DL9wbrUlSxFBGNm.jpg" alt="img"></p>
<p>如果我们一句话宏观地归纳预训练模型要做的事情（参考上图），其实很好理解，就是下面这句话：</p>
<p>在Transformer作为特征抽取器基础上，选定合适的模型结构，通过某种自监督学习任务，逼迫Transformer从大量无标注的自由文本中学习语言知识。这些语言知识以模型参数的方式，存储在Transformer结构中，以供下游任务使用。</p>
<p>我们见到的形形色色的预训练模型，无非就是，实现上述思路的具体做法而已。你可以换个模型结构，可以换个学习任务，也可以换个其它的部件，无非就是各种试，当然，有些做法相对有效，有些做法效果差些。一般而言，通常所说的预训练模型，都是从自由文本中学习语言知识，很明显，我们可以引入新型的知识或数据，比如人类已经挖掘好的结构化知识、多模态数据、多语言数据等，引入这些知识来促进模型理解语言，或者解决特殊类型的任务。</p>
<p>后文会先介绍预训练模型中常见的几种模型结构，并给出目前能得出的结论。然后，我们会找出目前表现比较好的那些预训练模型，并分析它们起作用的主要因素是什么。接下来，会简要介绍几种非自由文本类知识学习的预训练基本方法。</p>
<p>在谈这些之前，我们先从RoBERTa讲起。如果时光倒退半年多，你会发现，这是一个价值被严重低估的模型，其实，它很重要。</p>
<h3 id="预训练模型中的强基准roberta">预训练模型中的强基准：RoBERTa</h3>
<p>严格来说，原始的Bert模型是个未完成的半成品，而RoBERTa才是遵循Bert思路的完成品，或者说，Bert是进行时中的RoBERTa，也就是说下列等式成立Bert=RoBERTing。为什么这么说呢？因为，我们可以把RoBERTa看作是得到充分训练的Bert模型，而原始版本的Bert模型训练不够充分，这种模型是否得到充分训练的微小差异，能够极大提升原始版本Bert模型的效果。</p>
<p><img src="https://i.loli.net/2020/10/30/wvSEJsGYH1cpFbx.jpg" alt="img"></p>
<p>在原始Bert模型的基础上，RoBERTa通过实验，证明了如下几点：</p>
<ol type="1">
<li>进一步增加预训练数据数量，能够改善模型效果；</li>
<li>延长预训练时间或增加预训练步数，能够改善模型效果；</li>
<li>急剧放大预训练的每个Batch的Batch Size，能够明显改善模型效果；</li>
<li>拿掉预训练任务中的Next Sentence Prediction子任务，它不必要存在；</li>
<li>输入文本的动态Masking策略有帮助；</li>
</ol>
<p>上面列出的五项改进中，第四项和第五项改动，对最终的模型效果影响不大，暂时可忽略。第一点改进增加预训练数据对模型效果有帮助，这个符合直觉。第二项和第三项则涉及到模型是否得到充分训练，本质上这两项相结合，代表了更充分训练的Bert模型。如上面的性能对比图所示，如果以Bert Large作为对比基准，可以发现：仅仅将Batch Size放大，三个数据集上的效果就获得了明显提升，如果再加入新的数据，以及不断增加训练步数，还能持续获得效果的进一步提升。可以看出，RoBERTa效果明显比Bert large好，在相同数据情况下，甚至超过了知名度很高的XLNet。这主要归功于数据规模的增加，以及更充分的训练过程，其中更充分的训练过程发挥的作用更大些。这是为何说RoBERTa 在某种意义上，其实是一个完成版本或者加强版本的Bert模型。</p>
<p>纵观目前公开的预训练模型，我们可以发现，RoBERTa是其中一个效果非常好的超强基准模型。这句话有几个意思：</p>
<p>首先，尽管看上去RoBERTa也没做啥技术或者模型改进，只是把Bert模型训练得更充分了一些，但是，它的效果是非常好的。目前为止，效果能够明显超过它的模型很少，屈指可数，这个“屈指可数”，不是虚指，是它的字面含义。这个事实，其实隐含了很大的信息量，它说明了一个什么问题呢？您可以想一想，我的看法在后面小节内容会提到。</p>
<p>其次，对于一个改进模型来说，理论上都应该引入RoBERTa作为对比Baseline，而改进模型的效果，如果不能具备说服力地超过RoBERTa的话，那么这种改进的有效性，多少是成问题的，除非你强调改进模型的优势不在效果好，而在其它方面，比如更小更快等。</p>
<p>再次，后续的改进预训练模型，从策略角度讲，应该在设计之初，就站在RoBERTa的巨人肩膀上，就是说在增加一定数据量的前提下，增大Batch Size，加长预训练时间，让模型得到充分训练。因为，如果你不这么做，大概率你的效果是很难比过RoBERTa的，而目前我们能够见到的效果很突出的模型，你如果细究，会发现其实都已经引入了RoBERTa的关键要素了，关于这一点，在后面小节我们会做分析。</p>
<p>还有，对于追求落地效果的人来说，比如公司里做业务的同学，建议以RoBERTa为基础模型来做应用。</p>
<h3 id="预训练的发动机模型结构">预训练的发动机：模型结构</h3>
<p>对于预训练模型来说，目前的主流模型大都采用Transformer作为特征抽取器，现阶段看，Transformer的潜力仍然没有被充分挖掘，还有很大潜力可挖，意思是，Transformer效果足够好，而且还可以更好，貌似改进Transformer并非当务之急的事情。预训练模型的知识，是通过Transformer在训练迭代中从数据中不断学习，并以模型参数的形式编码到模型中的。虽然，大家都是用的Transformer，但是怎么用它搭建模型结构学习效率更高？这是一个问题。所谓学习效率高，就是给定相同大小规模的训练数据，它能编码更多的知识到模型里，这就意味着它的学习效率更高。不同的Transformer用法，会产生不同的模型结构，就会导致不同结构的差异化的学习效率。本节我们归纳下目前能得到的，关于模型结构的现有研究结论，会介绍常见的五种模型结构。当然，这里用模型结构来表达不足够确切，因为除了模型结构外，一般还包含自监督的学习方法，常见的学习方法包括AutoEncoding(简称AE)和AutoRegressive(简称AR)。AE即我们常说的双向语言模型，而AR则代表从左到右的单向语言模型。</p>
<ul>
<li><strong>Encoder-AE结构</strong></li>
</ul>
<p><img src="https://i.loli.net/2020/10/30/1klNabKIiu73QJq.jpg" alt="img"></p>
<p>Encoder-AE结构如上图所示。这其实是包括原始版本Bert在内的，大多数后续改进模型采取的结构。整个结构就是一个标准的Transformer，在语言模型预训练的时候，采用AE方法。也就是说，输入句中的未被Mask的任意单词两两可见，但是被Mask掉的单词之间都相互独立，互不可见。在预测某个被Mask掉的单词的时候，所有其它被Mask的单词都不起作用，但是句内未被Mask掉的所有单词，都可以参与当前单词的预测。可以看出，Encoder-AE是个采用双向语言模型的单Transformer结构。</p>
<p><img src="https://i.loli.net/2020/10/30/isTaJS6GdQLPBZU.jpg" alt="img"></p>
<p>模型结构比较（From BART）</p>
<p><img src="https://i.loli.net/2020/10/30/Opc45h7Z3PF1yq6.jpg" alt="img"></p>
<p>模型结构比较（From Google T5）</p>
<p>从目前对比实验看（上面两图），除了下文要讲述的Encoder-Decoder结构外，貌似对于语言理解类的NLP任务，这种结构都是效果最好的，但是对于语言生成类的任务，这种结构效果相对很差。也就是说，这种结构比较适合做语言理解类的任务。</p>
<ul>
<li><strong>Decoder-AR结构</strong></li>
</ul>
<p><img src="https://i.loli.net/2020/10/30/LWPdHeDOo5XRQK2.jpg" alt="img"></p>
<p>Decoder-AR结构如上图所示。它和Encoder-AE结构相同，都是采用单个的标准Transformer，主要区别在于：语言模型预训练的时候，采用AR方法，就是从左到右逐个生成单词，第i个单词 Wi 只能看到它之前的第1到第（i-1）个单词W1,...,Wi-1 ，不能看到后面的单词。采用这种结构的典型模型就是GPT1、GPT2、GPT3系列了。GPT3在文本生成任务方面的表现，确实是出乎意料地好。当然，这不能仅仅归功于这个结构本身，更复杂的模型和更大量的数据可能是主因。可以看出，Decoder-AR结构是个单向语言模型的单Transformer结构。</p>
<p>从目前对比实验看（参考Encoder-AE小节的两张效果对比图），除了Encoder-Decoder结构外，貌似对于语言生成类的任务，这种结构是效果最好的结构之一。但是相应的，语言理解类的任务，采用这种结构，效果比Encoder-AE结构差距非常明显，这也好理解，因为只看到上文看不到下文，对于很多语言理解类任务而言，信息损失很大，所以效果不好也在情理之中。也就是说，这种结构比较适合做语言生成类的任务。</p>
<ul>
<li><strong>Encoder-Decoder结构</strong></li>
</ul>
<p>既然Encoder-AE比较适合做语言理解类的任务，Encoder-AR比较适合做语言生成类的任务。那么，我们能否结合两者的优势，使得预训练模型既能做好生成类NLP任务，又能做好理解类任务呢？这是个很自然的想法，而Encoder-Decoder结构就是如此将两者结合的。最早明确提出使用Encoder-Decoder结构做通用领域预训练的，应该是微软提出的MASS模型，不过和这里介绍的做法有差异。</p>
<p><img src="https://i.loli.net/2020/10/30/7NG2hnjXitKTOcZ.jpg" alt="img"></p>
<p>Encoder-Decoder结构如上图所示。这种结构在Encoder侧，单独使用一个Transformer，采用了Encoder-AE的结构。也就是说，编码阶段采用双向语言模型，任意两个单词两两可见，以更充分地编码输入信息；而在Decoder侧，使用另外一个Transformer，采用了Decoder-AR结构，从左到右逐个生成单词。</p>
<p>当然，Decoder侧和标准的Decoder-AR不同的地方还是有的：Decoder侧生成的单词 Wi，除了像Decoder-AR结构一样能看到在它之前生成的单词序列 W1,...,Wi-1 外，还能看到Encoder侧的所有输入单词 。而这一般是通过Decoder侧对Encoder侧单词，进行Attention操作方式来实现的，这种Attention一般放在Encoder顶层Transformer Block的输出上。</p>
<p>在进行预训练的时候，Encoder和Decoder会同时对不同Mask部分进行预测：Encoder侧双向语言模型生成被随机Mask掉的部分单词；Decoder侧单向语言模型从左到右生成被Mask掉的一部分连续片断。两个任务联合训练，这样Encoder和Decoder两侧都可以得到比较充分地训练。</p>
<p><img src="https://i.loli.net/2020/10/30/SOAQbIj9czkhVK6.jpg" alt="img"></p>
<p>模型结构比较（From UniLM v2）</p>
<p>从目前对比实验看，无论是语言理解类的任务（参考Encoder-AE部分Google T5论文中展示的效果对比图），还是语言生成类的任务（参考上面来自于UniLM v2的效果对比），貌似Encoder-Decoder结构相对其它几种结构来说，效果都是最好的之一。而且，它有另外一个优点，就是用这个结构，可以同时做生成类和理解类的NLP任务，基本做到了不同任务在模型结构上的统一，这点还是很好的，一个结构可以到处使用，比较方便。但是，它也有个问题，因为两侧各用了一个Transformer，所以相对其它结构参数量翻倍，计算量也增加了，就是说比其它模型笨重。而且，Encoder-Decoder结构比其它结构效果好，很可能主要原因来自于参数量增加导致的模型容量增大，当然这是个人猜测。目前，采用这个结构的效果很好的模型包括Google T5以及BART等模型。</p>
<ul>
<li><strong>Prefix LM</strong></li>
</ul>
<p><img src="https://i.loli.net/2020/10/30/2b1usgtZPwJa58D.jpg" alt="img"></p>
<p>Prefix LM结构是Google T5论文中给出的叫法，这种结构最早由UniLM模型提出，我们沿用Google T5的这种称谓。如果深入分析的话，Prefix LM其实是Encoder-Decoder模型的变体：标准的Encoder-Decoder模型，Encoder和Decoder各自使用一个独立的Transformer；而Prefix LM，相当于Encoder和Decoder通过分割的方式，分享了同一个Transformer结构，Encoder部分占用左部，Decoder部分占用右部，这种分割占用是通过在Transformer内部使用Attention Mask来实现的。与标准Encoder-Decoder类似，Prefix LM在Encoder部分采用AE模式，就是任意两个单词都相互可见，Decoder部分采用AR模式，即待生成的单词可以见到Encoder侧所有单词和Decoder侧已经生成的单词，但是不能看未来尚未产生的单词，就是说是从左到右生成。</p>
<p>目前的一些对比实验证明，在其它条件相同的情况下，关于语言理解类的任务（参考Encoder-AE部分Google T5论文中的相关实验），Prefix LM结构的效果要弱于标准Encoder-Decoder结构。这里是值得深入思考下的，因为看上去Prefix LM和标准的Encoder-Decoder结构是等价的。那么，为什么它的效果比不过Encoder-Decoder结构呢？我想，一方面的原因估计是两者的参数规模差异导致的；另外一方面，可能与它这种模式的Decoder侧对Encoder侧的Attention机制有关。在Decoder侧，Transformer的每层 Block对Encoder做Attention的时候，标准的Encoder-Decoder模式，Attention是建立在Encoder侧的最后输出上，这样可以获得更全面完整的全局整合信息；而Prefix LM这种结构，Decoder侧的每层Transformer对Encoder侧的Attention，是建立在Encoder的对应层上的，因为这种模式的Encoder和Decoder分割了同一个Transformer结构，Attention只能在对应层内的单词之间进行，很难低层跨高层。这可能是影响这种结构效果的原因之一。当然这只是个人猜测，无证据证明，还请谨慎参考。</p>
<p>关于语言生成类的任务，Prefix LM效果虽然要弱于Encoder-Decoder结构（参考Encoder-Decoder小节UniLM v2论文效果对比图），但是总体而言，两者相差不大，相对其它模型，Prefix LM结构在生成类任务表现也比较突出。</p>
<p>Prefix LM因为是Encoder-Decoder的变体，所以可以看出，它的优势也在于可以同时进行语言理解和语言生成类任务，而且相对Encoder-Decoder来说，因为只用了一个Transformer，所以模型比较轻，这是Prefix LM的优势。缺点则是在效果方面，貌似要弱于Encoder-Decoder模型的效果，语言理解类任务相对有明显差距，生成类任务的效果相差不大。</p>
<ul>
<li><strong>Permuted Language Model(PLM)</strong></li>
</ul>
<p>PLM最早是在XLNet的论文中提出的，目前有些后续模型也在PLM上进行改进，所以我们把PLM也放在这里一起说一下。</p>
<p>PLM一样采用单个Transformer模型作为主干结构，但是从训练方法上来说，是个很另类也很有创意的做法，是种“形为AR，实为AE”的做法。在语言模型预训练过程中，它看上去遵循AR从左到右的输入过程，这符合一般生成任务的外在表现形式，但是在内部通过Attention Mask，实际做法其实是AE的做法，无非是把AE的做法隐藏在Transformer内部。它和AE从细节来说，主要有两个区别：首先，预训练过程中，输入句子去掉了Mask标记，改为内部Attention Mask，以保持预训练过程和下游任务Fine-tuning的一致性。关于这一点，目前有实验证明这个虽然有积极影响，但是影响不大（ELECTRA针对预训练过程是否带Mask 标记做了效果对比，带Mask标记的Bert模型GLUE得分82.2，去掉Mask标记利用其它单词代替的对比模型GLUE得分82.4）；其次，也是它和AE的最主要区别，PLM认为被Mask掉的单词之间是相互有影响的，先产生的被Mask掉的单词，应该对后生成的被Mask掉的单词，在预测的时候发生作用，而标准的AE则认为被Mask掉的单词是相互独立的，相互之间不产生作用。</p>
<p><img src="https://i.loli.net/2020/10/30/eqPo6kEBMm1Unaz.jpg" alt="img"></p>
<p>其实，如果你仔细分析下PLM的预训练过程，会发现本质上PLM是Prefix LM的一种变体。上图给出了个例子来说明这种情况，对于某个输入句子，PLM首先会进行单词顺序随机变换，然后选定变换后句子的末尾一部分单词进行Mask，被Mask的单词预测顺序是有序的，按照变换后在句中先后顺序来预测，上面例子中会先预测 X1 ，然后再预测 X5 。在预测 X1 的时候，未被Mask的上下文 [X2,X3,X4] 会对预测 X1 有帮助；假设已经预测并输出了 X1 ，在预测 X5 的时候，未被Mask掉的上下文 [X2,X3,X4] ，以及刚预测出的 X1 ，会对预测 X5 有帮助。其实你想，这等价于什么？等价于以 X4 作为边界切割开的Prefix LM模型，Encoder端包含 [X2,X3,X4] ，Decoder侧包含 [X1,X5] ，在预测 X5 的时候，不仅能看到Encoder侧的所有输入，也能看到Decoder侧之前的输出 X1 。当然，因为每个输入句子的长度各异，被Mask掉的单词个数也不固定，所以看上去Encoder和Decoder的边界根据输入句子，边界是在动态变化的。所以，PLM其实是一种边界变化的Prefix LM变体结构。当然，上面纯属个人推理过程，不保证正确性，谨慎参考。</p>
<p>如果不考虑XLNet里的其它因素，单纯看PLM结构的话，目前有些对比实验，貌似PLM在语言理解类任务中，效果不及Encoder-AE（参考UniLM v2论文中的对比实验，未在本文列出，可参考论文）；在语言生成类任务中，效果略微优于Encoder-AE，但是距离Decoder-AR差距较大（参考Encoder-AE描述部分BART的对比实验）。在两类任务中，都有点上不着村，下不着店的感觉，就是都还可以，但都不够好的感觉。XLNet效果确实是很好的，但是，这说明XLNet效果好，真正起作用的貌似不是PLM，而是其它因素。</p>
<p>上面内容简述了常见的五种预训练模型结构，如果总结一下的话：</p>
<p>首先，从模型效果来看，Encoder-Decoder结构无论在语言理解类还是语言生成类任务中，都是效果最好的。当然，效果好的原因很可能在于模型参数多，模型容量大，而不一定是自身结构带来的优势。它的优点一个是效果好，一个是能够将理解和生成任务统一在一个框架下；缺点是参数多计算多，所以模型比较重。采用这个结构的代表模型包括Google T5和BART。</p>
<p>其次，因为Encoder-Decoder模型比较重，所以，如果从相对轻量结构里进行选择的话，对于语言理解类任务，Encoder-AE结构相对而言效果较好，代表模型很多，典型的比如ALBert、RoBERTa；对于语言生成类任务，Decoder-AR结构和Prefix LM结构相对而言效果较好，都可考虑，Decoder-AR的代表模型是GPT系列，Prefix LM的代表模型是UniLM。语言理解类任务应该用AE任务，语言生成类任务应该用AR任务，这点也很明确了。</p>
<p>谈完了模型结构，下面我们来盘点下表现比较好的预训练模型，并分析下效果好背后的原因。</p>
<h3 id="强者的狂欢为什么有些模型表现这么好">强者的狂欢：为什么有些模型表现这么好</h3>
<p>目前Bert的改进模型有很多，有的表现非常突出，有的表现一般。我的主要目的是想找出那些表现好的模型，并分析下，到底是哪些因素导致这些模型效果超群的。</p>
<p>首先，我们需要先找出那些表现特别好的模型出来，我这里说的表现好，主要是从模型效果角度来说的，就是那些在公开数据集上指标比较高的模型。一种比较简单的方法就是：找GLUE、SuperGLUE、SQuAD 2.0这几个大规模NLP数据上，那些打榜模型中排名前列的。你可以看一下，自从Bert出现后，这几个榜单，都长年被预训练模型霸榜，指标在被各种新的预训练模型快速刷高，直到超过人类的水准。一般而言，能够打榜把指标刷到前列的，都是好模型，说明这些模型真的能打（插句闲话，这点其实特别值得推荐领域借鉴，就是有个大规模高难度数据集，供各种模型长年刷榜，这其实是促进领域技术进步很好的手段）。当然，也有一些新模型，可能未必会去打榜，所以作为补充措施，我又从比较新的文献中，找出一些模型，前提是它在文献中报道的效果要比RoBERTa好。这样，我筛出了一批表现优秀的模型，包括：RoBERTa，Google T5，ALBERT，ELECTRA，XLNet，GPT3，BART，UNILM v2, StructBert，MacBert。这些模型要么在某个榜单前几名，要么论文实验结果显示效果非常好，二者占其一。这里面，GPT3是个纯生成模型，ELECTRA相对而言方法比较特殊，在后面我会单独说下它。需要说明的是，ERNIE和NEZHA模型，效果也是非常好的，能够排在某些榜单前列。但是因为它们对应的论文比较早，我猜测现在打榜的模型，估计和原始论文中的做法，已经做了变动，但是具体怎么变的不清楚，所以没有在上面列表中列出。上述表单，应该基本囊括了目前时间（2020年9月）绝大多数效果最好的预训练模型了。</p>
<p>上述模型，都能找到对应的文章，可供仔细分析模型的有效因素。如果你仔细分析上述各个模型的共性，会发现，那些真正有效的因素会慢慢浮出水面。我在这里归纳一下：促进模型性能快速提高的因素，主要包含下列几方面。而且，这几方面的因素是可叠加的，就是说，如果一个模型采纳其中越多的因素，那么这个模型的效果表现可能会更好。</p>
<p>首先，更高质量、更多数量的预训练数据。</p>
<p><img src="https://i.loli.net/2020/10/30/E8VF4gCRMDjSlbm.jpg" alt="img"></p>
<p>关于预训练数据对模型效果的影响，Google T5做了大量对比实验，目前的结论，如果归纳一下的话，应该是这样的：在保证预训练数据质量的前提下，数据规模越大模型效果越好。这里需要注意的是，数据规模越大越好，这点其实从Bert一出来，就是一个容易想到的重要因素。因为数据量越多，数据里蕴含的知识也越多，那么模型能学到的东西越多，所以模型效果会更好，这是一个靠简单推理就能得出的结论。但是，它是有前提的，前提是数据质量要高，光数据量大不行，很多乱七八糟的数据，反而会对模型效果带来负面影响。</p>
<p>第二，增加模型容量及复杂度。</p>
<p><img src="https://i.loli.net/2020/10/30/YuIyA7FQh3JCxpW.jpg" alt="img"></p>
<p>所谓增加模型容量及复杂度，指的是增加Transformer模型的参数量，一般而言，模型容量越大，模型的表达能力越强。最直接的增加模型容量的方式就是增加Transformer Block层深，比如可以从Bert base的12层，增加到Bert Large的24层，还可以继续增加到比如36层，这是纵向增加复杂度，Google T5走的这条路（从上图可以看出，模型容量增加到4倍后，有些数据集效果相对Baseline有大幅度的提升）。除此外，还可以横向增加模型复杂度，比如在固定Transformer层深的情况下，可以通过放大Transformer中构件的大小，比如Hidden Size的增大，FFN层对隐层的放大，Multi-Head Self Attention的Attention头的增加，等多种方式来做到这一点。ALBERT走的这条路，它的xxLarge模型效果最好，只用了12层Transformer Block，但是Hidden Size达到了4096。</p>
<p>这两种模式还可以相互结合，就是同时纵向和横向增加模型复杂度，GPT 3即是如此，将模型复杂度这点推到了极致。单词特征的Embedding不会放的太大，一般采用64或者128大小，ALBERT证明了如果单词特征Embedding跟着Transformer内部的Hidden Size同步放大，效果反而会降低。也就是说，增加模型容量指的是放大Transformer模型本身的参数量，但不包括输入层Embedding的参数。</p>
<p>第三，更充分地训练模型；</p>
<p>这里所谓的“更充分”，一般指的是放大Batch Size、增加预训练步数，就是RoBERTa做的那两个事情。这块上文有述，这里不再赘述。</p>
<p>第四，有难度的预训练任务；</p>
<p><img src="https://i.loli.net/2020/10/30/sILxebBoQuO7wt4.jpg" alt="img"></p>
<p>原始的Bert预训练，有两个训练任务：一个是单词级的Mask语言模型MLM，一个是句子级的下一句预测任务NSP。RoBERTa证明了NSP对于模型效果没什么影响，所以拿掉了这个任务。有很多研究集中在这一块，采取了五花八门的预训练任务（如上图所示）。那么哪些预训练任务相对而言更有效呢？目前已经能够得出些比较明确的结论。</p>
<p>如果归纳一下的话，应该是这样的：对于单词级的Mask语言模型来说，Span类的预训练任务效果最好。所谓Span类的任务，就是Mask掉的不是一个独立的单词，而是一个连续的单词片断，要求模型正确预测片断内的所有单词。Span类任务，只是一个统称，它会有一些衍生的变体，比如N-Gram，就是Span模型的一个变体，再比如Mask掉的不是单词而是短语，本质上也是Span类任务的变体，这里我们统称为Span类任务。</p>
<p>目前有相当多的研究证明Span类任务是效果最好的，最近有些工作（微软的ProphetNet和百度的ERNIE-GEN）进一步说明，Span内多个单词独立被生成效果会更好。所谓独立生成，举个例子，假设被Mask掉的片断是：X1,X2,X3 ，之前一般Span类的预训练是顺序生成片断内的单词，就是先生成 X1 ，然后根据上下文及 X1 ，生成 X2 ，这么个顺序，就是说序列生成片断内单词。而独立生成，就是根据上下文，同时生成 X1,X2,X3 , 被生成的单词之间无影响。所以目前单词级的Mask语言模型，独立生成的Span类任务，应该是目前效果最好的。</p>
<p>对于句子级的任务，NSP任务学习两个句子是否连续句：正例由两个连续句子构成，负例则随机选择一句跟在前一句之后，要求模型预测两者是否连续句子。本质上，NSP在预测两个句子是否表达相近主题，而这个任务，相对MLM来说，过于简单了，导致模型学不到什么知识。ALBERT采用了句子顺序预测SOP（Sentence Order Prediction）：跟NSP一样，两个连续出现的句子作为正例，但是在构造负例的时候，则交换句子正确顺序，要求模型预测两个句子出现顺序是否正确，这样增加任务难度，StructBERT也采取了类似的做法。实验证明SOP是有效的句子级预测任务。</p>
<p>总而言之，目前证明Span类任务是有效的单词级任务，SOP是有效的句子级任务。目前看，预训练任务越有难度，则预训练模型越能高效率地学习知识，所以寻找更新的更有难度的预训练任务是有较大探索空间以及成功可能的。</p>
<p>上面列了四个主要因素，那么，还有其它因素么？我的猜测是基本没有了，尽管可能还有一些差异化的改进点是有效的，但它对最终效果的贡献，应该不是特别大，起码不像上述四个因素那么大。上面四个因素，如果进一步要划分重要性的话，估计前三个都很重要，第四个相对而言影响稍小一些。当然，同样地，这是我个人的猜测，谨慎参考。</p>
<p><img src="https://i.loli.net/2020/10/30/jeYAPHi61MaJFNf.jpg" alt="img"></p>
<p>如果我们根据上述可叠加的有效因素，来分析现有模型，可得出如上图所示列表（具备某因素的模型，对应的格子做了标记）。从上表中，我们可以得出一些结论：</p>
<p>首先，所有这些效果表现突出的模型，都增加了更多的高质量预训练数据。另外，通过增大Batch Size以及增加预训练步数方式，都使得模型得到更充分地训练。也就是说，所有这些表现突出的模型，都是站在RoBERTa模型的肩膀上的。其实，只要你站在RoBERTa肩膀上，效果都不会太差，剩下的问题是能比它好多少的问题。</p>
<p>其次，如果我来冒昧地做个判断的话，貌似对于语言理解类任务来说，估计Google T5和ALBERT是效果最好的预训练模型；而对于语言生成类的任务来说，估计GPT3是效果最好的模型。对于Google T5和ALBERT模型来说，两者都采纳了绝大部分有效因素，主要不同在于预训练任务，Google T5采用了Span类单词级任务，而ALBERT采用了SOP类句子级任务。这三个表现最突出的模型，和其它模型最大的区别，大概率在于它们在增加更多高质量数据的同时，走了大规模提升模型容量的路子。也就是说，在增加数据规模基础上大规模增加模型容量，这应该是拉开不同模型效果最主要的因素。</p>
<p>再次，我们可以据此预测，如果一个模型，采纳了上述所有有效因素，那么可以获得当前技术水准下的最好模型效果，就如上表中最后一行展示的，目前仍未知的Model X那样。就是说，这个模型应该是这样的：在RoBERTa模型基础上，增加更多高质量数据的同时，充分放大模型容量，而预训练任务则是单词类Span任务和句子类SOP任务的结合。当然，估计这里面起到主要作用的还是大量数据+大模型的因素。</p>
<p><img src="https://i.loli.net/2020/10/30/BPVtOXFLJye1ZEC.jpg" alt="img"></p>
<p>这里单独说下ELECTRA，这是一个比较独特的预训练方法(参考上图)。 它形式上采取了类似GAN的模式，但是本质上并非GAN，因为缺乏GAN最关键的生成器和判别器的对抗训练过程。ELECTRA联合训练了小的生成器以及大的判别器，它强迫判别器对生成器产生的所有单词，做个是否经过改写的判断，这无疑增加了模型的学习效率，因为原先的MLM只学习15%的被Mask单词，而ELECTRA对所有单词都要进行判断，并从中学习。ELECTRA论文做了分析，模型的绝大多数收益来自于全部单词参与训练这一步。这意味着，ELECTRA这种所有单词全员参与训练过程的模式，能够在其它条件相同的情况下（模型复杂度，数据量等），使得模型获得更高的学习效率，这个结论和做法还是很有价值的。本质上，ELECTRA这种提升模型效率的方法，和上面所述其它模型的各种做法，是相互互补的。就是说，在ELECTRA的训练模式下，增加训练数据、增加模型规模、模型充分训练，有可能获得更好的模型效果。</p>
<h2 id="暴力美学简单粗暴但有效"><strong>暴力美学：简单粗暴但有效</strong></h2>
<p>前文有述，RoBERTa是个非常强的Baseline，相对目前表现最强的Google T5和ALBERT模型，其实RoBERTa与这两个天花板模型之间，它们之间的性能Gap并不是特别大。其它表现突出的模型，要我猜，性能应该介于RoBERTa这个Baseline和两个天花板模型之间。而所有这些模型之间的主要差异，极有可能是模型容量的大小差异带来的。</p>
<p>从某种角度上看，我们可以认为：RoBERTa可以被看作是经过更充分训练的Bert模型，而ALBERT/Google T5可以理解为进一步增加了模型复杂度的RoBERTa增强版本。从Bert到RoBERTa，再到ALBERT/Google T5，这三类模型，很可能代表了自Bert出现来的最主要技术进展。所以，从模型改进的角度看，自从Bert诞生后近两年，并没有出现特别有效的模型改进方法。尽管从解决NLP任务效果的角度看，新的预训练模型相比Bert有了巨大的提升，但是这些提升，大致可以理解为是因为引入更多高质量数据、采用更多模型参数、模型训练更充分以及增加训练任务难度这几点综合导致的。而其中，在RoBERTa这种充分训练的模型基础上，增加数据，并加上更大的模型，可能在其中起到了主导作用。</p>
<p>由此进一步推理，我们可以得出如下结论：目前预训练模型都采用的Transformer结构，从模型容量或模型复杂度来说是足够复杂的。就是说，Transformer结构本身，目前并非制约预训练模型效果的瓶颈，我们可以仅仅通过增加高质量数据、增加模型复杂度配以更充分地模型训练，就仍然能够极大幅度地提升Bert的性能。</p>
<p>这说明了什么呢？这说明了大数据+大模型的暴力美学，这条粗暴简洁但有效的路子，还远远没有走到尽头，还有很大的潜力可挖。尽管这带来的副作用是：好的预训练模型，训练成本会非常高，这不是每个研究者都能够承受的。但是，我的意见，这其实是个好事情。如果仅仅通过加数据、扩模型就能获得更好的效果，这么简单的方式就能推动模型效果不断上升，推动更多应用获得更好效果，这不是天大的好事么？ 至于由此带来的大模型落地难的问题，我相信可以通过搭配知识蒸馏等把模型做小的方案来获得解决。就是说，很可能预训练模型发展会走出一个哑铃模式：两头大，中间小。两个大头中，一头是越来越大的预训练模型，一头是追求各种技术来实用化地把模型做小，这两端会越来越重要。</p>
<p>如果上述假设成立，即预训练领域的暴力美学依然暴力且美丽，那么从今往后的模型改进，我们应该怎么走呢？我的感觉，应该优先探索大数据+大模型的路，先走到暴力美学的尽头，然后再集中精力探索模型本身的改进。就是说，我们应该先把数据红利吃完，而不是优先发展新型模型，当然两者可以并行做，但是原则上，新型模型优先级不如先把数据红利吃完。为什么这么说呢？因为，目前很多研究表明：大多数改进新模型带来的提升，根本比不过提升数据质量数量的同时扩充模型容量带来的收益。而一些新模型的有效性，在数据量小的时候可能是有效的，但很可能发生的一幕是，当数据增大模型容量加大后，很多改进不再有效。也就是说，目前很多新模型的作用，很可能是增加了特殊类型的语言知识的编码和泛化能力，但是，这是完全可以通过增加数据数量和质量，并加大模型来达成的，这种方式又比较简单直观。所以，这是为何我觉得应该先把精力放到“大数据+大模型” 上，然后再集中精力进行模型改进的主要原因。</p>
<h2 id="知识补习班其它知识的引入"><strong>知识补习班：其它知识的引入</strong></h2>
<p>本文开头讲过，大多数预训练模型是从自由文本中学习语言知识。但是，很明显，我们能让模型学的，肯定不止自由文本这一种类型。理论上，任何包含知识的数据，都有些先验知识可供预训练模型学习。我的感觉，预训练模型的发展，会越来越像人脑，日益变成一个黑盒子。就是说，我们可以通过一定手段，喂给它数据，它就会学会其中包含的知识。但是，它是怎么学会的，学到了什么，这很可能对我们来说，会越来越难以理解，就是说，随着预训练模型学习领域的拓展，这个黑盒子，可能会越来越黑。下面我们介绍两个典型的其它领域，看看预训练模型是怎么学的。当然，我相信这种预训练方式，会拓展到越来越多的其它类型的数据或领域，这也是预训练模型领域，一个比较明晰的发展趋势。</p>
<ul>
<li><strong>显示知识的引入</strong></li>
</ul>
<p>原始Bert的语言学知识，是从大量自由文本中自主学习的，那么很自然的一个问题就是：我们过去已经通过一些技术手段，归纳出大量的结构化知识，比如知识图谱；或者已经建立了很多知识分析工具，比如命名实体识别系统等。那么能否利用这些知识识别工具，抑或已有的结构化知识，让预训练模型能够直接学到这些知识？</p>
<p>目前也有很多工作在做这个事情，就是让预训练模型能够编码更多的结构化知识或者语言知识。至于如何做，有两种典型的思路：一种以百度ERNIE为代表；一种以清华ERNIE为代表。这两个工作是最早做这个事情的，差不多同时出来，但思路不同，正好是两种具备代表性的方案。</p>
<p><img src="https://i.loli.net/2020/10/30/OjoakV1tr5MJYEN.jpg" alt="img"></p>
<p>百度ERNIE的思路是：在预训练阶段被Mask掉的对象上做文章，我们可以使用比如命名实体识别工具／短语识别工具，将输入中的命名实体或者部分短语Mask掉（参考上图），这些被Mask掉的片断，代表了某种类型的语言学知识，通过这种方式，强迫预训练模型去强化地学习相关知识。</p>
<p><img src="https://i.loli.net/2020/10/30/mwGZYnuSbeJj6xV.jpg" alt="img"></p>
<p>清华ERNIE则是另外一种思路：我们已经有些结构化知识或者实体关系知识等现成的外部知识库，可以在预训练的过程中，通过工具找出句中的命名实体，句中的命名实体可以触发知识库中其它相关实体，然后预训练模型通过特殊的结构，来融合文本和结构化知识，以进一步促进语言的理解（参考上图）。这是另外一种思路。</p>
<p>关于知识的融入，后续还有很多工作，但是大体走的是上面两条路线之一。关于将显示知识或者结构化知识引入预训练模型，我是这么看的，纯属个人意见：</p>
<p>我觉得，假设说我们用来预训练的数据量特别特别大，而且特征抽取器的能力特别强。理论上，结构化知识是蕴含在这些文本内的，因为我们的外部知识库也是通过技术手段从自由文本里挖掘出来的。假设上面两个条件同时能够被满足，理论上，不太需要单独再把结构化知识独立补充给Bert这类预训练模型，预训练模型应该能够直接从自由文本中就学会这些知识。但是，以我们目前的技术条件，上面两个条件完全被满足，还是有一定难度的。于是，在这种约束下，感觉独立强化知识，让Bert在编码的时候更重视这些结构化知识，看上去是有一定补充作用的。我猜测，比较高频出现的知识，已经能够通过常规的语言模型预训练能够捕获了，很可能对于那些偏冷门的知识，引入结构化知识，会对预训练模型做下游任务有直接促进作用。而可以预见的是：随着机器资源能力越来越强大，如果在第一个预训练阶段，不断加大数据数量和质量，不断增加Transformer模型容量，那么，单独补充结构化知识给预训练模型，收益可能会越来越小。当然，以目前的技术发展阶段，感觉这个事情还有空间和潜力可挖掘。当然，上面说的是通用知识，如果手上的外部知识库，领域性很强，通用训练数据中包含的相关领域数据很少，那么，直接把知识引入，对于解决问题还是很有必要的。</p>
<ul>
<li><strong>多模态预训练</strong></li>
</ul>
<p>随着存储容量越来越大、网络传输速度越来越快、计算速度越来越强，除了传统的文字内容外，图片、视频、音频等各种多模态信息在互联网的内容占比中越来越多。如何融合多种模态信息进行内容理解，就变得越来越重要。那么，能否将多模态信息纳入预训练的框架之内呢？这是个非常有现实价值的问题。</p>
<p>前文有述，自由文本的预训练，本质上是让模型从海量自由文本中，通过语言模型等任务，来学习其中蕴含的的语言学知识。由此自然引发的问题就是：多模态预训练也是要将某种新型的知识塞到模型参数里，那么，这是一种什么样的知识呢？本质上，多模态预训练要学习的知识是两种模态之间，或者多种模态之间，的知识单元映射关系。比如对于文字-图片这两种多模态信息来说，我们可以把图片想像成一种特殊类型的语言，多模态预训练希望让模型学会这两种不同模态之间的语义映射关系，比如能够将单词“苹果”和图片中出现的苹果区域建立起联系。或者说，希望通过将不同模态的信息映射到相同的语义空间，来学会两者之间的语义映射关系。</p>
<p><img src="https://i.loli.net/2020/10/30/mrwA5gk478FcNHT.jpg" alt="img"></p>
<p>如果我们能够成功地学会这种不同媒介间的语义映射，那么就可以做很多有意思的事情，比如说句话，搜出与这句话语义相近的图片（参考上图）；或者反过来，输入一个图片，能够找到或者生成对应的文字描述。再比如VQA（参考上图），就是给定一张图片，你可以针对图片提出一些问题，AI系统能够回答你的问题，给出正确答案。这涉及到图片-文字的跨媒体问答以及一些跨媒体的知识推理。而要想实现这种能力，如何通过预训练模型，让模型学会两种模态之间的语义映射关系就是至关重要的。</p>
<p>我们面临的第一个问题是：从什么样的数据里来学习不同模态之间的语义映射关系呢？自由文本的预训练模型，可以采纳海量无标注数据来做，然而，多模态预训练要学习不同模态信息间的语义映射关系，所以需要有标注好的“模态1-模态2”的对齐数据，比如：标注好的“文本-图片”或者“文本-视频”平行数据。只有具备跨模态对齐数据，模型才有可能从中学习不同媒介类型之间的语义映射关系。从这个角度讲，相对自由文本预训练来说，多模态预训练因为需要模态对齐训练数据，而这种数据往往是需要人工标注的，所以可获得的数据难度及成本就高了很多，明显不如文本预训练那么自由。</p>
<p>总体而言，目前的多模态预训练任务中，通常都是“双模态”预训练，常见的包括“文本-图片”、“文本-视频”、“视频-音频”等模态类型组合。其中， 相对而言，“文本-图片”类型的任务技术发展比较快，其它类型的多模态类型发展相对缓慢，我猜测这里的主要原因在于可用标注数据的差异。“文本-图片”目前有一些规模达到几十万到上百万规模的标注数据集合，典型的比如MS-COCO、Visual Gnome等，而其它类型的模态组合数据貌似缺乏大规模数据集合，这严重影响了领域技术进展。下面我们从“文本-图片”这种模态组合来宏观介绍下多模态预训练的常规做法，其它模态组合的技术方案差不太多，所缺的可能主要是标注好的模态对齐数据。</p>
<p>我们从模型结构和训练目标这两个角度来阐述。目前的大多数技术方案大同小异，主要差异在于采用了不同的模型结构及与不同训练目标的差异组合。</p>
<p>假设我们有“文本-图片”两种模态数据，需要联合学习三种预训练模型：文本模态自身的预训练模型，图片模态自身的预训练模型，以及两个模态之间的语义对齐预训练模型。从模型结构来说，目前主流的结构有两种：双流交互模型以及单流交互模型。</p>
<p><img src="https://i.loli.net/2020/10/30/tkIUsf5xFBVwuq4.jpg" alt="img"></p>
<p>典型双流交互模型结构如上图LXMERT模型所示。文本编码器代表一个流，一般采用Transformer模型捕捉文本单词之间的关系；图片编码器代表另外一个流，一般也是采用Transformer模型，对于图片来说，一般用Faster-RCNN模型识别出图片中包含的多个物体及其对应的矩形位置信息，将高置信度的物体及其对应的位置信息作为图片侧Transformer的输入，用来学习图片中物品的相互关系；在两个流之上，再加入额外的Transformer模型，用于融合两个模态的语义映射关系。在这种双流结构上，模型同时学习文本预训练目标、图片预训练目标，以及图片-文本对齐预训练目标。一般文本预训练目标和标准的Bert做法类似，通过随机Mask一部分文本单词的语言模型来做；图片预训练目标类似，可以Mask掉图片中包含的部分物品，要求模型正确预测物品类别或者预测物品Embedding编码；为了能够让两个模态语义对齐，一般还要学习一个跨模态目标，常规做法是将对齐语料中的“文本-图片”作为正例，随机选择部分图片或者文本作为负例，来要求模型正确做二分类问题，通过这种方式逼迫模型学习两种模态间的对齐关系。典型的双流模型包括LXMERT、ViLBERT等。</p>
<p><img src="https://i.loli.net/2020/10/30/hyXxRlIvs63Tabm.jpg" alt="img"></p>
<p>典型的单流交互模型结构如上图Unicoder-VL模型所示。单流和双流的区别在于：单流模型只用一个Transformer，而双流模型，如上所述，需要三个Transformer各自分工协作。输入的图片，经过上述的Faster-RCNN物体识别和位置编码后，和文本单词拼接，整体作为Transformer模型的输入。也就是说，单流模型靠单个Transformer，同时学习文本内部单词交互、图片中包含物体之间大的交互，以及文本-图片之间的细粒度语义单元之间的交互信息。单流模型的预训练目标，与双流交互模型是类似的，往往也需要联合学习文本预训练、图片预训练以及对齐预训练三个目标。典型的单流模型包括Unicoder-VL、VisualBERT、VL-VERT、UNITER等。</p>
<p><img src="https://i.loli.net/2020/10/30/5aqhOLVsnGuNIjf.jpg" alt="img"></p>
<p>经过多模态预训练之后，是否模型能够建立起不同模态信息之间的语义映射关系呢？答案可以参考上图：经过预训练后，输入一句话以及对应的图片进入模型，对于文本中的某个单词，我们可以观察这个单词与图片中哪块区域联系密切（根据Attention强度信息可以看出）。从上图示例可以看出，预训练模型确实学会了不同模态单词语义之间的映射关系。</p>
<p>多模态模型经过预训练之后，针对具体的应用任务，可以采取第二阶段Fine-tuning的模式增强应用效果。从上述描述可见，单流模型结构相对简单，模型参数也相对少些，而且能够在模型底层及早对不同模态之间的语义直接建立联系，所以看起来比双流模式更有发展前景，但是从目前的各种研究对比实验结果看，貌似两种方法的效果在伯仲之间。不过，可以得出的结论是，采用预训练模型的多模态方法，比不用预训练的传统方法，在应用效果上是有明显提升的。</p>
<p>目前来看，如果希望多模态预训练有更快速的技术发展，以下几个方面是需要重点关注的：</p>
<p>首先，也是最重要的，可能是急需构建不同模态间的大规模对齐数据。目前，“图片-文本”类型的对齐数据规模尚可，但是继续扩大数据规模无疑是有益的；对其它类型的模态组合而言，大规模的标准对齐数据比较缺乏，这会严重制约多模态预训练的发展。所以明显需要数据先行，这是发展技术的前提条件；</p>
<p>其次，感觉在自由文本预训练研究领域中，目前得到的一些得到验证的经验，推理起来，应该是能够直接迁移到多模态预训练领域的。典型的经验，比如：在扩大数据规模的同时，增加模型复杂度。增加模型复杂度包括图片特征抽取器模型复杂度（已经有实验验证加深ResNet模型对效果提升明显），以及增加对应的Transformer层深，放大Transformer的Hidden Size等，相信这是能够大幅提升多模态预训练的首选手段；再比如文本预训练任务中的Mask对象，采用Span方式而非单词方式（已有工作这么做了），加大Batch Size延长训练时间等训练方法优化手段，想来都应该是有益的；从训练目标来说，目前的模态间对齐任务还是有点类似NSP这种句子分类任务，明显偏简单了一些，这块可以考虑引入更有难度的对齐任务，以及实体级别细粒度的对齐任务，来增强模态对齐模型的效果。</p>
<p>再次，可以考虑由目前的两模态向真正的多模态扩展，比如三模态动态联合训练，目前常见的是“文本-图片”，或者“文本-视频”，通常是两模态结构，后面可以考虑“文本-图片-音频”，或者“文本-视频-音频”等三模态甚至更多模态的联合预训练。当然，这么做的前提，仍然是得先有多模态的对齐数据。</p>
<h2 id="多多益善从两阶段模型到四阶段模型"><strong>多多益善：从两阶段模型到四阶段模型</strong></h2>
<p>经典的预训练模型框架下，一般我们解决NLP问题有两个阶段：第一阶段是模型预训练阶段，预训练模型从文本等信息中学习语言知识；第二阶段是Fine-tuning阶段，根据手上的有监督数据，对模型参数进行微调，以获得更好的任务效果。</p>
<p>前文有述，预训练阶段的最明显发展趋势是大数据+大模型，在数据质量有保障的前提下，数据量越大，模型容量越大，预训练阶段学到的语言知识效果越好。其实，关于预训练数据，目前还有很多研究，能够得出另外一个结论：从领域、题材、类型等不同角度看，如果预训练数据和手上任务数据越接近，则预训练模型带来的收益就越大。</p>
<p>很多时候，我们手头上的任务数据有很强的领域性，比如可能是计算机领域的，因为预训练数据一般具备通用性，即使大量预训练文本里包含部分计算机类的文本，整体占比也很小。于是，这种情况下，由于领域差异比较大，预训练模型带给手头任务的收益，就没期望中那么大。一种直观的，也是不少人在用的解决方案是：把领域性文本，也加入到预训练数据中，一同参与预训练过程，这样能够增加预训练文本和手上任务的相似性，就能提升任务效果。事实上，这样做也确实能解决这个问题。但是，有一个问题：预训练阶段往往会兼顾模型的通用性，尽可能兼顾各种下游任务，希望模型能在不同领域都有效。而且，从趋势看，数据规模和模型规模会越来越大，也就是训练成本会越来越高。所以，这种把领域数据添加到预训练数据一起训练的做法，一则影响模型通用性，二则实现成本高，看上去就不是特别好的方法。</p>
<p>目前看，要解决这个问题，比较好的方法是把两个阶段分离：第一阶段仍然采取大数据、大模型，走通用普适、各种任务都能受益的路子，不特意考虑领域特点，因为兼顾不过来；第二阶段，在第一阶段训练好的通用预训练模型基础上，利用领域数据，再做一次预训练，等于把通用的预训练模型往领域方向拉动一下。这样两个阶段各司其职，有独立的优化目标，也能兼顾通用性和领域适配性。</p>
<p><img src="https://i.loli.net/2020/10/30/ELKz135biWBOFyr.jpg" alt="img"></p>
<p>上面这个方法，我猜应该不少人都已经在这么做了，论文“Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks”也通过大量实验验证了领域数据预训练（DAPT）的有效性，再结合它得出的另外一个重要结论：用手上的任务数据，无论大小，如果做一次任务级数据预训练（TAPT），也就是拿着手上任务数据，在通用预训练模型基础上，再做一次预训练，也能够有效提升任务效果。综合这个文章和其它有关文章的结论，我们不难看出，要想更好地提升任务效果，我们应该从传统的两阶段模型，拓展到如下四阶段模型（参考上图）：</p>
<p>第一个阶段：通用预训练</p>
<p>这就是传统两阶段模式中的第一阶段。这个阶段不仅仅追求效果好，也追求领域通用性。它的优化目标是：在尽可能多的下游任务场景中，效果都尽可能好，但不单独考虑某个特殊领域的效果如何。这个阶段，目前看总的发展趋势是：在数据质量有保证的前提下，增加数据数量，以及数据的多样性，同时提升模型复杂度，这样可以提供普遍有效的模型增强能力。很明显，这个阶段，一般只有土豪公司才能做得起，而且从趋势看，会越来越如此。将来的发展模式可能是，超级土豪公司不断优化这个模型，然后放出来供大家用，有能力做这个事情的人，应该会越来越少。</p>
<p>第二个阶段：领域预训练</p>
<p>在第一阶段训练好的通用预训练模型基础上，利用不同领域的自由文本，构建多个、不同领域的领域预训练模型。比如我们可以分别收集计算机领域、生物领域、电商领域…等等，多个不同领域的无标注自由文本数据。在第一阶段通用模型基础上，分别用各个领域数据，再分别做一次预训练，这样我们就得到了适合解决各个不同领域的预训练模型：计算机领域、生物领域、电商领域…..等等多个不同的预训练模型。下游任务可以根据自己任务的领域，选择适配性好的领域预训练模型来使用。</p>
<p>这个阶段的预训练模型，在训练的时候，有个独特的问题需要解决：灾难遗忘问题。所谓“灾难遗忘”，就是说，当你用领域数据进行预训练的时候，因为会调整第一阶段预训练模型的参数，这种偏向领域性的参数调整，可能会导致第一阶段模型学好的参数被改写，这意味着：经过第二阶段预训练，第一阶段预训练模型里学会的很多通用语言知识，可能会被冲掉。灾难遗忘就是这个意思。灾难遗忘问题，对于预训练模型，尤其是领域预训练模型来说，是个很关键也很重要的问题，目前也有一些解决方案，限于篇幅，这里就不展开了。</p>
<p>这个阶段的预训练，因为数据量相比第一阶段会小很多，所以其实中农公司甚至贫农公司也能做得起，不存在土豪门槛，大家应该都能做。当然，一般我们只做跟自己手头任务相关的领域的预训练模型。如果你想做很多领域的预训练模型，那估计也要备足银行卡。估计后续也会有土豪公司做好很多不同领域的预训练模型，供大家个性化适配使用，虽说目前还没有，但是推断起来，这是个大概率会发生的事件。</p>
<p>第三个阶段：任务预训练</p>
<p>在前两个预训练模型基础上，比如从第二个阶段里面的多个不同的领域预训练模型中，选择和手头任务适配的那个领域预训练模型，在这个模型基础上，用手头数据，抛掉数据标签，再做一次预训练，无论手上任务数据有多少。比如手上任务是计算机领域的，那么从第二阶段的多个领域模型里面，选择计算机领域适配过的预训练模型，在这个模型基础上进行一次任务级别的预训练。这样应该能明显提升任务效果。</p>
<p>第四阶段：任务Fine-tuning</p>
<p>这是传统两阶段的第二阶段，做法一样，没什么好讲的。</p>
<p>当然，如果你手上的任务没有那么强的领域性，可以跳过第二阶段，也就是那个领域预训练模型阶段，走剩余的三阶段模式即可，无论如何，任务预训练都是值得做的一个事情。</p>
<h2 id="聚沙成塔如何建造强大的预训练模型"><strong>聚沙成塔：如何建造强大的预训练模型</strong></h2>
<p>上文从不同角度或维度，总结了预训练模型某个方面的一些结论，我们综合起来看一下。不论出于什么目的，打榜也好，把手头应用做得更出色也好，如果我们综合各个维度的现有信息，那么，在当前技术水准下，如何构造强大的预训练模型，貌似是可以得出相对明晰结论的。因为NLP里面既有语言理解类任务，也有语言生成类任务，两者差异较大，所以我们分头来看。</p>
<p><img src="https://i.loli.net/2020/10/30/VdANbfwutJ3MyWn.jpg" alt="img"></p>
<p>对于语言理解类任务，我假设你的任务不是领域性特别强那种类型的，建议采取如下技术方案：</p>
<p>使用三阶段模型：通用预训练+任务预训练+任务Fine-tuning。在做完第一阶段预训练后，用手头任务数据，抛掉标签，再做一次任务预训练，然后任务Fine-tuning。</p>
<p>模型结构建议采取Encoder+Decoder结构，或者Encoder-AE结构；预训练任务配置两个：独立生成Span类语言模型及SOP句子任务；在质量优先的前提下，增加预训练数据的数量；比较关键的一点是，一定要增加模型容量：可以纵向增加Transformer Block层深，或者横向调大Transformer相应位置可配置参数大小。当然，如果你不差钱，两个可以一起上。另外，要使得模型得到充分训练，就是说增大训练过程中的Batch Size和训练步长。</p>
<p>对于语言生成类任务，建议采取如下技术方案：</p>
<p>使用两阶段模型：通用预训练+任务Fine-tuning。模型结构建议采取Encoder+Decoder结构，或者Decoder-AR结构；预训练任务采用独立生成Span类语言模型；在质量优先的前提下，增加预训练数据的数量；同样，比较关键的一点是，一定要增加模型容量：可以纵向增加Transformer Block层深，或者横向调大Transformer相应位置可配置参数大小。当然，如果你不差钱，两个可以一起上。另外，也要使得模型得到充分训练，就是说增大训练过程中的Batch Size和训练步长。</p>
<p>相信采取上述技术方案，你能在打榜过程中获得很好的名次，或者在实际工作中能比较快地完成自己的KPI或OKR。当然，如果是走落地应用的路子，关于知识蒸馏等一系列如何将模型做小这方面，记得要多花点功夫。</p>
<h3 id="参考">参考</h3>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/254821426" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/254821426</a></p>
</blockquote>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
  </entry>
  <entry>
    <title>2020-10-29-深度学习调参技巧汇总</title>
    <url>/2020/10/29/2020-10-29-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%B0%83%E5%8F%82%E6%8A%80%E5%B7%A7%E6%B1%87%E6%80%BB/</url>
    <content><![CDATA[<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
  </entry>
  <entry>
    <title>2020-10-28-transformer综述</title>
    <url>/2020/10/28/2020-10-28-transformer%E7%BB%BC%E8%BF%B0/</url>
    <content><![CDATA[<h3 id="transformer家族1----transformer详解和源码分析">🚀Transformer家族1 -- Transformer详解和源码分析</h3>
<h4 id="transformer总体结构">1 Transformer总体结构</h4>
<p>近几年NLP领域有了突飞猛进的发展，预训练模型功不可没。当前利用预训练模型（pretrain models）在下游任务中进行fine-tune，已经成为了大部分NLP任务的固定范式。Transformer摒弃了RNN的序列结构，完全采用attention和全连接，严格来说不属于预训练模型。但它却是当前几乎所有pretrain models的基本结构，为pretrain models打下了坚实的基础，并逐步发展出了transformer-XL，reformer等优化架构。本文结合论文和源码，对transformer基本结构，进行详细分析。</p>
<p>Transformer是谷歌在2017年6月提出，发表在NIPS2017上。论文地址 <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need</a>。 分析的代码为Harvardnlp的代码，基于PyTorch， 地址 <a href="https://github.com/harvardnlp/annotated-transformer" target="_blank" rel="noopener">annotated-transformer</a></p>
<p>Transformer主体框架是一个<strong>encoder-decoder</strong>结构，去掉了RNN序列结构，完全基于attention和全连接。在WMT2014英语翻译德语任务上，bleu值达到了28.4，达到当时的SOTA。其总体结构如下所示</p>
<p><img src="https://i.loli.net/2020/10/28/6z5wSWXlaQnAF2E.png" alt="在这里插入图片描述"></p>
<p>总体为一个典型的encoder-decoder结构。代码如下</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 整个模型入口</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_model</span><span class="params">(src_vocab, tgt_vocab, N=<span class="number">6</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">               d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, h=<span class="number">8</span>, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">    <span class="string">"Helper: Construct a model from hyperparameters."</span></span><br><span class="line">    c = copy.deepcopy</span><br><span class="line"></span><br><span class="line">    <span class="comment"># multiHead attention</span></span><br><span class="line">    attn = MultiHeadedAttention(h, d_model)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># feed-forward</span></span><br><span class="line">    ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># position-encoding</span></span><br><span class="line">    position = PositionalEncoding(d_model, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 整体为一个encoder-decoder</span></span><br><span class="line">    model = EncoderDecoder(</span><br><span class="line">        <span class="comment"># encoder编码层</span></span><br><span class="line">        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),</span><br><span class="line"></span><br><span class="line">        <span class="comment"># decoder解码层</span></span><br><span class="line">        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 编码层输入，输入语句进行token embedding和position embedding</span></span><br><span class="line">        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 解码层输入，同样需要做token embedding和position embedding</span></span><br><span class="line">        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),</span><br><span class="line"></span><br><span class="line">        <span class="comment"># linear + softmax，查找vocab中概率最大的字</span></span><br><span class="line">        Generator(d_model, tgt_vocab))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># This was important from their code. </span></span><br><span class="line">    <span class="comment"># Initialize parameters with Glorot / fan_avg.</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">            nn.init.xavier_uniform(p)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"><span class="number">1234567891011121314151617181920212223242526272829303132333435363738</span></span><br></pre></td></tr></tbody></table></figure>
<p>make_model为Transformer模型定义的入口，它先定义了multi-head attention、feed-forward、position-encoding等一系列子模块，然后定义了一个encoder-decoder结构并返回。下面来看encoder-decoder定义。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderDecoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    一个标准的encoder和decoder框架，可以自定义embedding、encoder、decoder等</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, encoder, decoder, src_embed, tgt_embed, generator)</span>:</span></span><br><span class="line">        super(EncoderDecoder, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># encoder和decoder通过构造函数传入，可灵活更改</span></span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line"></span><br><span class="line">        <span class="comment"># src和target的embedding，也是通过构造函数传入，方便灵活更改</span></span><br><span class="line">        self.src_embed = src_embed</span><br><span class="line">        self.tgt_embed = tgt_embed</span><br><span class="line"></span><br><span class="line">        <span class="comment"># linear + softmax</span></span><br><span class="line">        self.generator = generator</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, src, tgt, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="string">"Take in and process masked src and target sequences."</span></span><br><span class="line">        <span class="comment"># 先对输入进行encode，然后再通过decode输出</span></span><br><span class="line">        <span class="keyword">return</span> self.decode(self.encode(src, src_mask), src_mask,</span><br><span class="line">                            tgt, tgt_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, src, src_mask)</span>:</span></span><br><span class="line">        <span class="comment"># 先对输入进行embedding，然后再经过encoder</span></span><br><span class="line">        <span class="keyword">return</span> self.encoder(self.src_embed(src), src_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span><span class="params">(self, memory, src_mask, tgt, tgt_mask)</span>:</span></span><br><span class="line">        <span class="comment"># 先对目标进行embedding，然后经过decoder</span></span><br><span class="line">        <span class="keyword">return</span> self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)</span><br><span class="line"><span class="number">12345678910111213141516171819202122232425262728293031</span></span><br></pre></td></tr></tbody></table></figure>
<p>encoder-decoder定义了一个标准的编码解码框架，其中编码器、解码器均可以自定义，有很强的泛化能力。模块运行时会调用forward函数，它先对输入进行encode，然后再通过decode输出。我们就不详细展开了。</p>
<h4 id="encoder">2 encoder</h4>
<h5 id="encoder定义">2.1 encoder定义</h5>
<p>encoder分为两部分</p>
<ol type="1">
<li><strong>输入层embedding</strong>。输入层对inputs文本做token embedding，并对每个字做position encoding，然后叠加在一起，作为最终的输入。</li>
<li><strong>编码层encoding</strong>。编码层是多层结构相同的layer堆叠而成。每个layer又包括两部分，multi-head self-attention和feed-forward全连接，并在每部分加入了残差连接和归一化。</li>
</ol>
<p>代码实现上也验证了这一点。我们看EncoderDecoder类中的encode函数，它先利用输入embedding层对原始输入进行embedding，然后再通过编码层进行encoding。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderDecoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, src, src_mask)</span>:</span></span><br><span class="line">        <span class="comment"># 先对输入进行embedding，然后再经过encoder</span></span><br><span class="line">        <span class="keyword">return</span> self.encoder(self.src_embed(src), src_mask)</span><br><span class="line"><span class="number">1234</span></span><br></pre></td></tr></tbody></table></figure>
<h5 id="输入层embedding">2.2 输入层embedding</h5>
<p>原始文本经过embedding层进行向量化，它包括token embedding和position embedding两层。</p>
<h6 id="token-embedding">2.2.1 token embedding</h6>
<p>token embedding对文本进行向量化，一般来说有两种方式</p>
<ol type="1">
<li>采用<strong>固定词向量</strong>，比如利用Word2vec预先训练好的。这种方式是LSTM时代常用的方式，比较简单省事，无需训练。但由于词向量是固定的，不能解决一词多义的问题，词语本身也不是contextual的，没有结合上下文语境信息，另外对于不在词向量中的词语，比如特定领域词语或者新词，容易出现OOV问题。</li>
<li>随机初始化，然后<strong>训练</strong>。这种方式比较麻烦，需要大规模训练语料，但能解决固定词向量的一系列问题。Transformer采用了这种方式。</li>
</ol>
<p>另外，基于Transformer的BERT模型在中文处理时，直接基于字做embedding，优点有</p>
<ol type="1">
<li>无需分词，故不会引入分词误差。事实上，只要训练语料充分，模型自然就可以学到分词信息了。</li>
<li>中文字个数固定，不会导致OOV问题</li>
<li>中文字相对词，数量少很多，embedding层参数大大缩小，减小了模型体积，并加快了训练速度。</li>
</ol>
<p>事实上，就算在LSTM时代，很多case中，我们也碰到过基于字的embedding的效果比基于词的要好一些。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Embeddings</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="comment"># token embedding，随机初始化训练，然后查表找到每个字的embedding</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, vocab)</span>:</span></span><br><span class="line">        super(Embeddings, self).__init__()</span><br><span class="line">        <span class="comment"># 构建一个随机初始化的词向量表，[vocab_size, d_model]。 bert中的设置为[21128, 768]</span></span><br><span class="line">        self.lut = nn.Embedding(vocab, d_model)</span><br><span class="line">        self.d_model = d_model</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># 从词向量表中查找字对应的embedding向量</span></span><br><span class="line">        <span class="keyword">return</span> self.lut(x) * math.sqrt(self.d_model)</span><br><span class="line"></span><br><span class="line"><span class="number">123456789101112</span></span><br></pre></td></tr></tbody></table></figure>
<p>由代码可见，Transformer采用的是随机初始化，然后训练的方式。词向量维度为[vocab_size, d_model]。例如BERT中为[21128, 768]，参数量还是很大的。ALBert针对embedding层进行矩阵分解，大大减小了embedding层体积。</p>
<h6 id="position-encoding">2.2.2 position encoding</h6>
<p>首先一个问题，为啥要进行位置编码呢。原因在于self-attention，将任意两个字之间距离缩小为1，丢失了字的位置信息，故我们需要加上这一信息。我们也可以想到两种方法</p>
<ol type="1">
<li><strong>固定编码</strong>。Transformer采用了这一方式，通过奇数列cos函数，偶数列sin函数方式，利用三角函数对位置进行固定编码。</li>
<li><strong>动态训练</strong>。BERT采用了这种方式。先随机初始化一个embedding table，然后训练得到table 参数值。predict时通过embedding_lookup找到每个位置的embedding。这种方式和token embedding类似。</li>
</ol>
<p>哪一种方法好呢？个人以为各有利弊</p>
<ol type="1">
<li>固定编码方式简洁，不需要训练。且不受embedding table维度影响，理论上可以支持任意长度文本。（但要尽量避免预测文本很长，但训练集文本较短的case）</li>
<li>动态训练方式，在语料比较大时，准确度比较好。但需要训练，且最致命的是，限制了输入文本长度。当文本长度大于position embedding table维度时，超出的position无法查表得到embedding（可以理解为OOV了）。这也是为什么BERT模型文本长度最大512的原因。</li>
</ol>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="comment"># 位置编码。transformer利用编码方式实现，无需训练。bert则采用训练embedding_lookup方式</span></span><br><span class="line">    <span class="comment"># 编码方式文本语句长度不受限，但准确度不高</span></span><br><span class="line">    <span class="comment"># 训练方式文本长度会受position维度限制（这也是为什么bert只能处理最大512个字原因），但训练数据多时，准确率高</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, dropout, max_len=<span class="number">5000</span>)</span>:</span></span><br><span class="line">        super(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 采用sin和cos进行position encoding</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) *</span><br><span class="line">                             -(math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)        <span class="comment"># 偶数列</span></span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)        <span class="comment"># 奇数列</span></span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        self.register_buffer(<span class="string">'pe'</span>, pe)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># token embedding和position encoding加在一起</span></span><br><span class="line">        x = x + Variable(self.pe[:, :x.size(<span class="number">1</span>)], </span><br><span class="line">                         requires_grad=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br><span class="line"></span><br><span class="line"><span class="number">123456789101112131415161718192021222324</span></span><br></pre></td></tr></tbody></table></figure>
<p>由代码可见，position encoding直接采用了三角函数。对偶数列采用sin，奇数列采用cos。 <img src="https://i.loli.net/2020/10/28/zPU8Z9scbDNkS5A.png" alt="在这里插入图片描述"></p>
<h5 id="编码层">2.3 编码层</h5>
<p>Encoder层是Transformer的核心，它由<strong>N层相同结构的layer</strong>（默认6层）堆叠而成。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Core encoder is a stack of N layers"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layer, N)</span>:</span></span><br><span class="line">        super(Encoder, self).__init__()</span><br><span class="line">        <span class="comment"># N层堆叠而成，每一层结构都是相同的，训练参数不同</span></span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># layer normalization</span></span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        <span class="number">45</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask)</span>:</span></span><br><span class="line">        <span class="comment"># 1 经过N层堆叠的multi-head attention + feed-forward</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2 对encoder最终输出结果进行layer-norm归一化。层间和层内子模块都做过 add + dropout + layer-norm</span></span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br><span class="line"><span class="number">1234567891011121314151617</span></span><br></pre></td></tr></tbody></table></figure>
<p>encoder的定义很简洁。先经过N层相同结构的layer，然后再进行归一化输出。重点我们来看layer的定义。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Encoder is made up of self-attn and feed forward (defined below)"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, self_attn, feed_forward, dropout)</span>:</span></span><br><span class="line">        super(EncoderLayer, self).__init__()</span><br><span class="line">        <span class="comment"># 1 self_attention</span></span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2 feed_forward</span></span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3 残差连接。encoder和decoder，每层结构，每个子结构，都有残差连接。</span></span><br><span class="line">        <span class="comment"># add + drop-out + layer-norm</span></span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">2</span>)</span><br><span class="line">        self.size = size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask)</span>:</span></span><br><span class="line">        <span class="comment"># 经过self_attention, 然后和输入进行add + layer-norm</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, mask))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 经过feed_forward， 此模块也有add + layer-norm</span></span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">1</span>](x, self.feed_forward)</span><br><span class="line"></span><br><span class="line"><span class="number">12345678910111213141516171819202122</span></span><br></pre></td></tr></tbody></table></figure>
<p>encoder layer分为两个子模块</p>
<ol type="1">
<li><strong>self attention</strong>, 并对输入attention前的和经过attention输出的，做残差连接。残差连接先经过layer-norm归一化，然后进行dropout，最后再做add。后面我们详细分析</li>
<li><strong>feed-forward</strong>全连接，也有残差连接的存在，方式和self attention相同。</li>
</ol>
<h6 id="multiheadedattention">2.3.1 MultiHeadedAttention</h6>
<p>MultiHeaded Attention采用多头self-attention。它先将隐向量切分为h个头，然后每个头内部进行self-attention计算，最后再concat再一起。</p>
<p><img src="https://i.loli.net/2020/10/28/K4lYeqtN7jUR5fx.png" alt="在这里插入图片描述"> 代码如下</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadedAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, h, d_model, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        super(MultiHeadedAttention, self).__init__()</span><br><span class="line">        <span class="keyword">assert</span> d_model % h == <span class="number">0</span></span><br><span class="line">        <span class="comment"># d_model为隐层维度，也是embedding的维度，h为多头个数。</span></span><br><span class="line">        <span class="comment"># d_k为每个头的隐层维度，要除以多头个数。也就是加入了多头，总隐层维度不变。</span></span><br><span class="line">        self.d_k = d_model // h</span><br><span class="line">        self.h = h</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 线性连接</span></span><br><span class="line">        self.linears = clones(nn.Linear(d_model, d_model), <span class="number">4</span>)</span><br><span class="line">        self.attn = <span class="literal">None</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, mask=None)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 输入mask，在decoder的时候有用到。decode时不能看到要生成字之后的字，所以需要mask</span></span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        nbatches = query.size(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 1) q, k, v形状变化，加入多头， [batch, L, d_model] =&gt; [batch, h, L, d_model/h]</span></span><br><span class="line">        query, key, value = [l(x).view(nbatches, <span class="number">-1</span>, self.h, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">             <span class="keyword">for</span> l, x <span class="keyword">in</span> zip(self.linears, (query, key, value))]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2) attention计算</span></span><br><span class="line">        x, self.attn = attention(query, key, value, mask=mask, </span><br><span class="line">                                 dropout=self.dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3) 多头结果concat在一起，还原为初始形状</span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(nbatches, <span class="number">-1</span>, self.h * self.d_k)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 4）最后经过一个线性层</span></span><br><span class="line">        <span class="keyword">return</span> self.linears[<span class="number">-1</span>](x)</span><br><span class="line"><span class="number">123456789101112131415161718192021222324252627282930313233</span></span><br></pre></td></tr></tbody></table></figure>
<p>下面重点来看单个头的self-attention。也就是论文中的“Scaled Dot-Product Attention”。attention本质上是一个向量的加权求和。它探讨的是每个位置对当前位置的贡献。步骤如下</p>
<ol type="1">
<li>q向量和每个位置的k向量计算点积，然后除以向量长度的根号。计算点积可以认为是进行权重计算。除以向量长度原因是向量越长，q*k值理论上会越大，故需要在向量长度上做归一化。</li>
<li><strong>attention-mask</strong>。mask和输入矩阵shape相同，mask矩阵中值为0位置对应的输入矩阵的值更改为-1e9，一个非常非常小的数，经过softmax后趋近于0。decoder中使用了mask，后面我们详细分析。</li>
<li>softmax归一化，使得q向量和每个位置的k向量的score分布到（0, 1）之间</li>
<li>加权系数乘以每个位置v向量，然后加起来。</li>
</ol>
<p>公式如下：<img src="https://i.loli.net/2020/10/28/C8lDMeBadc4yftg.png" alt="在这里插入图片描述"> 代码如下</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention</span><span class="params">(query, key, value, mask=None, dropout=None)</span>:</span></span><br><span class="line">    <span class="comment"># attention计算，self_attention和soft-attention都是使用这个函数</span></span><br><span class="line">    <span class="comment"># self-attention, q k v 均来自同一文本。要么是encoder，要么是decoder</span></span><br><span class="line">    <span class="comment"># soft-attention, q来自decoder，k和v来自encoder，从而按照decoder和encoder相关性，将encoder信息融合进来</span></span><br><span class="line">    d_k = query.size(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 利用q * k计算两向量间相关度，相关度高则权重大。</span></span><br><span class="line">    <span class="comment"># 除以根号dk的原因是，对向量长度进行归一化。q和k的向量长度越长，q*k的值越大</span></span><br><span class="line">    scores = torch.matmul(query, key.transpose(<span class="number">-2</span>, <span class="number">-1</span>)) / math.sqrt(d_k)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># attention-mask，将 mask中为1的 元素所在的索引，在a中相同的的索引处替换为 value</span></span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        scores = scores.masked_fill(mask == <span class="number">0</span>, <span class="number">-1e9</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># softmax归一化</span></span><br><span class="line">    p_attn = F.softmax(scores, dim = <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># dropout</span></span><br><span class="line">    <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        p_attn = dropout(p_attn)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 最后利用归一化后的加权系数，乘以每一个v向量，再加和在一起，作为attention后的向量。每个字对应一个向量</span></span><br><span class="line">    <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br><span class="line"><span class="number">1234567891011121314151617181920212223</span></span><br></pre></td></tr></tbody></table></figure>
<p>self-attention和soft-attention共用了这个函数，他们之间的唯一区别是<strong>q k v向量的来源不同</strong>。self-attention中q k v 均来自同一文本。而decoder的soft-attention，q来自于decoder，k和v来自于encoder。它体现的是encoder对decoder的加权贡献。</p>
<h6 id="positionwisefeedforward">2.3.2 PositionwiseFeedForward</h6>
<p>feed-forward本质是一个两层的全连接，全连接之间加入了relu非线性和dropout。比较简单，代码如下</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionwiseFeedForward</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="comment"># 全连接层</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, d_ff, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        super(PositionwiseFeedForward, self).__init__()</span><br><span class="line">        <span class="comment"># 第一层全连接  [d_model, d_ff]</span></span><br><span class="line">        self.w_1 = nn.Linear(d_model, d_ff)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第二层全连接 [d_ff, d_model]</span></span><br><span class="line">        self.w_2 = nn.Linear(d_ff, d_model)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># dropout</span></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># 全连接1 -&gt; relu -&gt; dropout -&gt; 全连接2</span></span><br><span class="line">        <span class="keyword">return</span> self.w_2(self.dropout(F.relu(self.w_1(x))))</span><br><span class="line"><span class="number">12345678910111213141516</span></span><br></pre></td></tr></tbody></table></figure>
<p>总体过程是：<strong>全连接1 -&gt; relu -&gt; dropout -&gt; 全连接2</strong>。两层全连接内部没有shortcut，这儿不要搞混了。</p>
<h6 id="sublayerconnection">2.3.3 SublayerConnection</h6>
<p>在每层的self-attention和feed-forward模块中，均应用了残差连接。残差连接先对输入进行layerNorm归一化，然后送入attention或feed-forward模块，然后经过dropout，最后再和原始输入相加。这样做的好处是，让每一层attention和feed-forward模块的输入值，均是经过归一化的，保持在一个量级上，从而可以加快收敛速度。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SublayerConnection</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A residual connection followed by a layer norm.</span></span><br><span class="line"><span class="string">    Note for code simplicity the norm is first as opposed to last.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, dropout)</span>:</span></span><br><span class="line">        super(SublayerConnection, self).__init__()</span><br><span class="line">        <span class="comment"># layer-norm 归一化</span></span><br><span class="line">        self.norm = LayerNorm(size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># dropout</span></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, sublayer)</span>:</span></span><br><span class="line">        <span class="comment"># 先对输入进行layer-norm, 然后经过attention等相关模块，再经过dropout，最后再和输入相加</span></span><br><span class="line">        <span class="keyword">return</span> x + self.dropout(sublayer(self.norm(x)))</span><br><span class="line"><span class="number">12345678910111213141516</span></span><br></pre></td></tr></tbody></table></figure>
<p>从forward函数可见，先对输入进行layer-norm, 然后经过attention等相关模块，再经过dropout，最后再和输入相加。残差连接的作用就不说了，参考ResNet。</p>
<h4 id="decoder">3 decoder</h4>
<p>decoder结构和encoder大体相同，也是堆叠了N层相同结构的layer（默认6层）。不同的是，decoder的每个子层包括三层。</p>
<ol type="1">
<li><strong>masked multi-head self-attention</strong>。这一部分和encoder基本相同，区别在于decoder为了保证模型不能看见要预测字的后面位置的字，加入了mask，从而避免未来信息的穿越问题。mask为一个上三角矩阵，上三角全为1，下三角和对角线全为0</li>
<li><strong>multi-head soft-attention</strong>。soft-attention和self-attention结构基本相同，甚至实现函数都是同一个。唯一的区别在于，self-attention的q k v矩阵来自同一个，所以叫self-attention。而soft-attention的q来自decoder，k和v来自encoder。表征的是encoder的整体输出对于decoder的贡献。</li>
<li><strong>feed-forward</strong>。这一块基本相同。</li>
</ol>
<p>另外三个模块均使用了残差连接，步骤仍然为 layerNorm -&gt; attention等模块 -&gt; dropout -&gt; 和输入进行add</p>
<p>decoder每个layer代码如下</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Decoder is made of self-attn, src-attn, and feed forward (defined below)"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, self_attn, src_attn, feed_forward, dropout)</span>:</span></span><br><span class="line">        super(DecoderLayer, self).__init__()</span><br><span class="line">        self.size = size</span><br><span class="line"></span><br><span class="line">        <span class="comment"># self-attention 自注意力</span></span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line"></span><br><span class="line">        <span class="comment"># soft-attenton, encoder的输出对decoder的作用</span></span><br><span class="line">        self.src_attn = src_attn</span><br><span class="line"></span><br><span class="line">        <span class="comment"># feed-forward 全连接</span></span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 残差连接</span></span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, memory, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="comment"># memory为encoder最终输出</span></span><br><span class="line">        m = memory</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1 对decoder输入做self-attention, 再和输入做残差连接</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, tgt_mask))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2 对encoder输出和decoder当前进行soft-attention，此处也有残差连接</span></span><br><span class="line">        x = self.sublayer[<span class="number">1</span>](x, <span class="keyword">lambda</span> x: self.src_attn(x, m, m, src_mask))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3 feed-forward全连接，也有残差连接</span></span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">2</span>](x, self.feed_forward)</span><br><span class="line"><span class="number">123456789101112131415161718192021222324252627282930</span></span><br></pre></td></tr></tbody></table></figure>
<h4 id="输出层">4 输出层</h4>
<p>decoder的输出作为最终输出层的输入，经过两步</p>
<ol type="1">
<li>linear线性连接，也即是w * x + b</li>
<li>softmax归一化，向量长度等于vocabulary的长度，得到vocabulary中每个字的概率。利用beam-search等方法，即可得到生成结果。</li>
</ol>
<p>这一层比较简单，代码如下</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Define standard linear + softmax generation step."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, vocab)</span>:</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        self.proj = nn.Linear(d_model, vocab)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># 先经过linear线性层，然后经过softmax得到归一化概率分布</span></span><br><span class="line">        <span class="comment"># 输出向量长度等于vocabulary的维度</span></span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(self.proj(x), dim=<span class="number">-1</span>)</span><br><span class="line"><span class="number">12345678910</span></span><br></pre></td></tr></tbody></table></figure>
<h4 id="总结">5 总结</h4>
<p>Transformer相比LSTM的优点</p>
<ol type="1">
<li><strong>完全的并行计算</strong>，Transformer的attention和feed-forward，均可以并行计算。而LSTM则依赖上一时刻，必须串行</li>
<li><strong>减少长程依赖</strong>，利用self-attention将每个字之间距离缩短为1，大大缓解了长距离依赖问题</li>
<li><strong>提高网络深度</strong>。由于大大缓解了长程依赖梯度衰减问题，Transformer网络可以很深，基于Transformer的BERT甚至可以做到24层。而LSTM一般只有2层或者4层。网络越深，高阶特征捕获能力越好，模型performance也可以越高。</li>
<li><strong>真正的双向网络</strong>。Transformer可以同时融合前后位置的信息，而双向LSTM只是简单的将两个方向的结果相加，严格来说仍然是单向的。</li>
<li><strong>可解释性强</strong>。完全基于attention的Transformer，可以表达字与字之间的相关关系，可解释性更强。</li>
</ol>
<p>Transformer也不是一定就比LSTM好，它的缺点如下</p>
<ol type="1">
<li>文本长度很长时，比如篇章级别，<strong>计算量爆炸</strong>。self-attention的计算量为O(n^2), n为文本长度。Transformer-xl利用层级方式，将计算速度提升了1800倍</li>
<li>Transformer位置信息只靠<strong>position encoding</strong>，效果比较一般。当语句较短时，比如小于10个字，Transformer效果不一定比LSTM好</li>
<li>Transformer参数量较大，在大规模数据集上，效果远好于LSTM。但在<strong>小规模数据集</strong>上，如果不是利用pretrain models，效果不一定有LSTM好。</li>
</ol>
<h3 id="transformer家族2----编码长度优化transformer-xllongformer">🚀Transformer家族2 -- 编码长度优化（Transformer-XL、Longformer）</h3>
<h4 id="背景">1 背景</h4>
<p>NLP中经常出现长程依赖问题，比如一个词语可能和它距离上千位置的另一个词语有关系。长程关系的建立十分困难。常见序列结构模型都有一些难点，如下。</p>
<ol type="1">
<li>在RNN中，由于反向传播梯度衰减和梯度爆炸问题，使得模型只能捕获较短距离。</li>
<li>LSTM利用门限机制，将连乘转变了为连加，提升了模型长程捕获能力，但梯度弥散问题没有从根本上得到解决，故其最大程度只能在400左右。</li>
<li>Transformer利用self-attention机制进行建模，使得任何两个位置token距离都为1。如果没有内存和算力的限制，Transformer理论上可以编码无限长的文本。但由于attention计算量十分大，而且计算复杂度和序列长度为O(n^2)关系，导致序列长度增加，内存和计算量消耗飞快增加。实际中由于内存和算力有限，一般只能编码一定长度，例如512。</li>
</ol>
<p>为了提升模型的长程编码能力，从而提升模型在长文本，特别是document-level语料上的效果，我们必须对Transformer编码长度进行优化。本文带来了Transformer-XL、Longformer，详细分析他们如何实现编码长度优化。</p>
<p>LongFormer通过降低attention计算所需内存和算力，来实现长文本编码。我们也可以把它归入到算力优化中。但鉴于其名字就重点体现了它的长距离能力，故还是放在了编码长度优化中，和Transformer-XL一起来分析</p>
<h4 id="transformer-xl">2 Transformer-XL</h4>
<p><img src="https://i.loli.net/2020/10/28/ctFMhR8IEdPVSZ1.png" alt="在这里插入图片描述"> 论文信息：2019年01月，谷歌 &amp; CMU，ACL 2019 论文地址 https://arxiv.org/abs/1901.02860 代码和模型地址 https://github.com/kimiyoung/transformer-xl</p>
<h5 id="为什么需要transformer-xl">2.1 为什么需要Transformer-XL</h5>
<p>为了解决长文本编码问题，原版Transformer采用了固定编码长度的方案，例如512个token。将长文本按照固定长度，切分为多个segment。每个segment内部单独编码，segment之间不产生交互信息。这种方式的问题如下</p>
<ol type="1">
<li>模型无法建模超过固定编码长度的文本</li>
<li>segment之间没有交互信息，导致了文本碎片化。长语句的编码效果有待提升。</li>
<li>predict阶段，decoder每生成一个字，就往后挪一个，没有重复利用之前信息，导致计算量爆炸</li>
</ol>
<p>train和evaluate过程如下<img src="https://i.loli.net/2020/10/28/8YW7R4Q5J6zbmD1.png" alt="在这里插入图片描述"></p>
<h5 id="实现方法">2.2 实现方法</h5>
<h6 id="segment-level-recurrence-with-state-reuse-片段级递归和信息复用">2.2.1 Segment-Level Recurrence with State Reuse 片段级递归和信息复用</h6>
<p>Transformer-XL在编码后一个segment时，将前一个segment的隐层缓存下来。后一个segment的self-attention计算，会使用到前一个segment的隐层。后一个segment的第n+1层，对前一个segment的第n层隐层进行融合。故最大编码长度理论上为O(N × L)。在预测阶段，由于对segment隐层使用了缓存，故每预测一个词，不需要重新对之前的文本进行计算。大大提升了预测速度，最大可达到原始Transformer的1800倍。如下图所示 <img src="https://i.loli.net/2020/10/28/KmazXviordxyZuw.png" alt="在这里插入图片描述"></p>
<h6 id="relative-positional-encodings-相对位置编码">2.2.2 Relative Positional Encodings 相对位置编码</h6>
<p>segment递归中有个比较大的问题，就是如何区分不同segment中的相同位置。如果采用原版Transformer中的绝对编码方案，两者是无法区分的。如下 <img src="https://i.loli.net/2020/10/28/38l7XhVjZnqyuBe.png" alt="gh">不同segment中的相同位置，其position encoding会相同。这显然是有问题的。Transformer-XL将绝对位置编码改为了q和k之间的相对位置编码，代表了两个token之间的相对位置。从语义上讲，是make sense的。我们来看看具体实现方式。</p>
<p>绝对位置编码的attention计算如下 <img src="https://i.loli.net/2020/10/28/IKCVE7riwaQ9Zl1.png" alt="在这里插入图片描述">分为四部分</p>
<ol type="1">
<li>query的token encoding和 key的token encoding，之间的关联信息</li>
<li>query的token encoding和 key的position encoding，之间的关联信息。Uj为绝对位置j的编码向量</li>
<li>query的position encoding和 key的token encoding，之间的关联信息。Ui为绝对位置i的编码向量</li>
<li>query的position encoding和 key的position encoding，之间的关联信息</li>
</ol>
<p>而采用相对位置编码后，attention计算如下 <img src="https://i.loli.net/2020/10/28/Id9fV3bxSnmHyu2.png" alt="在这里插入图片描述">同样包含四部分，仍然为二者token encoding和position encoding之间的关联关系。区别在于</p>
<ol type="1">
<li>Ri-j为i和j之间相对位置编码，其中R为相对位置编码矩阵</li>
<li>u和v为query的位置编码，采用一个固定向量。因为采用相对位置编码后，无法对单个绝对位置进行编码了。文中称为global content bias，和global positional bias</li>
</ol>
<p>也有其他文章，采用了不同的相对位置编码方案。比如"Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.2018. Self-attention with relative position representations. arXiv preprint arXiv:1803.02155." 中只有a和b两部分，丢掉了c和d。Transformer-XL对两种方案进行了对比实验，证明前一种好。</p>
<h5 id="实验结果">2.3 实验结果</h5>
<h6 id="长文本编码效果">长文本编码效果</h6>
<p><img src="https://i.loli.net/2020/10/28/IyoR924J3BQWrOu.png" alt="在这里插入图片描述"> 在WikiText-103上的实验结果。WikiText-103包含词级别的超长文本，平均每篇文章长度为3.6K token。利用它可以验证模型的长文本编码能力。实验结果表明Transformer-XL large的PPL最低，效果最好。同时作者在One Billion Word、enwik8、text8上均进行了实验，都表明Transformer-XL效果最好。</p>
<h6 id="有效编码长度">有效编码长度</h6>
<p><img src="https://i.loli.net/2020/10/28/oKQv3akDcXWOZ7B.png" alt="在这里插入图片描述"> 模型可编码的有效长度如上，r为top-r难度样本上的表现。Transformer-XL比RNN长80%，比Transformer长450%。证明Transformer-XL可编码长度最长，长程捕获能力最强。</p>
<h6 id="预测速度">预测速度</h6>
<p><img src="https://i.loli.net/2020/10/28/g2TyZhWtRSEA7pB.png" alt="在这里插入图片描述"> 在不同segment长度下，模型预测速度的对比。和原始Transformer相比，预测速度最大可以提升1874倍。</p>
<h6 id="消融分析">消融分析</h6>
<p>文章对片段级递归和相对位置编码两个Method进行了消融分析，如下 <img src="https://i.loli.net/2020/10/28/4EeY78JcQojSuhD.png" alt="在这里插入图片描述"> 两个改进点均有作用，其中片段级递归作用更大。</p>
<h4 id="longformer">3 Longformer</h4>
<p><img src="https://i.loli.net/2020/10/28/KhcS2VIyvNEAwrg.png" alt="在这里插入图片描述"> 论文信息：2020年04月，allenai 论文地址 https://arxiv.org/abs/2004.05150 代码和模型地址 https://github.com/allenai/longformer</p>
<h5 id="改进方法">3.1 改进方法</h5>
<h6 id="attention稀疏化">3.1.1 attention稀疏化</h6>
<p>Transformer不能捕获长距离信息，本质原因还是因为计算量过大导致的。那我们通过降低attention计算量，是不是就可以提升长距离编码能力呢。答案是肯定的，LongFormer提出了三种attention稀疏化的方法，来降低计算量。 <img src="https://i.loli.net/2020/10/28/hDmpknyqTSIUEW7.png" alt="在这里插入图片描述"> a是原始的全连接方式的attention。后面三种为文章使用的稀疏attention。</p>
<ol type="1">
<li>Sliding Window attention。滑窗方式的attention。假设序列长度为n，滑窗大小w，则每个位置的attention只和滑窗范围内的token进行计算，复杂度从O(n^2)变为了O(n * w)。当w &lt;&lt; n时，可以大大降低计算量。</li>
<li>Dilated Sliding Window attention。受到空洞卷积的启发，提出了空洞滑窗attention。看下面这张图就明白了。</li>
</ol>
<p><img src="https://i.loli.net/2020/10/28/ZwAy1H3kB7SjIP5.png" alt="在这里插入图片描述"></p>
<ol type="1">
<li>Global Attention + sliding window。某些关键位置采用全局attention，这些位置的attention和所有token进行计算。而其他相对不关键的位置则采用滑窗attention。那什么叫关键位置呢？作者举例，分类问题中[CLS]为关键位置，需要计算全局attention。QA中question每个位置均为关键位置，同样计算全局attention。</li>
</ol>
<h6 id="tensor-virtual-machine-tvm">3.1.2 Tensor Virtual Machine (TVM)</h6>
<p>作者使用了TVM构建CUDA kernel，加快了longformer的速度，并降低了显存需求。这个又是另一个模型加速方面的话题，我们就先不展开了。</p>
<h5 id="实验结果-1">3.2 实验结果</h5>
<h6 id="大小模型效果">大小模型效果</h6>
<p><img src="https://i.loli.net/2020/10/28/ZThJDNXByQp2Lod.png" alt="在这里插入图片描述">作者在大小模型上均实验了LongFormer的效果。小模型为12 layers，512 hidden。大模型为30 layers，512 hidden。在text8和enwik8数据集上。小模型达到了SOTA。大模型比18层的Transformer-XL好，虽然不如Adaptive-span-Transformer和Compressive，但胜在可以pretrain-finetune</p>
<h6 id="消融分析-1">消融分析</h6>
<p><img src="https://i.loli.net/2020/10/28/pMCoW2ZLv6gq3j4.png" alt="在这里插入图片描述">消融分析中，可以发现</p>
<ol type="1">
<li>Dilation空洞，有一定的收益</li>
<li>top layer滑窗大小比bottom layer大时，效果好一些。这个也是make sense的。因为top layer捕获高维语义，关联信息距离相对较远，窗口应该尽量大一些。</li>
</ol>
<h6 id="语料长度">语料长度</h6>
<p><img src="https://i.loli.net/2020/10/28/IjGfH7Mi1LK2hsC.png" alt="在这里插入图片描述"> 从上表中我们发现，语料都是特别长的长文本。LongFormer真的是document级别的Transformer。</p>
<h6 id="下游任务finetune效果">下游任务finetune效果</h6>
<p><img src="https://i.loli.net/2020/10/28/JdZUrh6L3QFBHIn.png" alt="在这里插入图片描述"> <img src="https://i.loli.net/2020/10/28/WlOZR3iaJSA2nIU.png" alt="在这里插入图片描述"> 第一个table为RoBERTa和LongFormer在问答、指代消解、分类任务中的对比。第二个table为这几个任务数据集平均文本长度。每项任务都是超出RoBERTa，当文本长度大于512时，performance提升特别明显。更加说明了长程捕获能力在NLP中的重要性。</p>
<h3 id="transformer家族3----计算效率优化adaptive-spanreformerlite-transformer">🚀Transformer家族3 -- 计算效率优化（Adaptive-Span、Reformer、Lite-Transformer）</h3>
<h4 id="背景-1">1 背景</h4>
<p>上文我们从编码长度优化的角度，分析了如何对Transformer进行优化。Transformer-XL、LongFormer等模型，通过片段递归和attention稀疏化等方法，将长文本编码能力提升到了很高的高度。基本已经克服了Transformer长文本捕获能力偏弱的问题，使得下游任务模型performance得到了较大提升，特别是文本较长（大于512）的任务上。</p>
<p>但Transformer计算量和内存消耗过大的问题，还亟待解决。事实上，Transformer-XL、LongFormer已经大大降低了内存和算力消耗。毕竟Transformer之所以长距离编码能力偏弱，就是因为其计算量是序列长度的平方关系，对算力需求过大，导致当前GPU/TPU不能满足需求。编码长度优化和计算量优化，二者是相辅相成的。但着眼于论文的出发点，我们还是分为两个不同的章节进行分析。毕竟总不能所有模型都放在一个章节吧（_）。</p>
<p>本文我们带来Adaptive-Span Transformer、Reformer、Lite-Transformer等几篇文章</p>
<h4 id="adaptive-span-transformer">2 Adaptive-Span Transformer</h4>
<p><img src="https://i.loli.net/2020/10/28/SZIDO7KcoGHqvhF.png" alt="在这里插入图片描述"> 论文信息：2019年5月，FaceBook，ACL2019 论文地址 https://arxiv.org/pdf/1905.07799.pdf 代码和模型地址 https://github.com/facebookresearch/adaptive-span</p>
<h5 id="为什么需要adaptive-span">2.1 为什么需要Adaptive-Span</h5>
<p>之前Transformer-XL将长文本编码能力提升到了较高高度，但是否每个layer的每个head，都需要这么长的attention呢？尽管使用了多种优化手段，长距离attention毕竟还是需要较大的内存和算力。研究发现，大部分head只需要50左右的attention长度，只有少部分head需要较长的attention。这个是make sense的，大部分token只和它附近的token有关联。如下图 <img src="https://i.loli.net/2020/10/28/nOstVdScKG68CDz.png" alt="在这里插入图片描述">我们是否可以实现attention span长度的自适应呢？让不同的layer的不同的head，自己去学习自己的attention span长度呢？Adaptive-Span Transformer给出了肯定答案。</p>
<h5 id="实现方案">2.2 实现方案</h5>
<p>文章设定每个attention head内的token计算，都使用同一个span长度。我们就可以利用attention mask来实现自适应span。对每个head都添加一个attention mask，mask为0的位置不进行attention计算。文章设计的mask函数如下 <img src="https://i.loli.net/2020/10/28/yRVqrjfJPgBKlIk.png" alt="在这里插入图片描述"> R为超参，控制曲线平滑度。其为单调递减函数，如下图。 <img src="https://i.loli.net/2020/10/28/hvoFmDIp1BjgV5f.png" alt="在这里插入图片描述"></p>
<h5 id="实验结果-2">2.3 实验结果</h5>
<p><img src="https://i.loli.net/2020/10/28/EDsBxfipL4HcOwF.png" alt="在这里插入图片描述"> 和Transformer家族其他很多模型一样，Adaptive-span也在字符级别的语言模型上进行了验证，数据集为text8。如上，Transformer注意力长度固定为512，结论如下</p>
<ol type="1">
<li>Transformer-XL长程编码能力确实很强，平均span可达3800。</li>
<li>注意力长度确实不需要总那么长，Adaptive-Span大模型上，平均长度只有245</li>
<li>Adaptive-Span在算力需求很小（只有XL的1/3）的情况下，效果可以达到SOTA。</li>
</ol>
<p><img src="https://i.loli.net/2020/10/28/IDjAR2ucfELsegw.png" alt="在这里插入图片描述">上面是在enwik8上的结果。Adaptive-Span又一次在算力很小的情况下，达到了最优效果。值得注意的是，64层的Transformer居然需要120G的计算量，又一次证明了原版Transformer是多么的吃计算资源。另外Transformer-XL在节省计算资源上，其实也算可圈可点。</p>
<h4 id="reformer">3 Reformer</h4>
<p><img src="https://i.loli.net/2020/10/28/G8AVxR7fuCZdPHn.png" alt="在这里插入图片描述"> 论文信息：2020年1月，谷歌，ICLR2020 论文地址 https://arxiv.org/abs/2001.04451 代码和模型地址 https://github.com/google/trax/tree/master/trax/models/reformer</p>
<h5 id="为什么需要reformer">3.1 为什么需要Reformer</h5>
<p>Transformer内存和计算量消耗大的问题，一直以来广为诟病，并导致其一直不能在长文本上进行应用。（BERT、RoBERTa均设置最大长度为512）。Reformer认为Transformer有三大问题</p>
<ol type="1">
<li>attention层计算量和序列长度为平方关系，导致无法进行长距离编码</li>
<li>内存占用和模型层数呈N倍关系，导致加深Transformer层数，消耗的内存特别大</li>
<li>feed-forward的dff比隐层dmodel一般大很多，导致FF层占用的内存特别大</li>
</ol>
<p>针对这几个问题，Reformer创新性的提出了三点改进方案</p>
<ol type="1">
<li>LOCALITY-SENSITIVE HASHING 局部敏感hash，使得计算量从 O(L^2)降低为O(L log L) ,L为序列长度</li>
<li>Reversible Transformer 可逆Transformer，使得N层layers内存消耗变为只需要一层，从而使得模型加深不会受内存限制。</li>
<li>Feed-forward Chunking 分块全连接，大大降低了feed-forward层的内存消耗。</li>
</ol>
<p>Reformer是Transformer家族中最为关键的几个模型之一（去掉之一貌似都可以，顶多Transformer-XL不答应），其创新新也特别新颖，很多思想值得我们深入思考和借鉴。其效果也是特别明显，大大提高了内存和计算资源效率，编码长度可达64k。下面针对它的三点改进方案进行分析，有点难懂哦。</p>
<h5 id="实现方案-1">3.2 实现方案</h5>
<h6 id="locality-sensitive-hashing-局部敏感hash">3.2.1 LOCALITY-SENSITIVE HASHING 局部敏感hash</h6>
<p>局部敏感hash有点难懂，Reformer针对Transformer结构进行了深度灵魂拷问</p>
<h6 id="query和key必须用两套吗">Query和Key必须用两套吗</h6>
<p>Transformer主体结构为attention，原版attention计算方法如下 <img src="https://i.loli.net/2020/10/28/Z1fYDpO6BnLyHj4.png" alt="在这里插入图片描述"> 每个token，利用其query向量，和其他token的key向量进行点乘，从而代表两个token之间的相关性。归一化后，利用得到的相关性权重，对每个token的value向量进行加权求和。首先一个问题就是，query和key向量可以是同一套吗？我们可否利用key向量去和其他token的key计算相关性呢？</p>
<p>为此文章进行实验分析，证明是可行的。个人认为这一点也是make sense的。 <img src="https://i.loli.net/2020/10/28/YyTZDG1Bohkswit.png" alt="在这里插入图片描述">在文本和图像上，Q=K的attention，和普通attention，效果差别不大。</p>
<h6 id="必须和每个token计算相关性吗">必须和每个token计算相关性吗</h6>
<p>原版attention中，一个token必须和序列中其他所有token计算相关性，导致计算量随序列长度呈平方关系增长，大大制约了可编码最大长度。那必须和每个token计算相关性吗？其实之前Adaptive-Span Transformer也深度拷问过这个话题。它得出的结论是，对于大部分layer的multi-head，长度50范围内进行attention就已经足够了。不过Adaptive-Span采取的方法还是简单粗暴了一点，它约定每个head的attention span长度是固定的，并且attention span为当前token附近的其他token。</p>
<p>Adaptive-Span Transformer的这种方法显然还是没有抓住Attention计算冗余的痛点。Attention本质是加权求和，权重为两个token间的相关性。最终结果取决于较大的topk权重，其他权值较小的基本就是炮灰。并且softmax归一化更是加剧了这一点。小者更小，大者更大。为了减少计算冗余，我们可以只对相关性大的其他token的key向量计算Attention。</p>
<h6 id="怎么找到相关性大的向量呢">怎么找到相关性大的向量呢</h6>
<p>我们现在要从序列中找到与本token相关性最大的token，也就是当前key向量与哪些key向量相关性大。极端例子，如果两个向量完全相同，他们的相关性是最高的。确定两个高维向量的相关性确实比较困难，好在我们可以利用向量Hash来计算。</p>
<p>Reformer采用了局部敏感hash。我们让两个key向量在随机向量上投影，将它们划分到投影区间内。 <img src="https://i.loli.net/2020/10/28/gSHlhWRaoqfk9pN.png" alt="在这里插入图片描述"> 如图所示，划分了四个区间（4个桶bucket），进行了三次Hash。第一次Hash，使得上面两个向量分别归入bucket0和bucket3中，下面两个向量都归入bucket0。第二次Hash，上面两个向量和下面两个，均归入到bucket2中了。我们可以发现</p>
<ol type="1">
<li>相似的向量，也就是相关性大的，容易归入到一个bucket中</li>
<li>局部敏感Hash还是有一定的错误率的，我们可以利用多轮Hash来缓解。这也是Reformer的做法，它采取了4轮和8轮的Hash。</li>
</ol>
<h6 id="整个流程">整个流程</h6>
<p>经过局部敏感Hash后，我们可以将相关性大的key归入同一个bucket中。这样只用在bucket内进行普通Attention即可，大大降低了计算冗余度。为了实现并行计算，考虑到每个bucket包含的向量数目可能不同，实际处理中需要多看一个bucket。整个流程如下 <img src="https://i.loli.net/2020/10/28/ZS5dCT8nVcaiob4.png" alt="在这里插入图片描述"></p>
<ol type="1">
<li>让query等于key</li>
<li>局部敏感Hash（LSH）分桶。上图同一颜色的为同一个桶，共4个桶</li>
<li>桶排序，将相同的桶放在一起</li>
<li>为了实现并行计算，将所有桶分块（chunk），每个chunk大小相同</li>
<li>桶内计算Attention，由于之前做了分块操作，所以需要多看一个块。</li>
</ol>
<h6 id="多轮lsh">多轮LSH</h6>
<p>为了减少分桶错误率，文章采用了多次分桶，计算LSH Attention，Multi-round LSH attention。可以提升整体准确率。如下表。<img src="https://i.loli.net/2020/10/28/L4BDoiwTfaRjInv.png" alt="在这里插入图片描述"></p>
<h6 id="reversible-transformer-可逆transformer">3.2.2 REVERSIBLE TRANSFORMER 可逆Transformer</h6>
<p>LSH局部敏感Hash确实比较难理解，可逆Transformer相对好懂一些。这个方案是为了解决Transformer内存占用量，随layers层数线性增长的问题。为什么会线性增长呢？原因是反向传播中，梯度会从top layer向bottom layer传播，所以必须保存住每一层的Q K V向量，也就导致N层就需要N套Q K V。</p>
<p>那有没有办法不保存每一层的Q K V呢？可逆Transformer正是这个思路。它利用时间换空间的思想，只保留一层的向量，反向传播时，实时计算出之前层的向量。所以叫做Reversible。Reformer每一层分为两部分，x1和x2。输出也两部分，y1和y2。计算如下</p>
<p><img src="https://i.loli.net/2020/10/28/gKZT1mRFhEj48nU.png" alt="image-20201028214552153"></p>
<p>采用可逆残差连接后，模型效果基本没有下降。这也是make sense的，毕竟可逆是从计算角度来解决问题的，对模型本身没有改变。 <img src="https://i.loli.net/2020/10/28/hElWr4uHtUomVYG.png" alt="在这里插入图片描述"></p>
<h6 id="feed-forward-chunking-ff层分块">3.2.3 Feed-Forward chunking FF层分块</h6>
<p>针对fead-forward层内存消耗过大的问题，Reformer也给出了解决方案，就是FF层分块。如下 <img src="https://i.loli.net/2020/10/28/rpzwVXOSUavEs18.png" alt="在这里插入图片描述"></p>
<h5 id="实验结果-3">3.3 实验结果</h5>
<h6 id="内存和时间复杂度">内存和时间复杂度</h6>
<p>Reformer三个创新点，大大降低了内存和时间复杂度，消融分析如下 <img src="https://i.loli.net/2020/10/28/5ixhnpvKeQHrBw1.png" alt="在这里插入图片描述"></p>
<h6 id="模型效果">模型效果</h6>
<p>如下为在机器翻译上的效果。Reformer减少了算力消耗，同时也大大增加了长文本编码能力，故模型效果也得到了提升。如下。 <img src="https://i.loli.net/2020/10/28/8EtHVoBsfYxnmhO.png" alt="在这里插入图片描述"></p>
<h4 id="lite-transformer">4 Lite Transformer</h4>
<p><img src="https://i.loli.net/2020/10/28/9UfkunTVEpL5XBG.png" alt="在这里插入图片描述"> 论文信息：2020年4月，MIT &amp; 上海交大，ICLR2020 论文地址 https://arxiv.org/abs/2004.11886 代码和模型地址 https://github.com/mit-han-lab/lite-transformer</p>
<h5 id="为什么要做lite-transformer">4.1 为什么要做Lite Transformer</h5>
<p>主要出发点仍然是Transformer计算量太大，计算冗余过多的问题。跟Adaptive-Span Transformer和Reformer想法一样，Lite Transformer也觉得没必要做Full Attention，很多Attention连接是冗余的。不一样的是，它通过压缩Attention通道的方式实现，将多头减少了一半。与Base Transformer相比，计算量减少了2.5倍。并且文章使用了量化和剪枝技术，使得模型体积减小了18.2倍。</p>
<h5 id="实现方案-2">4.2 实现方案</h5>
<p>实现方案很简单，仍然采用了原版Transformer的seq2seq结构，创新点为</p>
<ol type="1">
<li>multiHead self-attention变为了两路并行，分别为一半的通道数（多头）。如下图a所示。其中左半部分为正常的fully attention，它用来捕获全局信息。右半部分为CNN卷积，用来捕获布局信息。最终二者通过FFN层融合。这个架构称为LONG-SHORT RANGE ATTENTION (LSRA)，长短期Attention。</li>
<li>为了进一步降低计算量，作者将CNN转变为了一个depth wise卷积和一个线性全连接。dw卷积在mobileNet中有讲过，不清楚可自行谷歌。</li>
</ol>
<p><img src="https://i.loli.net/2020/10/28/WQtJvpbP9rUczhs.png" alt="在这里插入图片描述"></p>
<h5 id="实验结果-4">4.3 实验结果</h5>
<h6 id="计算复杂度">计算复杂度</h6>
<p><img src="https://i.loli.net/2020/10/28/w6b1LlqIpZgt8fX.png" alt="在这里插入图片描述"> 如上图，在文本摘要任务上，Lite Transformer计算量相比Base Transformer，减少了2.5倍。同时Rouge指标基本没变。</p>
<h6 id="模型体积">模型体积</h6>
<p><img src="https://i.loli.net/2020/10/28/JYzeocGBUr6jy4W.png" alt="在这里插入图片描述"> Lite Transformer模型体积只有Transformer的2.5分之一，通过8bit量化和剪枝，最终模型体积下降了18.2倍。</p>
<h4 id="其他">5 其他</h4>
<p>其他几篇文章，也建议拜读下</p>
<ol type="1">
<li><a href="https://arxiv.org/abs/1904.10509" target="_blank" rel="noopener">Generating Long Sequences with Sparse Transformers</a> (OpenAI, 2019.04)</li>
<li><a href="https://arxiv.org/abs/1909.00015" target="_blank" rel="noopener">Adaptively Sparse Transformers</a> (EMNLP2019, 2019.09)</li>
<li><a href="https://arxiv.org/abs/1911.05507" target="_blank" rel="noopener">Compressive Transformers for Long-Range Sequence Modelling</a> (2019.11)</li>
<li><a href="https://arxiv.org/abs/2002.06170" target="_blank" rel="noopener">Transformer on a Diet</a> (2020.02)</li>
</ol>
<h3 id="transformer家族4----通用性优化universal-transformer">🚀Transformer家族4 -- 通用性优化（Universal-Transformer）</h3>
<h4 id="背景-2">1 背景</h4>
<p>之前讲Transformer的时候，也提到过它的通用性的缺点。相比于RNN，Transformer不是图灵完备的，虽然大多数任务都是吊打RNN，但在某些看起来极为简单的任务上，却表现很差，比如字符串拷贝等。这个问题其实也不算大，但谷歌还是给出了他的解决方案，也就是Universal Transformer。这篇看看就好了，个人感觉实际应用中作用有限。</p>
<h4 id="universal-transformer">2 Universal-Transformer</h4>
<p><img src="https://i.loli.net/2020/10/28/Lk3ymRQKZHpwN8e.png" alt="在这里插入图片描述">论文信息：2018年7月，谷歌，ICLR2019 论文地址 https://arxiv.org/abs/1807.03819 代码和模型地址 https://github.com/tensorflow/tensor2tensor</p>
<h5 id="为什么需要universal-transformer">2.1 为什么需要Universal-Transformer</h5>
<p>主要的出发点是原版Transformer不是图灵完备的，有些很简单的任务表现很差，比如字符串拷贝。序列任务还是比较偏好于迭代和递归变换，RNN正好满足了这一点，而Transformer不满足。这一点文章称作归纳偏置（Inductive Bias）。<a href="https://www.zhihu.com/question/41404496/answer/627673667" target="_blank" rel="noopener">深度学习的归纳偏置是什么？</a></p>
<h5 id="实现方案-3">2.2 实现方案</h5>
<h6 id="模型结构">模型结构</h6>
<p><img src="https://i.loli.net/2020/10/28/6aOdB5QqIYKSlEb.png" alt="在这里插入图片描述"> 如上所示为Universal-Transformer的结构，仍然为一个基于multiHead self-attention的seq2seq，几点不同</p>
<ol type="1">
<li>引入了时间步step，从而实现了循环递归。除了第一次是原始信息作为输入，之后都是由前一个step的输出作为后一个的输入。</li>
<li>Feed-forward换成了Transition函数。根据task不同，可选择separable convolution分解卷积和fully-connected neural network全连接神经网络。</li>
<li>时间和位置编码，TimeStep embedding和Position embedding，新引入了TimeStep embedding，二者的编码公式和Transformer中的位置编码很像，如下</li>
</ol>
<p><img src="https://i.loli.net/2020/10/28/tdJkSqcoZ4FGHNQ.png" alt="在这里插入图片描述"></p>
<h6 id="adaptive-computation-timeact-自适应计算时间">Adaptive Computation Time（ACT） 自适应计算时间</h6>
<p>前人已经提到过ACT了，作者在模型中引用了。序列问题中，有些词语比其他的更模糊。他们需要进行更多次的计算。Universal-Transformer利用了ACT机制，可以对每个token设置自适应计算时间。模型会动态调整每个位置所需的计算steps。当某个位置停止计算后，直接copy它的隐状态到下一step。当所有位置都停止计算后，整个过程才停止。<img src="https://i.loli.net/2020/10/28/pXaRS5Q7fVDuxyW.png" alt="在这里插入图片描述"> 如上，不同位置token所需的计算steps是不同的。</p>
<h5 id="实验结果-5">2.3 实验结果</h5>
<h6 id="字符串任务">字符串任务</h6>
<p><img src="https://i.loli.net/2020/10/28/iT5HIsel3WjDCxA.png" alt="在这里插入图片描述">字符串复制、翻转、添加操作的效果。可以发现</p>
<ol type="1">
<li>Transformer效果确实比较差，比LSTM差很多。这也验证了Transformer通用性确实有些问题，也就是本文的出发点</li>
<li>Universal-Transformer效果很好，超过LSTM很多，成功解决了原版Transformer的问题</li>
</ol>
<h6 id="机器翻译">机器翻译</h6>
<p><img src="https://i.loli.net/2020/10/28/LZTwqn9hjpBiAyr.png" alt="在这里插入图片描述">机器翻译上的结果，Universal-Transformer的BLEU比原版Transformer提高了0.9%</p>
<h3 id="transformer家族5----推理加速faster-transformer-turbotransformers">🚀Transformer家族5 -- 推理加速（Faster-Transformer 、TurboTransformers）</h3>
<h4 id="背景-3">1 背景</h4>
<p>之前介绍了从编码长度、计算效率、通用性等角度对Transformer进行优化，并介绍了几个重要模型。本文介绍如何进行Transformer推理加速。相比于离线训练，在线推理加速更加关键。一方面由于在线流量大，加速可带来硬件成本的节省。另一方面在线推理加速，可大大提升AI应用的用户体验。</p>
<p>事实上，之前的多种方法，特别是计算效率优化，对推理加速很有帮助。这些模型从算法的角度，进行了推理速度优化。本文主要从框架层的角度，讲解如何对推理进行加速。主要带来NVIDIA的Faster-Transformer框架和腾讯的Turbo-Transformer框架。</p>
<h4 id="faster-transformer">2 Faster-Transformer</h4>
<p>PPT资料：https://on-demand.gputechconf.com/gtc-cn/2019/pdf/CN9468/presentation.pdf 代码地址：https://github.com/NVIDIA/DeepLearningExamples/tree/master/FasterTransformer</p>
<h5 id="实现方案-4">实现方案</h5>
<p>Faster-Transformer算法结构和原版Transformer基本一致，主要是从框架层角度来实现计算加速。主要方法有</p>
<ol type="1">
<li>算子融合。对除矩阵乘法外的所有算子，进行了合并。比如Add、Sub。从而减少了GPU kernel调度和显存读写。</li>
<li>半精度F16优化。</li>
<li>GELU激活函数、层正则化、softmax等调用频次很高的操作的优化</li>
</ol>
<h5 id="效果">效果</h5>
<p><img src="https://i.loli.net/2020/10/28/AGKsp9WPLQ2julY.png" alt="在这里插入图片描述"> Encoder效果对比如上。Faster-Transformer基本吊打TF XLA，提升速度一倍多。<img src="https://i.loli.net/2020/10/28/yYDIensgFQmv3HG.png" alt="在这里插入图片描述"> Decoder效果对比如上。对比了32bit和16bit的结果。Decoding FP32和Decoding FP16为Faster-Transformer 的结果，也是吊打原始TensorFlow。</p>
<h4 id="turbotransformers">3 <strong>TurboTransformers</strong></h4>
<p><img src="https://i.loli.net/2020/10/28/IKjLfbDRNgMd5rE.png" alt="在这里插入图片描述">代码地址 https://github.com/Tencent/TurboTransformers</p>
<h5 id="实现方案-5">实现方案</h5>
<ol type="1">
<li>和Faster-Transformer一样，进行了算子融合。从而减少GPU kernel调用和显存占用</li>
<li>对于LayerNorm和softmax，由于不适合并行计算，重新开发并实现了并行计算版本。</li>
<li>内存缓存，避免频繁释放和分配内存。</li>
</ol>
<h5 id="和其他方案的对比">和其他方案的对比</h5>
<p><img src="https://i.loli.net/2020/10/28/BzXjLlFERMe34x2.png" alt="在这里插入图片描述"></p>
<h5 id="效果-1">效果</h5>
<p><img src="https://i.loli.net/2020/10/28/FnUxJrzqgL6eEks.png" alt="在这里插入图片描述">V100上的QPS，越高代表框架性能越好。对比了PyTorch、TensorFlow、Faster-Transformer、turboTransformers的效果，其中turboTransformers效果最好</p>
<h3 id="邱锡鹏教授nlp预训练模型综述">🚀邱锡鹏教授：NLP预训练模型综述</h3>
<h4 id="引言"><strong>1.引言</strong></h4>
<p>随深度学习的发展，多种神经网络都被应用在 NLP 任务中，比如 CNN、RNN、GNN 和 attention 机制等，但由于现有的数据集对于大部分有监督 NLP 任务来说都很小，因此，早期的模型对 NLP 任务来说都很“浅”，往往只包含 1-3 层。</p>
<p>而预训练模型（Pre-trained Models, PTMs）的出现将NLP带入一个新的时代，更“深”的模型和训练技巧的增强也使得 PTMs 由“浅”变“深”，在多项任务都达到了 SOTA 性能。</p>
<p>近日，复旦大学的邱锡鹏老师等人发布了预训练模型综述 *<strong>Pre-trained Models for Natural Language Processing: A Survey*</strong>，从背景、分类到应用与前景对 PTMs 做了详细而全面的调研。</p>
<p><img src="https://i.loli.net/2020/10/28/lLPuxzv8IVRTAQO.png" alt="img"></p>
<p><strong>论文标题：</strong>Pre-trained Models for Natural Language Processing: A Survey</p>
<p><strong>论文链接：</strong> https://arxiv.org/abs/2003.08271</p>
<h4 id="背景-4"><strong>2.背景</strong></h4>
<h5 id="语言表示学习"><strong>2.1 语言表示学习</strong></h5>
<p>对于语言来说，一个好的表示应当描绘语言的内在规则比如词语含义、句法结构、语义角色甚至语用。</p>
<p><img src="https://i.loli.net/2020/10/28/ZmF3yXaiHPME1LQ.png" alt="img"></p>
<p>而分布式表示的核心思想就是通过低维实值向量来描述一段文本的意义，而向量的每一个维度都没有对于意义，整体则代表一个具体的概念。图 1 是 NLP 的通用神经体系架构。</p>
<p>有两种 embedding（词嵌入）方式：上下文嵌入和非上下文嵌入，两者的区别在于词的 embedding 是否根据词出现的上下文动态地改变。</p>
<p><strong>非上下文嵌入：</strong>表示语言的第一步就是将分离的语言符号映射到分布式嵌入空间中。也就是对于词汇表中的每个单词（词根），通过 lookup table 映射到一个向量。</p>
<p>这种嵌入方式有两个局限：一是一个词通过这种方法获得的词嵌入总是静态且与上下文无关的，无法处理多义词；二是难以解决不在词汇表中的词（针对这个问题，很多 NLP 任务提出了字符级或词根级的词表示，如 CharCNN、FastText、Byte-Pair Encoding (BPE)）。</p>
<p><strong>上下文嵌入：</strong>为解决多义性和上下文相关的问题，将词在不同上下文的语义做区分。通过对词（词根）的 token 加一层 Neural Contextual Encoder（神经上下文编码器）得到词的上下文嵌入。</p>
<h3 id="section"></h3>
<h5 id="神经上下文编码器"><strong>2.2 神经上下文编码器</strong></h5>
<p><img src="https://i.loli.net/2020/10/28/DeQYroznLq4cCkt.png" alt="img"></p>
<p>如图 2 中所示，大部分的神经上下文编码器都可以被分为三类：卷积模型、序列模型、基于图的模型。</p>
<p><strong>卷积模型 ：</strong>卷积模型通过卷积操作将输入句子中的 embeddings 与其相邻的局部信息集成。</p>
<p><strong>序列模型 ：</strong>序列模型通常使用 RNN（如 LSTM 和 GRU）来描述词的上下文表示。实践中，双向 RNN 常用于收集词的两边信息，但表现往往会受到长程依赖问题的影响。</p>
<p><strong>基于图的模型 ：</strong>基于图的模型将词视做节点，通过预先定义的语言结构（如句法结构和语义联系）来学习上下文表示。但如何构造一个好的图结构往往严重依赖于专家知识和外部 NLP 工具，如依存分析器。</p>
<p>实际操作中往往直接通过一个全连接图来建模并让模型自己学习结构（一般通过自注意力机制）。一个典型的成功运用就是 Transformer。</p>
<p><strong>分析：</strong>卷积模型和序列模型都很难解决词之间的长程依赖问题，而 Transformer 虽然能更好地描述词之间的深层联系，却往往需要非常大的语料来训练，且容易在中等规模的数据集上过拟合。</p>
<h5 id="为什么要预训练"><strong>2.3 为什么要预训练？</strong></h5>
<p>正如上文提到的，模型参数的数量增长迅速，而为了训练这些参数，就需要更大的数据集来避免过拟合，而大规模的标注数据集成本又非常高。而相比之下，大规模未标注的语料却很容易构建。</p>
<p>为了利用大量的未标注文本数据，我们可以先从其中学习一个好的表示，再将这些表示用在别的任务中。这一通过 PTMs 从未标注大规模数据集中提取表示的预训练过程在很多 NLP 任务中都取得了很好的表现。</p>
<p>预训练的优点可以总结为以下三点：1 在大规模语料上通过预训练学习通用语言表示对下游任务很有帮助；2) 预训练提供了更好的模型初始化参数，使得在目标任务上有更好的泛化性能和更快的收敛速度；3) 预训练是一种有效的正则化方法，能够避免在小数据集上过拟合。</p>
<h4 id="ptms概述"><strong>3.PTMs概述</strong></h4>
<p>PTMs 的主要区别在于上下文编码器的使用、预训练任务和目标。上下文编码器已在 2.2 中做了叙述，接下来对预训练任务进行分析，并提出一种 PTMs 分类方法。</p>
<p><img src="https://i.loli.net/2020/10/28/LhoxNVG5rF3McDK.png" alt="img"></p>
<p>如图 3，这一部分内容作者在文中有一张非常详细的分类图可供参考。</p>
<p>表 1 从多个角度区分了文中提到的一些 PTMs。</p>
<p><img src="https://i.loli.net/2020/10/28/ZI176zJekFciyGr.png" alt="img"></p>
<h5 id="预训练任务"><strong>3.1 预训练任务</strong></h5>
<p>PTMs 按照预训练任务类型可以被分为两类：有监督学习、无监督学习/自监督学习。</p>
<p>有监督学习的预训练任务主要有机器翻译 (MT)，典型的模型是 CoVe。而下文进一步根据实现思路将自监督/无监督任务分为两类，一是基于上下文的 (LM, DAE, PLM)，二是基于对比的 (CTL)。</p>
<h4 id="section-1"></h4>
<h6 id="语言模型-lm"><strong>3.1.1 语言模型 (LM)</strong></h6>
<p>作为 NLP 中最常见的无监督任务，LM 一般指自回归 LM (auto-regressive LM) 或者单向 LM (unidirectional LM)。具体训练过程是基于一个大的语料，通过最大似然估计 (MLE) 训练计算一个句子出现的概率。</p>
<p>然而单向 LM 的缺点则是只能编码一个词左侧的文本和其自身，而更好的上下文应该编码左右两侧的文本。针对这一缺点，解决方案是双向 LM (BiLM)，即一个从左到右和一个从右到左的模型的组合。</p>
<h6 id="去噪声自编码器-denoising-autoencoder-dae"><strong>3.1.2 去噪声自编码器 (Denoising Autoencoder, DAE)</strong></h6>
<blockquote>
<p>这里将原文中 Masked Language Modeling (MLM) 与 DAE 合并为一个部分，因为一般将 BERT 中提出的 MLM 看作是基于 DAE 的思路实现的。</p>
</blockquote>
<p>DAE 的目的是通过向输入文本中添加噪声，利用含噪声的样本去重构不含噪声的输入。主要有五个实现方式：挡住 (MASK) token、删除 token、填充 token、句子排列、文本轮换。</p>
<p>MLM 随机选出一些词用 [MASK] 标记，然后去预测被 MASK 的词。但由于被 MASK 的词并不出现在 fine-tuning 的过程中，会导致预训练和微调的过程出现不一致性。针对这种情况，BERT 通过 80% [MASK]，10% 随机 token,10% 原 token 的方式来进行 mask。</p>
<p>而 MLM 的一种变体，<strong>Seq2SeqMLM</strong>，则是通过将 encoder-decoder (Seq2Seq) 应用到 MLM 上，这种变体有利于 Seq2Seq 类型的下游任务，比如 QA，总结和机器翻译。这一结构主要用在 MASS 和 T5 中。</p>
<p>而在 BERT 之后的很多论文都对 MLM 做了一些改进以增强性能，作者将其总结为 E-MLM (Enhanced Masked Language Modeling)。</p>
<p>其中 RoBERTa 使用动态 masking，UniLM 将对 mask 的预测扩展到三种任务：单向、双向和 Seq2Seq。XLM 通过一种串联并行双语句对叫做 TLM (translation language modeling) 的模型实现 MLM。</p>
<p>而 SpanBERT 和 StructBERT 则是引入了结构化信息。而 ERINE (Baidu) 则是选择 MASK 实体和短语，E-BERT 和 ERINE (THU) 则是利用了实体 embedding 方法，这三者都是借助了外部知识来丰富 MLM。</p>
<h6 id="排列语言模型plm"><strong>3.1.3 排列语言模型（PLM）</strong></h6>
<p>针对 MLM 中使用 MASK 导致的预训练与微调过程的不一致，Permuted Language Modeling (PLM) 对于一个给定序列，生成其所有可能排列进行采样作为训练的目标。值得注意的是，PLM 并不改变原始文本的位置，而是重新定义 token 预测的顺序。</p>
<h6 id="对比学习ctl"><strong>3.1.4 对比学习（CTL）</strong></h6>
<p>CTL (Contrastive Learning) 基于一种“learning by comparison”的思路，假设某些观测文本对比随机采样文本在语义上更相似，通过构建正样本和负样本并度量距离来实现学习。CTL 通常比 LM 具有更少的计算复杂度，也因此成为一个值得选择的 PTMs 训练标准。</p>
<h6 id="deep-infomax-dim"><strong>3.1.5 Deep InfoMax (DIM)</strong></h6>
<p>DIM 最初是在 CV 领域提出的用于最大化图像全局特征与局部特征之间的互信息（Mutual Information）的方法。</p>
<p>InfoWord 将 DIM 引入到语义表达学习中，提出用 DIM objective 以最大化句子的全局表示和一个 N-gram 的具备表示之间的互信息。</p>
<p>噪声对比估计（Noise-Contrastive Estimation，NCE）通过训练一个二元分类器来区分真实样本和假样本，训练词嵌入。NCE 的思想也被用在 word2vec 中。</p>
<h6 id="replaced-token-detection-rtd"><strong>3.1.6 Replaced Token Detection (RTD)</strong></h6>
<p>RTD 和 NCE 大体相同，根据上下文来预测 token 是否替换。</p>
<p>CBOW 的 negetive sampling 就可以看作是一个 RTD 的简单版本，其中采样是根据词汇表中的分布进行采样。</p>
<p>ELECTRA 基于 RTD 提出了一种新的 generator-discriminator 框架。首先用 MLM 任务训练 generator，再用 generator 的权重初始化 discriminator，再用判别任务（判别哪些 token 被 generator 替换过）训练 discriminator。</p>
<p>最终在下游任务只需要对 discriminator 进行 fine-tuning。TRD 也是一种很好的解决 MLM 导致的不一致问题的方法。</p>
<p>WKLM 则是通过在实体层面（entity-level）进行词替换，替换为同一个实体类型的实体名。</p>
<h5 id="section-2"></h5>
<h6 id="next-sentence-prediction-nsp"><strong>3.1.7 Next Sentence Prediction (NSP)</strong></h6>
<p>NSP 训练模型区分两个输入语句是否为训练语料中连续的片段，在选择预训练句对时，第二个句子 50% 是第一个句子实际的连续片段，50% 是语料中的随机段落。NSP 能够教会模型理解两个输入句子之间的联系，从而使得如 QA 和 NLI 这种对此类信息敏感的下游任务受益。</p>
<p>然而，近来 NSP 的必要性也遭到了质疑，XLNet 的作者发现不用 NSP loss 的单句训练优于使用 NSP 的句对训练。RoBERTa 的作者进一步分析表明：在对单个文本中的文本块训练时，去除 NSP 会在下游任务稍微提高性能。</p>
<h6 id="sentence-order-prediction-sop"><strong>3.1.8 Sentence Order Prediction (SOP)</strong></h6>
<p>NSP 结合了主题预测相关性预测，而因为主题预测更容易，模型将更依赖于主题预测。为更好建模句子之间的相关性，ALBERT 提出使用 SOP loss 替换 NSP loss，SOP 使用一个文档中的两个连续片段作为正样本，将这两个片段交换顺序作为负样本。</p>
<p>采用了 SOP 的 ALBERT 在多项下游任务中结果都优于 BERT。StructBERT 和 BERTje 也使用 SOP 作为自监督学习任务。</p>
<h3 id="section-3"></h3>
<h5 id="ptms的拓展"><strong>3.2 PTMs的拓展</strong></h5>
<h6 id="引入知识的ptms"><strong>3.2.1 引入知识的PTMs</strong></h6>
<p>通常 PTMs 都是用大量语料训练通用的语言表示，而将外部的领域知识引入到 PTMs 被证明式有效的。自 BERT 以来，就有很多预训练任务用以将外部知识纳入 PTMs，如：</p>
<p><strong>LIBERT：</strong>linguistically-informed BERT ，通过附加语言约束任务纳入了语言知识。</p>
<p><strong>SentiLR：</strong>通过对每个单词添加情感极性，将 MLM 拓展至 Label-Aware MLM (LA-MLM)，在多个情感分类任务达到 SOTA。</p>
<p><strong>SenseBERT：</strong>不仅能预测被 mask 的 token，还能预测 WordNet 中的 supersense。</p>
<p><strong>ERINE (THU)：</strong>将知识图谱中预训练的实体嵌入与文本中相应的实体提及相结合，以增强文本表示。</p>
<p><strong>KnowBERT：</strong>端到端将带实体连接模型与实体表示集成。</p>
<p><strong>KEPLER：</strong>将知识嵌入和语言模型对象联合。</p>
<p><strong>K-BERT：</strong>不同于以上几个模型通过实体嵌入引入知识图谱中的结构化信息，K-BERT 通过直接将知识图谱中相关三元组引入句子，获得一个 BERT 的拓展的树形输入。</p>
<p><strong>K-Adapter：</strong>针对不同预训练任务独立训练不同的适配器以引入多种知识，以解决上述模型在注入多种知识出现的遗忘问题。</p>
<h6 id="多模态ptms"><strong>3.2.2 多模态PTMs</strong></h6>
<p>随 PTMs 在 NLP 领域的广泛应用，一些多模态 PTMs 也被设计出来，在一些语音、视频、图像数据集上进行了预训练，比如：</p>
<ul>
<li><strong>视频-语言：</strong>VideoBERT、CBT</li>
<li><strong>图像-语言：</strong>用于 visual question answering (VQA) and visual commonsense reasoning (VCR)，如 ViLBERT、LXMERT、VisualBERT、B2T2、VLBERT、 Unicoder-VL、UNITER</li>
<li><strong>音频-文本：</strong>用于端到端 Speech Question Answering (SQA) 任务，如 SpeechBERT</li>
</ul>
<h4 id="section-4"></h4>
<h6 id="领域预训练ptms"><strong>3.2.3 领域预训练PTMs </strong></h6>
<p>大多数 PTMs 都是在 Wikipedia 这样的通用领域语料库上训练的，这就限制了他们在特定领域内的表现。</p>
<p>近期有一些用专业领域语料训练的 PTMs，比如：生物医学领域的 BioBERT，科学领域的 SciBERT，临床医学领域的 ClinicalBERT。还有一些工作尝试将预训练模型更好地使用目标应用，比如生物医学实体归一化、专利分类等。</p>
<h6 id="多语言与特定语言ptms"><strong>3.2.4 多语言与特定语言PTMs </strong></h6>
<p>学习多语言文本表示对于跨语言 NLP 任务是很重要的。早期工作着力于学习来自同一语义环境下的多语言词嵌入，这一方法往往缺乏语言间的校准。近期有如下几个多语言 PTMs：</p>
<p><strong>Multilingual-BERT：</strong>M-BERT，在 Wikipedia 上 104 种种语言的文本上进行 MLM 训练，每个训练样本都是单语言的，也没有专门设计跨语言目标，但即便如此，M-BERT 在跨语言任务上表现还是非常好。</p>
<p><strong>XLM：</strong>通过结合跨语言任务 TLM (translation language modeling)，提升了 M-BERT 的性能。</p>
<p><strong>Unicoder：</strong>提出三个跨语言预训练任务：1) cross-lingual word recovery; 2) cross-lingual paraphrase classification; 3) cross-lingual masked language model。</p>
<p>除此之外还有一些单语言的 PTMs：BERT-wwm，ZEN，NEZHA，ERNIE (Baidu)，BERTje，CamemBERT， FlauBERT ，RobBERT 。</p>
<h5 id="如何压缩ptms"><strong>3.3 如何压缩PTMs</strong></h5>
<p>预训练模型往往包含至少几千万个参数，这也使得模型难以部署到生活中的线上服务以及资源有限的设备上，这就使得模型压缩成为一条可能能够压缩模型尺寸并提高计算效率的方法。表 2 展示了一些压缩的 PTMs 的对比。</p>
<p><img src="https://i.loli.net/2020/10/28/6cBzpUQ3J7e1Xd2.png" alt="img"></p>
<p>压缩 PTMs 一般有四个方法：</p>
<ul>
<li><strong>剪枝（pruning）：</strong>去除不那么重要的参数（e.g. 权重、层数、通道数、attention heads）</li>
<li><strong>量化（weight quantization）：</strong>使用占位更少（低精度）的参数</li>
<li><strong>参数共享（parameter sharing）：</strong>相似模型单元间共享参数</li>
<li><strong>知识蒸馏（knowledge diistillation）：</strong>用一些优化目标从大型 teacher 模型学习一个小的 student 模型，一些利用知识蒸馏的 PTMs 见表 3。</li>
</ul>
<p><img src="https://i.loli.net/2020/10/28/5tdJqBHve3XYuCo.png" alt="img"></p>
<h4 id="如何将ptms应用至下游任务"><strong>4.如何将PTMs应用至下游任务</strong></h4>
<h5 id="迁移学习"><strong>4.1 迁移学习</strong></h5>
<p>迁移学习就是将源任务中的知识适应到目标任务，将 PTMs 适应到下游任务是一种顺序迁移学习任务。那么，如何迁移呢？我们需要考虑以下几个问题：</p>
<ul>
<li><strong>选择合适的预训练任务</strong>：近期，LM 是最流行的预训练任务，也有效解决了很多 NLP 问题。但不同的预训练任务在不同的下游任务上有不同的效果，比如 NSP 任务能帮助 PTM 理解句子之间的关系，因此 PTM 对于 QA 和 NLI 这样的下游任务很有帮助。</li>
<li><strong>选择合适的模型架构</strong>：比如 BERT 使用的 MLM 和 Transformer 结构使其擅长 NLU 任务，却很难生成语言。</li>
<li><strong>选择合适的语料</strong>：下游任务的数据应该接近 PTMs 的预训练任务。</li>
<li><strong>选择合适的layers</strong>：在“深”的预训练模型中，不同的 layer 往往描绘不同种类的信息。有三种选择 layers 的方式：1) 只用 Embedding，如 word2vec 和 Glove；2) Top Layer，如 BERT；3) All Layers，如 ELMo。</li>
<li><strong>是否进行fine-tune</strong>：模型迁移一般有两种方法：特征提取和 fine-tuning。特征提取的参数是冻结的，且往往需要特定任务的体系结构。fine-tunig 的参数是非冻结的，比特征提取方法更为通用且方便。</li>
</ul>
<h5 id="fine-tuning的策略"><strong>4.2 fine-tuning的策略</strong></h5>
<p>自 ULMFit 和 BERT 起，fine-tuning 已经成为 PTMs 主要的适配方法。这里有一些实用的 fine-tunig 策略：</p>
<ul>
<li>两阶段 fine-tuning：两阶段迁移的方法在预训练和 fine-tuning 阶段引入了一个中间阶段。在第一阶段，通过中间任务或语料来微调模型。在第二阶段，通过目标任务微调模型。</li>
<li>多任务 fine-tuning：liu等人在多任务学习框架下对 BERT 进行了微调，结果显示多任务学习和预训练是互补的方法。</li>
<li>采用额外的适配器 fine-tuning：fine-tuning 的主要缺点是参数效率低，在每一个下游任务上都有各自的 dine-tuning 参数。对此的解决方案是在固定原始参数时引入一些可以 fine-tuning 的适配器。</li>
<li>其他：逐层解冻而非连续 fine-tune 所有层；self-ensemble 和 self-distillation</li>
</ul>
<h4 id="一些ptms的资源"><strong>5.一些PTMs的资源</strong></h4>
<h5 id="一些开源的应用"><strong>一些开源的应用：</strong></h5>
<p><img src="https://i.loli.net/2020/10/28/jTUZBNqcrlm9hR2.png" alt="img"></p>
<p><strong>word2vec:</strong></p>
<p>https://github.com/tmikolov/word2vec</p>
<p><strong>GloVe:</strong></p>
<p>https://nlp.stanford.edu/projects/glove</p>
<p><strong>FastText:</strong></p>
<p>https://github.com/facebookresearch/fastText</p>
<p><strong>Transformers:</strong></p>
<p>https://github.com/huggingface/transformers</p>
<p><strong>Fairseq:</strong></p>
<p>https://github.com/pytorch/fairseq</p>
<p><strong>Flair:</strong></p>
<p>https://github.com/flairNLP/flair</p>
<p><strong>AllenNLP:</strong></p>
<p>https://github.com/allenai/allennlp</p>
<p><strong>FastNLP:</strong></p>
<p>https://github.com/fastnlp/fastNLP</p>
<p><strong>Chinese-BERT:</strong></p>
<p>https://github.com/ymcui/Chinese-BERT-wwm</p>
<p><strong>BERT:</strong></p>
<p>https://github.com/google-research/bert</p>
<p><strong>RoBERTa:</strong></p>
<p>https://github.com/pytorch/fairseq/tree/master/examples/roberta</p>
<p><strong>XLNet:</strong></p>
<p>https://github.com/zihangdai/xlnet/</p>
<p><strong>ALBERT:</strong></p>
<p>https://github.com/google-research/ALBERT</p>
<p><strong>T5:</strong></p>
<p>https://github.com/google-research/text-to-text-transfer-transformer</p>
<p><strong>ERNIE (Baidu):</strong></p>
<p>https://github.com/PaddlePaddle/ERNIE</p>
<p><strong>相关资源：</strong></p>
<p><strong>论文列表：</strong></p>
<p>https://github.com/thunlp/PLMpapers</p>
<p>https://github.com/tomohideshibata/BERT-related-papers</p>
<p>https://github.com/cedrickchee/awesome-bert-nlp</p>
<p><strong>BERT Lang Street（收集 BERT 在不同数据集和任务上的表现）：</strong></p>
<p>https://bertlang.unibocconi.it/</p>
<p><strong>BERTViz（应用 transformer 的模型的注意力可视化）：</strong></p>
<p>https://github.com/jessevig/bertviz</p>
<h4 id="应用"><strong>6.应用</strong></h4>
<h5 id="通用评估标准"><strong>6.1 通用评估标准</strong></h5>
<p>GLUE (The General Language Understanding Evaluation) 标准是一个集合了 9 个自然语言理解任务的标准。</p>
<p>其中包括：单个句子分类任务（CoLA和SST-2）、文本对分类任务（MNLI, RTE, WNLI, QQP, MRPC）、文本相似度任务（STSB）、相关性排行任务（QNLI）。GLUE 标准能够能够很好地评估模型的鲁棒性和通用性。</p>
<p>而近期 NLP 的快速发展促使了新的标准 SuperGLUE 的提出，相比 GLUE，SuperGLUE 有更多富有挑战性且多种多样的任务，如指代消解和 QA。</p>
<h3 id="section-5"></h3>
<h5 id="机器翻译-1"><strong>6.2 机器翻译</strong></h5>
<p>机器翻译（Machine Translation, MT）也是 NLP 的一项重要任务。几乎所有 MT 模型都使用了 encoder-decoder 框架。而近期随预训练模型的发展，也有不少尝试将 BERT 之类的预训练模型用于初始化 encoder，取得了一定成效。</p>
<h3 id="section-6"></h3>
<h5 id="问答系统"><strong>6.3 问答系统</strong></h5>
<p>问答系统（Question answering, QA）或是狭义概念的机器阅读理解（machine reading comprehension, MRC）也是 NLP 的重要任务。</p>
<p>从易到难，有三种类型的 QA 任务：单回合提取 QA (single-round extractive QA, SQuAD)、多回合生成QA (multi-round generative QA, CoQA)、多跳问答 (multi-hop QA, HotpotQA)。</p>
<p>针对提取 QA，有通过 PTM 初始化 encoder 的回溯阅读架构（retrospective reader architecture）；针对多回合生成 QA，有“PTM+Adversarial Training+Rationale Tagging+Knowledge Distillation”架构；针对多跳 QA，有“Select, Answer, and Explain” (SAE) 系统。</p>
<h5 id="情感分析"><strong>6.4 情感分析</strong></h5>
<p>BERT 通过在广泛使用的情感分析数据集 SST-2 上进行微调后，表现超过了先前的 SOTA 模型。而后又有很多将 BERT 进行调整以应用在 aspect 级的情感分析（ABSA）任务上。</p>
<h5 id="总结-1"><strong>6.5 总结</strong></h5>
<p>从长文本中总结出短文本也是近期 NLP 的热点。也有很多尝试将 PTM 应用在总结文本任务上，如将 BERT 通过插入 [CLS] token 来学习句子表示的模型 BERTSUM。</p>
<h5 id="命名实体识别"><strong>6.6 命名实体识别</strong></h5>
<p>命名实体识别（Named Entity Recognition, NER）也是知识提取的一个基础任务，在很多 NLP 任务上都有重要作用。TagLM 和 ELMo 利用预训练语言模型的最后一层的输入和各层的加权总和作为词嵌入的一部分。</p>
<h4 id="未来方向"><strong>7.未来方向</strong></h4>
<h5 id="ptms的上界"><strong>7.1 PTMs的上界</strong></h5>
<p>随 BERT 的出现，我们可以发现，很多模型都可以通过更长的训练步长不在和更大的语料来提升性能，比如去年的 T5 使用的 C4 数据集。而我们也可以通过加深模型来提升性能，比如 Turing-NLG 使用了 72 个 transformer 层。</p>
<p>PTMs 的共同目标都是学习语言的本质通用知识(或者说是世界的知识)，然而，随着模型的不断加深，语料的不断增大，训练模型的花销也越来越大。一种更可行的解决方案是设计更有效的模型架构、自监督预训练任务、优化器和软硬件方面的技巧等。ELECTRA 就是这个方向上一个很好的尝试。</p>
<h5 id="面向任务的预训练与模型压缩"><strong>7.2 面向任务的预训练与模型压缩</strong></h5>
<p>在实践中，不同的下游任务要求 PTMs 拥有不同的功能。而 PTMs 与下游目标任务间的差异通常表现在两方面：模型架构与数据分布。较大的 PTMs 通常情况下会有更好的性能，但实际问题是如何在低容量设备和低时延应用上使用如此庞大的 PTM。</p>
<p>除此之外，我们可以通过模型压缩来将通用 PTMs 教给面向对象的 PTM。尽管 CV 中对 CNNs 的压缩已经非常成熟，但 Tansformer 的全连接结构使得模型压缩非常具有挑战性。</p>
<h5 id="ptms架构"><strong>7.3 PTMs架构</strong></h5>
<p>Transformer 是 PTMs 的一个高效的框架，但 Transformer 的局限在于计算复杂度。由于 GPU 显存大小的限制，目前大多数 PTM 无法处理序列长度超过 512 个 token 的序列。搭配这一限制需要改进 Transformer 的结构，如 Transformer-XL。因此，寻求更有效的模型架构对于解决长程文本信息也是很重要的。</p>
<h5 id="fine-tunig中的知识迁移"><strong>7.4 Fine-tunig中的知识迁移 </strong></h5>
<p>Fine-tuning 是目前将 PTM 的知识迁移至下游任务的主要方法，但参数效率却很低，每个下游任务都有特定的 fine-tuned 参数。</p>
<p>一个可以改进的解决方案是固定 PTMs 的原始参数，并为特定任务添加小型的可微调的适配器，这样就可以在不同的下游任务使用共享的 PTMs。从 PTM‘s 中挖掘知识也可以更灵活，比如：知识提取、知识蒸馏、数据增加、将 PTMs 作为外部知识等等。</p>
<h5 id="ptms的可解释性与可靠性"><strong>7.5 PTMs的可解释性与可靠性 </strong></h5>
<p>PTMs 的深且非线性的架构使得决策制定的过程非常不透明。近期，可解释人工智能（explainable artificial intelligence, XAI）成为热点。通过对模型词嵌入的研究我们可以分析 PTMs 中的语言和世界知识，但更多有关注意力机制的可解释性的问题还值得探讨。</p>
<p>PTMs 这种深模型很容易受到对抗样本的扰动而产生错误的预测。在 CV 领域，对抗攻击与防御已经被广泛学习，而由于语言的特性，文本的对抗还非常具有挑战性。PTMs 的对抗防御也对于提升 PTMs 的鲁棒性很重要。</p>
<h4 id="总结-2"><strong>8.总结</strong></h4>
<p>邱锡鹏老师的这篇综述很全面地概括了预训练模型，也非常适合初学者当作一个 roadmap 来阅读。我们可以看到 NLP 的发展过程是非常令人感动的，从最开始的“要表示语言”的目标，使用词袋模型和 N-gram。</p>
<p>再想到“词语具有多义性”，所以需要有上下文，使用 LSTM。LSTM 只有单向，那就使用双向 LSTM。“想要更大范围的上下文”，就产生了 transformer。</p>
<p>“再大一些”，有了 transformer-XL。还是不够好，怎么办？“更多知识”，于是不断加大语料库，不断堆 GPU，直到 T5 探索了“Limits of Transfer Learning with a Uniﬁed Text-to-Text Transformer”。</p>
<p>模型太大，成本太高，那就压缩模型，改进框架，于是有了 ELECTRA。预训练模型缺乏尝试推理能力，那就知识提取，于是有了 COMET。每一步尝试都是在靠近语言的本质与世界的知识。</p>
<p><em>“The whole of science is nothing more than a refinement of everyday thinking.”</em></p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
  </entry>
  <entry>
    <title>2020-10-26-BERT论文阅读及详解</title>
    <url>/2020/10/26/2020-10-26-BERT%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E5%8F%8A%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<h3 id="前言">前言</h3>
<p>BERT是谷歌发布的基于双向 Transformer的大规模预训练语言模型，该预训练模型能高效抽取文本信息并应用于各种NLP任务，并刷新了 11 项 NLP 任务的当前最优性能记录。</p>
<p>BERT的全称是基于Transformer的双向编码器表征，<strong>其中“双向”表示模型在处理某一个词时，它能同时利用前面的词和后面的词两部分信息</strong>。</p>
<p>BERT的本质上是通过在海量的语料的基础上运行自监督学习方法<strong>为单词学习一个好的特征表示</strong>，所谓自监督学习是指在没有人工标注的数据上运行的监督学习。</p>
<p>在以后特定的NLP任务中，<strong>我们可以直接使用BERT的特征表示作为该任务的词嵌入特征。</strong>所以BERT提供的是一个供其它任务迁移学习的模型，该模型可以根据任务微调或者固定之后作为特征提取器。</p>
<p>BERT仍然使用的是Transformer模型，它pretraining的不是普通的语言模型，而是Mask语言模型。在介绍Mask语言模型之前我们先介绍BERT的输入表示。</p>
<h3 id="section"></h3>
<h3 id="bert的总体结构">BERT的总体结构</h3>
<p>如图2-1，是Devlin等人在论文中给出的BERT结构示意图。BERT的输入是token序列对应的嵌入向量序列。在生命周期的不同阶段，输出是不同的：</p>
<p>在<strong>预训练阶段</strong>，BERT采用<strong>多任务策略</strong>，输出包括“下一个词语”和“是否为下一句”。</p>
<p>在<strong>微调和推断阶段</strong>，BERT(针对<strong>具体的任务</strong>)输出NER标签、答案位置等等。</p>
<p>这个示意图非常概括，BERT内部细节比较模糊。后面进行更详细的介绍。</p>
<p><img src="https://i.loli.net/2020/11/10/wo4WusfDvTS7ekZ.jpg" alt="img"></p>
<p>图 2-1 《BERT》中提供的BERT结构原图</p>
<h3 id="输入表示">输入表示</h3>
<p>BERT的输入的编码向量（长度是512）是3个嵌入特征的单位和，如图4，这三个词嵌入特征是：</p>
<ol type="1">
<li>WordPiece 嵌入[6]：WordPiece是指将单词划分成一组有限的公共子词单元，能在单词的有效性和字符的灵活性之间取得一个折中的平衡。例如图4的示例中‘playing’被拆分成了‘play’和‘ing’；</li>
<li>位置嵌入（Position Embedding）：位置嵌入是指将单词的位置信息编码成特征向量，位置嵌入是向模型中引入单词位置关系的至关重要的一环。位置嵌入的具体内容参考我之前的<a href="https://link.zhihu.com/?target=https%3A//senliuy.gitbooks.io/advanced-deep-learning/content/di-er-zhang-ff1a-xu-lie-mo-xing/attention-is-all-you-need.html">分析</a>；</li>
<li>分割嵌入（Segment Embedding）：用于区分两个句子，例如B是否是A的下文（对话场景，问答场景等）。对于句子对，第一个句子的特征值是0，第二个句子的特征值是1。</li>
</ol>
<p>最后，说明一下图4中的两个特殊符号<code>[CLS]</code>和<code>[SEP]</code>，其中<code>[CLS]</code>表示该特征用于分类模型，对非分类模型，该符合可以省去。<code>[SEP]</code>表示分句符号，用于断开输入语料中的两个句子。</p>
<p><img src="https://i.loli.net/2020/11/10/9LqQ7Vk58lNs2dM.jpg" alt="img"></p>
<blockquote>
<p>对于情感分类这样的任务，只有一个句子，因此Segment id总是0；而对于Entailment任务，输入是两个句子，因此Segment是0或者1。</p>
</blockquote>
<p>BERT模型要求有一个固定的Sequence的长度，比如128。如果不够就在后面padding，否则就截取掉多余的Token，从而保证输入是一个固定长度的Token序列，后面的代码会详细的介绍。第一个Token总是特殊的[CLS]，它本身没有任何语义，因此它会(必须)编码整个句子(其它词)的语义。</p>
<h3 id="mask-lm">Mask LM</h3>
<p>为了解决只能利用单向信息的问题，<strong>BERT使用的是Mask语言模型而不是普通的语言模型</strong>。Mask语言模型有点类似与完形填空——给定一个句子，<strong>把其中某个词遮挡起来，让人猜测可能的词</strong>。</p>
<p>这里会随机的Mask掉15%的词，然后让BERT来预测这些Mask的词，通过调整模型的参数使得模型预测正确的概率尽可能大，这等价于交叉熵的损失函数。这样的Transformer在编码一个词的时候会(必须)参考上下文的信息。</p>
<p>但是这有一个问题：<strong>在Pretraining Mask LM时会出现特殊的Token [MASK]，但是在后面的fine-tuning时却不会出现，这会出现Mismatch的问题。</strong></p>
<p>因此BERT中，如果某个Token在被选中的15%个Token里，则按照下面的方式随机的执行：</p>
<ul>
<li><p>80%的概率替换成[MASK]，比如my dog is hairy → my dog is [MASK]</p></li>
<li><p>10%的概率替换成随机的一个词，比如my dog is hairy → my dog is apple</p></li>
<li><p>10%的概率替换成它本身，比如my dog is hairy → my dog is hairy</p>
<p><img src="https://i.loli.net/2020/11/10/FOB4nNjGZQ6eWKr.jpg" alt="img"></p></li>
</ul>
<blockquote>
<p>这样做的好处是，BERT并不知道[MASK]替换的是哪一个词，而且任何一个词都有可能是被替换掉的，比如它看到的apple可能是被替换的词。这样<strong>强迫模型在编码当前时刻的时候不能太依赖于当前的词，而要考虑它的上下文，甚至更加上下文进行”纠错”。</strong>比如上面的例子模型在编码apple是根据上下文my dog is应该把apple(部分)编码成hairy的语义而不是apple的语义。</p>
</blockquote>
<h3 id="预测句子关系">预测句子关系</h3>
<p>在有些任务中，比如问答，<strong>前后两个句子有一定的关联关系，我们希望BERT Pretraining的模型能够学习到这种关系。因此BERT还增加了一个新的任务——预测两个句子是否有关联关系</strong>。这是一种Multi-Task Learing。BERT要求的Pretraining的数据是一个一个的”文章”，比如它使用了BookCorpus和维基百科的数据，BookCorpus是很多本书，每本书的前后句子是有关联关系的；而维基百科的文章的前后句子也是有关系的。对于这个任务，<strong>BERT会以50%的概率抽取有关联的句子(注意这里的句子实际只是联系的Token序列，不是语言学意义上的句子)，另外以50%的概率随机抽取两个无关的句子，然后让BERT模型来判断这两个句子是否相关</strong>。比如下面的两个相关的句子：</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">[CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP]</span><br></pre></td></tr></tbody></table></figure>
<p>下面是两个不相关的句子：</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">[CLS] the man [MASK] to the store [SEP] penguin [MASK] are flight ##less birds [SEP]</span><br></pre></td></tr></tbody></table></figure>
<h3 id="fine-tuning">Fine-Tuning</h3>
<p>BERT的Fine-Tuning如下图所示，共分为4类任务。</p>
<p><img src="https://i.loli.net/2020/11/10/tyI65WJibLkm8hB.png" alt="img"> <em>图：BERT的Fine-Tuning</em></p>
<p>对于普通的分类任务，输入是一个序列，如图中右上所示，<strong>所有的Token都是属于同一个Segment(Id=0)，我们用第一个特殊Token [CLS]的最后一层输出接上softmax进行分类，用分类的数据来进行Fine-Tuning</strong>。</p>
<p>对于相似度计算等输入为两个序列的任务，过程如图左上所示。两个序列的Token对应不同的Segment(Id=0/1)。我们也是用第一个特殊Token [CLS]的最后一层输出接上softmax进行分类，然后用分类数据进行Fine-Tuning。</p>
<p>第三类任务是<strong>序列标注，比如命名实体识别</strong>，输入是一个句子(Token序列)，<strong>除了[CLS]和[SEP]的每个时刻都会有输出的Tag</strong>，比如B-PER表示人名的开始，本章的序列标注部分已经介绍过怎么把NER变成序列标注的问题了，这里不再赘述。然后用输出的Tag来进行Fine-Tuning，过程如图右下所示。</p>
<p>第四类是问答类问题，比如SQuAD v1.1数据集，<strong>输入是一个问题和一段很长的包含答案的文字(Paragraph)，输出在这段文字里找到问题的答案。</strong></p>
<p>比如输入的问题是：</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">Where do water droplets collide with ice crystals to form precipitation?</span><br></pre></td></tr></tbody></table></figure>
<p>包含答案的文字是：</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">... Precipitation forms as smaller droplets coalesce via collision with other rain drops or ice crystals within a cloud. ...</span><br></pre></td></tr></tbody></table></figure>
<p>正确答案是”within a cloud”。</p>
<p>论文中作者提到了另外的两个模型，分别是OpenAI GPT和ELMo。</p>
<p>图3展示了这3个模型架构的对比：</p>
<p><img src="https://i.loli.net/2020/11/10/boz1UqSNnXdhHvJ.png" alt="image-20201026200944962"></p>
<ul>
<li>BERT使用了双向的Transformer架构，预训练阶段使用了MLM和NSP。</li>
<li>OpenAI GPT使用了left-to-right的Transformer。</li>
<li>ELMo分别使用了left-to-right和right-to-left进行独立训练，然后将输出拼接起来，为下游任务提供序列特征。</li>
</ul>
<p>上面的三个模型架构中，只有BERT模型的表征在每一层都联合考虑到了左边和右边的上下文信息。另外，除了架构不同，还要说明的一点是：BERT和OpenAI GPT是基于fine-tuning的方法，而ELMo是基于feature-based的方法。</p>
<hr>
<h3 id="其它">其它</h3>
<p>[CLS]就是classification的意思，可以理解为用于下游的分类任务。</p>
<p>主要用于以下两种任务：</p>
<ul>
<li>单文本分类任务：对于文本分类任务，BERT模型在文本前插入一个[CLS]符号，并将该符号对应的输出向量作为整篇文本的语义表示，用于文本分类，如下图所示。可以理解为：<strong>与文本中已有的其它字/词相比，这个无明显语义信息的符号会更“公平”地融合文本中各个字/词的语义信息。</strong></li>
</ul>
<p><img src="https://i.loli.net/2020/11/10/89gX7PbO35AdpwB.png" alt="img"></p>
<ul>
<li>语句对分类任务：该任务的实际应用场景包括：问答（判断一个问题与一个答案是否匹配）、语句匹配（两句话是否表达同一个意思）等。<strong>对于该任务，BERT模型除了添加[CLS]符号并将对应的输出作为文本的语义表示，还对输入的两句话用一个[SEP]符号作分割，并分别对两句话附加两个不同的文本向量以作区分</strong>，如下图所示。</li>
</ul>
<p><img src="https://i.loli.net/2020/11/10/ZdvuYK6DyTrBFbA.png" alt="img"></p>
<hr>
<h4 id="模型架构">模型架构</h4>
<h4 id="输入表示-1">输入表示</h4>
<ul>
<li><p>我们的输入表示能够在一个标记序列中明确地表示单个文本句子或一对文本句子（例如，[问题，答案]）。一个词的输入=词的embeding+段embeding+位置embeding</p>
<p><img src="https://i.loli.net/2020/11/10/jkpW6lnm1QcIhut.png" alt="img"></p></li>
<li><p>对于词embeding论文使用<a href="https://arxiv.org/pdf/1609.08144.pdf" target="_blank" rel="noopener">WordPiece embeddings</a></p></li>
<li><p>每个序列的第一个字符始终是特殊分类embedding([CLS])。对应于该字符的最终隐藏状态（即，Transformer的输出）被视为<strong>整个序列表示</strong>常用于聚合用作分类任务。<strong>对于非分类任务，将忽略此向量。</strong></p></li>
</ul>
<h4 id="预训练">预训练</h4>
<ul>
<li>任务一：Masked LM
<ul>
<li>标准条件语言模型只能从左到右或从右到左进行训练，因为双向条件语言模型将允许每个单词在多层self-attention中间接看到自己。<strong>为了避免当前要预测的词在self-attention中看到要预测的答案我们采样的方法是：随机屏蔽掉(mask)输入序列中一定比例的输入词，然后仅预测那些被屏蔽的词,称这个方法叫masked LM(MLM)，最后我们将这个被mask的词的最后隐藏层输出，输入到softmax层中预测这个被mask的词</strong></li>
<li>在论文的实验中我们<strong>每次mask掉一个序列的15%词</strong></li>
<li>该任务的两个缺点：
<ul>
<li>第一个：这种操作使得预训练和微调之间不匹配，因为在微调期间可能没有[MASK]字符。为了缓解这种情况我们不总是用[MASK]词来替换被mask掉的词，而是80%的用[MASK]词来替换被mask掉的词，10%用一个随机词来替换被mask掉的词，再，10%保存源词不变。例子：
<ul>
<li>原句：my dog is hairy 我们要mask掉hairy</li>
<li>80%：my dog is [MASK]</li>
<li>10%：my dog is apple</li>
<li>10%：my dog is hairy</li>
</ul></li>
<li>第二个：<strong>每个batch中只预测了15％的词，这表明模型可能需要更多的预训练步骤才能收敛。实验证明该任务的训练略微慢一点比起预测每一个词的语言模型。</strong></li>
</ul></li>
</ul></li>
<li>任务二：Next Sentence Prediction
<ul>
<li>为了训练理解句子关系的模型，我们预先训练下一句话预测任务，该任务可以从任何单语言语料库中生成。具体地，在构建每个预训练样本时，选择句子A和B，50％B是A的实际下一句子， 50％B是来自语料库的一个随机句子，例子如下：
<ul>
<li><strong>Input</strong> = [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP]</li>
<li><strong>Label</strong> = IsNext</li>
<li><strong>Input</strong> = [CLS] the man [MASK] to the store [SEP] penguin [MASK] are flight ##less birds [SEP]</li>
<li><strong>Label</strong> = NotNext</li>
</ul></li>
</ul></li>
<li>预训练过程设置：
<ul>
<li>输入序列长度为512，batch_size=256,训练1000000步近似在33亿词的预料库上40 epochs</li>
<li>使用Adam优化器，learning_rate=1e-4, β_1= 0.9, β_2= 0.999,权重的L2正则项系数为0.01，学习率是预热步数：10000，学习率线性衰退，在每一层使用概率为0.1的dropout，激活函数使用<a href="https://arxiv.org/abs/1606.08415" target="_blank" rel="noopener">gelu</a></li>
</ul></li>
</ul>
<h4 id="微调">微调</h4>
<ul>
<li><p>对于序列水平的分类任务，我们<strong>获取第一个词[CLS]的最后隐藏层状态,再将C经过一个全连接层得到最后的预测分布，其中K是类别数。</strong>W也是这种特殊任务唯一添加的模型参数。</p></li>
<li><p>在微调的过程中BERT和W被同时微调。</p></li>
<li><p><strong>在微调中，大多数模型超参数与预训练相同，一般修改的超参数是：batch_size, learning_rate, epochs。 Dropout的概率始终保持在0.1</strong>。理论上说最佳超参数值随特定于任务不同而不同，但我们发现以下范围的可能值可以在所有任务中很好地工作：</p>
<ul>
<li>Batch size: 16, 32</li>
<li>Learning rate (Adam): 5e-5, 3e-5, 2e-5</li>
<li>Number of epochs: 3, 4</li>
</ul></li>
<li><p>我们还观察到，大数据集对超参数选择的敏感性远小于小数据集</p></li>
<li><p>微调总结图：</p>
<p><img src="https://i.loli.net/2020/11/10/w2IJaCbm6tqKVyo.png" alt="img"></p>
<p>图一：预测两个句子的关系，图二：是对当个句子分类。</p></li>
</ul>
<h4 id="模型对比">模型对比</h4>
<p><img src="https://i.loli.net/2020/11/10/CN4jbLg5SQIyYZP.png" alt="img"></p>
<h4 id="总结">总结</h4>
<h5 id="词嵌入语言模型的方法">词嵌入语言模型的方法</h5>
<ul>
<li><p>NLP词嵌入语言模型的方法：</p>
<ul>
<li><p>Feature-based方法</p>
<ul>
<li><p>Feature-based指利用预先训练好的语言模型的结果,作为当前特定任务模型（task-specific）的一个额外的特征引入到当前特定任务模型中，例如下图的语言模型</p>
<p><img src="https://i.loli.net/2020/11/10/t8e7J3joTYKqHzw.png" alt="img"></p>
<p>上图中，左边部分为序列标注模型，也就是task-specific model，每个任务可能不同，右边是两个预训练好的前向LM(Left-to-right)和后向LM(Right-To-Left), 将两个LM的结果进行了合并，并将LM embedding与词向量、第一层RNN输出、第二层RNN输出进行了concat操作</p></li>
<li><p>通常feature-based方法包括两步：</p>
<ul>
<li>首先在大的语料A上无监督地训练语言模型，训练完毕得到语言模型。</li>
<li>然后构造task-specific model例如序列标注模型，采用有label的语料B来有监地训练task-sepcific model，将语言模型的参数固定，语料B的训练数据经过语言模型得到LM embedding，作为task-specific model的额外特征</li>
</ul></li>
<li><p>ELMo是这方面的典型代表</p></li>
</ul></li>
</ul></li>
<li><p>Fine-tuning方法</p>
<ul>
<li><p>Fine-tuning方式是指在已经训练好的语言模型的基础上，加入少量的task-specific parameters, 例如对于分类问题在语言模型基础上加一层softmax网络，然后在新的语料上重新训练来进行fine-tune。</p></li>
<li><p>OpenAI GPT 是这一方法的典型代表，其模型如下所示:</p>
<p><img src="https://i.loli.net/2020/11/10/Ho3JzRbLeAkjiFM.png" alt="img"></p>
<p>GPT首先语言模型采用了Transformer Decoder的方法来进行训练，采用文本预测作为语言模型训练任务，训练完毕之后，加一层Linear Project来完成分类/相似度计算等NLP任务。</p></li>
<li><p>Fine-Tuning的方法工作包括两步：</p>
<ul>
<li>构造语言模型，采用大的语料A来训练语言模型</li>
<li>在语言模型基础上增加少量神经网络层来完成specific task model例如序列标注、分类等，然后采用有label的语料B来有监督地训练模型，这个过程中语言模型的参数并不固定.</li>
</ul></li>
<li><p>而BERT采用了fine-tuning的方法，并且在许多task-specific model中取得了最好的效果</p></li>
</ul></li>
</ul>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
  </entry>
  <entry>
    <title>2020-10-22-MAML论文</title>
    <url>/2020/10/22/2020-10-22-MAML%E8%AE%BA%E6%96%87/</url>
    <content><![CDATA[<h3 id="model-agnostic-meta-learning-for-fast-adaptation-of-deep-networks">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</h3>
<blockquote>
<p>ICML 2017</p>
<p>authors ： Chelsea Finn，Pieter Abbeel， Sergey Levine （University of California ,Berkeley； Open AI）</p>
<p>code url (official tf) : <a href="https://github.com/cbfinn/maml" target="_blank" rel="noopener" class="uri">https://github.com/cbfinn/maml</a></p>
<p>code url (unofficial torch): <a href="https://github.com/dragen1860/MAML-Pytorch" target="_blank" rel="noopener" class="uri">https://github.com/dragen1860/MAML-Pytorch</a></p>
<p>被引用次数：2427</p>
</blockquote>
<h4 id="背景">背景</h4>
<p>解决小样本学习问题很有挑战 - &gt; 利用元学习的方法框架</p>
<p>元学习学习到一个模型，这个模型可以在少量新数据中快速学习。</p>
<h4 id="问题">问题</h4>
<p>前人通过学习update function或learning rule的训练方法，需要通过扩充模型的参数量或是限制模型结构（如限定RNN网络）等手段来提高准确率。</p>
<h4 id="解决">解决</h4>
<p>model-agnostic：模型无关。</p>
<p>MAML可以认为是一个框架，提供一个meta-learner用于训练base-learner。这里的meta-learner即MAML的精髓所在，用于 learning to learn；而base-learner则是在目标数据集上被训练，并实际用于预测任务的真正的数学模型。</p>
<p>绝大多数深度学习模型都可以作为base-learner无缝嵌入MAML中，而MAML甚至可以用于强化学习中，这就是MAML中model-agnostic的含义</p>
<p>本文的想法是训练一组初始化参数，<strong>通过在初始参数的基础上进行一或多步的梯度调整，来达到仅用少量数据并且一次或几次的梯度更新就能快速适应新task的目的（能够有好的表现，小loss）</strong>。</p>
<p>为了达到这一目的，训练模型需要最大化新task的loss function的参数敏感度（<em>maximizing the sensitivity of the loss functions of new tasks with respect to the parameters</em>），当敏感度提高时，极小的参数（参数量）变化也可以对模型带来较大的改进。</p>
<h4 id="贡献">贡献</h4>
<p>本文提出的算法可以被轻松地使用在全连接网络、卷积网络以及递归神经网络中，并且可以使用多种loss函数，可以适用于多个领域，包括少样本的回归、图像分类，以及强化学习，并且使用更少的参数量达到了当时（2017年）最先进的专注于少样本分类领域的网络的准确率。</p>
<h4 id="模型">模型</h4>
<h5 id="算法图解">算法图解</h5>
<p><img src="E:\myBlog\source_posts\image-20201103104804241.png" alt="image-20201103104804241"></p>
<p>既然希望使用训练好的meta-learner仅通过几步梯度迭代便可适用于新的task，作者便将目标设定为，通过梯度迭代，找到对于task敏感的参数 θ 。</p>
<p>训练完成后的模型具有对新task的学习域分布最敏感的参数，因此可以在仅一或多次的梯度迭代中获得最符合新任务的 θ* ，达到较高的准确率。</p>
<h5 id="算法流程">算法流程</h5>
<p><img src="E:\myBlog\source\_posts\image-20201103105129835.png" alt="image-20201103105129835" style="zoom:67%;"></p>
<p>表示的是MAML预训练阶段的算法</p>
<p>gradient through a gradient</p>
<h5 id="损失函数">损失函数</h5>
<p>梯度的计算是需要确定loss function的，MAML中loss根据不同的问题处理有不同的选择：</p>
<p>对于可监督回归问题，采用MSE</p>
<p>对于可监督分类问题，采用交叉熵</p>
<p>总的损失函数计算</p>
<p><img src="E:\myBlog\source_posts\image-20201103135150758.png" alt="image-20201103135150758"></p>
<p><img src="E:\myBlog\source\_posts\image-20201103105239355.png" alt="image-20201103105239355" style="zoom: 67%;"></p>
<p><del>为什么在support set 中梯度只更新一次？</del></p>
<ol type="1">
<li>增加速度</li>
<li>因为在实际的应用中</li>
<li>小样本学习中</li>
</ol>
<h4 id="实验">实验</h4>
<p><img src="E:\myBlog\source_posts\image-20201103105805219.png" alt="image-20201103105805219"></p>
<p>first order approximation (一阶近似）：泰勒展开式对函数展开后，取前两项 ，和原始的不进行一阶近似的差别不大</p>
<h4 id="参考">参考</h4>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/57864886" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/57864886</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/72920138" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/72920138</a></p>
</blockquote>
<p>Learning to Compare: Relation Network for Few-Shot Learning</p>
<p>应用领域： 图像分类</p>
<p>使用方法：两个模块： embedding module (CNNEncoder 普通的四层卷积) 和 relation module (RelationNetwork)</p>
<p><img src="E:\myBlog\source_posts\image-20201023150506263.png" alt="image-20201023150506263"></p>
<p>思路2： transformer可以嵌入到function函数中，作为特征提取器</p>
<p>思路3： 结合使用</p>
<p>Adaptive Subspaces for Few-Shot Learning</p>
<p><img src="E:\myBlog\source_posts\image-20201023150053311.png" alt="image-20201023150053311"></p>
<p><img src="E:\myBlog\source_posts\image-20201023150105936.png" alt="image-20201023150105936"></p>
<p><img src="E:\myBlog\source_posts\image-20201023144851908.png" alt="image-20201023144851908"></p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
  </entry>
  <entry>
    <title>2020-10-20-transformer和GNN的关系理解</title>
    <url>/2020/10/20/2020-10-20-transformer%E5%92%8CGNN%E7%9A%84%E5%85%B3%E7%B3%BB%E7%90%86%E8%A7%A3/</url>
    <content><![CDATA[<h3 id="transformer和gnn的关系">transformer和GNN的关系</h3>
<blockquote>
<p>Transformer和GNN有什么关系？一开始可能并不明显。但是通过这篇文章，你会从GNN的角度看待Transformer的架构，对于原理有更清楚的认知。</p>
</blockquote>
<p>通过这篇博文，现为南洋理工大学助理研究员Chaitanya Joshi 将为读者介绍图神经网络和 Transformer 之间的内在联系。具体而言，作者首先介绍 NLP 和 GNN 中模型架构的基本原理，使用公式和图片来加以联系，然后讨论怎样能够推动这方面的进步。</p>
<p><strong>transformer是GNN的一种特例</strong></p>
<h4 id="nlp-中的表示学习">NLP 中的表示学习</h4>
<p>从一个很高的角度来看，所有的神经网络架构都是<strong>对输入数据构建表示</strong>(<em>representations</em>)——以向量或嵌入矩阵的形式。这种方法将有用的统计或语义信息进行编码。</p>
<p>这些隐表示(<em>latent</em> or <em>hidden</em> representations)可以被用来进行一些有用的任务，如图像分类或句子翻译。<strong>神经网络通过反馈（即损失函数）来构建更好的表示。</strong></p>
<p>对于 NLP 来说，传统上，RNN 对每个词都会建立一个表示——<strong>使用序列的方式</strong>。例如，每个时间步一个词。从直观上来说，我们可以想象，一个 RNN 层是一个传送带。词汇以自回归(<em>autoregressively</em>)的方式从左到右被处理。在结束的时候，我们可以得到每个词在句子中的隐藏特征，然后将这些特征输入到下一个 RNN 层中，或者用到任务中去。</p>
<p><img src="https://i.loli.net/2020/10/20/Do7nd4r83wpJIuQ.jpg" alt="img"></p>
<p>从机器翻译开始，Transformer 就逐渐开始取代 RNN。<strong>这一模型有着新的表示学习策略</strong>。它不再使用递归，而是使用注意力机制对每个词构建表示——即每个词语在句子中的重要程度。知道了这一点，词的特征更新则是所有词的线性变换之和——通过其重要性进行加权。</p>
<h4 id="解析transformer">解析Transformer</h4>
<p>通过将前一段翻译成数学符号以及向量的方式去创建对整个体系结构的认知。将长句 S 中的第 i 个单词的隐藏特征 h 从 ℓ 层更新至ℓ+1 层：</p>
<p><img src="https://i.loli.net/2020/10/20/ZPO4o7kHUsy9GT2.png" alt="img"></p>
<p>其中 j∈S 为句子中单词的集合，Q<sup>ℓ、K</sup>ℓ、V^ℓ为可学习的线性权重（分别表示注意力计算的 Query、Key 以及 Value）。针对句子中每个单词的并行执行注意力机制，从而在 one shot 中（在 RNNs 转换器上的另外一点，逐字地更新特征）获取它们的更新特征。</p>
<p>我们可通过以下途径更好地理解注意力机制：</p>
<p><img src="https://i.loli.net/2020/10/19/M7EcF9nmQRfLOBA.jpg" alt="img" style="zoom:67%;"></p>
<p>考虑到 h_j^l； ∀j∈S 句中 h_i^l 和其他词的特征，通过点积计算每对（i，j）的注意力权重，然后在所有 j 上计算出 softmax。最后通过所有 h_j^l 的权重进行相应的加权，得到更新后的单词特征 h_i^l+1。</p>
<h4 id="多头注意力机制">多头注意力机制</h4>
<p>让点积注意力机制发挥作用是被证明较为棘手：糟糕的随机初始化可能会破坏学习过程的稳定性，此情况可以通过并行执行多头注意力将结果连接起来，从而克服这个问题（<strong>而每个「head」都有单独的可学习权重</strong>）：</p>
<p><img src="https://i.loli.net/2020/10/20/8R6hMowJm1zYlPX.png" alt="img"></p>
<p>其中 Q<sup>k,ℓ、K</sup>k,ℓ、V^k,ℓ是第 K 个注意力 head 的可学习权重，O^ℓ 是向下的投影，用以匹配 h_i^l+1 和 h_i^l 跨层的维度。</p>
<p>通过观察上一层中隐藏特征的不同的变换过程以及方面，多头机制允许注意力机制从本质上“规避风险”。关于这点，我们将在后面详细讨论。</p>
<h4 id="尺度问题和前向传播子层">尺度问题和前向传播子层</h4>
<p>促使形成最终形态的Transformer结构的<strong>关键问题点</strong>是，注意机制之后的<strong>词的特征</strong>可能在不同的尺度或重要性上：1）这可能是由于某些词在将其他词的特征累加时具有非常集中或非常分散的注意力权重 w_ij；（2）在单个特征/向量输入级别，跨多个注意力头（每个可能会以不同的比例输出值）进行级联可以导致最终向量 h_i^ℓ+1的输入具有一个大范围的值。遵循传统的机器学习思路，在上述流程中增加一个归一化层似乎是一个合理的选择。</p>
<p><img src="https://i.loli.net/2020/10/20/LpmV5fOXb3NMQAR.png" alt="image-20201019200900240"></p>
<p><strong>Transformers使用LayerNorm克服了问题（2）</strong>，LayerNorm在特征层级上进行归一化并学习一种仿射变换。此外，通过求特征维度的平方根来缩放点积注意力有助于抵消问题（1）。</p>
<p>最后，作者提出了控制尺度问题的另一个“技巧”：<strong>具有特殊结构的考虑位置的双层MLP</strong>。在多头注意力之后，他们通过一个可学习的权重 h_i^ℓ+1 将投影到一个更高的维度，在该维度中， h_i^ℓ+1 经过ReLU非线性变换，然后投影回其原始维度，然后再进行另一个归一化操作：</p>
<p><img src="https://i.loli.net/2020/10/20/8mlU4tj9YgzvGKX.png" alt="image-20201019201217429"></p>
<p>说实话，我不确定超参数化前馈子层背后的确切理由是什么，似乎也没有人对此提出疑问！我认为LayerNorm和缩放的点积不能完全解决突出的问题，因此大型MLP是一种可以相互独立地重新缩放特征向量的手段。</p>
<p>Transformer层的最终形态如下所示：</p>
<p><img src="https://i.loli.net/2020/10/19/2YgkMdw7l5JyPGT.png" alt="img" style="zoom:50%;"></p>
<p>Transformer架构也非常适合非常深的网络，使NLP界能够在模型参数和扩展数据这两方面进行延伸。</p>
<p>每个多头注意力子层和前馈子层的输入和输出之间的残差连接是堆叠Transformer层的关键（但为了清楚起见，在上图中省略了）。</p>
<h4 id="gnns构建图的表示">GNNs构建图的表示</h4>
<p>图卷积网络是图神经网络的一个分类</p>
<p>图神经网络（GNNs）或图卷积网络（GCNs）在图数据中建立节点和边的表示（representations）。它们是通过<strong>邻域聚合</strong>（或消息传递）（<strong>neighbourhood aggregation</strong> or message passing）来实现的，<strong>在邻域聚合中，每个节点从其邻域收集特征，以更新其周围的局部图结构表示</strong>。通过堆叠多个GNN层使得该模型可以将每个节点的特征传播到整个图中，从其邻居传播到邻居的邻居，依此类推。</p>
<p><img src="https://i.loli.net/2020/10/20/WD2hXpu18kmSqKU.png" alt="image-20201019201748102"></p>
<p>以这个表情符号社交网络为例：由GNN产生的节点特征可用于预测性任务，例如识别最有影响力的成员或提出潜在的联系。</p>
<p>在他们最基本的形式中，GNNs通过以下方法来更新节点i在ℓ层的隐藏层特征h（例如，😆），也就是先将节点自身的特征 h_i^l 和每个邻居节点 j∈N(i) 特征 h_j^l 的聚合相累加，然后再整体做一个<strong>非线性变换</strong>，如下：</p>
<p><img src="https://i.loli.net/2020/10/20/Meg8lO7IcuADTd3.png" alt="image-20201019202233150"></p>
<p>其中U<sup>l,V</sup>l是GNN层的可学习的权重矩阵，而<img src="https://i.loli.net/2020/10/20/rtfhOwx4lTWRjXz.png" alt="image-20201019202245048">是一个非线性变换，例如ReLU。</p>
<p>在上述例子中，N (😆) ={ 😘, 😎, 😜, 🤩 }。</p>
<p>邻域节点 j∈N(i) 上的求和可以被其他输入大小不变的聚合函数代替，例如简单的均值/最大值函数或其他更强大的函数（如通过注意机制的加权和）。</p>
<p>这听起来熟悉吗？</p>
<p>也许这样一条流程可以帮助建立连接：</p>
<p><img src="https://i.loli.net/2020/10/20/Askv9i74DwX3zIP.png" alt="image-20201019202551797"></p>
<blockquote>
<p><strong>如果我们要执行多个并行的邻域聚合头，并且用注意力机制（即加权和）替换领域上的求和 ，我们将获得图注意力网络（GAT）。加上归一化和前馈MLP，瞧，我们就有了Graph Transformer！</strong></p>
</blockquote>
<h3 id="句子就是由词全连接而成的图">句子就是由词全连接而成的图</h3>
<p>Sentences are fully-connected word graphs</p>
<p>为了使连接更加清晰，可以将一个句子看作一个完全连接的图，其中每个单词都连接到其他每个单词。现在，我们可以<strong>使用GNN来为图（句子）中的每个节点（单词）构建特征</strong>，然后我们可以使用它来执行NLP任务。</p>
<p><img src="https://i.loli.net/2020/10/20/YbeLK31vHIjUNRp.png" alt="image-20201019203155390"></p>
<p>广义上来讲，这就是Transformers正在做的事情：Transformers是以多头注意力作为邻居聚合函数的GNNs（<strong>GNNs with multi-head attention</strong> as the neighbourhood aggregation function ）。<strong>标准GNNs从其局部邻域节点j∈N(i) 聚合特征，而NLP的<code>Transformers将整个句子视为局部邻域</code>，在每个层聚合来自每个单词j∈S的特征。</strong></p>
<p>重要的是，各种特定于问题的技巧（如位置编码、因果/掩码聚合、学习率表和大量的预训练）对于Transformers的成功至关重要，但在GNN界中却很少出现。同时，从GNN的角度看Transformers可以启发我们摆脱模型结构中的许多花哨的玩意。</p>
<h3 id="可以从transformers和gnn中学到什么">可以从Transformers和GNN中学到什么？</h3>
<p>现在我们已经在Transformers和GNN之间建立了联系，接着让我们来探讨一些新的问题...</p>
<h4 id="全连接图是nlp的最佳输入格式吗">全连接图是NLP的最佳输入格式吗？</h4>
<p>在统计NLP和ML之前，Noam Chomsky等语言学家致力于发展语言结构的最新理论，如语法树/图。Tree LSTMs已经尝试过这一点，但是也许Transformers/GNNs是可以让语言理论和统计NLP的领域结合得更加紧密的更好的架构？</p>
<p><img src="https://i.loli.net/2020/10/20/XuSVtxfpclmaCjs.png" alt="image-20201019203855101"></p>
<h4 id="如何学习到长期依赖">如何学习到长期依赖？</h4>
<p><strong>完全连通图使得学习词与词之间的非常长期的依赖关系变得非常困难，这是完全连通图的另一个问题。</strong>这仅仅是因为图中的边数与节点数成二次平方关系，即在n个单词的句子中，Transformer/GNN将在n^2对单词上进行计算。如果n很大，那将会是一个非常棘手的问题。</p>
<p>NLP界对长序列和依赖性问题的看法很有意思：例如，使注意力机制在输入大小方面稀疏或自适应，在每一层中添加递归或压缩，以及使用对局部性敏感的哈希法进行有效的注意，这些都是优化Transformers有希望的新想法。</p>
<p>有趣的是，还可以看到一些GNN界的想法被混入其中，例如，用于句子图稀疏化的二进制分区似乎是另一种令人兴奋的方法。</p>
<p><img src="https://i.loli.net/2020/10/20/7JndG8ZPg3zUWE1.png" alt="image-20201019210515688"></p>
<h4 id="transformers在学习神经网络的句法吗">Transformers在学习神经网络的句法吗？</h4>
<p>NLP界有几篇关于Transformers可能学到什么的有趣论文。其基本前提是，对句子中的所有词对使用注意力机制（目的是确定哪些词对最有趣），可以让Transformers学习特定任务句法之类的东西。</p>
<p>多头注意力中的不同头也可能“关注”不同的句法属性。</p>
<p>从图的角度来看，通过在完全图上使用GNN，我们能否从GNN在每一层执行邻域聚合的方法中恢复最重要的边线及其可能带来的影响？我还不太相信这种观点。</p>
<p><img src="https://i.loli.net/2020/10/20/cNW2ZmO7yExKo4s.png" alt="image-20201019211524983"></p>
<h4 id="为什么要用多头注意力为什么要用注意力机制">为什么要用多头注意力？为什么要用注意力机制？</h4>
<p>我更赞同多头机制的优化观点——拥有多个注意力可以改进学习，克服不好的随机初始化。例如，这些论文表明，Transformers头可以在训练后“修剪”或“删除”，并且不会产生重大的性能影响。</p>
<p>多头邻聚合机制在GNNs中也被证明是有效的，例如在GAT使用相同的多头注意力，MoNet使用多个高斯核来聚合特征。虽然多头技巧是为了稳定注意力机制而发明的，但它能否成为提炼出额外模型性能的标准？</p>
<p>相反，具有简单聚合函数（如sum或max）的GNNs不需要多个聚合头来维持稳定的训练。如果我们不需要计算句子中每个词对之间的成对兼容性，对Transformers来说不是很好吗？</p>
<p>Transformers能从抛弃注意力中获益吗？Yann Dauphin和合作者最近的工作提出了另一种ConvNet架构。Transformers也可能最终会做一些类似于ConvNets的事情。</p>
<p><img src="https://i.loli.net/2020/10/20/x8a9EmGXlARLJpw.png" alt="img"></p>
<h4 id="为什么transformers这么难训练">为什么Transformers这么难训练？</h4>
<p>阅读新的Transformer论文让我觉得，在确定最佳学习率表、预热策略和衰减设置时，训练这些模型需要一些类似于黑魔法的东西。这可能仅仅是因为模型太大，而且所研究的NLP任务非常具有挑战性。</p>
<p>但是最近的结果表明，这也可能是由于结构中归一化和残差连接的特定组合导致的。</p>
<p>在这一点上我很在意，但是也让我感到怀疑：我们真的需要代价昂贵的成对的多头注意力结构，超参数化的MLP子层以及复杂的学习计划吗？</p>
<p>我们真的需要具有大量碳足迹的（译者注：有人提出现在训练一个模型相当于5辆汽车一天的排碳量）大规模模型吗？</p>
<p>具有良好归纳偏差的架构难道不容易训练吗？</p>
<p>参考</p>
<blockquote>
<p><a href="https://graphdeeplearning.github.io/post/transformers-are-gnns/" target="_blank" rel="noopener" class="uri">https://graphdeeplearning.github.io/post/transformers-are-gnns/</a></p>
<p><a href="https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/104666156" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/104666156</a></p>
</blockquote>
<h3 id="总结如下">总结如下</h3>
<p>句子就是由词全连接而成的图</p>
<p>如果我们要执行多个并行的邻域聚合头，并且用注意力机制（即加权和）替换领域上的求和 ，我们将获得图注意力网络（GAT）。加上归一化和前馈MLP，瞧，我们就有了Graph Transformer！</p>
<p>Transformers是以多头注意力作为邻居聚合函数的GNNs（<strong>GNNs with multi-head attention</strong> as the neighbourhood aggregation function ）。<strong>标准GNNs从其局部邻域节点j∈N(i) 聚合特征，而NLP的<code>Transformers将整个句子视为局部邻域</code>，在每个层聚合来自每个单词j∈S的特征。</strong></p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
  </entry>
  <entry>
    <title>2020-10-17-论文分享</title>
    <url>/2020/10/17/2020-10-17-%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/</url>
    <content><![CDATA[<h3 id="前言">前言</h3>
<p>本次阅读的论文是《Greedy Layerwise Learning Can Scale to ImageNet》，本周日论文分享组会中汇报</p>
<h3 id="论文">论文</h3>
<blockquote>
<p>《Greedy Layerwise Learning Can Scale to ImageNet》</p>
<p>ICML 2019</p>
<p>code url （pytorch） ：<a href="https://github.com/eugenium/layerCNN" target="_blank" rel="noopener" class="uri">https://github.com/eugenium/layerCNN</a></p>
</blockquote>
<p>两年前的code了 ，最近一次提交也是一年前，更新了readme说明部分。但是为什么在论文中没有说明呢？</p>
<p>看了代码，发现好像没有什么特别之处，就是这样一个逻辑</p>
<h4 id="背景">背景</h4>
<p>浅层监督的1隐藏层（ 1-hidden layer ）神经网络具有许多有利的属性，这些属性使它们比深层次的同类神经网络更易于解释，分析和优化，但缺乏表示能力。</p>
<p>深度神经网络（CNN）并不一定需要共同学习各个CNN层以获得高性能。</p>
<h4 id="问题">问题</h4>
<p>同时多层网络较为复杂，尚不清楚各层如何协同工作以实现高精度的预测。</p>
<p>就计算和内存资源而言，端到端的反向传播效率可能较低。</p>
<h4 id="解决">解决</h4>
<p>本文主要工作就是使用1隐藏层学习问题来逐层顺序构建深度网络，从而可以从浅层网络继承有利属性。</p>
<p>将上述这种greedy layerwise learning 拓展到了imagenet和CIFAR-10数据集等大型数据集上，应用场景是图像分类任务。</p>
<p>greedy approach 将更少地依赖于获得完整的梯度。不需要存储大多数中间激活，也不需要计算大多数中间梯度。 这样不需要很多的计算资源。本文就是解决将分层训练策略（ layer-wise training strategies）应用到大规模数据集的问题</p>
<h4 id="结果">结果</h4>
<p>使用一组简单的架构和培训构想，我们发现解决顺序出现的1隐藏层辅助问题会导致CNN超过ImageNet上的AlexNet性能。</p>
<h4 id="背景知识">背景知识</h4>
<h5 id="深层网络的贪婪逐层预训练方法greedy-layer-wise-train">深层网络的贪婪逐层预训练方法（greedy layer-wise train）</h5>
<p>每次只训练网络中的一层，即我们首先训练一个只含一个隐藏层的网络，仅当这层网络训练结束之后才开始训练一个有两个隐藏层的网络，以此类推。</p>
<p>在每一步中，我们把已经训练好的前k-1层固定，然后增加第k层（也就是将我们已经训练好的前k-1的输出作为输入）。每一层的训练可以是有监督的（例如，将每一步的分类误差作为目标函数），但更通常使用无监督方法（例如自动编码器）。</p>
<p>这些各层单独训练所得到的权重被用来初始化最终（或者说全部）的深度网络的权重，然后对整个网络进行“微调”（即把所有层放在一起来优化有标签训练集上的训练误差）。</p>
<p>考虑一个神经网络，如下图所示。它的输入是6维向量，输出是3维向量，代表输入样本属于三个类别的概率。</p>
<p><img src="https://i.loli.net/2020/10/18/OUnJaYGuvNDTzq1.png" alt="img"></p>
<p>最开始我们通过高斯分布随机初始化网络参数，然后逐层地优化网络参数。首先第一层。如下图，我们只保留输入层Input和第一个隐藏层Features I，其余层去掉。</p>
<p>之后，加入一个输出层，该<strong>输出层的输出向量维度和输入层一样</strong>，从而构成一个自编码器。我们训练这个自编码器，便可以得到第一层的网络参数，即绿线部分。</p>
<p><img src="https://i.loli.net/2020/10/18/SeutKCU7da1ImxF.png" alt="img"></p>
<p>然后是第二层的网络参数。如下图，我们只保留原始神经网络中的第一个隐藏层和第二个隐藏层，其余层去掉。</p>
<p>之后添加一个输出层，其输出向量维度和第一个隐藏层维度一样，从而构成一个自编码器，自编码器的输入是第一个隐藏层。</p>
<p>优化这个自编码器，我们就可以得到第二层网络参数，即红线部分。</p>
<p><img src="https://i.loli.net/2020/10/18/HVRc9A8xG5y2Mvj.png" alt="img"></p>
<p>优化这两个自编码器的过程就是逐层贪婪预训练。由于每个自编码器都只是优化了一层隐藏层，所以每个隐藏层的参数都只是局部最优的。</p>
<p>优化完这两个自编码器之后，我们把优化后的网络参数作为神经网络的初始值，之后微调（fine tune）整个网络，直到网络收敛。</p>
<h4 id="模型">模型</h4>
<h5 id="模型架构">模型架构</h5>
<p><img src="https://i.loli.net/2020/10/18/3wXjSyEbUKnsP7A.png" alt="image-20201018104055941"></p>
<p><img src="https://i.loli.net/2020/10/18/jR9JAHOolUP6Bmv.png" alt="image-20201018104658134"></p>
<p><img src="https://i.loli.net/2020/10/18/IXaq52BcrGbgyDk.png" alt="image-20201018104744620"></p>
<p>Xj+1 ：先对上一次训练的结果进行下采样 （ invertible downsampling 可逆下采样操作），然后进行CNN ， 再进行RELU函数</p>
<p>Ｚj+1 : 对Xj+1 进行辅助分类器操作得到，预测zj计算中间分类输出。</p>
<p><img src="https://i.loli.net/2020/10/18/ZlFPWJGkA5mHBtv.png" alt="image-20201018105215075"></p>
<p>根据1式，xj已经经过了一层卷积，又W是从0开始的，所以下标只有k-2</p>
<p><img src="https://i.loli.net/2020/10/18/dPVRxjHkbNmGvws.png" alt="image-20201018105539176"></p>
<h5 id="损失函数">损失函数</h5>
<p><img src="https://i.loli.net/2020/10/18/8vO62gaeNxE9Bnl.png" alt="image-20201018104427451"></p>
<h5 id="算法流程">算法流程</h5>
<p><img src="https://i.loli.net/2020/10/18/2PJMlYuLHEW4XQN.png" alt="image-20201018104104058"></p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
  </entry>
  <entry>
    <title>2020-10-13-小样本学习</title>
    <url>/2020/10/13/2020-10-13-%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<h3 id="前言">前言</h3>
<p>最近了解了一下小样本学习的相关知识，这几天看了几篇论文介绍如下</p>
<h3 id="综述">综述</h3>
<p>Generalizing from a Few Examples : A Survey on Few-Shot Learning</p>
<h4 id="简介">简介</h4>
<p>问题</p>
<p>（a）例如典型的 MNIST 分类问题，一共有 10 个类，训练集一共有 6000 个样本，平均下来每个类大约 600 个样本，但是我们想一下我们人类不需要这么多样本，这表明当前的深度学习技术和我们人类智能差距还是很大的，要想弥补这一差距，<strong>小样本学习是一个很关键的问题。</strong></p>
<ol start="2" type="a">
<li>如果想要构建新的数据集（以分类数据集为例），我们需要<strong>标记大量的数据</strong>，但是有的时候标记数据集需要某些领域的专家（例如医学图像的标记），这费时又费力，因此<strong>如果我们可以解决小样本学习问题，只需要每个类标记几张图片就可以高准确率的给剩余大量图片自动标记。</strong></li>
</ol>
<p><strong>以上两个原因使得小样本学习的研究成为热点。</strong></p>
<p><strong>小样本学习（Few-Shot Learning，以下简称 FSL ）</strong>用于解决当可用的数据量比较少时，如何提升神经网络的性能。</p>
<h4 id="定义">定义</h4>
<p>以下是FSL的一些符号术语</p>
<p><img src="https://i.loli.net/2020/10/15/Wy3Yzfv8OibZsgS.png" alt="image-20201015093502514"></p>
<p><img src="https://i.loli.net/2020/10/15/3Dy7SHqXgEIPwUB.png" alt="image-20201015164223838"></p>
<p>FSL是机器学习的一个子领域，所以先来介绍一下机器学习定义</p>
<p>机器学习：</p>
<p><img src="https://i.loli.net/2020/10/15/kNUTzD2WLYIgpZf.png" alt="image-20201015093951256"></p>
<p>给定一个任务T，任务的性能P，给定一些经验E，比如通过训练学习得到的标注数据，可以提升任务T的性能P</p>
<p>以下是简单例子</p>
<p><img src="https://i.loli.net/2020/10/15/4pRq6PcLUYgbKC5.png" alt="image-20201015094231448"></p>
<p>在传统机器学习中需要很多样本信息，但是在实际中是很困难的，所以FSL就是解决这类问题。在训练集Dtrain中提供的监督信息有限的情况下，包括（输入xi和对应的输出yi），来获得好的学习性能</p>
<p>FSL 小样本学习</p>
<p><img src="https://i.loli.net/2020/10/15/KT7aSbA6hg3iXVq.png" alt="image-20201015094815702"></p>
<p>和机器学习定义类似，只是E包含了有限的监督信息的样例（example）。也即，每个类class中包含了很少的有标签样例。</p>
<p><strong>小样本分类</strong>主要就是学习一个分类器h，能够预测每一个输入数据xi的标签yi，通常使用的是N-way-K-shot分类方法，N个类，每个类有K个例子。其中 Dtrain包括I=KN个例子</p>
<p><img src="https://i.loli.net/2020/10/20/h2EcFBrTmZvMGL4.png" alt="image-20201015095811521"></p>
<p>只要关注的是image classification .可以看到相比于机器学习，FSL在经验E部分多了一个prior knowledge，也就是如果只是一些少的监督信息的样例不足以去解决tesk T中的问题，如图像分类。所以还是<strong>需要结合一些先验知识。</strong></p>
<blockquote>
<p>处理目标T（target T）时，在E中的监督信息如果只有一个例子的话，那么就是one-shot learning。</p>
<p>处理目标T（target T）时，当E中没有任何监督信息例子的话，那么就是zero-shot learning。</p>
<p>Zero-shot Learing <strong>就是训练样本里没有这个类别的样本，但是如果我们可以学到一个好的映射，这个映射好到我们即使在训练的时候没看到这个类，但是我们在遇到的时候依然能通过这个映射得到这个新类的特征。</strong></p>
<p><strong>即：训练集</strong>中<strong>没有出现过</strong>的<strong>类别</strong>，能自动创造出相应的映射。</p>
</blockquote>
<p>主要问题</p>
<p>在机器学习中寻找最适合的假设时通常都是通过找到一组最优的参数来确定这个假设，并通过给定的训练集，最小化损失函数这一目标来指示最优参数的搜索，<strong>最小化损失函数</strong>如下所示：</p>
<p>　　　　<img src="https://i.loli.net/2020/10/15/kHsi7YgaSIVQeGr.png" alt="img"></p>
<p>　　在训练模型中，我们是通过训练集来拟合真实分布，我们训练出来的分布和真实分布往往不一样，这<strong>中间的差值称为期望风险（期望损失）</strong>，表达式如下：</p>
<p>　　　　<img src="https://i.loli.net/2020/10/15/sXLuTmn7SzlaFUV.png" alt="img"></p>
<p>　理论上说，<strong>让期望风险最下化才能逼近真实分布</strong>，但因为你并不知道真实分布，所有最小化期望风险是无法实现的</p>
<p>在机器学习中通常用经验风险来替换期望风险，经验风险就是在训练集上预测的结果和真实结果的差异，也是我们常说的<strong>损失函数</strong>，表达式如下：</p>
<p>　　　　<img src="https://i.loli.net/2020/10/15/UO37Pd9GIqjxBDL.png" alt="img"></p>
<p>　　我们给出下面一个符号描述：</p>
<p>　　　　<img src="https://i.loli.net/2020/10/15/OK6DQwaZ34Bbct2.png" alt="img"></p>
<p>h^是真实分布的假设</p>
<p>h∗是假设空间H中最接近h^的假设</p>
<p>而hI是通过最小化经验损失得到的假设。根据机器学习中的误差分解可得：</p>
<p>　　　　<img src="https://i.loli.net/2020/10/15/h9Loiz4wqbFX2Qj.png" alt="img"></p>
<p>　　等式右边第一项表示的是<strong>假设空间H中最优的假设和真实假设的误差</strong>，这一项其实<strong>由所选择的模型和参数的初始化分布决定的</strong>，这也就是为什么有的时候，模型选择的简单了，给再多的数据也训练不好，欠拟合。</p>
<p>第二项就是<strong>训练得到的假设和H中最优假设的误差</strong>，我们训练得到的假设也是从假设空间H中选择的，但有时候会陷入局部最优，或者提供的训练数据分布有偏差，导致无法到全局最优。</p>
<p>　　但理论上对于第二项，当样本数量II足够大时，有：</p>
<p>　　　　<img src="https://i.loli.net/2020/10/15/wnJhz4qAXgjUrQR.png" alt="img"></p>
<p><img src="https://i.loli.net/2020/10/15/diHcTjyW1hPGMNQ.png" alt="image-20201015102743694"></p>
<p>　<strong>传统的机器学习都是建立在大规模的训练数据上的，因此εest(H,I)是很小的，但是在FSL任务上，训练数据很小，因此εest(H,I)是很大的，</strong>此时采用传统的训练模式，如softmax+交叉熵，是极容易陷入过拟合的。</p>
<p><strong>所以需要更多的先验知识</strong></p>
<p>具体的图如下：</p>
<p><img src="https://i.loli.net/2020/10/15/mp3Bjruof89JskS.png" alt="image-20201015103318798"></p>
<p>解决方法-分类：</p>
<p><img src="https://i.loli.net/2020/10/15/XS5ZnfOL8mMHow2.png" alt="image-20201015103338811"></p>
<p>假设空间的确定就是模型函数的可行性范围</p>
<p><strong>Data</strong></p>
<p>　　Data就是通过先验知识来做<strong>数据增强</strong>， 数据量增大可以获得可靠的hI，自然能解决上述问题。</p>
<p>通常进行手动操作对FSL进行数据预处理。例如在图像上，比如图片的旋转剪切放缩等，句子中的同义词替换等，以及复杂的生成模型生成和真实数据相近的数据。数据增强的方式有很多种，大量的合适的增强一定程度上可以缓解FSL问题，但需要耗费大量的精力，以及很强的域知识，只是针对特定的数据集，很难应用到其它数据集中，因此<strong>不能很好的解决FSL问题</strong></p>
<p>分类</p>
<p><img src="https://i.loli.net/2020/10/15/umBnUeEhCTFfrbo.png" alt="image-20201015110602917"></p>
<p><img src="https://i.loli.net/2020/10/15/VgKI5RENmQXtipo.png" alt="image-20201015110557216"></p>
<p><strong>Model</strong></p>
<p>　　通过先验知识来<strong>限制模型复杂度，降低假设空间H的大小</strong>，使得当前的数据集可以满足</p>
<p>如果我们想使用机器学习模型来解决FSL问题，我们需要使用假设空间H很小的模型，这样样本复杂度也就小了，对于一些简单的任务，这样是可行的，但是对于复杂的任务，小的模型会导致εapp(H)很大，而现实中大多数任务都很复杂，它们的特征很多，且特征维度也很高。</p>
<p>因此我们<strong>只能一开始给一个假设空间H很大的模型，然后通过一些先验知识将这个空间中无效的hypothesis去掉，缩小假设空间H</strong>，但实际上和<strong>模型剪枝</strong>中的理念类似，你一开始给一个小的模型，这个模型空间离真实假设太远了，而你给一个大的模型空间，它离真实假设近的概率比较大，然后通过先验知识去掉哪些离真实假设远的假设。</p>
<p><img src="https://i.loli.net/2020/10/15/1R2cuKxSPkaFwUb.png" alt="image-20201015110729588"></p>
<p><strong>Algorithm</strong></p>
<p>　　通过先验知识来提供一个好的搜索策略，可以是一个好的搜索起始点，也可以是一个明确的搜索策略，来寻找最优点。</p>
<p>在机器学习中我们<strong>通常使用SGD以及它的变体，如ADAM，RMSProp等来更新参数，寻找最优的参数</strong>，对应到假设空间H中最优的假设h∗。这种方式在有大量的数据集的情况下可以用来慢慢迭代更新寻找最优参数，但是在FSL任务中，样本数量很少，这种方法就失效了。在这一节，我们不再限制假设空间。</p>
<p>根据使用不同的先验知识，可以将ALGORITHM分为下面3类：</p>
<p><img src="https://i.loli.net/2020/10/15/lr86fNGOuwLtPZn.png" alt="image-20201015110816197"></p>
<p>　　接下来的工作都是围绕这几个方向展开来求解FSL问题。</p>
<p><img src="https://i.loli.net/2020/10/15/a5x4EvrUSj3WAl1.png" alt="image-20201015103435972"></p>
<h3 id="应用">应用</h3>
<p><strong>Few-shot Learning Meta Learning 在监督学习领域的应用。</strong></p>
<p><strong>Meta Learning研究Task！</strong></p>
<p>Meta Learning，又称为 learning to learn，在 meta training 阶段将数据集分解为不同的 meta task，去学习类别变化的情况下模型的泛化能力，在 meta testing 阶段，面对全新的类别，不需要变动已有的模型，就可以完成分类。<strong>如果我们构建的深度学习系统能够学到先验知识，并且能够利用这些知识，我们就可以在新的问题上学的更快更好</strong>！那么，这个就是Meta Learning要做的事情了</p>
<p><strong>不是要学一个具体的模型，我们要学的是一个先验知识</strong>。<strong>如果我们已有的先验知识来帮助我们解决新的问题，那么我们对于新的问题就可以不需要那么多的样本，从而解决 few-shot 问题</strong>。</p>
<p>形式化来说，few-shot 的训练集中包含了很多的类别，每个类别中有多个样本。在训练阶段，会在训练集中<strong>随机抽取</strong> C 个类别，每个类别 K 个样本（总共 CK 个数据），构建一个 meta-task，作为模型的支撑集（support set）输入；再从这 C 个类中剩余的数据中抽取一批（batch）样本作为模型的预测对象（batch set）。即要求模型从 C*K 个数据中学会如何区分这 C 个类别，<strong>这样的任务被称为 C-way K-shot 问题。</strong></p>
<p>图 1 展示的是一个 2-way 5-shot 的示例，可以看到 meta training 阶段构建了一系列 meta-task 来让模型学习如何根据 support set 预测 batch set 中的样本的标签；meta testing 阶段的输入数据的形式与训练阶段一致（2-way 5-shot），但是会在全新的类别上构建 support set 和 batch。每一行都是一个task，包含了task的train set和test set。</p>
<p>我们可以把<strong>每一个task当做一个meta learning的训练样本</strong>。我们要通过多种task的训练，从而在Meta-test的时候也就是在新的task上取得好效果。</p>
<p><img src="https://i.loli.net/2020/10/15/n9C1hakXsYGuQAf.jpg" alt="img">▲ 图1：Few-shot Learning示例</p>
<h4 id="通常解决办法">通常解决办法</h4>
<h4 id="hypernetwork-生成参数">HyperNetwork 生成参数</h4>
<p><img src="https://i.loli.net/2020/10/15/oEJPShp9M2k5Nzi.png" alt="image-20201015163737823"></p>
<p>HyperNetwork 简单说就是<strong>用一个网络来生成另外一个网络的参数</strong>。</p>
<p>那么我们这里非常直接，我们的设想就是<strong>希望用一个hypernetwork输入训练集数据，然后给我输出我的对应模型也就是上图f的参数，我们希望输出的这个参数能够使得在测试图片上取得好的识别效果</strong>。</p>
<p>有了这样设计，这个hypernetwork其实就是一个meta network。大家可以看到，<strong>本来基本的做法是用训练集直接训练这个模型f，但是现在我们用这个hypernetwork不训练了，直接给你输出参数，这等价于hypernetwork学会了如何学习图像识别，这也是为什么meta learning也同时叫做learning to learn的原因。</strong></p>
<h4 id="训练">训练</h4>
<p><strong>训练过程</strong>中，每次训练<strong>（episode）</strong>（Dtrain | Dtest）都会采样得到不同 meta-task，所以总体来看，训练包含了不同的类别组合，这种机制使得模型学会不同 meta-task 中的共性部分，比如如何提取重要特征及比较样本相似等，忘掉 meta-task 中 task 相关部分。通过这种学习机制学到的模型，在面对新的未见过的 meta-task 时，也能较好地进行分类。</p>
<p>这里有个所谓的<strong>episodic training</strong>！一个<strong>episode就是包含了一个task，有训练集有测试集</strong>。我们<strong>使用训练集输入到hypernetwork，得到f的参数，然后使用测试集输入到f 得到预测的标签，最后用测试集的样本标签得到模型的loss，之后就用梯度下降进行训练。</strong>所以我们可以看到，整个模型是端到端的。通过大量的episodic training，也就是大量的task进行训练，我们就可以训练出一个模型出来。</p>
<p>在 meta training 阶段将数据集分解为不同的 meta task，<strong>去学习类别变化的情况下模型的泛化能力，</strong>在 meta testing 阶段，面对全新的类别，<strong>不需要变动已有的模型，就可以完成分类</strong>。</p>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/61215293" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/61215293</a></p>
</blockquote>
<h3 id="其它">其它</h3>
<h4 id="inductive-learning-与-transductive-learning">inductive learning 与 transductive learning</h4>
<p>在训练过程中，已知testing data（unlabelled data）是transductive learing</p>
<p>在训练过程中，并不知道testing data ，训练好模型后去解决未知的testing data 是inductive learing</p>
<p>通俗地来说inductive learning是特殊到一般的学习，测试数据只是用来测试这个通用模型的好坏；transductive learning是特殊到特殊的学习，目的就是解决target domain的问题。</p>
<p><img src="https://i.loli.net/2020/10/15/2HOTN5fb3Lvz7pD.png" alt="img"></p>
<p>现在有这个问题，已知ABC的类别，求问号的类别，</p>
<p>inductive learning就是只根据现有的ABC，用比如kNN距离算法来预测，在来一个新的数据的时候，还是只根据5个ABC来预测。</p>
<p>transductive learning直接以某种算法观察出数据的分布，这里呈现三个cluster，就根据cluster判定，不会建立一个预测的模型，如果一个新的数据加进来 就必须重新算一遍整个算法，新加的数据也会导致旧的已预测问号的结果改变</p>
<h4 id="representation-learning">representation learning</h4>
<p>在机器学习领域，表征学习（或<strong>特征学习</strong>）是一种将原始数据转换成为能够被机器学习有效开发的一种技术的集合。在特征学习算法出现之前，机器学习研究人员需要利用手动特征工程（manual feature learning）等技术从原始数据的领域知识（domain knowledge）建立特征，然后再部署相关的机器学习算法。</p>
<p>特征学习弥补了这一点，它使得机器不仅能学习到数据的特征，并能利用这些特征来完成一个具体的任务。</p>
<p>表征学习的目标不是通过学习原始数据预测某个观察结果，而是学习数据的底层结构（underlying structure），从而可以分析出原始数据的其它特性。</p>
<p>特征学习可以被分为两类：监督式特征学习（Supervised Representation Learning）和无监督式特征学习（Unsupervised Representation Learning）。</p>
<p>在监督特征学习中，被标记过的数据被当做特征用来学习。例如神经网络（Neural Networks），多层感知器（Multi-Layer Perception），监督字典学习（Supervised Dictionary Learning）。</p>
<p>在无监督特征学习中，未被标记过的数据被当做特征用来学习。例如无监督字典学习（Unsupervised Dictionary Learning），主成分分析（Principal Component Analysis），独立成分分析（Independent Component Analysis），自动编码（Auto-encoders），矩阵分解（Matrix Factorization） ，各种聚类分析（Clustering）及其变形。</p>
<blockquote>
<p><a href="https://www.jiqizhixin.com/graph/technologies/64d4c374-6061-46cc-8d29-d0a582934876" target="_blank" rel="noopener" class="uri">https://www.jiqizhixin.com/graph/technologies/64d4c374-6061-46cc-8d29-d0a582934876</a></p>
</blockquote>
<h3 id="论文-edge-labeling-graph-neural-network-for-few-shot-learning">论文-Edge-Labeling Graph Neural Network for Few-shot Learning</h3>
<p>CVPR 2019</p>
<p>code url： <a href="https://github.com/khy0809/fewshot-egnn" target="_blank" rel="noopener" class="uri">https://github.com/khy0809/fewshot-egnn</a></p>
<h4 id="abstract">Abstract</h4>
<p>在本文中，提出了一种新颖的边标记图神经网络（edge-labeling graph neural network ）（EGNN），该网络将边标记图上的深层神经网络应用于小样本学习。</p>
<p>以前在小样本学习中使用的图神经网络（GNN）方法是基于节点标记框架（ node-labeling framework）的，该框架对聚类内的相似度和聚类间不相似度进行隐式地建模（implicitly modeling）。 相反，提出的EGNN学会<strong>预测图上的边标签</strong>，而不是节点标签，从而通过直接利用聚类内的相似性和聚类间不相似性来<strong>迭代更新边标签</strong>，从而实现显式聚类的进化。 <strong>它也非常适合在各种类别上执行而无需重新训练，并且可以轻松扩展以执行直推推理（transductive inference）。</strong></p>
<p>EGNN的参数是通过带有边标记损失（edge-labeling loss）的episodic training来学习的，从而获得了针对未见的低数据问题的可普遍推广的模型。</p>
<p>在带有两个基准数据集的有监督和半监督的小样本图像分类任务上，提出的EGNN大大提高了现有GNN的性能。</p>
<p>对GNN的改进，提高了GNN的性能</p>
<p>预测图上的边标签，而不是节点标签，直接利用聚类内的相似性和聚类间不相似性来<strong>迭代更新边标签</strong>，从而实现显式聚类的进化。</p>
<h4 id="背景">背景</h4>
<p>GNN可以很好的处理数据之间丰富的关系结构，是迭代地通过消息传递（message passing）从邻居节点进行特征聚合。而小样本学习算法已显示要求充分利用support集和query集之间的关系，因此使用GNN可以自然地解决小样本学习问题。</p>
<p>GNN解决小样本学习问题的思路：</p>
<p>先建立一个从support到query的全连接的图，节点使用嵌入向量和独热编码label表示，通过邻居聚合迭代的更新节点feature，完成对query的分类。</p>
<h4 id="问题">问题</h4>
<p>然而以前在小样本学习中使用的图神经网络（GNN）方法是基于节点标记框架（ node-labeling framework）的，该框架对聚类内的相似度和聚类间不相似度进行隐式建模（implicitly modeling）。</p>
<h4 id="解决">解决</h4>
<p>提出的EGNN学会<strong>预测图上的边标签</strong>，而不是节点标签，从而通过直接利用聚类内的相似性和聚类间不相似性来<strong>迭代更新边标签</strong>，从而实现显式聚类的进化。</p>
<p><img src="https://i.loli.net/2020/10/15/ZewrSL3qdRy68iD.png" alt="image-20201015191818980"></p>
<h4 id="贡献">贡献</h4>
<p>EGNN首次使用边标注的方式构建，利用的是 episodic training framework</p>
<p>在小样本分类的有监督和半监督任务中，表现超过了所有的GNN。同时，证明显式聚类和分别利用类内相似、类间不同都是有效的。</p>
<h4 id="模型">模型</h4>
<p>术语介绍</p>
<p><img src="https://i.loli.net/2020/10/15/63wSKIRlfJGjEXB.png" alt="image-20201015193021049"></p>
<p>过程</p>
<p>每个episode中的支持集S都用作标记的训练集，在该训练集上训练模型以最小化其在查询集Q上的预测损失。此训练过程逐个episode反复进行，直到收敛为止。</p>
<p><img src="https://i.loli.net/2020/10/15/SaoD8MkpsU5uPnA.png" alt="image-20201015193456782"></p>
<p><img src="https://i.loli.net/2020/10/15/VIE751M3FlTBjKw.png" alt="image-20201015200148142"></p>
<p>给出tesk所有样本的特征表示，那么就可以构建一个全连接图，其中每个节点代表一个样本，每个边代表两个连接点之间的关系。</p>
<h4 id="伪代码">伪代码</h4>
<p><img src="https://i.loli.net/2020/10/15/vIdrJxt1TLEwNRU.jpg" alt="在这里插入图片描述"></p>
<p>我们可以看到，这个整体更新的方式像极了EM算法。 第一步是获取特征，<strong>这个embedding的网络在文章的图3。</strong></p>
<p>点特征先通过图(a)的卷积嵌入网络进行初始化， 边特征也被初始化如下面公式 第二步是初始化边</p>
<p><img src="https://i.loli.net/2020/10/15/LU549hpFxOYynQR.png" alt="image-20201015194613945"></p>
<p>如果两个点是同一个标签，那么就是1， 否则就是0</p>
<p><img src="https://i.loli.net/2020/10/15/xACgHdYarn32fpD.jpg" alt="在这里插入图片描述"></p>
<p>如果其是同一类，或者其不是同一类，或者其相邻节点不属于支持集</p>
<p>第三步是进入一个更新循环。</p>
<p>第四步是更新节点 <img src="https://i.loli.net/2020/10/15/eV9Zj1zlRCDBXTw.jpg" alt="在这里插入图片描述"></p>
<p>是把边的不同维数的特征（分别代表类内，和类间（相似性和不相似性））和节点特征相乘，然后2个结果做连接操作，作为参数传入神经网络，得到更新之后的节点特征，这个时候的节点特征就是包含了相应的边的语意信息了。信息更加饱满。</p>
<p>可以看到图中就是先进行算法，然后连接操作，然后进入一个多层感知机，最后得到更新之后的节点信息。 其中：eijd是做了归一化操作，f是点转移网络（transformation network）</p>
<p>第五步是更新边 <img src="https://i.loli.net/2020/10/15/kcoI5NE1KxQgACq.jpg" alt="在这里插入图片描述"></p>
<p>然后不断进行第四五步的循环L次，结束后计算出我们要测试的数据所属于的类的概率。</p>
<p>最终的边标签预测结果就是最后的边特征</p>
<p><img src="https://i.loli.net/2020/10/15/I36BJMaP2WlZy8g.png" alt="image-20201015200438652"></p>
<h4 id="损失函数">损失函数</h4>
<p><img src="https://i.loli.net/2020/10/15/sXD6vTOF1p7weiC.png" alt="image-20201015200559689"></p>
<p><img src="https://i.loli.net/2020/10/15/XwjzkxMNTQ1hGv9.png" alt="image-20201015200725295"></p>
<p>L代表第L层，M代表M个任务，这个是episodic training可以理解为多任务。</p>
<p>λ是学习率，L是二元交叉滴损失，Y是真实的标签，Yˆ是预测的标签。就是说损失函数是所有M个任务L层的所有损失的和。</p>
<h3 id="小样本的预训练">小样本的预训练</h3>
<p><strong>预训练</strong>是（Pre-training）大家都熟悉且非常有效的获取先验知识的方法。具体就是在大型数据集上，学习一个强大的神经网络作为特征提取器，例如CV里面常见的在ImageNet上预训练的ResNet网络，或是NLP里面在Wikipedia上训练的BERT，都代表一种特征表达的先验知识。</p>
<p><strong>在预训练基础上，我们只需在样本数量少的目标任务中，微调部分（例如只训练最后一层fc分类器）或者全部网络的参数，便得到了一个可以解决小样本学习问题的模型。</strong></p>
<p>预训练相当于给了小样本学习一个好的起点，就像一个人在上课前预习了大量的知识点。不过想要更上一层楼，还需要<strong>有效的学习方法</strong>。<strong>元学习</strong>（meta learning）的目的就是找到这种方法。具体来说，我们可以从<strong>预训练集</strong>中，每次采样出来一个“沙盒”版小样本任务，例如选5个类，每个类选5张图片作为训练集（support set），再选15张作为测试集（query set），然后我们要求模型在support set训练的结果，能够在query set上面取得好的表现。其实这种学习策略在我们身边随处可见，例如准备考试的时候，我们会提前做一些模拟测试，了解题型，规划答题节奏等等，这就是一种元学习。在小样本学习的实际操作中，我们可以使用元学习训练一个模型的初始化参数（MAML），或是一个分类器参数的生成网络（LEO）等等。通过元学习得到的知识，就构成了一种学习方法的先验知识，在预训练的网络之上，进一步提升小样本学习的表现。</p>
<p><strong>预训练是小样本学习中一个核心的环节</strong>，无论是基于微调的，还是基于元学习的方法，都以预训练为开始。那么从常理来说，更强的预训练，应该会带来更好的小样本学习的表现，例如在现有文献中，使用更深层的神经网络架构<strong>WRN-28-10</strong>的微调结果，往往会比相对较浅的<strong>ResNet-10</strong>表现好很多。</p>
<blockquote>
<p>利用其它的网络进行预训练，然后再进行元学习+微调，或者直接微调操作</p>
</blockquote>
<p><strong>小样本学习的解决思路</strong>，可以用下面这张图来概括：我们先在一个大的数据集 D 上面预训练一个特征提取网络Ω ，之后我们既可以直接使用 Ω在每一个小样本任务中微调(红色方块的Fine-Tuning);</p>
<p>也可以进一步使用元学习(Meta-Learning)，将D 拆成一个个由support set S和query set Q 组成的沙盒任务（Si，Qi） ，训练高效的学习方法；元学习结束以后，我们就可以用这种高效的学习方法，在小样本学习的任务中进行微调(绿色方块的Fine-Tuning)。</p>
<p><img src="https://i.loli.net/2020/10/20/SGetivHsnoBFrIk.jpg" alt="img">小样本学习的两种解决思路。</p>
<hr>
<h3 id="小样本的演变">小样本的演变</h3>
<p>小样本学习一般会简化为N-way K-shot问题，如图[1]。其中N代表类别数量，K代表每一类中(支持集)的样本量；</p>
<p><img src="https://i.loli.net/2020/10/20/9lRfqxs1U8uP6Fo.jpg" alt="图[1] N-way K-shot"></p>
<p>解决分类问题，人们最先想到的是采用传统监督学习的方式，直接在训练集上进行训练，在测试集上进行测试，如图[2]，但神经网络需要优化的参数量是巨大的，<strong>在少样本条件下，几乎都会发生过拟合</strong>；</p>
<p><img src="https://i.loli.net/2020/10/20/xrJlIDiOtwfLoj8.jpg" alt="图[2] 传统监督学习"></p>
<p>为了解决上述问题，人们首先想到的是通过<strong>使用迁移学习+Fine-tune的方式</strong>，<strong>利用Base-classes中的大量数据进行网络训练，得到的Pre-trained模型迁移到Novel-classes进行Fine-tune</strong>，如图[3]。虽然是Pre-trained网络+Fine-tune微调可以避免部分情况的过拟合问题，但是当数据量很少的时候，<strong>仍然存在较大过拟合的风险</strong>。</p>
<p><img src="https://i.loli.net/2020/10/20/H84WS5KRIuoQ2bs.jpg" alt="图[3] Pre-trained网络+Fine-tune微调"></p>
<p>接下来讲的就是小样本学习中极具分量的<strong>Meta-learning</strong>方法，现阶段绝大部分的小样本学习都使用的是Meta-learning方法。Meta-learning，即learn to learn，翻译成中文是元学习。Meta-learning共分为Training和Testing两个阶段，Training阶段的思路如图[4]。简单描述下流程：</p>
<p>1：将训练集采样成Support set和Query set两部分；</p>
<p>2：基于Support set生成一个分类模型；</p>
<p>3：利用模型对Query set进行分类预测生成predict labels；</p>
<p>4：通过query labels和predict labels进行Loss(e.g., cross entropy loss )计算，从而对分类模型中的参数θ进行优化。</p>
<p><img src="https://i.loli.net/2020/10/20/yGt6YfLuJADW5rx.jpg" alt="图[4] Meta-learning Training阶段思路"></p>
<p>Testing阶段的思路如图[5]，利用Training阶段学来的分类模型在Novel class的Support set上进行进一步学习，学到的模型对Novel class的Query set进行预测。</p>
<p><img src="https://i.loli.net/2020/10/20/oE2RtPAfVxOrI9L.jpg" alt="图[5] Meta-learning Testing阶段思路"></p>
<p>介绍到这里，Meta-learning的整体流程的流程就介绍完了，如图[6];</p>
<p>现在反过来看，Meta-learning核心点之一是如何通过少量样本来学习这个分类模型，即图[6]中的keyu部分。</p>
<p><img src="https://i.loli.net/2020/10/20/FNi5PM4fruDKWaH.jpg" alt="图[6] Meta-learning整体流程以及key point"></p>
<h3 id="star-预训练与微调"><span class="github-emoji" style="display:inline;vertical-align:middle;color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8">⭐</span> 预训练与微调</h3>
<p>假设我们想从图像中识别出不同种类的椅子，然后将购买链接推荐给用户。一种可能的方法是先找出100种常见的椅子，为每种椅子拍摄1,000张不同角度的图像，然后在收集到的图像数据集上训练一个分类模型。这个椅子数据集虽然可能比Fashion-MNIST数据集要庞大，但样本数仍然不及ImageNet数据集中样本数的十分之一。这可能会导致适用于ImageNet数据集的复杂模型在这个椅子数据集上过拟合。同时，因为数据量有限，最终训练得到的模型的精度也可能达不到实用的要求。</p>
<p>为了应对上述问题，一个显而易见的解决办法是收集更多的数据。然而，收集和标注数据会花费大量的时间和资金。例如，为了收集ImageNet数据集，研究人员花费了数百万美元的研究经费。虽然目前的数据采集成本已降低了不少，但其成本仍然不可忽略。</p>
<p>另外一种解决办法是应用迁移学习（transfer learning），将从源数据集学到的知识迁移到目标数据集上。例如，虽然ImageNet数据集的图像大多跟椅子无关，但在该数据集上训练的模型可以抽取较通用的图像特征，从而能够帮助识别边缘、纹理、形状和物体组成等。这些类似的特征对于识别椅子也可能同样有效。</p>
<p>本节我们介绍迁移学习中的一种常用技术：<strong>微调（fine tuning）</strong>。如图所示，微调由以下4步构成。</p>
<ol type="1">
<li><p>在源数据集（如ImageNet数据集）上预训练一个神经网络模型，即源模型。</p></li>
<li><p>创建一个新的神经网络模型，即目标模型。它复制了源模型上除了输出层外的所有模型设计及其参数。我们假设这些模型参数包含了源数据集上学习到的知识，且这些知识同样适用于目标数据集。我们还假设源模型的输出层与源数据集的标签紧密相关，因此在目标模型中不予采用。</p></li>
<li><p>为目标模型添加一个输出大小为目标数据集类别个数的输出层，并随机初始化该层的模型参数。</p></li>
<li><p>在目标数据集（如椅子数据集）上训练目标模型。我们将从头训练输出层，而其余层的参数都是基于源模型的参数微调得到的。</p>
<p><img src="https://i.loli.net/2020/10/20/xMtWkTzv9hufrni.png" alt="image-20201019213025514"></p></li>
</ol>
<p><strong>当目标数据集远小于源数据集时，微调有助于提升模型的泛化能力。</strong></p>
<h4 id="小结">小结</h4>
<ul>
<li>迁移学习将从源数据集学到的知识迁移到目标数据集上。微调是迁移学习的一种常用技术。</li>
<li>目标模型复制了源模型上除了输出层外的所有模型设计及其参数，并基于目标数据集微调这些参数。而目标模型的输出层需要从头训练。</li>
<li>一般来说，微调参数会使用较小的学习率，而从头训练输出层可以使用较大的学习率。</li>
</ul>
<p><span class="github-emoji" style="display:inline;vertical-align:middle;color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8">⭐</span> 具体实例可以参考下面的链接</p>
<blockquote>
<p><a href="http://zh.gluon.ai/chapter_computer-vision/fine-tuning.html" target="_blank" rel="noopener" class="uri">http://zh.gluon.ai/chapter_computer-vision/fine-tuning.html</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/35890660" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/35890660</a></p>
</blockquote>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
  </entry>
  <entry>
    <title>2020-10-12-transformer图像处理论文</title>
    <url>/2020/10/12/2020-10-12-transformer%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E8%AE%BA%E6%96%87/</url>
    <content><![CDATA[<h3 id="前言">前言</h3>
<h3 id="end-to-end-object-detection-with-transformers">End-to-End Object Detection with Transformers</h3>
<p>CVPR 2020</p>
<p>code url ： <a href="https://github.com/facebookresearch/detr" target="_blank" rel="noopener" class="uri">https://github.com/facebookresearch/detr</a></p>
<p>简易 code：</p>
<p><a href="https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/detr_demo.ipynb" target="_blank" rel="noopener" class="uri">https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/detr_demo.ipynb</a></p>
<h4 id="abstract">Abstract</h4>
<p>我们把目标检测看做是一种set prediction的问题，我们的方法也直接移除了一些人工设计的组件，例如NMS和anchor的生成。</p>
<p>我们的框架DETR，由两个部分构成，一是set-based的全局loss，使用bipartite matching (二分匹配)生成唯一的预测，二是transformer的encoder-decoder 结构。</p>
<p>只需提供固定大小学习到的目标查询集合，DETR推理出目标与全局图像上下文，直接并行地预测出结果。新的模型非常简单，不需要特定的库来支持。DETR在coco数据集上有着可以和faster-rcnn媲美的准确率与效率。而且它也能完成全景分割的任务。</p>
<h4 id="问题">问题</h4>
<p>目标检测的目标是预测一个bbox的集合和各个bbox的标签。目前的检测器不是直接预测一个目标的集合，而是使用替代的回归和分类去处理大量的propoasls、anchors或者window centers。</p>
<p>模型的效果会受到一系列问题的影响：后处理去消除大量重叠的预测、anchors的设计、怎么把target box与anchor关联起来。为了简化流程，我们提出一种直接set prediction的方式来消除这些替代的方法。</p>
<h4 id="解决">解决</h4>
<p>将目标检测看做是一种set prediction（序列预测）的问题，我们的方法也直接移除了一些人工设计的组件，例如NMS和anchor的生成。</p>
<p><img src="https://i.loli.net/2020/10/16/36UT8nHdBIKNQ4m.png" alt="DETR模型的大体结构"></p>
<p>DETR模型的大体结构</p>
<p>DETR可一次预测所有对象，并通过设置损失函数进行端到端训练，该函数执行预测对象与真实对象之间的<strong>二分匹配</strong>。</p>
<p>与大多数现有的检测方法不同，DETR不需要任何自定义层，因此可以在任何包含标准CNN和转换器类的框架中轻松重现。</p>
<h4 id="两大核心思想">两大核心思想</h4>
<p>1、transformer保证了attention，确保对一个实例的识别，是在整幅图的知识下进行的。</p>
<p>2、二分最大匹配，确保了一一对应的关系。</p>
<h4 id="局限性">局限性</h4>
<p>DETR在大型物体上表现出明显更好的性能，这可能是由于transformer的非局部计算所致。然而在小型物体上就表现出一般的性能</p>
<h4 id="模型">模型</h4>
<p><img src="https://i.loli.net/2020/10/16/cS3QMqNTJGykKZd.png" alt="image-20201016094016507"></p>
<p>DETR的整体结构Transformer类似：Backbone得到的特征铺平，加上Position信息之后送到一Encoder里，得到上下文信息。这100个candidates是被Decoder<strong>并行解码的</strong>（显存就很大，但实现的时候可写成不并行的），以得到最后的检测框。</p>
<p><img src="https://i.loli.net/2020/10/16/ZiGXu5nE3OF8syv.png" alt="image-20201013142352470"></p>
<h5 id="detr-encoder"><strong>DETR Encoder</strong></h5>
<p>网络一开始是使用Backbone（比如ResNet）提取一些feature，然后降维到d×HW。</p>
<p><img src="https://i.loli.net/2020/10/16/3C4qzOXKLsJco5H.png" alt="image-20201016094956398"></p>
<p>Feature降维之后与<strong>Spatial Positional Encoding相加</strong>，然后被送到Encoder里。</p>
<p>为了体现图像在x和y维度上的信息，作者的代码里<strong>分别计算了两个维度的Positional Encoding，然后Cat到一起。</strong></p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">pos_x = torch.stack((pos_x[:, :, :, <span class="number">0</span>::<span class="number">2</span>].sin(), pos_x[:, :, :, <span class="number">1</span>::<span class="number">2</span>].cos()), dim=<span class="number">4</span>).flatten(<span class="number">3</span>)</span><br><span class="line">pos_y = torch.stack((pos_y[:, :, :, <span class="number">0</span>::<span class="number">2</span>].sin(), pos_y[:, :, :, <span class="number">1</span>::<span class="number">2</span>].cos()), dim=<span class="number">4</span>).flatten(<span class="number">3</span>)</span><br><span class="line">pos = torch.cat((pos_y, pos_x), dim=<span class="number">3</span>).permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>FFN、LN等操作也与Transformer类似。<strong>Encoder最后得到的结果是对N个物体编码后的特征。</strong></p>
<h5 id="detr-decoder"><strong>DETR Decoder</strong></h5>
<p>DETR Decoder的结构也与Transformer类似，<strong>区别在于Decoder并行解码N个object。</strong></p>
<p>每个Decoder有两个输入：一路是Object Query（或者是上一个Decoder的输出），另一路是Encoder的结果。</p>
<p>We add prediction FFNs and Hungarian loss after each decoder layer. All predictions FFNs share their parameters。</p>
<p>在每一个decode层都会添加FFN和Hungarian loss，并行计算出N个object ， FFN是共享参数的</p>
<p>object queries会输入到每一层中，我一开始不明白Object Query是怎样得到的。后来从代码看，<strong>Object Query是一组nn.Embedding的weight（就是一组学到的参数）。</strong></p>
<p><strong>最后一个Decoder后面接了两个FFN，分别预测检测框及其类别。</strong></p>
<p>仔细看论文和代码，才发现它的输出是定长的（N）：100个检测框和类别，这种操作可能跟COCO评测的时候取top 100的框有关。100比一个图像中普遍的目标数量都要多。</p>
<h4 id="损失函数---bipartite-matching">损失函数 - Bipartite Matching</h4>
<p>一个难点就是如何去评价预测目标和真实目标（class 、边框大小位置）</p>
<p><strong>由于输出物体的顺序不一定与ground truth的序列相同</strong>，作者使用二元匹配将GT框与预测框进行匹配。其匹配策略如下：</p>
<p>（y和y^都是N大小，用no object填充）</p>
<p><img src="https://i.loli.net/2020/10/16/7HnIS2eMsPoRqpO.jpg" alt="img"></p>
<p>但是Lmatch中yi和y^的最佳分配需要用到<strong>匈牙利算法</strong>（Hungarian algorithm），参考的是前人的做法</p>
<h5 id="匈牙利算法"><strong>匈牙利算法</strong></h5>
<p>寻找二分图的最大匹配</p>
<p>最后的损失函数：</p>
<p><img src="https://i.loli.net/2020/10/16/wsH8nTdmuaekDl5.jpg" alt="img"></p>
<p>所谓二分的最大匹配，即保证预测值与真值实现最大的匹配，保证预测的N的实例（包括∅）按照位置与真值对应起来。实现一一对应之后，便能够利用分类Loss以及boundingbox Loss进行优化。这种一一对应的关系，同时也另一个好处是，不会多个预测值对应到同一个真实值上，然后再通过NMS进行后处理。</p>
<hr>
<p><img src="https://i.loli.net/2020/10/16/VaPDHrkwgthvSTb.png" alt="image-20201013000531053"></p>
<p>个人觉得最直白的理解方式就是用positional embedding替代了原本的anchor。</p>
<p>第一步用CNN提feature，然后展开成一维之后加上位置信息进入encoder加工。之后decoder里的object queries，实际上是另一组可学习的positional embedding，其功能类似于anchor。之后每个query进过decoder后算一个bbox和class prob。</p>
<p>网络的结构是非常简单的，先是CNN提取特征，然后将CNN提取的特征送入到transformer中，而由于transformer是位置无关，所以为了保持位置信息，需要送入CNN特征的同时，送入位置的编码信息，确保整个链路中位置信息不丢失。在transformer中编码之后，送入到解码器，同时送入到解码器的还包括object queries（即文中说的N个查询对象），N个对象以及编码器的输入在解码器的综合作用下，获取N个输出，这N个输出在FFN的作用下，产生N个位置以及每个位置对应的类别。</p>
<p>至此，网络的便具备物体检测的能力。与原始的transformer不同的地方在于decoder每一层都输出结果，计算loss。这种思想还是相对简单并且work的，EV-FlowNet以及龙明盛迁移学习的某一个版本中均有类似的操作。如果仔细探究的话，我想一定会有一种更合计的叠加方式，而不是这种简单的加在一起，毕竟每一层理论上的物理意义都不同，这种叠加loss的方法，限制了decoder只有第一层完成了大部分任务，更多的层只是一个上采样和细化的过程。</p>
<p>概括而言，文章的两大核心思想为：</p>
<p>1、transformer保证了attention，确保对一个实例的识别，是在整幅图的知识下进行的。注意力机制本质是在跑message passing去对提取的特征进行一种滤波，这里面在很大程度上就是<strong>实现了其他分析中的去提取不同位置不同物体之间的相互关系这个功能，通过发掘这个约束提高了对物体识别的可靠性。</strong></p>
<p>2、二分最大匹配，确保了一一对应的关系。</p>
<p><img src="https://i.loli.net/2020/10/16/E7CjvLOxHBSMR25.jpg" alt="img"></p>
<h4 id="二分图最大匹配问题与匈牙利算法的核心思想">二分图最大匹配问题与匈牙利算法的核心思想</h4>
<p><a href="https://liam.page/2016/04/03/Hungarian-algorithm-in-the-maximum-matching-problem-of-bigraph/" target="_blank" rel="noopener" class="uri">https://liam.page/2016/04/03/Hungarian-algorithm-in-the-maximum-matching-problem-of-bigraph/</a></p>
<p>图上的object queries实际上是N个emebding，更具体得说应该是N个实例query的embedding(我理解是这样)，退一步不准确一点可以简单理解成位置。N是固定值但是emebding完之后N个quries都不太一样。所以差不多的意思就是告诉模型要100个实例，然后decoder根据encoder得到特征的位置和显著性decoder出100个抽象点代表instance，其中部分是前景instance，部分是背景instance，前景的就class+box loss，背景的就当背景。这就是训练过程。推理过程也就很简单了，前景的就直接用，背景的就丢掉。</p>
<p>Transformer encoder： 注意力机制本质是在跑message passing去对提取的特征进行一种滤波，这里面在很大程度上就是<strong>实现了其他分析中的去提取不同位置不同物体之间的相互关系这个功能，通过发掘这个约束提高了对物体识别的可靠性。</strong></p>
<hr>
<h3 id="an-image-is-worth-16x16-words-transformers-for-image-recognition-at-scale">AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE</h3>
<p>ICLR 2021 under review</p>
<p>code url (非官方) : <a href="https://github.com/lucidrains/vit-pytorch" target="_blank" rel="noopener" class="uri">https://github.com/lucidrains/vit-pytorch</a></p>
<h4 id="abstract-1">Abstract</h4>
<p>patch： 图像块</p>
<p>尽管Transformer体系结构已成为自然语言处理任务的实际标准，但其在计算机视觉中的应用仍然受到限制。 在视觉领域，注意力要么与卷积网络一起应用，要么用于替换卷积网络的某些组件，同时将其整体结构保持在适当的位置。 我们表明，这种对CNN的依赖不是必需的，并且当直接应用于图像块序列时，纯transformer可以很好地执行图像分类任务。 当对大量数据进行预训练并转移到多个识别基准（ImageNet，CIFAR-100，VTAB等）时，与最先进的卷积网络相比，Vision Transformer可获得出色的结果，而在训练中所需更少的计算资源。</p>
<h4 id="问题-1">问题</h4>
<p>在视觉领域中，transformer模型存在着对CNN的依赖，无法做到纯transformer模型。</p>
<h4 id="解决-1">解决</h4>
<p>当直接应用于图像块（patch）序列时，纯transformer可以很好地执行图像分类任务。 对transformer进行尽可能少的修改，这样有利于以后模型的拓展。为此，我们将图像拆分为小块，并提供这些小块的线性嵌入序列作为transformer的输入。图像块与NLP中的token（单词）的处理方式相同，应用在图像分类中。</p>
<h4 id="性能">性能</h4>
<p><img src="https://i.loli.net/2020/10/16/t9nKLb241IBUXxM.png" alt="image-20201016142258890"></p>
<p>在一些 mid-sized datasets 数据集（中型数据集）上表现一般，可能是transformer缺少CNN的归纳偏置特性。</p>
<p>然而在大型数据集中表现超过了CNN的归纳偏置。</p>
<h4 id="模型-1">模型</h4>
<h5 id="整体架构">整体架构</h5>
<p><img src="https://i.loli.net/2020/10/16/3UnGydo5C8aIOhm.png" alt="image-20201016143430430"></p>
<p><img src="https://i.loli.net/2020/10/16/o3VuAQ5IYZtmMej.png" alt="image-20201016143922712"></p>
<p>图像transformer遵循为NLP设计的体系结构。</p>
<p>标准的Transformer接收一维token嵌入的序列作为输入。 为了处理2D图像，我们将图像x∈R H×W×C reshape为一系列flatten的2D的patch，xp∈R N×（P 2·C）。</p>
<p>（H，W）是原始图像的分辨率，（P，P）是每个图像块的分辨率。 那么N = HW / P2是transformer的有效序列长度。</p>
<p>transformer在所有图层上使用恒定的宽度(d_model)，因此可训练的线性投影将每个向量图形块映射到模型尺寸D，我们将其输出称为图像块嵌入（patch embedding）。</p>
<h5 id="总体流程">总体流程</h5>
<p><img src="https://i.loli.net/2020/10/16/8KpGO94DXLlIZCS.png" alt="image-20201016145353457"></p>
<p>位置嵌入： 没有使用行和列的位置嵌入（DETR），和token的位置嵌入一样</p>
<p>为什么位置向量是随机生成的，并进行优化的呢？ 原transformer是计算得到的？？？</p>
<p><img src="https://i.loli.net/2020/10/16/XCLARSYyO3tewTg.png" alt="image-20201016150139040"></p>
<p><img src="https://i.loli.net/2020/10/16/pRZHo6XKFYltTfg.png" alt="image-20201016150253070"></p>
<p>class embeding: 可学习的标签嵌入</p>
<h5 id="预训练与微调">预训练与微调</h5>
<p>在实验阶段，在大型数据集上进行预训练，然后在较小的数据集上进行微调，作为tesk。</p>
<p>在微调时，去掉了pre-trained prediction head （Xclass），并将最后的num_classes改为数据集需要的类数目</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
  </entry>
  <entry>
    <title>2020-09-28-windows-terminal探索</title>
    <url>/2020/09/28/2020-09-28-windows-terminal%E6%8E%A2%E7%B4%A2/</url>
    <content><![CDATA[<h3 id="前言">前言</h3>
<p>优化方法终于考完了，最近几天都是在复习。可以放松两天，折腾一下</p>
<h3 id="windows-terminal">windows terminal</h3>
<h4 id="安装windows-terminal">安装Windows Terminal</h4>
<p>在Windows上，可以安装<code>Windows Terminal</code>。有点类似于MacOS上的<code>iTerm</code>，可以说是Windows下最舒适的终端。</p>
<p>在<code>windows store</code>中安装即可，比较方便，如下图所示</p>
<p><img src="https://i.loli.net/2020/09/30/KLziDFhWNBg9qYR.png" alt="image-20200930205359665"></p>
<h3 id="安装ubuntu子系统">安装Ubuntu子系统</h3>
<p>此时，我们仅仅安装了一个命令行终端而已，还是需要在<code>Windows</code>上安装<code>Ubuntu</code>。</p>
<p>只需要两步</p>
<p>1.在系统中开启子系统功能</p>
<p>2.在<code>windows store</code>安装linux版本即可。我安装的是<code>ubuntu 18.04 LTS</code>版本的，和实验室服务器一个版本号，利于操作。</p>
<blockquote>
<p>关于LTS</p>
<p>1.LTS= 长期支持版本，你会在较长的时间内获得安全、维护和(有时有)功能的更新。</p>
<p>2.LTS 版本被认为是最稳定的版本，它经历了广泛的测试，并且大多包含了多年积累的改进。</p>
<p>3.对于ubuntu，没两年发布一个LST版本，比如ubuntu 16.04 ubuntu 18.04等等，代表的是发布的年份。</p>
<p>4.最新的 LTS 版本是 Ubuntu 20.04 ，它将被支持到 2025 年 4 月，支持5年的软件更新和修补。换句话说，Ubuntu 20.04 在那之前都会收到软件更新。非 LTS 版本只支持九个月。</p>
</blockquote>
<p>如下图，在控制面板，找到程序选项，点击 “启用或关闭Windows功能”。</p>
<p><img src="https://i.loli.net/2020/09/28/KQ3LocsNrBxiRf5.png" alt="image-20200928105917992"></p>
<p>从弹出的对话框里，划到最下边，然后给“适用于Linux的Windows子系统“，打勾，完事！</p>
<p><img src="https://i.loli.net/2020/09/28/HPVhtscoGj8iCRA.png" alt="image-20200928105907117"></p>
<p>在windows中访问ubuntu系统</p>
<p>可以认为在windows 文件资源管理器中开辟一个空间用来储存ubuntu系统，但是如何找到位置呢？</p>
<p>执行如下命令：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /home  </span><br><span class="line">explorer.exe .  <span class="comment">#用文件资源管理器来打开当前home目录所在位置</span></span><br></pre></td></tr></tbody></table></figure>
<p>可以看到是在<code>网络</code>一栏中， 可以看到ubuntu的文件目录。但是返回到<code>网络</code>根目录，却显示是无文件夹。不知道为什么。</p>
<p>为了操作方便，我把这个长长的目录，右键映射到了Z盘上。如图，下次在访问Linux的时候，直接访问Z盘就可以了。</p>
<p><img src="https://i.loli.net/2020/09/28/XqMOBfSKRkwHC93.png" alt="image-20200928111036904"></p>
<p>这时，就可以看到在我的电脑里就有了Z盘</p>
<p><img src="https://i.loli.net/2020/09/28/V9MFRujrqgl46me.png" alt="image-20200928111104530"></p>
<blockquote>
<p><strong>映射网络驱动器</strong>目的就是为了让远程网络中的资源和本地共享，在本地可以对远程网络中的资源进行访问，并且可以创建文件。</p>
</blockquote>
<h3 id="工作区快捷键">工作区快捷键</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">Win 快捷键</th>
<th style="text-align: left;">作用</th>
<th style="text-align: left;">备注</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Ctrl + Shift + P</strong>，F1</td>
<td style="text-align: left;">显示命令面板</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Ctrl + B</strong></td>
<td style="text-align: left;">显示/隐藏侧边栏</td>
<td style="text-align: left;">很实用</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>Ctrl + \</code></td>
<td style="text-align: left;"><strong>创建多个编辑器</strong></td>
<td style="text-align: left;">【重要】抄代码利器</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Ctrl + 1、2</strong></td>
<td style="text-align: left;">聚焦到第 1、第 2 个编辑器</td>
<td style="text-align: left;">同上重要</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>ctrl +/-</strong></td>
<td style="text-align: left;">将工作区放大/缩小（包括代码字体、左侧导航栏）</td>
<td style="text-align: left;">在投影仪场景经常用到</td>
</tr>
<tr class="even">
<td style="text-align: left;">Ctrl + J</td>
<td style="text-align: left;">显示/隐藏控制台</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Ctrl + Shift + N</strong></td>
<td style="text-align: left;">重新开一个软件的窗口</td>
<td style="text-align: left;">很常用</td>
</tr>
<tr class="even">
<td style="text-align: left;">Ctrl + Shift + W</td>
<td style="text-align: left;">关闭软件的当前窗口</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Ctrl + N</td>
<td style="text-align: left;">新建文件</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">Ctrl + W</td>
<td style="text-align: left;">关闭当前文件</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
  </entry>
  <entry>
    <title>2020-09-19-论文分享（第2期）</title>
    <url>/2020/09/19/2020-09-19-%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB%EF%BC%88%E7%AC%AC2%E6%9C%9F%EF%BC%89/</url>
    <content><![CDATA[<h4 id="联邦学习">联邦学习</h4>
<p>参考 <a href="https://zhuanlan.zhihu.com/p/79284686" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/79284686</a></p>
<p>背景</p>
<p>1.现实生活中，除了少数巨头公司能够满足，绝大多数企业都存在数据量少，数据质量差的问题，不足以支撑人工智能技术的实现；</p>
<p>2.同时国内外监管环境也在逐步加强数据保护，因此数据在安全合规的前提下自由流动，成了大势所趋，所以不能获取很多涉及用户隐私的信息。</p>
<p>3.数据的不充分交流，同时也导致即使在同一个公司内，数据也往往以孤岛形式出现。</p>
<p>基于以上不足以支撑实现、不允许粗暴交换、不愿意贡献价值三点，</p>
<p>现在大量存在的<strong>数据孤岛</strong>，以及隐私保护问题，联邦学习被提出。</p>
<p>概念</p>
<p>本质：联邦学习本质上是一种<strong>分布式</strong>机器学习技术，或机器学习<strong>框架</strong>。</p>
<p>目标：联邦学习的目标是在保证数据隐私安全及合法合规的基础上，实现共同建模，提升AI模型的效果。</p>
<p>前身：联邦学习最早在 2016 年由谷歌提出，原本用于解决安卓手机终端用户在本地更新模型的问题；</p>
<p><img src="E:\myBlog\source_posts\v2-657a9f63512351691e60af9d88a34605_720w.jpg" alt="img"></p>
<h2 id="横向联邦学习">3.1 横向联邦学习</h2>
<p><strong>适用场景：</strong></p>
<p>横向联邦学习的本质是<strong>样本的联合</strong>，适用于参与者间业态相同但触达客户不同，即特征重叠多，用户重叠少时的场景，比如不同地区的银行间，他们的业务相似（特征相似），但用户不同（样本不同）</p>
<p><strong>学习过程：</strong></p>
<p><img src="E:\myBlog\source_posts\v2-23616816b92a6d62be206b0aa28ba393_720w.jpg" alt="img"></p>
<p>step1：参与方各自从服务器A下载最新模型；</p>
<p>step2：每个参与方利用本地数据训练模型，加密梯度上传给服务器A，服务器A聚合各用户的梯度更新模型参数；</p>
<p>step3：服务器A返回更新后的模型给各参与方；</p>
<p>step4：各参与方更新各自模型。</p>
<p><strong>步骤解读：</strong>在传统的机器学习建模中，通常是把模型训练需要的数据集合到一个数据中心然后再训练模型，之后预测。在横向联邦学习中，可以看作是<strong>基于样本的分布式模型训练</strong>，分发全部数据到不同的机器，每台机器从服务器下载模型，然后利用本地数据训练模型，之后返回给服务器需要更新的参数；服务器聚合各机器上的返回的参数，更新模型，再把最新的模型反馈到每台机器。</p>
<p>在这个过程中，每台机器下都是<strong>相同且完整的模型</strong>，且机器之间不交流不依赖，在预测时每台机器也可以<strong>独立预测</strong>，可以把这个过程看作成基于样本的分布式模型训练。谷歌最初就是采用横向联邦的方式解决安卓手机终端用户在本地更新模型的问题的。</p>
<h2 id="简介"><strong>简介</strong></h2>
<p>NAS</p>
<p>深度学习可以自动学习出有用的特征，脱离了对特征工程的依赖，在图像、语音等任务上取得了超越其他算法的结果。这种成功很大程度上得益于新神经网络结构的出现，如ResNet、Inception、DenseNet等。但设计出高性能的神经网络需要大量的专业知识与反复试验，成本极高，限制了神经网络在很多问题上的应用。神经结构搜索（Neural Architecture Search，简称NAS）是一种自动设计神经网络的技术，可以通过算法根据样本集自动设计出高性能的网络结构，在某些任务上甚至可以媲美人类专家的水准，甚至发现某些人类之前未曾提出的网络结构，这可以有效的降低神经网络的使用和实现成本。</p>
<p>NAS的原理是给定一个称为搜索空间的候选神经网络结构集合，用某种策略从中搜索出最优网络结构。神经网络结构的优劣即性能用某些指标如精度、速度来度量，称为性能评估。这一过程如下图所示。</p>
<p><img src="E:\myBlog\source_posts\v2-261f4e89d5c60e5d336052e7fc6d116d_720w.png" alt="img"></p>
<p>在搜索过程的每次迭代中，从搜索空间产生“样本”即得到一个神经网络结构，称为“子网络”。在训练样本集上训练子网络，然后在验证集上评估其性能。逐步优化网络结构，直至找到最优的子网络。</p>
<p>搜索空间，搜索策略，性能评估策略是NAS算法的核心要素。搜索空间定义了可以搜索的神经网络结构的集合，即解的空间。搜索策略定义了如何在搜索空间中寻找最优网络结构。性能评估策略定义了如何评估搜索出的网络结构的性能。对这些要素的不同实现得到了各种不同的NAS算法，本节将选择有代表性的进行介绍。</p>
<h3 id="fisher-information">Fisher Information</h3>
<p>反映了我们对参数估计的准确度，它越大，对参数估计的准确度越高，即代表了越多的信息。</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>论文分享</category>
      </categories>
      <tags>
        <tag>论文</tag>
        <tag>论文分享</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-09-14-修改pytorch版transformer代码</title>
    <url>/2020/09/14/2020-09-14-%E4%BF%AE%E6%94%B9pytorch%E7%89%88transformer%E4%BB%A3%E7%A0%81/</url>
    <content><![CDATA[<p><del>源代码是<code>main3.py</code> ,在此基础上进行修改，修改后文件为<code>main3-2.py</code></del></p>
<p>740中<code>annotated-transformer</code>中<code>main.py</code>和哈佛的一样</p>
<p>复制到了本地<code>main.py</code> ,再复制到<code>annotated-transformer1</code>中的main.py</p>
<p><strong>所以改前的代码是<code>main.py</code> ，改后的代码是<code>main-1.py</code></strong></p>
<p>注：</p>
<p><strong><code>python main.py &gt;main.txt 2&gt;&amp;1</code>，在将结果重定向到main.txt中，会覆盖main.txt之前的内容</strong></p>
<p><strong>每次跑实验的预测都是不一样的，但是都是和输入差不多</strong></p>
<ol type="1">
<li>将<code>attention</code>函数去掉，合并到<code>MultiHeadedAttention</code>中，服务器上测试<strong>可行</strong></li>
</ol>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
  </entry>
  <entry>
    <title>2020-09-11-gpu实验加速</title>
    <url>/2020/09/11/2020-09-11-gpu%E5%AE%9E%E9%AA%8C%E5%8A%A0%E9%80%9F/</url>
    <content><![CDATA[<h3 id="前言">前言</h3>
<p>将深度学习应用到实际问题中，一个非常大的问题在于训练深度学习模型需要的计算量太大。为了加速训练过程，本文将介绍如何如何在TensorFlow中使用单个GPU进行计算加速</p>
<h3 id="简介">简介</h3>
<h4 id="cuda">CUDA</h4>
<p><a href="https://developer.nvidia.com/cuda-toolkit-archive" target="_blank" rel="noopener">CUDA</a>（Compute Unified Device Architecture,点击进入安装网站），是显卡厂商NVIDIA推出的运算平台。 CUDA™是一种由NVIDIA推出的通用并行计算架构，该架构使GPU能够解决复杂的计算问题。安装GPU版tensorflow,必须有这个环境。</p>
<p>CUDA是NVIDIA推出的用于自家GPU的并行计算框架，也就是说CUDA只能在NVIDIA的GPU上运行，而且只有当要解决的计算问题是可以大量并行计算的时候才能发挥CUDA的作用。</p>
<h4 id="cudnn">cuDNN</h4>
<p>NVIDIA <a href="https://developer.nvidia.com/cuda-toolkit-archive" target="_blank" rel="noopener">cuDNN</a>是用于深度神经网络的GPU加速库。它强调性能、易用性和低内存开销。</p>
<h3 id="安装">安装</h3>
<p>必须要安装对应版本的CUDA、cuDNN和tensorflow</p>
<p>我在实验室服务器R740上的安装版本如下，是可以运行的</p>
<blockquote>
<p>CUDA： V10.1 # nvcc --version</p>
<p>cuDNN：V7</p>
<p>tensorflow-gpu：1.14.0</p>
</blockquote>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">conda install python==3.6.10  <span class="comment">#这样可以 ，但是为什么 python==3.6.1 和  python==3.6.0 是不可以的呢？？</span></span><br><span class="line">pip install tensorflow-gpu==1.14.0  <span class="comment">#安装成功gpu版本</span></span><br><span class="line"><span class="comment">#用conda装tensorflow时候，会自动下载cuda和cudnn，所以推荐用pip安装</span></span><br><span class="line"></span><br><span class="line">pip install tensorflow-gpu==1.2 <span class="comment">#如果安装错误，可以用pip卸载，没测试过。 或者直接再新建一个虚拟环境也可以</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="测试tensorflow-gpu">测试tensorflow-gpu</h3>
<p>测试安装的tensorflow是否可用GPU，测试如下：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">pyhton <span class="comment">#进入python操作环境</span></span><br><span class="line"></span><br><span class="line">import tensorflow as tf </span><br><span class="line">sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))</span><br></pre></td></tr></tbody></table></figure>
<p>显示如下则表示tensorflow支持的，输出如下：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">2020-09-11 08:30:54.735834: Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA</span><br><span class="line">2020-09-11 08:30:54.821023:  Successfully opened dynamic library libcuda.so.1</span><br><span class="line">2020-09-11 08:30:55.698894:  XLA service 0x5654b4f86600 executing computations on platform CUDA. Devices:</span><br><span class="line">2020-09-11 08:30:55.699000:   StreamExecutor device (0): Tesla M60, Compute Capability 5.2</span><br><span class="line">2020-09-11 08:30:55.699022:   StreamExecutor device (1): Tesla M60, Compute Capability 5.2</span><br><span class="line">2020-09-11 08:30:55.699042:   StreamExecutor device (2): Tesla M60, Compute Capability 5.2</span><br><span class="line">2020-09-11 08:30:55.699062:   StreamExecutor device (3): Tesla M60, Compute Capability 5.2</span><br><span class="line">2020-09-11 08:30:55.732911:   CPU Frequency: 2100000000 Hz</span><br><span class="line">2020-09-11 08:30:55.738953: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5654b54aa810 executing computations on platform Host. Devices:</span><br><span class="line">2020-09-11 08:30:55.739001: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;</span><br><span class="line">2020-09-11 08:30:55.741878: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: </span><br><span class="line">name: Tesla M60 major: 5 minor: 2 memoryClockRate(GHz): 1.1775</span><br><span class="line">pciBusID: 0000:b1:00.0</span><br><span class="line">2020-09-11 08:30:55.742665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: </span><br><span class="line">name: Tesla M60 major: 5 minor: 2 memoryClockRate(GHz): 1.1775</span><br><span class="line">pciBusID: 0000:b2:00.0</span><br><span class="line">2020-09-11 08:30:55.743420: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties: </span><br><span class="line">name: Tesla M60 major: 5 minor: 2 memoryClockRate(GHz): 1.1775</span><br><span class="line">pciBusID: 0000:da:00.0</span><br><span class="line">2020-09-11 08:30:55.744263: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties: </span><br><span class="line">name: Tesla M60 major: 5 minor: 2 memoryClockRate(GHz): 1.1775</span><br><span class="line">pciBusID: 0000:db:00.0</span><br><span class="line">2020-09-11 08:30:55.744692: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library <span class="string">'libcudart.so.10.0'</span>; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/dell2/.mujoco/mjpro150/bin:/usr/<span class="built_in">local</span>/cuda-10.1/lib64:</span><br><span class="line">2020-09-11 08:30:55.744798: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library <span class="string">'libcublas.so.10.0'</span>; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/dell2/.mujoco/mjpro150/bin:/usr/<span class="built_in">local</span>/cuda-10.1/lib64:</span><br><span class="line">2020-09-11 08:30:55.744891: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library <span class="string">'libcufft.so.10.0'</span>; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/dell2/.mujoco/mjpro150/bin:/usr/<span class="built_in">local</span>/cuda-10.1/lib64:</span><br><span class="line">2020-09-11 08:30:55.744980: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library <span class="string">'libcurand.so.10.0'</span>; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/dell2/.mujoco/mjpro150/bin:/usr/<span class="built_in">local</span>/cuda-10.1/lib64:</span><br><span class="line">2020-09-11 08:30:55.745070: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library <span class="string">'libcusolver.so.10.0'</span>; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/dell2/.mujoco/mjpro150/bin:/usr/<span class="built_in">local</span>/cuda-10.1/lib64:</span><br><span class="line">2020-09-11 08:30:55.745166: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library <span class="string">'libcusparse.so.10.0'</span>; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/dell2/.mujoco/mjpro150/bin:/usr/<span class="built_in">local</span>/cuda-10.1/lib64:</span><br><span class="line">2020-09-11 08:30:55.750141: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7</span><br><span class="line">2020-09-11 08:30:55.750170: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1663] Cannot dlopen some GPU libraries. Skipping registering GPU devices...</span><br><span class="line">2020-09-11 08:30:55.750542: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:</span><br><span class="line">2020-09-11 08:30:55.750706:       0 1 2 3 </span><br><span class="line">2020-09-11 08:30:55.750797:  0:   N Y Y Y </span><br><span class="line">2020-09-11 08:30:55.750887:  1:   Y N Y Y </span><br><span class="line">2020-09-11 08:30:55.750974:  2:   Y Y N Y </span><br><span class="line">2020-09-11 08:30:55.751059:  3:   Y Y Y N </span><br><span class="line">Device mapping:</span><br><span class="line">/job:localhost/replica:0/task:0/device:XLA_GPU:0 -&gt; device: XLA_GPU device</span><br><span class="line">/job:localhost/replica:0/task:0/device:XLA_GPU:1 -&gt; device: XLA_GPU device</span><br><span class="line">/job:localhost/replica:0/task:0/device:XLA_GPU:2 -&gt; device: XLA_GPU device</span><br><span class="line">/job:localhost/replica:0/task:0/device:XLA_GPU:3 -&gt; device: XLA_GPU device</span><br><span class="line">/job:localhost/replica:0/task:0/device:XLA_CPU:0 -&gt; device: XLA_CPU device</span><br><span class="line">2020-09-11 08:30:55.757190: I tensorflow/core/common_runtime/direct_session.cc:296] Device mapping:</span><br><span class="line">/job:localhost/replica:0/task:0/device:XLA_GPU:0 -&gt; device: XLA_GPU device</span><br><span class="line">/job:localhost/replica:0/task:0/device:XLA_GPU:1 -&gt; device: XLA_GPU device</span><br><span class="line">/job:localhost/replica:0/task:0/device:XLA_GPU:2 -&gt; device: XLA_GPU device</span><br><span class="line">/job:localhost/replica:0/task:0/device:XLA_GPU:3 -&gt; device: XLA_GPU device</span><br><span class="line">/job:localhost/replica:0/task:0/device:XLA_CPU:0 -&gt; device: XLA_CPU device</span><br></pre></td></tr></tbody></table></figure>
<p>表示tensorflow支持device：CPU：0 ，支持device：<code>GPU：0,1,2,3</code>，共4块GPU</p>
<p>比如CPU在TensorFlow中的名称为/cpu:0。<strong>在默认情况下，即使机器有多个CPU，TensorFlow也不会区分它们，所有的CPU都使用/cpu:0作为名称。</strong></p>
<p>而一台机器上不同GPU的名称是不同的，第n个GPU在TensorFlow中的名称为/gpu:n。比如第一个GPU的名称为/gpu:0，第二个GPU名称为/gpu:1，以此类推。</p>
<p>作者：博文视点 链接：https://www.jianshu.com/p/26ac409dfb38 来源：简书 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>
<h3 id="ubuntu中查看显卡信息">Ubuntu中查看显卡信息</h3>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">lspci | grep -i vga <span class="comment">#显卡</span></span><br></pre></td></tr></tbody></table></figure>
<p>显示结果如下：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">03:00.0 VGA compatible controller: Matrox Electronics Systems Ltd. Integrated Matrox G200eW3 Graphics Controller (rev 04)</span><br><span class="line">b1:00.0 VGA compatible controller: NVIDIA Corporation GM204GL [Tesla M60] (rev a1)</span><br><span class="line">b2:00.0 VGA compatible controller: NVIDIA Corporation GM204GL [Tesla M60] (rev a1)</span><br><span class="line">da:00.0 VGA compatible controller: NVIDIA Corporation GM204GL [Tesla M60] (rev a1)</span><br><span class="line">db:00.0 VGA compatible controller: NVIDIA Corporation GM204GL [Tesla M60] (rev a1)</span><br></pre></td></tr></tbody></table></figure>
<h3 id="ubuntu中查看nvidia-gpu">Ubuntu中查看nvidia GPU</h3>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">lspci | grep -i nvidia <span class="comment">#查看gpu信息</span></span><br></pre></td></tr></tbody></table></figure>
<p>显示结果如下：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">b1:00.0 VGA compatible controller: NVIDIA Corporation GM204GL [Tesla M60] (rev a1)</span><br><span class="line">b2:00.0 VGA compatible controller: NVIDIA Corporation GM204GL [Tesla M60] (rev a1)</span><br><span class="line">da:00.0 VGA compatible controller: NVIDIA Corporation GM204GL [Tesla M60] (rev a1)</span><br><span class="line">db:00.0 VGA compatible controller: NVIDIA Corporation GM204GL [Tesla M60] (rev a1)</span><br></pre></td></tr></tbody></table></figure>
<h3 id="查看nvidia的显卡信息和使用情况">查看Nvidia的显卡信息和使用情况</h3>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">nvidia-smi</span><br></pre></td></tr></tbody></table></figure>
<p>显示如下：</p>
<p><img src="https://i.loli.net/2020/09/11/OQZjHpocke5S9FE.png" alt="image-20200910221947824"></p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">ps aux | grep train.py #我的实验名称为train.py</span><br></pre></td></tr></tbody></table></figure>
<p><img src="https://i.loli.net/2020/09/11/7lujWDbiqnV6zwG.png" alt="image-20200910222011793"></p>
<p>可以看到，我的实验进程号是<code>21195</code>，在<code>processes</code>中可以看到使用了<code>GPU1,2</code></p>
<h3 id="指定gpu实验加速">指定GPU实验加速</h3>
<p>如果电脑有多个GPU，tensorflow默认全部使用。</p>
<p>如果想只使用部分GPU，可以设置<code>CUDA_VISIBLE_DEVICES</code>。</p>
<h4 id="命令行指定">命令行指定</h4>
<p>在执行python程序时，可以通过：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=1 python train.py <span class="comment">#只使用GPU1</span></span><br></pre></td></tr></tbody></table></figure>
<p>以下为一些使用指导：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">Environment Variable Syntax      Results</span><br><span class="line"></span><br><span class="line">CUDA_VISIBLE_DEVICES=1           Only device 1 will be seen</span><br><span class="line">CUDA_VISIBLE_DEVICES=0,1         Devices 0 and 1 will be visible</span><br><span class="line">CUDA_VISIBLE_DEVICES=<span class="string">"0,1"</span>       Same as above, quotation marks are optional</span><br><span class="line">CUDA_VISIBLE_DEVICES=0,2,3       Devices 0, 2, 3 will be visible; device 1 is masked</span><br><span class="line">CUDA_VISIBLE_DEVICES=<span class="string">""</span>          No GPU will be visible1234567</span><br></pre></td></tr></tbody></table></figure>
<h4 id="代码中指定">代码中指定</h4>
<p>在Python代码中添加以下内容：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="string">"1"</span> <span class="comment">#只使用GPU1</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="设置tensorflow使用的显存大小">设置tensorflow使用的显存大小</h3>
<h4 id="定量设置显存">定量设置显存</h4>
<p>默认tensorflow是使用GPU尽可能多的显存（内存）。</p>
<p>用Tensorflow创建session的时候要注意设置内存使用情况，特别是内存资源不够而且要和别人共享一块GPU的时候（留一点给别人用）</p>
<p>可以通过下面的方式，来设置使用的GPU显存：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=<span class="number">0.7</span>)</span><br><span class="line">sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))</span><br></pre></td></tr></tbody></table></figure>
<p>上面分配给tensorflow的GPU显存大小为：GPU实际显存*0.7。 可以按照需要，设置不同的值，来分配显存。</p>
<h4 id="按需设置显存">按需设置显存</h4>
<p>上面的只能设置固定的大小。如果想按需分配，可以使用allow_growth参数</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">gpu_options = tf.GPUOptions(allow_growth=<span class="literal">True</span>)</span><br><span class="line">sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))  </span><br><span class="line"><span class="comment"># 使用allow_growth option，刚一开始分配少量的GPU容量，然后按需慢慢的增加，由于不会释放内存，所以会导致碎片</span></span><br></pre></td></tr></tbody></table></figure>
<p>如果一个 TensorFlow 的 operation 中兼有 CPU 和 GPU 的实现, 当这个算子被指派设备时, GPU 有优先权. 比如<code>matmul</code>中 CPU 和 GPU kernel 函数都存在. 那么在 <code>cpu:0</code> 和 <code>gpu:0</code> 中, <code>matmul</code> operation 会被指派给 <code>gpu:0</code> .</p>
<h4 id="记录设备指派情况">记录设备指派情况</h4>
<p>为了获取你的 operations 和 Tensor 被指派到哪个设备上运行, 用 <code>log_device_placement</code> 新建一个 <code>session</code>, 并设置为 <code>True</code>.</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 新建一个 graph.</span></span><br><span class="line">a = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], shape=[<span class="number">2</span>, <span class="number">3</span>], name=<span class="string">'a'</span>)</span><br><span class="line">b = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], shape=[<span class="number">3</span>, <span class="number">2</span>], name=<span class="string">'b'</span>)</span><br><span class="line">c = tf.matmul(a, b)</span><br><span class="line"><span class="comment"># 新建session with log_device_placement并设置为True.</span></span><br><span class="line">sess = tf.Session(config=tf.ConfigProto(log_device_placement=<span class="literal">True</span>))</span><br><span class="line"><span class="comment"># 运行这个 op.</span></span><br><span class="line"><span class="keyword">print</span> sess.run(c)</span><br></pre></td></tr></tbody></table></figure>
<p>你应该能看见以下输出:</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">Device mapping:</span><br><span class="line">/job:localhost/replica:0/task:0/gpu:0 -&gt; device: 0, name: Tesla K40c, pci bus</span><br><span class="line">id: <span class="number">0000</span>:<span class="number">05</span>:<span class="number">00.0</span></span><br><span class="line">b: /job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/gpu:<span class="number">0</span></span><br><span class="line">a: /job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/gpu:<span class="number">0</span></span><br><span class="line">MatMul: /job:localhost/replica:<span class="number">0</span>/task:<span class="number">0</span>/gpu:<span class="number">0</span></span><br><span class="line">[[ <span class="number">22.</span>  <span class="number">28.</span>]</span><br><span class="line"> [ <span class="number">49.</span>  <span class="number">64.</span>]]</span><br></pre></td></tr></tbody></table></figure>
<h4 id="section"></h4>
<h3 id="gpu和cpu">GPU和CPU</h3>
<p>一个GPU被多个实验使用，但是如果实验超过显存大小，就会都被挂掉，会显示<code>stopped</code>字样</p>
<p>一个实验可以用多个GPU，但是需要更改部分代码，让其支持多GPU</p>
<p>不要tensorflow-gpu和tensorflow(cpu版)一起装，因为这样装有个先后顺序问题，先安装tensorflow-gpu再安装tensorflow，gpu版本直接不能用了。</p>
<p>如果想测试cpu和gpu版本性能的，最好创建两个python的虚拟环境，一个装tensorflow-gpu，另一个装tensorflow。</p>
<hr>
<p>在Tensorflow中使用gpu和cpu是有很大的差别的。在小数据集的情况下，cpu和gpu的性能差别不大。不过在大数据集的情况下，cpu的时间显著增加，而gpu变化并不明显。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> timeit</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cpu_run</span><span class="params">(num)</span>:</span></span><br><span class="line">  <span class="keyword">with</span> tf.device(<span class="string">'/cpu:0'</span>):</span><br><span class="line">    cpu_a=tf.random.normal([<span class="number">1</span>,num])</span><br><span class="line">    cpu_b=tf.random.normal([num,<span class="number">1</span>])</span><br><span class="line">    c=tf.matmul(cpu_a,cpu_b)</span><br><span class="line">  <span class="keyword">return</span> c</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gpu_run</span><span class="params">(num)</span>:</span></span><br><span class="line">  <span class="keyword">with</span> tf.device(<span class="string">'/gpu:0'</span>):</span><br><span class="line">    gpu_a=tf.random.normal([<span class="number">1</span>,num])</span><br><span class="line">    gpu_b=tf.random.normal([num,<span class="number">1</span>])</span><br><span class="line">    c=tf.matmul(gpu_a,gpu_b)</span><br><span class="line">  <span class="keyword">return</span> c</span><br><span class="line">k=<span class="number">10</span></span><br><span class="line">m=<span class="number">7</span></span><br><span class="line">cpu_result=np.arange(m,dtype=np.float32)</span><br><span class="line">gpu_result=np.arange(m,dtype=np.float32)</span><br><span class="line">x_time=np.arange(m)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">  k=k*<span class="number">10</span></span><br><span class="line">  x_time[i]=k</span><br><span class="line">  cpu_str=<span class="string">'cpu_run('</span>+str(k)+<span class="string">')'</span></span><br><span class="line">  gpu_str=<span class="string">'gpu_run('</span>+str(k)+<span class="string">')'</span></span><br><span class="line">  <span class="comment">#print(cpu_str)</span></span><br><span class="line">  cpu_time=timeit.timeit(cpu_str,<span class="string">'from __main__ import cpu_run'</span>,number=<span class="number">10</span>)</span><br><span class="line">  gpu_time=timeit.timeit(gpu_str,<span class="string">'from __main__ import gpu_run'</span>,number=<span class="number">10</span>)</span><br><span class="line">  <span class="comment"># 正式计算10次，取平均时间</span></span><br><span class="line">  cpu_time=timeit.timeit(cpu_str,<span class="string">'from __main__ import cpu_run'</span>,number=<span class="number">10</span>)</span><br><span class="line">  gpu_time=timeit.timeit(gpu_str,<span class="string">'from __main__ import gpu_run'</span>,number=<span class="number">10</span>)</span><br><span class="line">  cpu_result[i]=cpu_time</span><br><span class="line">  gpu_result[i]=gpu_time</span><br><span class="line"></span><br><span class="line">print(cpu_result)</span><br><span class="line">print(gpu_result)</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.set_xscale(<span class="string">"log"</span>)</span><br><span class="line">ax.set_adjustable(<span class="string">"datalim"</span>)</span><br><span class="line">ax.plot(x_time,cpu_result)</span><br><span class="line">ax.plot(x_time,gpu_result)</span><br><span class="line">ax.grid()</span><br><span class="line">plt.draw()</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p><img src="https://i.loli.net/2020/09/11/tRxAb2wY5qKGF4D.png" alt="在这里插入图片描述"> 蓝线是cpu的耗时，而红线是gpu的耗时。</p>
<p>更多gpu内容可参考</p>
<blockquote>
<p><a href="https://docs.pythontab.com/tensorflow/how_tos/using_gpu/" target="_blank" rel="noopener">tensorflow官方文档，使用 GPUs</a></p>
<p><a href="https://www.cnblogs.com/nxf-rabbit75/p/10639833.html" target="_blank" rel="noopener">Tensorflow检验GPU是否安装成功 及 使用GPU训练注意事项</a></p>
<p><a href="https://www.jianshu.com/p/26ac409dfb38" target="_blank" rel="noopener">TensorFlow：实战Google深度学习框架（第2版）:GPU加速</a></p>
</blockquote>
<h3 id="tensorflow匹配的关系">tensorflow匹配的关系</h3>
<p><img src="https://i.loli.net/2020/09/13/FqJ1cXThMzKHvA5.png" alt="image-20200913144848843"></p>
<p><img src="E:\myBlog\source_posts\FqJ1cXThMzKHvA5.png"></p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
  </entry>
  <entry>
    <title>2020-09-07-实验日志</title>
    <url>/2020/09/07/2020-09-07-%E5%AE%9E%E9%AA%8C%E6%97%A5%E5%BF%97/</url>
    <content><![CDATA[<h3 id="虚拟环境配置">虚拟环境配置</h3>
<h4 id="笔记本">笔记本</h4>
<table>
<thead>
<tr class="header">
<th>名称</th>
<th>配置</th>
<th>用处</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>sijian36</td>
<td>tf 1.9.0</td>
<td>普通跑实验</td>
</tr>
<tr class="even">
<td>python714</td>
<td>tf 1.14.0</td>
<td>RKN</td>
</tr>
<tr class="odd">
<td>ronghe</td>
<td>tf 1.14.0</td>
<td>transformer和RKN</td>
</tr>
</tbody>
</table>
<h4 id="r740服务器">R740服务器</h4>
<p>cuda-10.2</p>
<table>
<thead>
<tr class="header">
<th>名称</th>
<th>配置</th>
<th>用处</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>sijian1</td>
<td>tf 1.13.0</td>
<td>一般实验</td>
</tr>
<tr class="even">
<td>pytorth030</td>
<td>torch 0.3</td>
<td>哈佛torch版transformer</td>
</tr>
<tr class="odd">
<td>lsjRKN</td>
<td>tf 1.14.0</td>
<td>RKN</td>
</tr>
<tr class="even">
<td>ronghe</td>
<td>tf 1.14.0</td>
<td>transformer和RKN</td>
</tr>
<tr class="odd">
<td>ronghe6</td>
<td>tf-gpu1.14.0</td>
<td>gpu加速融合</td>
</tr>
</tbody>
</table>
<h3 id="日志">🚀日志</h3>
<h4 id="section"><strong>2020-09-06</strong></h4>
<h5 id="主要内容">主要内容</h5>
<p>笔记本的RKN实验</p>
<p>跑通是在RKNmaster文件跑</p>
<hr>
<p>哈佛torch版transformer实验</p>
<p>R740中 LSJ/annotated-transformer1/main5.py(pytorch030)</p>
<p>是之前上传到R740跑的实验</p>
<p>LSJ/annotated-transformer是前一阵为了将函数改成直通型流程而从笔记本上上传的</p>
<h5 id="出现问题">出现问题</h5>
<p>在R740跑RKN实验</p>
<p><code>attributeerror: module 'tensorflow.keras.initializers'  has no attribute 'normal'</code> 解决</p>
<p>将RKN.py 77行 normal 改为 <strong>RandomNormal</strong> 还是出错</p>
<p>再次出错 keep.dim出错</p>
<p>修改 将keep.dim=True参数去掉 再运行</p>
<p>​ <img src="https://i.loli.net/2020/09/08/h7Ma3lm82eFSuWX.png" alt="img"></p>
<ul>
<li>运行结果是没有tf.matrix_band_part 这个参数，于是百度发现，</li>
<li>新版本：tf.matrix_band_part变成tf.linalg.band_part 于是修改再运行</li>
</ul>
<p>运行结果显示</p>
<p>​ <img src="https://i.loli.net/2020/09/08/h7Ma3lm82eFSuWX.png" alt="img"></p>
<p>于是百度，原因是</p>
<p>The image from your input pipeline is of type 'uint8', you need to type cast it to 'float32', You can do this after the image jpeg decoder:</p>
<p>以下更改，在RKN.py中插入h = tf.cast(h, tf.float32)</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line"></span><br><span class="line">def _prop_through_layers(inputs, layers):</span><br><span class="line"></span><br><span class="line">​    """propagates inputs through layers"""</span><br><span class="line"></span><br><span class="line">​    h = inputs</span><br><span class="line"></span><br><span class="line">h = tf.cast(h, tf.float32)</span><br><span class="line"></span><br><span class="line">​    for layer in layers:</span><br><span class="line"></span><br><span class="line">​        h = layer(h)</span><br><span class="line"></span><br><span class="line">​    return h</span><br></pre></td></tr></tbody></table></figure>
<p>还是报错</p>
<p><strong>放弃使用sijian1 以及刚刚对RKNmaser的修改</strong></p>
<p>将笔记本中的RKNmaster 复制为rknmas上传到R740 名字为<strong>rknmas</strong></p>
<p>参考了笔记本中的虚拟环境，在R740新建lsjRKN的虚拟环境，<strong>tf版本为1.14 python：3.6</strong></p>
<p>可以跑通实验</p>
<p>实验可以在R740跑起来，但是为什么论文作者的github代码上tensorflow版本是1.13 不好使，但是在tensorflow1.14就可以跑起来？？？？</p>
<p>在笔记本上跑的 设置epoch=5</p>
<p>​ <img src="https://i.loli.net/2020/09/08/h7Ma3lm82eFSuWX.png" alt="img"></p>
<h4 id="section-1"><strong>2020-09-07</strong></h4>
<h5 id="主要内容-1">主要内容</h5>
<p>配置transformer和RKN融合的实验虚拟环境 测试代码</p>
<p>下载的是<a href="https://github.com/Kyubyong/transformer" target="_blank" rel="noopener">Kyubyong/transformer</a> 代码，准备融合RKN</p>
<p>具体的配置如下：</p>
<hr>
<p><strong>Requirements</strong></p>
<ul>
<li>python==3.x (Let's move on to python 3 if you still use python 2)</li>
<li>tensorflow==1.12.0</li>
<li>numpy&gt;=1.15.4</li>
<li>sentencepiece==0.1.8</li>
<li>tqdm&gt;=4.28.1 #显示进度条的包</li>
</ul>
<hr>
<p>github下载代码，放到<code>C:\Users\Administrator\PycharmProjects</code>目录下，文件名为 <code>transformer-master</code></p>
<p><code>python714</code>是可以运行RKN的，在笔记本上，根据<code>python714</code> clone了<code>ronghe</code> ，并添加所需要的包</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">conda install tqdm</span><br><span class="line"></span><br><span class="line">pip install  sentencepiece==0.1.8 <span class="comment"># conda 安装出错 ，于是用pip安装</span></span><br></pre></td></tr></tbody></table></figure>
<h5 id="出现问题-1"><strong>出现问题</strong></h5>
<p>此代码不是官方代码，虽然可以实现transformer，但是使用的数据集是小型的<code>IWSLT 2016 de-en</code>，而不是transformer论文中使用的大型数据集WMT，但是官方代码又很难读，而且有很多用不到的接口</p>
<p>在纠结，要用目前的代码进行融合，还是用官方的代码呢？</p>
<p>问过师兄，现在还是不用官方的transformer代码，就用目前的代码，只是验证，不用管实验数据集，先将现在的代码结合RKN再说</p>
<h4 id="section-2"><strong>2020-09-08</strong></h4>
<h5 id="主要内容-2">主要内容</h5>
<p>阅读整理RKN的代码</p>
<p>将昨天的transformer数据集无法读取的问题解决</p>
<p>将RKN在R740上跑，并保存在<code>test1.txt</code>文件中，可以用<code>less</code> 查看</p>
<h5 id="遇到问题">遇到问题</h5>
<p>RKN代码读的一脸懵</p>
<p>transformer代码bug还未修复 <span class="github-emoji" style="display:inline;vertical-align:middle;color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f62d.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f62d.png?v8">😭</span></p>
<h4 id="section-3"><strong>2020-09-09</strong></h4>
<h5 id="主要内容-3">主要内容</h5>
<p>在R740新建环境<code>ronghe</code>，根据虚拟环境<code>lsjRKN</code>来建的</p>
<p>第三方包也安装成功</p>
<h5 id="遇到问题-1"><strong>遇到问题</strong></h5>
<ol type="1">
<li></li>
<li>添加上encoding='ascii',error='ignore'就可以解决</li>
</ol>
<p><img src="https://i.loli.net/2020/09/11/vCfMGWJ975VEiRl.png" alt="image-20200909094144959"></p>
<h6 id="注">注</h6>
<p>在解决完之后，一定要看报错的位置，可能这个已解决，但是其它相同的问题不同位置也会报错，同样解决就可以了</p>
<ol start="2" type="1">
<li>在笔记本上跑此实验，发现内存不够，超出内容超过10%</li>
</ol>
<p><code>Allocation of 1196032000 exceeds 10% of system memory</code></p>
<p><strong>解决</strong></p>
<p>减少<code>banch_size</code> ， 但是还是超出，但是应该是在现有环境下实验可以跑通的，于是想着在R740上跑</p>
<p>在R740跑<code>prepro.py</code>实验，如下输出，并<code>INFO：done</code> （表示完成）</p>
<p><img src="https://i.loli.net/2020/09/11/BAb9krnJ8dCYKTX.png" alt="image-20200909132740936"></p>
<p>开始跑<code>train.py</code> 并将输出保存在train99.txt中（9月9日）</p>
<p><img src="https://i.loli.net/2020/09/11/7UpXhTBcnGNILtH.png" alt="image-20200909134011883"></p>
<p>这个WARNING是什么意思呢？</p>
<p>猜想：源代码需要的是<code>tf1.12</code>版本 我配置的是<code>tf 1.14</code>版本，不知道是不是这个原因 。晚上回寝百度一下</p>
<ol start="3" type="1">
<li>在740中跑的太慢了，不知道具体原因。在看源代码进行修改</li>
</ol>
<h4 id="section-4">2020-09-10</h4>
<h5 id="主要内容-4">主要内容</h5>
<p>更改虚拟环境，可以使用GPU对实验进行加速</p>
<p>阅读transformer的代码，明天融合</p>
<p>对跑实验的一些warning都已经修改了，复制项目名字为<code>transformer-mas</code></p>
<p>上传到R740中，命名<code>transformer-mas</code></p>
<h5 id="遇到问题-2">遇到问题</h5>
<p><img src="https://i.loli.net/2020/09/11/HUMOjGrYSiZlPo5.png" alt="image-20200910200948426" style="zoom:200%;"></p>
<p>新建<code>ronghe3</code>虚拟环境</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">conda install python==3.6.10  <span class="comment">#这样可以 ，但是为什么 python==3.6.1 和  python==3.6.0 是不可以的呢？？</span></span><br><span class="line">pip install tensorflow-gpu==1.13.1</span><br></pre></td></tr></tbody></table></figure>
<p>在已经安装了tensorflow-gpu的<code>ronghe3</code>基础上，克隆了<code>ronghe4</code>，进行接下来的操作</p>
<h6 id="注-1">注</h6>
<p>如果执行<code>conda install tensorflow==1.13.1</code></p>
<p>安装错误 ，导入不了tensorflow-gpu，应该是和CUDA版本不匹配</p>
<p><img src="https://i.loli.net/2020/09/11/CgXZf5vQzdODpyU.png" alt="image-20200910211644840"></p>
<h5 id="参考">参考</h5>
<p><a href="https://www.tensorflow.org/install/gpu?hl=zh-cn" target="_blank" rel="noopener">tensorflow官方，GPU 支持</a></p>
<p><code>ronghe5</code></p>
<p><code>pip install tensorflow-gpu==1.12.0</code></p>
<p>还是跑不通</p>
<p><code>ronghe6</code></p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">conda install python==3.6.10  <span class="comment">#这样可以 ，但是为什么 python==3.6.1 和  python==3.6.0 是不可以的呢？？</span></span><br><span class="line">pip install tensorflow-gpu==1.14.0</span><br></pre></td></tr></tbody></table></figure>
<p>终于可以跑通了，不会报错了！！！</p>
<p>测试安装的tensorflow是否可用GPU，可以使用。测试如下：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">pyhton <span class="comment">#进入python操作环境</span></span><br><span class="line"></span><br><span class="line">import tensorflow as tf </span><br><span class="line">sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))</span><br></pre></td></tr></tbody></table></figure>
<p>RKN实验跑完了，保存在test2.txt中</p>
<blockquote>
<p>tensorflow-gpu 1.5版本及以上要求CUDA版本为9.0</p>
<p>tensorflow-gpu 1.3及以上版本要求cudnn版本为V6及以上</p>
</blockquote>
<h4 id="section-5">2020-09-11</h4>
<h5 id="主要内容-5">主要内容</h5>
<p>解决在linux显示图形的问题</p>
<p>解决transformer实验报错</p>
<h5 id="遇到问题-3">遇到问题</h5>
<ol type="1">
<li>用xshell在服务器linux端只能显示控制台输出，如果想要显示图像，比如<code>matplotlib</code>包，则要下载<code>xmanage</code></li>
</ol>
<p>由于需要收费，没有下载</p>
<p><img src="https://i.loli.net/2020/09/11/zkqMvhElw2ubg9U.png" alt="image-20200911212759446"></p>
<p>解决方法： 可以用<code>plt.savafig</code>保存到服务器，再保存在本地笔记本</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line">matplotlib.use(<span class="string">'Agg'</span>)</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.font_manager <span class="keyword">import</span> FontProperties</span><br><span class="line">X = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</span><br><span class="line">dataY = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>]</span><br><span class="line">plt.xlabel(<span class="string">"x轴"</span>);</span><br><span class="line">plt.ylabel(<span class="string">"y轴"</span>);</span><br><span class="line">plt.savefig(<span class="string">"./lisijian.png"</span>,dpi=<span class="number">100</span>) <span class="comment">#保存在本文件夹下的lisijian.png</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p>报错<code>_tkinter.TclError: couldn't connect to display "localhost:32.0"</code></p>
<p>原因： 问题在于，您使用的是一个交互式后端，它试图为您创建图形窗口，但由于您断开了启动模拟时可用的x服务器，所以失败了。</p>
<p>解决方法：使用非交互式后端(请参见<a href="https://matplotlib.org/faq/usage_faq.html#what-is-a-backend" target="_blank" rel="noopener">后端</a>？)比如：Agg(用于Png格式，PDF, SVG或PS。在生成图形的脚本中，只需在import matplotlib.pyplot as plt之前调用matplotlib.use(）即可</p>
<p>比如<code>matplotlib.use('Agg')</code></p>
<ol start="2" type="1">
<li><p>在transformer实验中，才开始没注意，今天才发现有一个错误，如下:</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">AssertionError: Bad argument number <span class="keyword">for</span> Name: 3, expecting 4</span><br></pre></td></tr></tbody></table></figure></li>
</ol>
<p>解决方法：因为对结果的影响不可观,所以就没去在意 ,后面发现用其他docker并没有多少问题,而且每次都出现一堆warning很影响美观性,于是百度准备解决这个问题</p>
<p><strong>后来发现是有个gast的库版本太高,导致不兼容的问题,降级gast即可解决</strong></p>
<p>使用pip进行降级</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">pip install --user gast==0.2.2</span><br></pre></td></tr></tbody></table></figure>
<p><strong>待解决：</strong></p>
<p><strong>tensorflow的兼容性问题 cuda的兼容性问题 ？？</strong></p>
<p><strong>一般如果要对服务器上的实验进行更改的话，怎能会简单一些？？</strong></p>
<h4 id="section-6">2020-09-13</h4>
<h5 id="主要内容-6">主要内容</h5>
<p>解决transformer报错的问题</p>
<p>解决tensorflow目前不支持CUDA10.1的问题</p>
<p>修改：</p>
<p>将batch 由 128 改为 32</p>
<p>将maxlen1 和maxlen2 由100改为101</p>
<h5 id="遇到问题-4">遇到问题</h5>
<ol type="1">
<li><p>在运行transformer代码的时候，程序报错如下（部分内容，具体参考<code>train911.txt</code>）：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">"train.py"</span>, line 81, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    hypotheses = get_hypotheses(num_eval_batches, num_eval_samples, sess, y_hat, m.idx2token)</span><br><span class="line">  File <span class="string">"/home/dell2/LSJ/transformer-master/utils.py"</span>, line 144, <span class="keyword">in</span> get_hypotheses</span><br><span class="line">    h = sess.run(tensor)</span><br><span class="line">  File <span class="string">"/home/dell2/anaconda3/envs/ronghe6/lib/python3.6/site-packages/tensorflow/python/client/session.py"</span>, line 950, <span class="keyword">in</span> run</span><br><span class="line">    run_metadata_ptr)</span><br><span class="line">  File <span class="string">"/home/dell2/anaconda3/envs/ronghe6/lib/python3.6/site-packages/tensorflow/python/client/session.py"</span>, line 1173, <span class="keyword">in</span> _run</span><br><span class="line">    feed_dict_tensor, options, run_metadata)</span><br><span class="line">  File <span class="string">"/home/dell2/anaconda3/envs/ronghe6/lib/python3.6/site-packages/tensorflow/python/client/session.py"</span>, line 1350, <span class="keyword">in</span> _do_run</span><br><span class="line">    run_metadata)</span><br><span class="line">  File <span class="string">"/home/dell2/anaconda3/envs/ronghe6/lib/python3.6/site-packages/tensorflow/python/client/session.py"</span>, line 1370, <span class="keyword">in</span> _do_call</span><br><span class="line">    raise <span class="built_in">type</span>(e)(node_def, op, message)</span><br><span class="line">tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[111,100] = 100 is not <span class="keyword">in</span> [0, 100)</span><br><span class="line">	 [[node encoder_1/positional_encoding/embedding_lookup (defined at /home/dell2/LSJ/transformer-master/modules.py:290) ]]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Original stack trace <span class="keyword">for</span> <span class="string">'encoder_1/positional_encoding/embedding_lookup'</span>:</span><br><span class="line">  File <span class="string">"train.py"</span>, line 48, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    y_hat, eval_summaries = m.eval(xs, ys)</span><br><span class="line">  File <span class="string">"/home/dell2/LSJ/transformer-master/model.py"</span>, line 176, <span class="keyword">in</span> <span class="built_in">eval</span></span><br><span class="line">    memory, sents1, src_masks = self.encode(xs, False)</span><br><span class="line">  File <span class="string">"/home/dell2/LSJ/transformer-master/model.py"</span>, line 53, <span class="keyword">in</span> encode</span><br><span class="line">    enc += positional_encoding(enc, self.hp.maxlen1)</span><br><span class="line"></span><br><span class="line">  File <span class="string">"/home/dell2/LSJ/transformer-master/modules.py"</span>, line 290, <span class="keyword">in</span> positional_encoding</span><br><span class="line">     outputs = tf.nn.embedding_lookup(position_enc, position_ind)</span><br></pre></td></tr></tbody></table></figure>
<p>可以追溯到位置编码部分，出现了<code>InvalidArgumentError: indices[111,100] = 100 is not in [0, 100)</code>的错误</p>
<p>于是在我将超参数maxlen由100改为101，可以正常运行</p>
<h4 id="参考-1">参考</h4></li>
<li><p>在rognhe6中安装的tensorflow-gpu：1.14是不支持CUDA10.1版本的，只支持到CUDA10.0版本。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">tf.test.is_gpu_available()</span><br></pre></td></tr></tbody></table></figure>
<p>输出如下：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">pciBusID: 0000:db:00.0</span><br><span class="line">2020-09-13 09:32:43.541828: Could not dlopen library <span class="string">'libcudart.so.10.0'</span>; </span><br><span class="line">dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory;</span><br><span class="line"></span><br><span class="line">False</span><br></pre></td></tr></tbody></table></figure></li>
</ol>
<p>可见是不支持目前ubuntu中的CUDA环境，参考了博客，修改如下：</p>
<p>将<code>cudatoolkit=10.0</code>安装到当前环境下</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">conda install cudatoolkit=10.0</span><br></pre></td></tr></tbody></table></figure>
<p>问题解决</p>
<h5 id="参考-2">参考</h5>
<blockquote>
<p><a href="https://blog.csdn.net/qq_28193019/article/details/103146116" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/qq_28193019/article/details/103146116</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/115611908" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/115611908</a></p>
</blockquote>
<ol start="3" type="1">
<li>可以继续跑实验，可以用GPU，但是还是出现了一些问题</li>
</ol>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">Resource exhausted: OOM when allocating tensor with shape[1024,98,64] and <span class="built_in">type</span> <span class="built_in">float</span> on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc</span><br></pre></td></tr></tbody></table></figure>
<p>显示内存不够，于是我将batch_size从128改为32 ，可以正常运行了</p>
<p>或者可以考虑使用多个GPU呢？</p>
<h5 id="参考-3">参考</h5>
<blockquote>
<p><a href="https://blog.csdn.net/Will_Ye/article/details/89878588" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/Will_Ye/article/details/89878588</a></p>
<p><a href="https://blog.csdn.net/Will_Ye/article/details/89878588" target="_blank" rel="noopener">OOM ResourceExhaustedError 的完美解决方法</a></p>
</blockquote>
<h4 id="section-7">2020-09-13</h4>
<p>transformer-mas训练部分已经跑了8个epoch，只用了一个GPU，跑的有点慢，于是暂停，以后再跑。</p>
<p>开始跑test.py文件，但是在跑的时候，<code>TypeError: stat: path should be string, bytes, os.PathLike or integer, &gt; not NoneType</code></p>
<p>路径写的不对，在ckpt中添加路径即可</p>
<h4 id="section-8">2020-09-29</h4>
<p>利用最新的ckpt进行测试，显示的是</p>
<p>想着可能最新的epoch的图和数据没有完全写入文件中，所以我在log/1文件夹中将最新的ckpt删除了，让次新的ckpt来进行测试。</p>
<p>发现结果还是unk ，不知道为什么？ 是不是因为我一个epoch保存了多个ckpt</p>
<h4 id="section-9">2020-09-30</h4>
<p>今天的实验终于解决了，可以有好的结果了。这段时间真的太煎熬了。不过还是学到了不少东西。</p>
<p>之前修改的其它地方是没有问题的，不需要再变，是在epoch</p>
<p>计划以及疑问：</p>
<p>如何锁死进程， 多个的话，会显示显存不足</p>
<p>为什么必须要跑完才能显示结果呢？ 在哪体现的呢</p>
<p>平时想要快速测试代码是否好用？ 有什么办法</p>
<p>哪些人tensorflow用的好，以后经常请教</p>
<p>模型验证的作用是啥？ 在代码中没有体现出来啊</p>
<p>总结遇到的困难以及学到的知识</p>
<p>解决onetab保存的标签</p>
<h3 id="讨论">🚀讨论</h3>
<h4 id="section-10">2020.10.16</h4>
<p>1.transformer和图神经网络/图卷积网络的联系 2.传统方法处理小样本学习 论文 + 代码 3.目标检测和小样本学习的联系 论文+ 代码 4.根据读的上述论文，多跑小样本实验，了解基本思路框架逻辑 5.根据已经读过的论文，改进的transformer应用到图像处理中（transformer可以处理目标检测，那么也可以解决小样本学习），验证 6.将图像处理中flatten变换为1*1卷积 ，验证 7.transformer做时间序列聚类 ，验证</p>
<h4 id="section-11">2020.10.23</h4>
<p>任务（做完讨论）： 1.阅读BERT论文+跑代码</p>
<ol start="2" type="1">
<li>nlp领域的小样本学习研究情况</li>
</ol>
<p>研究方向: 1.参考BERT，将transformer-XL 改为双向模型，结合小样本学习的思想，用于nlp任务</p>
<p>2.在transformer处理图像的DETR模型中，加入小样本框架（MAML等），用于目标检测。 参考论文《Frustratingly Simple Few-Shot Object Detection》</p>
<p>3.可以用transformer来替换传统小样本学习模型框架中的特征提取器 （relation network、Adaptive Subspace等模型）</p>
<p>其它：</p>
<ol type="1">
<li>将论文《Adaptive Subspaces for Few-Shot Learning》发给翟滨</li>
</ol>
<h4 id="section-12">2020.10.28</h4>
<p>任务： 1. NLP + 小样本学习的论文详细阅读 ★ 2. 将BERT思想融合到transformer-XL代码中 ，处理transformer-XL模型的任务；熟练掌握这两个模型 ★</p>
<ol start="3" type="1">
<li>看GPT系列模型论文，要对transformer系列有清晰的认识</li>
<li>查找文本处理方面transformer系列最新的模型</li>
<li>查找基于对抗的文本处理模型</li>
</ol>
<h4 id="section-13">2020.11.6</h4>
<p>任务： 1.将T-CVAE代码由tf改为torch版本，用于融合模型 ★ 2.看BERT模型和GPT系列模型的论文，数据流向以及最后的输出逻辑搞明白 ★ 3.GPT系列的模型是否可以去处理句子级别的自回归，关注相关论文 4.查找一些GAN处理文本生成任务的论文，参考思路，尝试融合BERT和GPT 5.查找GPT的压缩模型 6.继续看最新的论文，了解研究近况</p>
<p>方向： 1.将T-CVAE模型的思想融入到OPTIMUS模型中，在小样本框架下处理NLP任务 ★ 2.在GAN框架下，将BERT作为判别器，将GPT作为生成器来处理文本生成任务</p>
<p>其它： 每周二下午和周俊阳一起开会讨论</p>
<h4 id="section-14">2020.11.10</h4>
<p>任务： 1.将BERT小样本论文看完 ，之后讲 2.查找一些GAN处理文本生成任务的论文（对抗文本生成），参考思路，尝试融合BERT和GPT 3.将T-CVAE代码由tf改为torch版本，用于融合模型 ★ 4.继续看最新的论文，了解研究近况</p>
<p>方向： 1.将T-CVAE模型的思想融入到OPTIMUS模型中，在小样本框架下处理NLP任务 ★ 2.在GAN框架下，将BERT作为判别器，将GPT作为生成器来处理文本生成任务 3. 参考Litte transformer， 模型 Lite BERT 在小样本框架下处理图像检测问题</p>
<p>其它： 将transformer论文以及笔记发给周俊阳 下周暂停一周讨论</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <tags>
        <tag>故障排除</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-09-06-学习工作杂记</title>
    <url>/2020/09/06/2020-09-06-%E5%AD%A6%E4%B9%A0%E5%B7%A5%E4%BD%9C%E6%9D%82%E8%AE%B0/</url>
    <content><![CDATA[<h3 id="前言">前言</h3>
<p>多做总结，提高效率，不要拖延</p>
<h3 id="怎样从熬夜中恢复过来">怎样从熬夜中恢复过来</h3>
<p>1． 不要打盹，5min不能够得到休息</p>
<p>2． 吃早餐 一个小时内吃早餐（全谷物和蛋白质） ，可以充满活力，认知能力可以提高，最好不摄入糖，会让人发困</p>
<p>3． 出去走。自然光可以让身体发热</p>
<p>4． 开始办公时候，喝一杯咖啡</p>
<p>5． 工作：首先完成最困难的部分，最开始的几个小时是一天中效率最高的时候</p>
<p>6． 会议之前可以喝一杯咖啡，有效时间是半小时</p>
<p>7． 午饭不吃过多的糖，会犯困</p>
<p>8． 下午可以喝一杯咖啡这时候是最困的时候。三点之后不能摄入咖啡，有效时间7 小时</p>
<p>9． 下午可以做一下简单的事情</p>
<h3 id="在家寝室学习">在家&amp;寝室学习</h3>
<ol type="1">
<li><p>行为影响态度。换掉睡衣，接近类似学校的状态。希望自己成为什么样子， 就穿成什么样子</p></li>
<li></li>
</ol>
<p><img src="https://i.loli.net/2020/09/06/QDutkAwlLGgKTBo.png"></p>
<ol start="3" type="1">
<li>先做一道题再说。（计划太多，无从下手。过分犹豫）不要想太多，直接动手</li>
</ol>
<p><img src="https://i.loli.net/2020/09/06/CerZI164jAU85qO.png"></p>
<ol start="4" type="1">
<li>拒绝含糖食物</li>
</ol>
<p>自控力需要能量的供给，学习前可以吃块糖，可以补充能量。但是减少高gi食物，如酸奶果汁薯片，或者碳水类食物（米饭面条土豆）。多吃瘦肉、蔬菜、水果，能够增强自控力，还能瘦</p>
<ol start="5" type="1">
<li><p>不要用时间做计划，用学习量做计划。（因为会拖延）拒绝整点学习、计划学习时间，采用今天背多少单词等，一个清晰明确的目标会事半功倍</p></li>
<li><p>保持工作区的整洁，不放无关的物品。使手机飞行模式、黑白模式</p></li>
</ol>
<p><img src="https://i.loli.net/2020/09/06/UO5JydTCmLIKa12.png"></p>
<h3 id="戒掉手机避免用意志力来克制">戒掉手机（避免用意志力来克制）</h3>
<p>1． 替代法 并不是真正想做，而是习惯了某种行为。可以买一个手机模型，终止大脑的无意识行为，给大脑一个选择的机会</p>
<p>2． 心理暗示。‘我不玩手机‘ 而不是’我不能玩手机‘</p>
<p>3． 优化环境。环境的影响很大。搭建一个良好的环境。睡觉前把手机放在客厅，学习时增加获得手机的难度</p>
<p>4． 负面反馈。人们对于损失和负面事件的敏感度高于正面事件的敏感</p>
<p>5． 看实时学习视频，看到别人学习 自己也不好意思玩</p>
<p>休息放空自己，会使得注意力更集中</p>
<p>把社交软件放在小文件夹里再放到手指不容易碰到的地方，如果一段时间又习惯了点这个位置的社交软件，就再更换桌面排布</p>
<h3 id="自己习惯">自己习惯</h3>
<p>对于我自己来说，习惯睡觉前进行一些文字记录的工作，比如写博客做总结，就是不会再去接触一些新知识。把第二天要做的事情列好，或者直接找好第二天最难工作内容的参考资料，对第二天工作内容有一个大概的印象，这样第二天一早就可以直攻克艰难的部分，避免其它琐碎的事情</p>
<p>起床的时候，提前找好第二天要穿的衣服，同时可以适量补充水分</p>
<p>在进行学习的时候，先设置5分钟，休息5分钟，再逐渐增加时间，进入状态，多学习时间不休息</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
  </entry>
  <entry>
    <title>2020-09-06-不确定性研究</title>
    <url>/2020/09/06/2020-09-06-%E4%B8%8D%E7%A1%AE%E5%AE%9A%E6%80%A7%E7%A0%94%E7%A9%B6/</url>
    <content><![CDATA[<h3 id="前言">前言</h3>
<p>近日，旷视上海研究院长危夷晨在将门技术社群做了一次题为《Uncertainty Learning for Visual Recognition》(不确定性学习在视觉计算中的应用) Online Talk，共分为4个部分：</p>
<ol type="1">
<li>Preliminary（基础知识）</li>
<li>Uncertainty in Deep Learning（深度学习中的不确定性问题）</li>
<li>Uncertainty in Computer Vision（不确定性的计算机视觉应用）</li>
<li>Summary（总结）</li>
</ol>
<p>我主要参考了前三部分的内容</p>
<h3 id="基础知识">基础知识</h3>
<h4 id="何为不确定性估计">何为不确定性估计</h4>
<p>要理解何为不确定性估计，我们可以先从<strong>确定性预测（deterministic prediction）</strong>开始。假设要对两张人脸进行对比，验证是否是同一个人的照片，那么可以使用人脸识别系统分别对这两张人脸图片提取特征，并使用某种度量指标衡量所提取的两个特征的相似程度，根据所预测出的相似程度来判断两张人脸图像是否从属同一个人。如果相似度很高（比如95%），则可以判断这两张人脸属于同一个人。这种通过预测一个确定性的人脸特征用来判断的方式被称为确定性预测（deterministic prediction）。</p>
<p>然而这个<strong>相似度分数并不总是有效</strong>，以下图中第二个例子为例，可以看到在输入图像中，一张非常清晰，另一张十分模糊，然而这个时候人脸识别系统依然给二者打出很高的相似度分数，那么面对这种情况，我们是否要相信系统给出的答案，我们是否有办法来判断系统给出这个分数的可靠程度？</p>
<p>为此，人们提出了另一个<strong>辅助判断的指标</strong>，即判断机器给出的答案是否可信，可信程度多少的分数被称为<strong>confidence score（置信度分数）</strong>。如下图第二行中，系统给出相似度95%，然而confidence score却只有10%，表明<strong>系统给出的相似度分数的可信度很低，因此我们在采纳系统给出的这个判断答案的时候需要十分谨慎。</strong></p>
<p>从这个案例可以知道，<strong>在confidence score分数背后存在一个核心思想，即很多时候机器学习系统给出的判断不一定是靠谱的，即，系统对于给出的判断具有一定程度的“不确定性”。</strong>那么此时人们就需要知道系统给出这个判断到底有几成把握，因此我们需要诸如置信度分数或者“不确定性”分数这样的额外信息来帮助我们做出更好的决策。</p>
<p><img src="https://i.loli.net/2020/09/06/xBqNOnrsRiM7o85.jpg" alt="img"></p>
<h4 id="为何不确定性重要">为何不确定性重要</h4>
<p>上面介绍完之后，我们再来谈谈它为什么重要。简单来讲，不确定性估计在深度学习之中有着广泛的应用场景，为其落地发挥着不可替代的重要作用，下面讲一些比较要代表性的场景：</p>
<ol type="1">
<li><strong>高风险应用场景</strong>。这类场景<strong>需要非常精确的估计</strong>，因为一旦估计错误，可能出现严重的后果，例如医疗图像诊断、自动驾驶。</li>
<li><strong>大量机器学习场景</strong>。比如，在主动学习（Active Learning）这种技术框架中，模型需要确定哪些样本更值得被打标签。这也涉及到系统对于估计样本“价值程度”不确定性。同时，研究人员往往也会发现单纯使用机器学习系统进行判断时，会存在少量样本系统无法做出很好的判断，因此这时人们会邀请专家来标记这部分困难样本，以训练模型。</li>
<li><strong>强化学习</strong>。强化学习由于经常要权衡exploration和exploitation操作，因此如何确定每一台机器的概率分布是否被准确估计，就是对这台机器模型参数的不确定性估计。</li>
<li><strong>对处于训练数据分布之外情况的检测</strong>。由于很多时候测试数据并不在训练数据中，因此如果测试数据超出了训练数据的数据分布，那这样的预测是没有准确度可言的，这时候就需要一个额外的不确定性估计来确认对当前的预测有多大把握。</li>
</ol>
<h4 id="两种不确定性">两种不确定性</h4>
<p>接下来，我们界定一下不确定性的分类问题。一般来讲，不确定性可以分为两类：</p>
<ol type="1">
<li><strong>数据的不确定性</strong>：也被称为偶然（Aleatoric）不确定性，它描述的是<strong>数据中内在的噪声，即无法避免的误差，这个现象不能通过增加采样数据来削弱。</strong>例如有时候拍照的手稍微颤抖画面便会模糊，这种数据是不能通过增加拍照次数来消除的。因此解决这个问题的方法一般是提升数据采集时候的稳定性，或者提升衡量指标的精度以囊括各类客观影响因素。</li>
<li><strong>模型的不确定性</strong>：也被称为认知（Epistemic）不确定性。它指出，<strong>模型自身对输入数据的估计可能因为训练不佳、训练数据不够等原因而不准确，与某一单独的数据无关</strong>。因此，认知不确定性测量的，是训练过程本身所估计的模型参数的不确定性。这种不确定性是可以通过有针对性的调整（增加训练数据等方式）来缓解甚至解决的。</li>
</ol>
<h3 id="深度学习中的不确定性问题">深度学习中的不确定性问题</h3>
<p><strong>如果单看深度学习网络本身，它是确定性的，例如简单的多层前馈网络，在训练好以后，其结构、权重以及对某一个样本所输出类别的概率都是确定的。因此，在深度神经网络中引入不确定性的一个方法就是引入贝叶斯法则，从而得到贝叶斯神经网络（BNN）。</strong></p>
<p>简单而言，如下图，贝叶斯神经网络的<strong>权重不像普通神经网络是一个具体数值，而是一个概率分布，表示每一个权重w遵循一个分布，而非之前是一个确定的数值</strong>。因此在训练和推理中，网络的权重会变化，<strong>根据分布来随机采样</strong>。通过这种方法可以建模各个参数本身存在的不确定性。</p>
<p><img src="https://i.loli.net/2020/09/06/6IRjbGw4cfT8mSB.jpg" alt="img"></p>
<p>然而，由于在实际应用中参数量十分巨大，要严格根据贝叶斯公式计算后验概率几乎不现实，因此为了将网络应用于大型数据集，就<strong>需要高效的近似计算方法</strong>。早期比较有名的方法是通过马尔科夫链蒙特卡洛采样法（MCMC-sampling）来逼近假定的参数分布，但是由于这种方法很慢，因此发展出了一系列更好的<strong>近似计算后验概率</strong>的方法，如下：</p>
<h4 id="变分推断">变分推断</h4>
<p>变分推断的基本方法就是<strong>引入变分分布对BNN优化过程中涉及到的后验概率进行近似估计，这种方法较为高效。</strong></p>
<p><img src="https://i.loli.net/2020/09/06/nQzKO2i1PYNkvuU.jpg"></p>
<h4 id="dropoutbnnvi">Dropout=BNN+VI</h4>
<p><img src="https://i.loli.net/2020/09/06/aQRWjzwDKIJl6eL.jpg" alt="img"></p>
<p>这种<strong>dropout方法</strong>也称为蒙特卡洛dropout，进一步简化了对后验概率分布的近似计算，它认为常见的dropout技术实际上等于在贝叶斯网络中进行变分推断。通过上图的对比，我们可以直观理解标准神经网络经过dropout之后，在每一层随机取消一些神经元，把连接变稀疏的网络是什么样子。</p>
<p>可以证明，<strong>在假设每一个神经元都服从一个离散的伯努利分布的情况下，经dropout方法处理的神经网络的优化过程实际上等价于在一个贝叶斯网络中进行变分推断。</strong>由于这种结构中每个节点的权重是被多个子网络共享的，因此它的训练和推理相对高效。这项理论成果近年来得到了较多的应用。</p>
<p>我们在前向传播的时候，让某个神经元的激活值以<strong>一定的概率p停止工作</strong>（每一个批次都是随机），这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征。</p>
<p><strong>dropout掉不同的隐藏神经元就类似在训练不同的网络，随机删掉一半隐藏神经元导致网络结构已经不同，整个dropout过程就相当于对很多个不同的神经网络取平均。而不同的网络产生不同的过拟合，一些互为“反向”的拟合相互抵消就可以达到整体上减少过拟合。</strong></p>
<h5 id="dropout具体工作流程">Dropout具体工作流程</h5>
<p>假设我们要训练这样一个神经网络，如图所示。</p>
<p><img src="https://i.loli.net/2020/09/06/SetbpsjYxEX1yQZ.jpg" alt="标准的神经网络"></p>
<p>输入是x输出是y，正常的流程是：我们首先把x通过网络前向传播，然后把误差反向传播以决定如何更新参数让网络进行学习。使用Dropout之后，过程变成如下：</p>
<p>（1）首先<strong>随机（临时）</strong>删掉网络中一半（dropout=0.5时）的隐藏神经元，输入输出神经元保持不变（图中虚线为部分临时被删除的神经元）</p>
<p><img src="https://i.loli.net/2020/09/06/GnirX4u39lSmyUg.jpg" alt="部分临时被删除的神经元"></p>
<p>（2） 然后把输入x通过修改后的网络前向传播，然后把得到的损失结果通过修改的网络反向传播。一小批训练样本执行完这个过程后，在<strong>没有被删除的神经元上</strong>按照随机梯度下降法更新对应的参数（w，b）。</p>
<p>（3）然后继续重复这一过程：</p>
<ul>
<li><strong>恢复被删掉的神经元</strong>（此时被删除的神经元保持原样，而没有被删除的神经元已经有所更新）</li>
<li>从隐藏层神经元中<strong>随机选择</strong>一个一半大小的子集临时删除掉（备份被删除神经元的参数）。</li>
<li>对一小批训练样本，先前向传播然后反向传播损失并根据随机梯度下降法更新参数（w，b） （没有被删除的那一部分参数得到更新，删除的神经元参数保持被删除前的结果）。</li>
</ul>
<p>不断重复这一过程。</p>
<h5 id="参考">参考</h5>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/38200980" target="_blank" rel="noopener">深度学习中Dropout原理解析</a></p>
</blockquote>
<h4 id="模型融合">模型融合</h4>
<p>这也是一种进行不确定性估计的基本方法，其大致思路是，<strong>从一个数据集中进行多次随机采样，分别训练模型，然后再将这些模型的推理结果综合，其均值作为预测结果，方差作为预测的不确定性。</strong>另外需要强调的是，蒙特卡洛dropout可以认为是一种特殊的模型融合方法。</p>
<p><img src="https://i.loli.net/2020/09/06/noXFIR2ACtwpmk1.jpg" alt="img"></p>
<h3 id="回归问题中的数据不确定性">回归问题中的数据不确定性</h3>
<p>这是一种数据估计的标准做法。<strong>给定输入x_i，解一个函数f(x_i)，使得它逼近ground truth y_i。假设这个函数f(x_i)遵循一个高斯分布，那么其均值就是y_i，方差就是σ（也依赖于x_i）。</strong></p>
<p><strong>这时，如果对这个高斯分布取似然度，再取负的log值，那么就可以得到下图中的损失函数L。因此在优化的时候，除了希望优化f(x_i)逼近y_i，同时也需要优化σ(x_i)，它表示这个高斯分布的不确定性，σ越大越不确定。</strong></p>
<p>因此当f很容易逼近y的时候，那么公式中第一项L2范数就会很小，这时即便σ也小，但结果依然不会很大；当f很难逼近y，即f很难学习的时候，第一项中的L2范数就会很大，这时优化过程就会使得σ也变大，从而使得整个第一项减小，因此学到的σ会随着数据学习的难度做自我调整。</p>
<p><img src="https://i.loli.net/2020/09/06/nlD62kW1pXygAV4.jpg" alt="img"></p>
<h4 id="简单例子">简单例子</h4>
<p>我们借助一个直观例子来理解模型不确定性与数据不确定性。首先这里的<strong>ground truth函数为一个正弦函数</strong>，即图中橙色的点是测试数据，而<strong>训练数据是从[-5，+5]区间采样的蓝色点，研究人员对每一个蓝色点都添加了高斯噪声，因此可以看到这些蓝色点明显偏离ground truth。</strong></p>
<p>下方左图是用贝叶斯网络加dropout进行的<strong>模型不确定性估计</strong>。<strong>红色曲线为估计出来的预测值，延其上下分布的黄色面积则为每一个点对应的方差</strong>。在进行模型不确定性估计时，系统会对每个输入点估计多次，每次会随机采样模型的权重，以求出对每个输入点多次预测所得到的均值和方差。可以发现，蓝色点区域之外的部分预测的方差很大，这是因为模型没有见过这样的数据。（<strong>因为蓝色是训练数据</strong>，其它是测试数据，没见过的，所以方差就会较大，也就是不确定性较高）</p>
<p>下方右图中红色曲线为估计出来的预测值，是<strong>数据不确定性估计</strong>，曲线上下的黄色跨度就是每一个点通过数据不确定性估计方法所学出的方差。可以发现，原本输入数据中有噪声的部分，其预测出的方差比较大，反映出模型对这样的输出拥有较大的不确定性。</p>
<p><img src="https://i.loli.net/2020/09/06/EjFQnPko6pVhYH5.jpg" alt="img"></p>
<h3 id="不确定性的计算机视觉应用">不确定性的计算机视觉应用</h3>
<p><img src="https://i.loli.net/2020/09/06/Gw6pNAvuisDe18a.png" alt="img"></p>
<p>尽管不确定性在机器学习中已经有很长历史，但是直到2017年（就我所知）随着NeurlPS论文<em>What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision</em>的提出，它才开始真正应用在基于深度学习的视觉预测任务中。这篇论文本身没有太多方法创新，通过将已知的方法 用于语义分割与深度回归任务，取得了不错的结果。<strong>通过在模型中引入不确定性估计的已有理论成果，使得原本任务的精度得到了显著提升。</strong></p>
<p>通过论文给出的定性结果可以较为直观的理解模型不确定性和数据不确定性。如下图，系统估计出来的不确定性是有明确含义即很容易理解的，图中上半部分做语义分割，下半部分做深度估计。</p>
<p><img src="https://picb.zhimg.com/80/v2-cf9cf0d7715af33b38dc8fa8b71b8aa0_1440w.jpg" alt="img"></p>
<p><strong>整张图的第4、5列分别是数据不确定性和模型不确定性的结果。红色部分表示不确定程度较大，蓝色部分表示较为确定。从数据不确定性结果（第4列）可以看到，红色部分往往出现在物体边界处，表示这些区域的像素更加具有二义性，系统不太清楚这部分像素究竟属于前景还是背景，另外这部分信息在训练数据中（即ground truth）往往也较模糊。可以发现，系统给出的数据不确定结果符合人类直观理解。</strong></p>
<p><strong>从模型不确定性结果（第5列）可以看到，模型对出现人的部分给出了很高的不确定性，这是因为模型在训练中很少遇到人的数据，因此模型很难估计出人所处位置的深度，将该区域标记为高度不确定。</strong></p>
<h4 id="物体检测中的数据不确定性">物体检测中的数据不确定性</h4>
<p><img src="https://i.loli.net/2020/09/06/RrKDqx7kFG8mJyl.jpg" alt="img"></p>
<p>在物体检测任务中，很大一部分不确定性来源于标注数据的不确定。上图给出了几个典型例子，可以看到，在标注边界框的时候，由于存在各种物体角度、遮挡，所以往往很难评价一个边界框标注的好坏。由于标注规则不一、数据本身存在的各种不确定性，因此具有二义性的数据标注会导致具有二义性的学习结果，从而将不确定性引入了模型，进一步输出结果也是不确定的。</p>
<p>针对这个问题，有研究人员在CVPR 2019、ICCV 2019提出了两篇颇有价值的论文，其核心思想类似，将每一个边界框的4个坐标均认为呈高斯分布，然后分别估计其均值和方差。用上述介绍的数据不确定性回归公式来替代传统的L1损失，将原来所需要预测的4个变量扩充为8个变量。</p>
<p>因此，这种方法除了可以估计边界框每一个坐标之外，还让它们都带有了一个不确定性参数。利用这些不确定性数据，可以进一步做很多事情（比如在NMS中作为权重来对边界框位置进行投票）。</p>
<h4 id="人脸识别中的模型不确定性">人脸识别中的模型不确定性</h4>
<p><img src="https://i.loli.net/2020/09/06/nWDBsXI2dNfaHTV.jpg" alt="img"></p>
<p>对于在人脸识别任务中如何估计模型不确定性，推荐大家上图中的论文工作，其核心思想是，将BNN+dropout用到人脸识别任务中，如图所示，dropout层（红色）被加在每一个卷积block之后，从而构建了一个蒙特卡洛Dropout网络。在训练过程中，每当流程到达这些层的时候，就会随机丢掉一些神经元，从而实现模拟参数分布的效果。在测试过程中，每一个图像都会经过该网络多次，进而可以多这些结果估计均值与方差，将方差作为预测结果的不确定性。</p>
<h4 id="人脸识别中的数据不确定性pfe方法">人脸识别中的数据不确定性：PFE方法</h4>
<p><img src="https://i.loli.net/2020/09/06/qPMvAkWN1UHK2e9.jpg" alt="img"></p>
<p>PFE方法全称为Probabilistic Face Embeddings，其核心思想是用概率分布来替代传统确定的人脸嵌入特征。传统的方法会将输入图像映射到一个高维空间中，得到一个表示特征的向量，然而对于这些方法而言，输出的向量都是确定的，因此被称为deterministic embedding。PFE引入了不确定性，将输出向量认为是一个概率分布，而不再是一个确定的向量，它有一个均值μ、方差σ。</p>
<p>均值代表对图像的嵌入，方差描述了模型对输入数据的不确定性。该方法希望同时估计μ和σ，并且σ能够根据输入图像的质量、包含噪声的程度进行自适应的学习。在上图右方的示例中可以看到，每一个输出的特征z不再是一个点，而是一个高斯形状的概率分布（椭圆），椭圆的大小就描述了估计的不确定性。</p>
<p><img src="https://i.loli.net/2020/09/06/O8mvS1VTIHodxZ5.png" alt="img"></p>
<p>从具体实现方法来看，PFE的创新值得借鉴，它并不直接去估计每一个均值μ，而是通过一个事先已经训练好的人脸识别模型来抽取每个样本的特征μ_i，然后研究人员再在网络中加入一个小分支，来对每个样本输出一个方差（比如假设μ_i是一个512维的向量，那么此时也会输出一个对μ_i的每一维度单独估计方差的512维方差向量）。</p>
<p>进一步，论文提出了一种新的metric——mutual likelihood score（MLS），来衡量两个分布间的距离。上图公式中x_i和x_j是两个样本在特定空间中的高斯分布，两个分布所得到的MLS数值就代表了其相似度。在训练过程中，针对所有positive 样本，计算负的MLS数值作为损失，并最小化该损失目标函数，进而可以估计新增加分支（估计方差的分支）的参数。</p>
<p><img src="https://picb.zhimg.com/80/v2-b2a72843a1b2199fb213df0e93e9d570_1440w.jpg" alt="img"></p>
<p>上图是论文对方差的解释，较为直观。可以发现红框标注出来的（方差超过一定阈值）图片都是姿态有较大变化、模糊、或者有遮挡的图片，系统都认为它们识别起来有较大不确定性；而正面、高清的图片不确定性普遍较小。为了进一步验证学习出来的不确定性是否能够有效解释图像质量，PFE在下方左图中进行了有关在低质量图像之间使用传统cosine相似度计算是否可靠的研究。</p>
<p>研究人员对清晰图片添加了不同程度的噪声，蓝色线代表原图与模糊图之间的相似度分数，而红色代表两张来自不同ID的图随模糊程度的增加所计算的相似度。可以发现对于同一ID（蓝线），随着模糊程度增加，相似度也逐渐降低；而对于不同ID，随着模糊程度增加相似度却在增加。这说明依据该相似度可能会将两张来自不同ID的模糊图像错认为是同一张图。这一现象在其它很多论文中也同样被观测到。</p>
<p><img src="https://picb.zhimg.com/80/v2-e38487eb964f4dc56b7ba89689ea5158_1440w.jpg" alt="img"></p>
<p>然而在经过PFE论文提出的MLS相似度修正之后，情况得到了很大改善。如右图，当图片模糊度增加时，对同样ID的图来说，其相似度没有变得太小，而不同ID图像的相似度也没有变得太大。这个实验证明这种计算图像相似度的新metric在面对低质量图片时更加鲁棒。</p>
<h4 id="pfe方法的缺陷">PFE方法的缺陷</h4>
<p>虽然PFE方法取得了重要进展，但是缺点也很明显，因为它并没有学习身份特征（identity-feature），每一个identity的特征嵌入是确定的，PFE只是增加了一个估计方差的小网络分支，这导致必须用一个新的metric（即MLS）来计算所有样本对的距离。而使用MLS这个度量函数带来的缺陷在实际工业应用中是代价较高的：第一，我们需要额外存储方差向量；第二，相比传统的余弦相似度，MLS相似度的计算资源消耗也更大。</p>
<p>受此启发，我们团队在投递给CVPR 2020的新论文中不仅做到了估计方差，同时也能更新每个样本的特征。下图为传统方法、PFE与我们团队方法的对比。</p>
<p><img src="https://i.loli.net/2020/09/06/pYSInMZ9R64euEC.jpg" alt="img"></p>
<p>可以发现，在图（a）中，虚线框出的蓝色椭圆代表一个类别，圈外存在一个正样本和负样本，而对于传统相似度计算方法来说，很难将负样本和正样本区分开来；而（b）中PFE方法对每个样本估计了一个分布，在带有分布的特征表示下，利用MLS就能够有效将正样本和负样本区分开来，但是PFE中正负样本本身是确定的；在（c）中，我们团队方法能够在估计正负样本方差的同时，也让特征本身修正得更好。</p>
<p><img src="https://i.loli.net/2020/09/06/ayVXfSxm1DYk5dG.png" alt="img"></p>
<p>上图是三种方法的对比，可以看到在最后计算相似度的时候，由于特征本身经过了调整，只需要使用cosine相似度来计算两个均值向量就可以得出答案。具体而言，我们团队提出了两种实现方法，如下：</p>
<h3 id="法1从头学习一个分类模型"><strong>法1：从头学习一个分类模型</strong></h3>
<p><img src="https://i.loli.net/2020/09/06/yZWVciHGd8bwK74.jpg" alt="img"></p>
<p>这种方法的主要部分与通用识别模型的结构一致，区别在于，在输出特征的位置，我们让模型输出一个有关每个样本特征的均值μ，以及一个方差σ。进一步，对于每个样本的每一次迭代而言，都随机采样一个ε（如上图最下方）。</p>
<p>通过这种方式得到的新样本特征s_i就是遵从均值μ、方差为σ的高斯分布采出的值，它可以模拟一个服从高斯分布的特征。通过这种简单的重新采样的技巧，就可以很好进行模型训练。在测试的时候不再需要采样，仅需要将已经得到的均值μ作为特征来计算相似度即可。</p>
<p><img src="https://i.loli.net/2020/09/06/8PcKyqBlErgUQAm.png" alt="img"></p>
<p>该方法的损失函数除了包含softmax以及其一切合理变种之外，还有一个KL损失，它使得每一个学出来特征的分布尽可能逼近单位高斯分布。这个损失项的引入来自于2016年一篇名为<em>Deep variational information bottleneck</em>的论文。进一步整个损失函数就可以用标准SGD方法来优化。下图解释了整个损失函数中softmax与kl损失是如何起到平衡的作用的。</p>
<p><img src="https://i.loli.net/2020/09/06/9wxoyQuj8M2AWem.jpg" alt="img"></p>
<h3 id="法2从现有模型出发学习回归模型"><strong>法2：从现有模型出发学习回归模型</strong></h3>
<p><img src="https://i.loli.net/2020/09/06/k8rDhcJY5ZaMQnU.jpg" alt="img"></p>
<p>这种方法假设输出的特征μ遵循高斯分布，目的是让它逼近期望的特征w。与PFE类似，假设输入的模型已经固定，且输出的特征μ属于类别c，则让μ逼近这个类别c的特征中心w_c（w_c来自事先训练好的人脸分类模型）。这种方法适用于当已经有一个训练好的模型，但依然希望做不确定性估计的情况。相对于PFE而言，它多做了样本特征的学习。下图解释了该损失函数中σ起到的平衡作用。</p>
<p><img src="https://i.loli.net/2020/09/06/3qWYVXC4QIx1vh8.png" alt="img"></p>
<p><strong>实验结果：</strong>在三种损失函数上的对比测试结果显示，我们团队提出的分类方法（HUM_cls）在最困难的数据集IJB-C（具有大量模糊、噪声图像）上效果最佳；在LFW、CFP-FP、YTF这些较成熟的数据集上我们提出的两种方法同其他方法区别不大；在较困难的MegFace(R1)数据集上我们团队的分类方法效果最佳。</p>
<p><img src="https://i.loli.net/2020/09/06/9e3k4ImOhET5Wgo.jpg" alt="img"></p>
<p>下图展示了在三种数据集上学习出来的方差分布情况，展示了位于不同方差位置的图像的样子。</p>
<p><img src="https://i.loli.net/2020/09/06/81X3Pk4o9IHZTxE.jpg" alt="img"></p>
<p>进一步，我们团队使用了ResNet-64作为backbone（与PFE的SOTA模型backbone深度一致），来将本文方法同SOTA方法在最困难的数据集IJB-C上进行性能对比，结果显示在每一个指标上我们团队方法均实现了领先。为了测试本文方法对噪声信息干扰的鲁棒性，团队对图片人工施加了高斯噪声（从0到40%），可以发现，当噪声越明显的时候，本文引入的不确定估计方法的优越性也约高。</p>
<p><img src="https://i.loli.net/2020/09/06/4VLqFkPgIvaYSm2.jpg" alt="img"></p>
<h3 id="参考-1">🚀参考</h3>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/95774787" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/95774787</a></p>
</blockquote>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
  </entry>
  <entry>
    <title>2020-09-05-知识点杂</title>
    <url>/2020/09/05/2020-09-05-%E7%9F%A5%E8%AF%86%E7%82%B9%E6%9D%82/</url>
    <content><![CDATA[<h3 id="对数似然">对数似然</h3>
<p>最大化对数似然，因此值越大越好。例如，对数似然值 -3 比 -7 好。</p>
<p>对数为负值是完全可能的，如下图log函数</p>
<p><img src="https://i.loli.net/2020/09/12/UFihCbxJk9gBfuz.png" alt="对数- 维基百科，自由的百科全书" style="zoom: 33%;"></p>
<h3 id="高斯分布中考虑对数似然而不是似然">高斯分布中考虑对数似然而不是似然</h3>
<p>通过最大似然函数来确定高斯分布中未知参数的值，实际上，<strong>最大化似然函数的对数更方便</strong>。因为对数是其论证的单调递增函数，函数的对数的最大化等价于函数本身的最大化。logaithm不仅简化了后续的数学分析，而且还有助于数学计算，<strong>因为大量小概率的乘积很容易使计算机的数值精度下降，但是log就可以通过计算总和来解决</strong>。</p>
<ol type="1">
<li>当要计算随机变量的joint likelihood时很有用，他们之间独立，并且分布相同。</li>
</ol>
<p><img src="https://i.loli.net/2020/10/20/qi8jgvAXN1S3Tco.png" alt="image-20200905213103449"></p>
<p>联合概率是所有点的概率的乘积：</p>
<p><img src="https://i.loli.net/2020/10/20/s1Jv75P8zauowL6.png" alt="image-20200905213136550"></p>
<p><strong>如果是log，则只需要求和即可</strong></p>
<ol start="2" type="1">
<li>由于是<strong>高斯分布</strong>，使用log避免了计算指数</li>
</ol>
<p><img src="https://i.loli.net/2020/10/20/xJ2fmL6EIK9h3sz.png" alt="image-20200905213239871"></p>
<p>可以写成：</p>
<p><img src="https://i.loli.net/2020/10/20/vGRU6lV7HgWjFuN.png" alt="image-20200905213249229"></p>
<ol start="3" type="1">
<li>ln x是单调递增的函数，因此log-likelihood和likelihood有相同的关系</li>
</ol>
<p><img src="https://i.loli.net/2020/10/20/cesuIUonJS8V1lR.png" alt="image-20200905213303446"></p>
<p><strong>负对数似然</strong>是一种用于解决分类问题的 损失函数 ，它是似然函数得一种自然对数形式，可用于测量两种概率分布之间的相似性，其取负号是为了让最大似然值和最小损失相对应，是最大似然估计及相关领域的常见函数形式。</p>
<p>机器学习中，习惯用优化 算法 求最小值，因此会用到负对数似然，这是分类问题中的常见的损失函数，且能拓展到 多分类 问题。</p>
<h3 id="负对数似然和似然估计">负对数似然和似然估计</h3>
<p><strong>负对数似然</strong>是一种用于解决分类问题的 损失函数 ，它是似然函数的一种自然对数形式，可用于测量两种概率分布之间的相似性，其取负号是为了让最大似然值和最小损失相对应，是最大似然估计及相关领域的常见函数形式。</p>
<p>机器学习中，习惯用优化 算法 求最小值，因此会用到负对数似然，这是分类问题中的常见的损失函数，且能拓展到 多分类 问题。</p>
<h3 id="最大似然估计">最大似然估计</h3>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/32803109" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/32803109</a></p>
</blockquote>
<h3 id="归纳偏置inductive-bias">归纳偏置(Inductive Bias)</h3>
<p>在机器学习中，很多学习算法经常会对学习的问题做一些<strong>假设</strong>，这些假设就称为归纳偏置(Inductive Bias)。</p>
<p><strong>归纳(Induction)</strong>是自然科学中常用的两大方法之一(归纳与演绎, induction and deduction)，指的是从一些例子中寻找共性、泛化，形成一个比较通用的规则的过程；<strong>偏置(Bias)</strong>是指我们对模型的偏好。</p>
<p>因此，归纳偏置可以理解为，从现实生活中观察到的现象中归纳出一定的规则(heuristics)，然后对模型做一定的约束，从而可以起到“模型选择”的作用，即从假设空间中选择出更符合现实规则的模型。其实，贝叶斯学习中的“<strong>先验(Prior)</strong>”这个叫法，可能比“归纳偏置”更直观一些。</p>
<p>在深度学习方面也是一样。以神经网络为例，各式各样的网络结构/组件/机制往往就来源于归纳偏置。</p>
<p>在卷积神经网络中，我们假设特征具有局部性(Locality)的特性，即当我们把相邻的一些特征放在一起，会更容易得到“解”；在循环神经网络中，我们假设每一时刻的计算依赖于历史计算结果；还有注意力机制，也是基于从人的直觉、生活经验归纳得到的规则。</p>
<p>CNN的inductive bias应该是locality和spatial invariance，即空间相近的grid elements有联系而远的没有，和空间不变性（kernel权重共享）</p>
<p>RNN的inductive bias是sequentiality和time invariance，即序列顺序上的timesteps有联系，和时间变换的不变性（rnn权重共享）</p>
<h3 id="图灵完备turing-complete">图灵完备（turing complete）</h3>
<p>在<a href="https://link.jianshu.com?t=https%3A%2F%2Fbaike.baidu.com%2Fitem%2F%E5%8F%AF%E8%AE%A1%E7%AE%97%E6%80%A7%E7%90%86%E8%AE%BA" target="_blank" rel="noopener">可计算性理论</a>里，如果一系列操作数据的规则（如指令集、编程语言、细胞自动机）按照一定的顺序可以计算出结果，被称为图灵完备（turing complete）。</p>
<p>一个有图灵完备指令集的设备被定义为<a href="https://link.jianshu.com?t=http%3A%2F%2Fbaike.baidu.com%2Fitem%2F%E9%80%9A%E7%94%A8%E8%AE%A1%E7%AE%97%E6%9C%BA" target="_blank" rel="noopener">通用计算机</a>。如果是图灵完备的，它（计算机设备）有能力执行条件跳转（if、while、goto语句）以及改变内存数据。 如果某个东西展现出了图灵完备，它就有能力表现出可以模拟原始计算机，而即使最简单的计算机也能模拟出最复杂的计算机。所有的通用编程语言和现代计算机的指令集都是图灵完备的（C++ template就是图灵完备的），都能解决内存有限的问题。图灵完备的机器都被定义有无限内存，但是机器指令集却通常定义为只工作在特定的、有限数量的RAM上。</p>
<h3 id="目标检测">🚀 目标检测</h3>
<blockquote>
<p><a href="https://bbs.cvmart.net/topics/3056" target="_blank" rel="noopener" class="uri">https://bbs.cvmart.net/topics/3056</a></p>
</blockquote>
<h3 id="二分图匹配bipartite-matching">🚀 二分图匹配（bipartite matching ）</h3>
<blockquote>
<p><a href="https://liam.page/2016/04/03/Hungarian-algorithm-in-the-maximum-matching-problem-of-bigraph/" target="_blank" rel="noopener" class="uri">https://liam.page/2016/04/03/Hungarian-algorithm-in-the-maximum-matching-problem-of-bigraph/</a></p>
</blockquote>
<h3 id="解决vscode乱码问题vscode设置自动推导文件编码">解决vscode乱码问题，VSCode设置自动推导文件编码</h3>
<blockquote>
<p><a href="https://blog.csdn.net/COCO56/article/details/100058599" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/COCO56/article/details/100058599</a></p>
</blockquote>
<h3 id="linux-杀死暂停继续后台运行进程">LINUX 杀死、暂停、继续、后台运行进程</h3>
<p>ctrl + z</p>
<p>可以将一个正在前台执行的命令放到后台，并且暂停</p>
<p>若想恢复到前台，则</p>
<ol type="1">
<li>jobs #查看当前有多少在后台运行的命令 会有序号 job号</li>
<li>fg 〔<em>job</em>号〕 将后台中的命令调至前台继续运行 如： fg %1</li>
</ol>
<blockquote>
<p><a href="https://blog.csdn.net/QQ1910084514/article/details/80390671" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/QQ1910084514/article/details/80390671</a></p>
</blockquote>
<h3 id="benchmark-和-baseline">Benchmark 和 baseline</h3>
<blockquote>
<p>benchmark：N-COUNT A <strong>benchmark</strong> is something whose quality or quantity is known and which can therefore be used as a standard with which other things can be compared.</p>
</blockquote>
<p>通俗的讲，一个算法之所以被称为benchmark，是因为它的<strong>性能已经被广泛研究，人们对它性能的表现形式、测量方法都非常熟悉，因此可以作为标准方法来衡量其他方法的好坏</strong>。 这里需要区别state-of-the-art（SOTA），能够称为SOTA的算法表明其性能在当前属于最佳性能。如果一个新算法以SOTA作为benchmark，这当然是最好的了，但如果比不过SOTA，能比benchmark要好，且方法有一定创新，也是可以发表的。</p>
<blockquote>
<p>baseline：N-COUNT A <strong>baseline</strong> is a value or starting point on a scale with which other values can be compared.</p>
</blockquote>
<p>通俗的讲，一个算法被称为baseline，基本上表示<strong>比这个算法性能还差的基本上不能接受的</strong>，除非方法上有革命性的创新点，而且还有巨大的改进空间和超越benchmark的潜力，只是因为是发展初期而性能有限。所以baseline有一个自带的含义就是“<strong>性能起点</strong>”。这里还需要指出其另一个应用语境，就是<strong>在算法优化过程中，一般version1.0是作为baseline的，即这是你的算法能达到的一个基本性能，在算法继续优化和调参数的过程中，你的目标是比这个性能更好</strong>，因此需要在这个base line的基础上往上跳。</p>
<p>简而言之， benchmark一般是和同行中比较牛的算法比较，比牛算法还好，那你可以考虑发好一点的会议/期刊； baseline一般是自己算法优化和调参过程中自己和自己比较，目标是越来越好，当性能超过benchmark时，可以发表了，当性能甚至超过SOTA时，恭喜你，考虑投顶会顶刊啦。</p>
<h3 id="object-detection与object-recognition区别">object detection与object recognition区别</h3>
<h4 id="object-recognition目标识别">object recognition（目标识别）</h4>
<ul>
<li><p>给定一幅图像</p></li>
<li><p>检测到图像中<strong>所有的目标</strong>（类别受限于训练集中的物体类别）</p></li>
<li><p>得到检测<strong>到的目标的矩形框</strong>，并对所有检测到的矩形框进行分类</p>
<p><img src="https://i.loli.net/2020/10/21/8dEGftYxesRUikb.png" alt="这里写图片描述"></p>
<blockquote>
<p>Object Recognition: In a given image you have to detect all objects (a restricted class of objects depend on your dataset), Localized them with a bounding box and label that bounding box with a label.</p>
</blockquote></li>
</ul>
<h4 id="object-detection目标检测">object detection（目标检测）</h4>
<ul>
<li><p>与object recognition目标类似</p></li>
<li><p>但只有两个类别，<strong>只需要找到目标所在的矩形框和非目标矩形框</strong></p></li>
<li><p>例如，人脸检测（人脸为目标、背景为非目标）、汽车检测（汽车为目标、背景为非目标）</p>
<p><img src="https://i.loli.net/2020/10/21/KmzLuSgPieZs2bj.png" alt="这里写图片描述"></p>
<p><img src="https://i.loli.net/2020/10/21/zJcGvyaeAuREpHZ.png" alt="这里写图片描述"></p>
<blockquote>
<p>Object Detection: it’s like Object recognition but in this task you have only two class of object classification which means object bounding boxes and non-object bounding boxes. For example Car detection: you have to Detect all cars in a given image with their bounding boxes.</p>
</blockquote></li>
</ul>
<blockquote>
<p><a href="https://blog.csdn.net/tina_ttl/article/details/51915618" target="_blank" rel="noopener" class="uri">https://blog.csdn.net/tina_ttl/article/details/51915618</a></p>
</blockquote>
<h3 id="lass-agnostic和class-specific的区别">lass-agnostic和class-specific的区别</h3>
<p>For a class-aware(class-specific) detector, if you feed it an image, <strong>it will return a set of bounding boxes, each box associated with the class of the object inside (i.e. dog, cat, car).</strong> It means that by the time the detector finished detecting, it knows what type of object was detected.</p>
<p>For class-agnostic detector, <strong>it detects a bunch of objects without knowing what class they belong to</strong>. To put it simply, they only detect “foreground” objects. Foreground is a broad term, but usually it is a set that contains all specific classes we want to find in an image, i.e. foreground = {cat, dog, car, airplane, …}. Since it doesn’t know the class of the object it detected, we call it class-agnostic.</p>
<h3 id="对抗攻击adversarial-attack">对抗攻击（adversarial attack）</h3>
<h4 id="概念">概念</h4>
<p>对抗攻击英文为adversarial attack。即对输入样本故意添加一些人无法察觉的细微的干扰，导致模型以高置信度给出一个错误的输出。</p>
<p>首先通过一些图片来对这个领域有一个直观的理解：对抗攻击就是使得DNN误判的同时，使得图片的改变尽可能少。</p>
<p><img src="https://i.loli.net/2020/11/02/XSyR7obILvfi9z4.jpg" alt="img"></p>
<p>从图中可以看的出来，DNN在左图中正常地把狗识别成狗，而在左图上添加一些扰动（perturbation）之后形成右图，在肉眼看来，这两张图片并没有什么区别，按正常左右两图的识别结果应该是一样的，但是DNN却在右图中不正常的把狗识别成了人、火车等等。</p>
<p><img src="https://i.loli.net/2020/11/02/xJGn6CgaQdUkWPe.jpg" alt="img"></p>
<p>在原图上加一个人肉分辨不出来的Perturbation之后，可以使得识别出现错误</p>
<p>对抗攻击从image attack起源，逐渐完善理论，然后慢慢扩大到了video attack以及在NLP、强化学习等领域中的应用。</p>
<p>其中的思想大致可以按下面理解：深度神经网络对输入图片的特征有了一个比较好的提取，但是具体提取了什么特征以及为什么提取这个特征不知道。所以我们需要试图找到那个模型认为很重要的位置，然后尝试着去改变这个位置的像素值，使得DNN对输入做出一个误判。</p>
<h4 id="参考">参考</h4>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/104532285" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/104532285</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/49755857" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/49755857</a></p>
</blockquote>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
  </entry>
  <entry>
    <title>2020-09-04-transformer直观理解</title>
    <url>/2020/09/04/2020-09-04-transformer%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3/</url>
    <content><![CDATA[<h3 id="直观attention-模型">直观“Attention 模型”</h3>
<p>本文试着从直观的角度解析“Attention模型”，应用场景：原文--译文，具体选择 中文--英文，即在<strong>将中文翻译为英文</strong>这一场景中，直观解析“Attention模型”。</p>
<h4 id="概括">概括</h4>
<p>一句中文A翻译为一句英文B主要是完成以下“<strong>两项任务</strong>”：</p>
<ol type="1">
<li><p>理解中文A的意思为X；</p></li>
<li><p>将意思X用英文表达出来，即英文B；</p></li>
</ol>
<p>在用计算机完成以上任务前，需要以下三点“<strong>准备工作</strong>”：</p>
<ol type="i">
<li><p>需要将中文的字和英文的单词转换为计算机可以理解（计算）的数（即一个字/单词对应转换为一个向量，称为字向量/单词向量），然后计算机才有可能完成以上两项任务，实现翻译； <code>单词-&gt;单词向量</code></p></li>
<li><p>另外针对一句中文翻译为一句英文，每个字在句子中的位置也对意思的表达会产生很大的影响，所以每个字在句子中的位置也要定义一个向量来表达（即一个位置对应转换为一个向量，称为位置向量）； <code>位置向量</code></p></li>
<li><p>将字向量/单词向量加上位置向量（定义两种向量的维度相同，如都是512维，便于此处元素相加），能更好更全面的代表这句话，为更好的翻译做好准备；<code>单词向量+位置向量</code></p></li>
</ol>
<p>“Attention模型”实现以上内容，具体情况如下图所示：</p>
<p><img src="https://i.loli.net/2020/09/05/bEpK5wcj9rkAGCU.jpg" alt="img"></p>
<p>以下针对“准备工作”、任务1和任务2展开讨论；</p>
<h4 id="准备工作">准备工作</h4>
<p>包括字向量/单词向量、位置向量；此处也称为<strong>词嵌入，位置编码</strong>；</p>
<ol type="a">
<li>字向量/单词向量分别是<code>随机产生产生的一组512维向量</code>，如字向量，假设选用了3000个常用汉字，每个字对应一个512维的随机向量，则整个字向量就是一个3000 X 512 的二维矩阵，每一行代表一个字；</li>
</ol>
<p><strong>之所以用随机且选择较大维度（如512维），是为了让生成的各个向量间尽可能的独立，即没有相关性</strong>，就像“你、我、他、拿、和、中”指代的具体意思在最初定义时是可以随机互换的，之间也无关系，他们之间的相关性/关系是在该语系语境中根据语义、语法、文化等因素形成的，即上述任务1需要完成的。</p>
<p>（词嵌入，每个词之间没有关系）</p>
<p><img src="https://i.loli.net/2020/09/05/9tSTwmsdnfuW7Ol.jpg" alt="img"></p>
<ol start="2" type="a">
<li>位置向量是代表一个字在句子中的位置信息，也定义为一个512维的向量，但并<strong>不是随机产生</strong>的，而是根据位置确切计算得来，即<strong>一个位置对应转化为一个512维向量；</strong></li>
</ol>
<p><img src="https://i.loli.net/2020/09/05/FAR6vuDZgBoJwTX.jpg" alt="img"></p>
<ol start="3" type="a">
<li>假设翻译时定义一句话最大长度是10个字，则该句话对应的字向量是一个10 X 512的二维矩阵（每一行代表一个字），位置向量也是一个10 X 512的二维矩阵（每一行代表对应字的位置信息）；<strong>两个矩阵相加得新的二维矩阵能更好更全面的表达这句话；</strong></li>
</ol>
<h4 id="任务1编码">任务1：编码</h4>
<p><strong>理解</strong>一句中文A的意思为X；此处也称为“编码”</p>
<ol type="i">
<li><p>翻译时中文中的“你”、“我”大多时候对应着英文的“you”、“me”，如果都是这样的简单一一对应关系，那翻译是很简单的；而实际情况是<strong>绝大多数都是一对多的关系，即同一个中文字在不同的语境中对应的英文是不一样的单词</strong>，如“和”字在不同语境中翻译为英文可能是“and”、“sum”、“peace”等。</p></li>
<li><p>一个字从多个可能的意思中选择一个是<strong>根据语境</strong>来确定的，即<strong>根据这个字与句子中所有字的相关关系来确定</strong>；<strong>一句话需要计算该句话中每个字与该句子中所有字的相关关系来确定这句话中每个字在该语境中的意思，即确认中文语境</strong>；</p></li>
<li><p><span class="github-emoji" style="display:inline;vertical-align:middle;color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8">⭐</span> 在计算相关性之前，对每一个<strong>字对应的向量进行相应的线性变换</strong>以便于<strong>更好的计算相关性</strong>确认最终意思；计算完相关性（确认中文语境）并以此更新向量矩阵后（即self-attention，确认每个字在当前这句话的语境下的“确切意思”），再<strong>进行一次线性变换</strong>，对这个“确切意思”进行再次拟合校准；</p></li>
</ol>
<p>具体情况，如下图所示；</p>
<p><img src="https://i.loli.net/2020/09/05/ZT4OvczK3tyCdGX.jpg" alt="img"></p>
<p>Notes：i~iii是一个处理单元，<strong>输入“向量矩阵”和输出“新向量矩阵X”的维度是一样的</strong>；完成任务1是以上处理单元循环N次（<strong>强化上述效果</strong>），设定义N=3（论文中N=8）；即由3个处理单元依次链接完成任务1，如下图所示：</p>
<p><img src="https://i.loli.net/2020/09/05/ZS4YMITXOWyr8kf.jpg" alt="img"></p>
<h4 id="任务2解码">任务2：解码</h4>
<p>将意思X用英文表达出来，即英文B；此处也称为“解码”</p>
<p>和任务1类似，<strong>差异</strong>在于：</p>
<p>I. 任务1仅考虑中文语境即可，任务2<strong>既考虑中文语境（vanilla-attention），也考虑英文语境（self-attention）；</strong></p>
<ol start="2" type="i">
<li>和任务1类似，经过N个处理单元后获得的向量矩阵，经过“最后一次线性变换”转换为对应英文语系中各个单词的值，然后由softmax转换为是各个英文单词的概率，完成翻译；</li>
</ol>
<p>图示如下：</p>
<p><img src="https://i.loli.net/2020/09/05/yKDA2Fs3Xc4HkIo.jpg" alt="img"></p>
<p>整体简化图示如下：</p>
<p><img src="https://i.loli.net/2020/09/05/6epU1uboH2AGP7k.jpg" alt="img"></p>
<h3 id="attention注意力">Attention注意力</h3>
<p><img src="https://i.loli.net/2020/09/05/R2XINUW16u4anTs.jpg" alt="img"></p>
<p>上图是attention模型的总体结构，包含了模型所有节点及流程（因为有循环结构，流程不是特别清楚，下文会详细解释）；模型总体分为两个部分：编码部分和解码部分，分别是上图的左边和右边图示；以下选取翻译情景，以<strong>模型训练</strong>为例解释整个过程；</p>
<p><strong>训练样本：原文译文(一一对应)</strong></p>
<h4 id="编码部分inputs">编码部分（inputs）</h4>
<h5 id="input-embedding">Input embedding:</h5>
<p>1.1 将原文的所有单词汇总统计频率，删除低频词汇（比如出现次数小于20次的统一</p>
<p>定义为’<unk>’）；</unk></p>
<p>此时总共选出了假设10000个单词，则用数字编号为0~9999，一一对应，定义该对应表为word2num；</p>
<p>然后用<code>xaviers方法</code>生成随机矩阵Matrix ：<strong>10000行N列</strong>（10000行是确定的，对应10000个单词，N列自定义）；这样就可以将10000个不同的单词通过word2num映射成10000个不同的数字（int），然后将10000个不同的数字通过Matrix映射成10000个不同的N维向量（如何映射？比如数字0，3，经过 Matrix映射分别变为向量Matrix[0],Matrix[3]，维度为N维）；</p>
<p>这样，<strong>任何一个单词，都可以被映射成为唯一的一个N维向量</strong>；</p>
<p><img src="https://i.loli.net/2020/09/05/86ByVmtDIGfd2bx.png" alt="img"></p>
<p><strong>Note：此处N自定义为512</strong></p>
<p>1.2 翻译的时候是<strong>一个句子一个句子的翻译</strong>，所以需要定义一个句子的标准长度，比如10个单词；如果一句话不足10个单词则用0填充（<strong>对应的word即word2num表中的<pad></pad></strong>），如果多了，删掉；</p>
<p>这样一句话就是标准的10个单词；比如句子 “中国人有中国梦。”，这句话共有八个字（最后一个是结束符），<strong>经过word2num变为一列X：<a href="注：100代表的word是结束符">1,7,3,2,1,7,6,100,0,0</a>,X经过Matrix映射为10行N列的矩阵matX</strong>= [Matrix[1], Matrix[7], Matrix[3], Matrix[2] , Matrix[1] , Matrix[7] , Matrix[6], Matrix[100] , Matrix[0] , Matrix[0]]; embedding 到此基本结束，即完成了将一句话变为 一个矩阵，矩阵的每一行代表一个特定的单词；此处还可以scale一下，即<code>matX*N**(1/2)</code>; （<code>**代表次方，即matX中的每一个元素都乘以N的1/2次方，此时N=512，以此来缩放</code>）</p>
<p><img src="https://i.loli.net/2020/09/05/xOJBLyNruSIX9s2.png" alt="img"></p>
<h5 id="positional-encoding">Positional encoding:</h5>
<p>2.1 单词在句子中的不同位置体现了不同信息，所以需要对位置进行编码，体现不同的信息情况，此处是对绝对位置进行编码，即位置数字0，1，2，3，…N等，进行运算编码，具体编码如下：</p>
<p>2.1.1 对于句子中的每一个字，其位置pos∈<a href="每句话10个字">0,1,2,…,9</a>,每个字是N（512）维向量，维度 i （i∈[ 0,1,2,3,4,..N]）带入<strong>函数计算</strong>，</p>
<p><img src="https://i.loli.net/2020/09/05/dR4rAa9DkZJKNQC.png" alt="img"></p>
<blockquote>
<p>用sin和cos是因为在后面运算过程中会近似出现sin(a)sin(b)+cos(a)cos(b)的形式，根据三角函数公式上式恰好等于cos(a-b)，当a和b差小时（即两个字离得近）值大，反之小。这在一定程度上可以表达两个字的距离。</p>
</blockquote>
<p>2.1.2 经过如上函数运行一次后，获得了一个<strong>10行N列的矩阵matP</strong>；每一行代表一个绝对位置信息，此时matP的shape和matX的shape相同；</p>
<p><img src="https://i.loli.net/2020/09/05/xFKDaYu1pSNZ7lo.png" alt="img"></p>
<p>2.1.3 <strong>对于矩阵matP的每一行，第0，2，4，6,...等偶数列上的值用sin()函数激 活，第1，3，5，。。。等奇数列的值用cos()函数激活，以此更新matP</strong>；即 matP[:,0::2]=sin(matP[:,0::2]), matP[:,1::2]=cos(matP[:,1::2])；</p>
<p><img src="https://i.loli.net/2020/09/05/w1FJXfzq5IRrPCS.png" alt="img"></p>
<p>2.2 至此positional encoding结束，最后通常也会scale一次，即对更新后的matP进行<code>matP*N**(1/2)</code>运算，得到再次更新的matP，此时的matP的shape还是和matX相同；<strong>然后将matP和matX相加即matEnc=matP+matX，矩阵matEnc其shape=[10，512]；</strong></p>
<h5 id="multi-head-attention循环单元">Multi-head attention循环单元</h5>
<p>3.1 然后matEnc进入模型编码部分的循环，即Figure1中左边红色框内部分，每个循环单元又分为4个小部分：multi-head attention, add&amp;norm, feedForward, add&amp;norm；</p>
<p>3.2 Multi-head attention</p>
<p><img src="https://i.loli.net/2020/09/05/np9H73tSVYogJG4.jpg" alt="img"></p>
<p>3.2.1 Multi-head attention 由三个输入，分别为V，K，Q，此处<strong>V=K=Q=matEnc</strong>（在解码部分multi-head attention中的VKQ三者不是这种关系）;</p>
<p>3.2.2 首先分别对V，K，Q三者分别进行线性变换，即将三者分别输入到三个单层神经网络层，激活函数选择relu，输出新的V，K，Q（三者shape都和原来shape相同，即<strong>经过线性变换时输出维度和输入维度相同</strong>）；</p>
<p>3.2.3 然后将Q在最后一维上进行切分为num_heads(假设为8)段，然后对切分完的矩阵在axis=0维上进行concat链接起来(纵向连接)；对V和K都进行和Q一样的操作；操作后的矩阵记为Q_,K_,V_；</p>
<p><strong>可以变化其维度， 由[1,10,512]变为[8,10,64]</strong></p>
<p><img src="https://i.loli.net/2020/09/05/PqJYGOb4fvTmsFw.png" alt="img"></p>
<p><img src="https://i.loli.net/2020/09/05/2npJkGobFc4RXwu.png" alt="img"></p>
<p>3.2.4 <strong>Q_矩阵相乘 K_的转置（对最后2维）</strong>，生成结果记为outputs，然后对outputs 进行scale一次更新为outputs；<strong>此次矩阵相乘是计算词与词的相关性，切成多个num_heads进行计算是为了实现对词与词之间深层次相关性进行计算；</strong></p>
<p><img src="https://i.loli.net/2020/09/05/imn5tKIUxzyfwLW.png" alt="img"></p>
<p><code>shape（outputs） = （8,10,10）</code></p>
<p>3.2.5 对outputs进行softmax运算，更新outputs，即outputs=softmax(outputs);</p>
<p>3.2.6 最新的outputs（即K和Q的相关性） 矩阵相乘 V_， 其值更新为outputs；</p>
<p><img src="https://i.loli.net/2020/09/05/oZalMTkQeRVqsUy.png" alt="img"></p>
<p><code>shape（outputs）= (8,10,64)</code></p>
<p>3.2.7 最后将outputs在axis=0维上切分为num_heads段，然后在axis=2维上合并， <strong>恢复原来Q的维度</strong>；</p>
<p><img src="https://i.loli.net/2020/09/05/JrFbIdWauXxA4nK.png" alt="img"></p>
<p>3.3 Add&amp;norm</p>
<p>3.3.1 类似ResNet，将<strong>最初的输入与其对应的输出叠加一次</strong>，即outputs=outputs+Q， 使网络有效叠加，<strong>避免梯度消失</strong>；</p>
<p><img src="https://i.loli.net/2020/09/05/falSI79c3xD2Ey8.png" alt="img"></p>
<p>3.3.2 标准化矫正一次，在outputs对最后一维计算均值和方差，用outputs减去均值除以方差+spsilon得值更新为outputs，然后变量gamma*outputs+变量beta；（Norm操作）</p>
<p>3.4 feed Forward （就是dense layer 全连接层）</p>
<p>3.4.1 对outputs进行第一次卷积操作，结果更新为outputs（卷积核为1*1，每一次卷积操作的计算发生在一个词对应的向量元素上，卷积核数目即最后一维向量长度，也就是一个词对应的向量维数）；</p>
<p>3.4.2 对最新outputs进行第二次卷积操作，卷积核仍然为1*1，卷积核数目为N；</p>
<p><img src="https://i.loli.net/2020/09/05/esShU1Nx32AYt8Z.png" alt="img"></p>
<p>3.5 Add&amp;norm : 和3.3相同，经过以上操作后，此时最新的output和matEnc的shape相同；</p>
<p>3.6 <strong>令matEnc=outputs, 完成一次循环，然后返回到3.2开始第二次循环</strong>；共循环Nx（自定义；每一次循环其结构相同，但对应的参数是不同的，即是独立训练的）；完成Nx次后，模型的编码部分完成，仍然令matEnc=outputs，准备进入解码部分；</p>
<p>解码部分：</p>
<p>​ <strong>此时的outputs指的是上一时间点解码器的输出</strong></p>
<ol type="1">
<li><p>Outputs：<strong>shifted right右移一位</strong>？？？？，是为了解码区最初初始化时第一次输入，并将其统一定义为特定值（在word2num中提前定义）；</p></li>
<li><p>Outputs embedding: 同编码部分；更新outputs；</p></li>
<li><p>Positional embedding：同编码部分；更新outputs； （前三步是准备工作）</p></li>
<li><p>进入解码区循环体； （以下是解码器的顺序操作）</p></li>
</ol>
<p>4.1 Masked multi-head attention: 和编码部分的multi-head attention类似，但是多了一 次<strong>masked</strong>，因为在解码部分，解码的时候是从左到右依次解码的，当解出第一个字的时候，第一个字只能与第一个字计算相关性，当解出第二个字的时候，只能计算出第二个字与第一个字和第二个字的相关性，...；所以需要进行一次mask；</p>
<p><img src="https://i.loli.net/2020/09/05/54eVnEgmh7CdqKH.jpg" alt="img"></p>
<p>为什么是10*10呢？？？</p>
<p>4.2 Add&amp;norm：同编码部分，更新outputs；</p>
<p>4.3 Multi-head attention：同编码部分，但是Q和K，V不再相同，Q=outputs，K=V=matEnc；(outputs是上层的输出，k,v是来自编码器的输出)</p>
<p>4.4 Add&amp;norm:同编码部分，更新outputs；</p>
<p>4.5 Feed-Forward：同编码部分，更新outputs；</p>
<p>4.6 Add&amp;norm: 同编码部分，更新outputs；</p>
<p>4.7 最新outputs和最开始进入该循环时候的outputs的shape相同；回到4.1，开始第 二次循环。。。；直到完成Nx次循环（自定义；<strong>每一次循环layer结构相同，但对应的参数是不同的，即独立训练的</strong>）；</p>
<ol start="5" type="1">
<li><p>Linear: 将最新的outputs，输入到单层神经网络中，输出层维度为“译文”有效单词总数；更新outputs；</p></li>
<li><p>Softmax: 对outputs进行softmax运算，确定模型译文和原译文比较计算loss，进行网络优化（参数更新）；</p></li>
</ol>
<h4 id="注">注</h4>
<p>1.解码器的<code>outputs embedding</code> ：在训练的时候就是对应原文的译文，其中第一字统一定义为0,作为输入；在预测时第一次输入也是全是0,然后每循环一次，预测一个字直到出现终止符。</p>
<p>2.对于matEnc=matP+matX，这里为什么要用add,而不是contact ?</p>
<p>matX是一个10行512列的矩阵，每一行代表一个字；</p>
<p>matP是一个10行512列的矩阵，每一行代表一个位置；</p>
<p><strong>对于不一样的句子，matX是不一样的，matP是完全一样的；</strong></p>
<p>则对于不一样的句子，add后是不一样的，contact后至少一半是一样的，从直观上，add似乎更好；</p>
<p>对于一个字，其出现的位置不同，可能表达的意思完全不一样，比如“和”，如果其在句首或者句中出现更可能是“and”的意思，如果在句末出现，更可能是“sum”的意思，而这两个意思几乎完全不一样，即他们的向量完全不一样似乎更合理，而非contact的至少一半一样；</p>
<p>matP矩阵的特点从上到下对应各元素是递增的，matX是随机产生的（比如均值为0的随机数），即大约在0附近波动的数，与matP做add运算后，相当于均值被依次提高，以此代表融入每个字位置信息；因为每次训练的时候均值被提高的量是一定的，所以可以期望模型训练后能“意识”到这一点；</p>
<p>add产生“信息混淆”，比如两个字在两个不同的位置上分别add后，结果相近，从直观上这可能会造成问题；这个问题可以通过加大向量维度来降低其出现概率，比如选择512维是很长的维度了，出现这种概率的问题还是很小的；</p>
<p>如果用contact实际就是在每个字向量后面追加一个位置信息以示区别，做这种区别无需太多维，也许一两维即可；</p>
<p>深度学习算法的可解释性差，分析大多属于理论上的“纸上谈兵”，最可靠的方式，仍是分别以add和contact两种方式建模，大量测试后的结果更为可靠。</p>
<ol start="3" type="1">
<li><p>待探究</p>
<p>你文章中的逻辑是，对原始Q/K/V做不同线性变换（三个权重矩阵）得到新的Q/K/V→对新的Q/K/V在最后一个维度做切分得到多头（8组Q/K/V）→各组Q/K/V计算attetion值→8组Q/K/V的attetion值concat得到最终的attention值。</p>
<p>而原论文的逻辑是，对原始的Q/K/V做不同的线性变换（8（组）×3个权重矩阵）得到新的8组Q/K/V值→各组Q/K/V计算attention值→8组Q/K/V的attention值concat→concat结果经过一个线性变换（为了还原到最初的维度）得到最终的attention值。</p>
<p>论文提到multi-head attention是为了从不同表征子空间提取信息。个人理解实现这种差异化的提取，是通过多组权重矩阵来实现的，而不是通过embedding值不同分段获取。</p></li>
<li><p>在<strong>预测阶段</strong>，每次预测后底部decoder的输入是可变的，首先是[<bos>]，然后是[<bos>, word1 ]，再输入[<bos>, word1, word2 ]……，那么decoder内部如何保证它送入linear层的输出是(1, N)的向量呢？</bos></bos></bos></p>
<p><strong>答</strong>：[<bos>]时，经过解码区的循环部分后 是一个[1, 512]的矩阵， 经过linear层是准备预测1个字的；</bos></p>
<p>[<bos>, word1 ]时，经过解码区的循环部分 是一个[2, 512]的矩阵；经过linear层是准备预测2个字的；以此类推。</bos></p>
<p>也就是说，输入bos，输出word1；然后将bos word1输入，再输出word1 word2；再输入bos word1 word2......每次都把输出的最后一个字加到下一轮输入。</p></li>
<li><p>假设target是<bos>我爱中国<eos>，这算6个字，训练时decoder是不是也输出6个vocab-size长度的向量，那么第一个vocab-size长度的向量预测的是<bos>还是"我"呢？</bos></eos></bos></p>
<p><strong>答</strong>：预测的第一个是“我”,<bos>作为一个起始引导使用。</bos></p></li>
<li><p>第一个问题是训练和预测时解码端如何运行，我理解训练时使用mask一次性对所有时间步并行进行解码，预测时则需要先预测出上一步的词，再输入预测下一步，所以不能并行。如果我上面说的没错的话，第一个问题是为什么训练时mask没有掩盖自身，也就是对角线不mask，这样的话不就泄露了要预测那个词吗？第二个问题是预测时该如何进行，因为训练时，输入多少个时间步的词就会输出多少个时间步的预测值，但是在预测解码阶段，假设为t，要预测t + 1该如何操作？难道是先将t + 1随便加一个pad上去然后看预测值softmax吗？</p>
<p><strong>答</strong>：<strong>在训练时，用mask是一次性的解码</strong>，因为训练时所有label是已知的，用mask实现同时并行运算；<strong>预测时label是未知的，需要一个一个词预测</strong>，当预测第一个词时只能知道第一个词和第一个词的相关性，然后再运行模型一遍，预测出第一个词和第二词，依次循环直到出现终止符，这个过程不是并行的。</p>
<p>个人理解： 在训练时，把一整句话都作为解码器的输入，这样可以实现并行运算，因为每一个label都是已知的。而在预测时，需要一步一步来</p></li>
</ol>
<h3 id="参考">🚀参考</h3>
<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/62397974" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/62397974</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/44731789" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/44731789</a></p>
</blockquote>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>transformer</category>
      </categories>
      <tags>
        <tag>transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-08-31-论文分享（第1期）</title>
    <url>/2020/08/31/2020-08-31-%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB(%E7%AC%AC1%E6%9C%9F)/</url>
    <content><![CDATA[<h3 id="reformerthe-eficient-transformer">reformer：the eficient transformer</h3>
<h4 id="论文概况">论文概况</h4>
<ul>
<li>来源：ICLR 2020</li>
<li>arXiv: 1901.02860</li>
<li>作者：Nikita Kitaev ，Anselm Levskaya</li>
<li>论文地址：<a href="https://openreview.net/forum?id=rkgNKkHtvB" target="_blank" rel="noopener" class="uri">https://openreview.net/forum?id=rkgNKkHtvB</a></li>
<li>Code url：<a href="https://github.com/google/trax/tree/master/trax/models/reformer" target="_blank" rel="noopener" class="uri">https://github.com/google/trax/tree/master/trax/models/reformer</a></li>
<li>论文组会报告于<code>2020.08.30</code></li>
</ul>
<h4 id="背景">背景</h4>
<p>Transformer架构被广泛用于自然语言处理中，并在许多任务上产生了最新的结果</p>
<h5 id="问题">问题</h5>
<ol type="1">
<li>大型的 Transformer 可以在许多任务上实现 sota，但是面临着参数过多的问题，导致所占内存过大，造成资源紧张.</li>
</ol>
<p>在最大的配置中，参数数量已经超过了 5亿/层，层数多达 64。</p>
<ol start="2" type="1">
<li><p>具有 <em>N</em> 层的模型要消耗 <em>N</em> 倍于单层模型的内存，因为每一层中的激活都需要存储以进行反向传播。</p></li>
<li><p>由于点乘注意力本身的局限性，导致不能处理长序列数据，否则会导致效率不高</p></li>
</ol>
<p>也就是说transformer的上下文窗口有限制范围。最多也就几千个单词。</p>
<blockquote>
<p>Transformer 的强大来源于注意力机制 ，通过这一机制，Transformer 将上下文窗口内所有可能的单词对纳入考虑，以理解它们之间的联系。因此，如果文本包含 10 万个单词，Transformer 将需要评估 100 亿单词对（10 万 x 10 万），这显然不切实际。</p>
<p>另一个问题是如何保存每个模型层的输出 。对于使用大型上下文窗口的应用来说，存储多个模型层输出的内存需求会迅速变得过大。这意味着，实际使用大量层的 Transformer 模型只能用于生成几小段落的文本或一小段的音乐。</p>
</blockquote>
<h5 id="解决方案">解决方案</h5>
<ol type="1">
<li><p>使用可逆残差（reversible residual layers）代替标准残差（standard residuals），这使得存储在训练过程中仅激活一次，而不是 n 次（此处 n 指层数），更有效地使用可用内存</p></li>
<li><p>将点乘注意力（dot-product attention）替换为一个使用局部敏感哈希（locality-sensitive hashing）的点乘注意力，将复杂度从 O(L2 ) 变为 O(L log L)，此处 L 指序列的长度，来降低长序列的处理复杂度</p></li>
</ol>
<p><strong>Reformer与使用完全Transformer所获得的结果相匹配，但运行速度要快得多，尤其是在文本任务上，并且内存效率要高几个数量级。</strong></p>
<h4 id="注意力问题">注意力问题</h4>
<h5 id="原始注意力">原始注意力</h5>
<p>公式如下：<span class="math inline">\(Attention (Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V\)</span></p>
<p>self-attention操作的核心 ——<span class="math inline">\(QK^T\)</span> 表示key和query之间的相似度得分</p>
<p>计算带有所有k的q的点积，并用√dk进行缩放，然后应用softmax函数来获得v的权重。用来消除hidden size这个参数对注意力分布的影响。对于每个query，我们在所有keys上计算一个softmax，以确保矩阵的每一行和为1—— 确保新的隐藏状态的大小不依赖于序列长度。</p>
<p>最后，我们用我们的注意力矩阵乘以我们的values矩阵，为每个token生成一个新的隐藏表示。</p>
<hr>
<blockquote>
<h5 id="例子">例子</h5>
<p>Key:(batch, length,d_model)</p>
<p>Query:(batch, length,d_model)</p>
<p>-&gt; <span class="math inline">\(QK^T\)</span>　:(batch, length,length)</p>
<p>-&gt; 复杂度： O(<span class="math inline">\(L^2\)</span> )</p>
<p>-&gt;原始的transformer结构难以处理过长的序列长度</p>
</blockquote>
<p><img src="https://i.loli.net/2020/09/03/9mdzrXcGKZ5FPi3.png" alt="image-20200903163000933"></p>
<p><img src="https://i.loli.net/2020/09/03/6QvMn1qxGXzwKmC.png" alt="image-20200903161441529"></p>
<p>其实，在softmax中，对于每个查询 <em>q</em>，我们只需要注意最接近 <em>q</em> 的键 <em>k</em>。<strong>并不一定需要那些注意力权重很小的token。</strong></p>
<p>例如，如果序列长度是 64K，对于每个 <em>q</em>，我们可以只考虑 32 或 64 个最近的键的一个小子集。因为这些是和<em>q</em>最需要注意的</p>
<h5 id="局部敏感哈希lsh">局部敏感哈希(LSH)</h5>
<p><img src="https://i.loli.net/2020/09/03/zQI9n2ubZYDV7SL.png" alt="image-20200903163152560"></p>
<p>局部敏感哈希使用<code>球形投影点的随机旋转</code>，通过argmax在有符号轴投影上<code>建立桶（bucket）</code>。 在此高度简化的2D描绘中，对于三个不同的角度hash，两个点x和y不太可能共享相同的哈希桶（上方），除非它们的球面投影彼此靠近（下方）。</p>
<p>该图演示了一个用<code>4个桶进行3轮哈希的设置</code>。下面的图中的向量映射到了同一个bucket，因为它们的输入很接近，而上一张图中的向量映射到第一个和最后一个bucket。</p>
<p>LSH是一组将高维向量映射到一组离散值(桶/集群)的方法。是解决在高维空间中快速找到最近邻居（最相似）的问题。</p>
<p><code>基本思想</code>：选择 <em>hash</em> 函数，对于两个点 p 和 q，如果 q 接近 p，那么很有可能我们有 hash(q) == hash(p)</p>
<h5 id="lsh注意力">LSH注意力</h5>
<p><img src="https://i.loli.net/2020/09/03/KgnJ7iB2kImcshX.png" alt="image-20200903163641310"></p>
<p>完全不同的方法来处理序列长度问题，丢弃了<code>query投影</code>（Q=K）（实验结果发现，学习不同的keys和queries的投影并不是严格必要的），<code>并将注意力权重替换为key的函数（hash函数）</code>，以此降低复杂度</p>
<p>步骤如下：</p>
<p>1.使用LSH为每个token计算一个桶</p>
<p>2.根据相同的桶进行归类排序</p>
<p>3.分块并将标准的点乘注意力应用到桶中的token的块上，从而大大降低计算负载</p>
<h4 id="内存问题">内存问题</h4>
<p>单层能够执行长序列的单模型。但是，当使用梯度下降训练多层模型时，由于需要保存每一层的激活（函数），以用于执行逆推。一个传统的 Transformer 模型具有十几个或更多的层，通过缓存这些层的值，内存将会很快用完。</p>
<p>可逆层：在反向传播时，按需重新计算每个层的输入，而不是将其保存在内存中。其中来自网络最后一层的激活用于还原来自任何中间层的激活。</p>
<p>原始的残差网络：<span class="math inline">\(Y=F(x)\)</span></p>
<p>可逆层的残差网络： 注意我们如何从它的输出(Y ₁, Y ₂)计算物体的输入(X ₁, X ₂)。</p>
<p><span class="math inline">\(\begin{array}{ll}y_{1}=x_{1}+F\left(x_{2}\right) &amp; y_{2}=x_{2}+G\left(y_{1}\right) \\ x_{2}=y_{2}-G\left(y_{1}\right) &amp; x_{1}=y_{1}-F\left(x_{2}\right) \\ Y_{1}=X_{1}+\text { Attention }\left(X_{2}\right) &amp; Y_{2}=X_{2}+\text { FeedForward }\left(Y_{1}\right)\end{array}\)</span></p>
<p>示意图如下</p>
<p><img src="https://i.loli.net/2020/09/03/WX3lcQrmB16pI2Y.png" alt="image-20200903164816966"></p>
<p><img src="https://i.loli.net/2020/09/03/FgHKa4QNrsjlzMh.png" alt="image-20200903164907447"></p>
<h4 id="实验">实验</h4>
<p>作者分别对图像生成任务 <em>imagenet64</em>(长度为 12K)和文本任务 <em>enwik8</em>(长度为 64K)进行了实验，评价了可逆 Transformer 和 LSH 哈希对内存、精度和速度的影响。</p>
<p>🎉可逆 Transformer 匹配基准：他们的实验结果表明，可逆的 Transformer 可以节省内存不牺牲精度：</p>
<p><img src="https://i.loli.net/2020/09/03/LwSPM5qFuBfQhaZ.png" alt="null"></p>
<p>在 enwik8 和 imagenet64 训练中，可逆性对性能的影响</p>
<p>🎉LSH 注意力匹配基准：注意 LSH 注意力是一个近似的全注意力，其准确性随着散列值的增加而提高。当哈希值为 8 时，LSH 的注意力几乎等于完全注意力：</p>
<p><img src="https://i.loli.net/2020/09/03/t4zNAXOf6iEZ7cH.jpg" alt="null"></p>
<p>LSH 注意力作为散列循环对 imagenet64 的影响</p>
<p>🎉他们也证明了传统注意力的速度随着序列长度的增加而变慢，而 LSH 注意力速度保持稳定，它运行在序列长度~ 100k 在 8GB 的 GPU 上的正常速度：</p>
<p><img src="https://i.loli.net/2020/09/03/aGRcA42EiNSBz8v.jpg" alt="null"></p>
<p>注意力评估的速度作为全注意力和 LSH 注意力的输入长度的函数</p>
<blockquote>
<p>与 Transformer 模型相比，最终的 Reformer 模型具有更高的存储效率和更快的存储速度。</p>
</blockquote>
<h4 id="参考">🚀参考</h4>
<blockquote>
<p><a href="https://www.6aiq.com/article/1583729200869" target="_blank" rel="noopener" class="uri">https://www.6aiq.com/article/1583729200869</a></p>
<p><a href="https://thinkwee.top/2020/02/07/reformer/" target="_blank" rel="noopener" class="uri">https://thinkwee.top/2020/02/07/reformer/</a></p>
<p><a href="https://aijishu.com/a/1060000000100293" target="_blank" rel="noopener" class="uri">https://aijishu.com/a/1060000000100293</a></p>
</blockquote>
<h3 id="transformer-xl-attentive-language-models-beyond-a-fixed-length-context">Transformer-XL : Attentive Language Models Beyond a Fixed-Length Context</h3>
<h4 id="论文概况-1">论文概况</h4>
<ul>
<li>来源：ACL 2019</li>
<li>arXiv: 1901.02860</li>
<li>作者：ZihangDai , ZhilinYang , YimingYang</li>
<li>论文地址： <a href="https://arxiv.org/abs/1901.02860" target="_blank" rel="noopener" class="uri">https://arxiv.org/abs/1901.02860</a></li>
<li>Code url：<a href="https://github.com/kimiyoung/transformer-xl" target="_blank" rel="noopener" class="uri">https://github.com/kimiyoung/transformer-xl</a></li>
<li>论文组会报告于<code>2020.08.15</code></li>
</ul>
<h4 id="背景-1">背景</h4>
<h5 id="问题-1">问题</h5>
<p>Transformer存在局限性：</p>
<p>1.在语言建模时的设置受到固定长度（segment）的限制。对长距离依赖的建模能力仍然不足</p>
<p>2.因为transformer将文本等分为相同的片段，导致了上下文碎片</p>
<h5 id="解决方案-1">解决方案</h5>
<p>使学习不再仅仅依赖于定长，且不破坏时间的相关性。</p>
<ol type="1">
<li>提出<strong>片段级递归机制(segment-level recurrence mechanism)</strong>，引入一个<strong>记忆(memory)</strong>模块（类似于cache或cell） 之前计算过了不需要重复计算，直接为后面片段使用。</li>
</ol>
<ul>
<li>使得<code>长距离依赖的建模</code>成为可能；</li>
<li>使得片段之间产生交互，解决上下文碎片化问题</li>
</ul>
<p>2.提出<strong>相对位置编码机制</strong>，代替绝对位置编码。 Transformer的绝对位置编码指的是一个片段中，为1 为2 。如果是多个片段同时考虑的话，那么这种1，2就会重复，所以使用了相对位置编码的方法。这样可以在多个片段（segment）中使用相对编码。具体内容见论文</p>
<p>注：两者是一起使用的，共同解决transformer存在的局限性</p>
<h4 id="模型-transformer-xl">模型 transformer-XL</h4>
<h5 id="原始transformer">原始transformer</h5>
<p><img src="https://i.loli.net/2020/09/04/YU3mC2hIOA9ncrj.gif" alt="v2-732805e00feb35e41f1d00f8df516950_b"></p>
<h5 id="片段注意力机制">片段注意力机制</h5>
<p>为了解决长距离依赖，文章引入一个memory状态。</p>
<p>在训练过程中，每个片段的表示为最后的隐层状态，表示片段的序号，表示片段的长度，表示隐层维度。</p>
<p>在计算片段的表示时，用memory缓存片段层的隐层状态，用来更新，这样就给下一个片段同了上文，长距离依赖也通过memory保存了下来。并且，最大可能的依赖长度线性增长，达到**N*L**</p>
<p><img src="https://i.loli.net/2020/09/04/IjUWo7DahsNPAkv.gif" alt="v2-a8210cd2f9bfb9307ba81d694dc4e4b4_b"></p>
<h5 id="评估阶段">评估阶段</h5>
<h6 id="原始transformer-1">原始transformer</h6>
<p><img src="https://i.loli.net/2020/09/04/epOcXjYTy835dJv.gif" alt="v2-13a38126e684b838e5ed207fd5cae944_b"></p>
<h6 id="transformer-xl">Transformer-XL</h6>
<p><img src="https://i.loli.net/2020/09/04/qdYL5RsQEOFS8nG.gif" alt="v2-502e1e1fec12b326ace579e059b3b3df_b"></p>
<h4 id="实验-1">实验</h4>
<p>实验部分是对基于Transformer-XL的语言模型进行评估，分为字符级和词级。评价指标分别是bpc(每字符位数)和PPL(困惑度)，越小越好。enwiki8和text8用的是bpc。Transformer-XL在多个语言模型基准测试中实现了最先进的结果。 Transformer-XL第一个在char级语言模型基准enwiki8上突破1.0。</p>
<p><strong>去除实验：</strong></p>
<p><img src="https://i.loli.net/2020/09/04/ZV1lpewdtanoWi9.png" alt="image-20200904153950861"></p>
<p>重点是本文设计的相对位置编码<strong>优于</strong>其他工作，memory的设计也有很大的提升。</p>
<p>最后，Transformer-XL在评估阶段的速度也明显快于 vanilla Transformer，特别是对于较长的上下文。例如，对于 800 个字符的上下文长度，Transformer-XL 比Vanilla Transformer 快 363 倍；而对于 3800 字符的上下文，Transformer-XL 快了 1874 倍。</p>
<h4 id="创新点">创新点</h4>
<ul>
<li>提出了片段级递归机制和相对位置编码机制</li>
<li>依赖关系比原始Transformer长450％，并且在评估过程中，其速度比原始Transformer快1800倍以上</li>
</ul>
<h4 id="参考-1">🚀参考</h4>
<blockquote>
<p><a href="https://www.cnblogs.com/shona/p/12041055.html" target="_blank" rel="noopener" class="uri">https://www.cnblogs.com/shona/p/12041055.html</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/83062195" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/83062195</a></p>
<p><a href="https://www.cnblogs.com/mj-selina/p/12373636.html" target="_blank" rel="noopener" class="uri">https://www.cnblogs.com/mj-selina/p/12373636.html</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/70745925" target="_blank" rel="noopener" class="uri">https://zhuanlan.zhihu.com/p/70745925</a></p>
</blockquote>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>论文分享</category>
      </categories>
      <tags>
        <tag>论文</tag>
        <tag>论文分享</tag>
        <tag>transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-08-31-git进阶</title>
    <url>/2020/08/31/2020-08-31-git%E8%BF%9B%E9%98%B6/</url>
    <content><![CDATA[<h3 id="前言">前言</h3>
<p>之前总结过git的一些基本命令，后来使用了更多git，写博客用于记录。不断更新 ，在实践中总结git知识点。</p>
<p>回顾下之前的git基本操作</p>
<ul>
<li>将现有的项目添加提交并上传到远程仓库</li>
</ul>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">git add . #添加当前文件夹下的所有文件</span><br><span class="line"></span><br><span class="line">git commit -m "first commit " # 引号内是本次的提交说明 </span><br><span class="line"></span><br><span class="line">git push -u origin master # 提交本地分支到远程分支</span><br><span class="line">		(若出现failed to push som refs to， 则执行git pull origin master，</span><br><span class="line">		将远程服务器github上的master拉下来，再重新push)</span><br></pre></td></tr></tbody></table></figure>
<ul>
<li>clone代码</li>
</ul>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">git clone   https://github.com/raymond-zhao/cat-mall.git   ../Github/cat-mall </span><br><span class="line">#将cat-mall代码克隆到  ../Github/cat-mall 中</span><br></pre></td></tr></tbody></table></figure>
<h3 id="git-status-和-git-diff">git status 和 git diff</h3>
<p>在对文件进行修改之后，可以用 <code>git status</code> 查看结果，可以让我们时刻掌握仓库当前的状态</p>
<p><img src="https://i.loli.net/2020/08/31/fTmxaAeZG1iSCLd.png" alt="image-20200831180231338" style="zoom:67%;"></p>
<p>可以看到在<code>modified</code>部分，可以看到有四个文件被修改了，<strong>但是还没有进行提交（<code>commit</code>）修改</strong></p>
<p>而下半部分的<code>untracked files</code>表示的是<strong>之前从未提交到仓库分支</strong>的文件（一个markd文件，一个照片）</p>
<p>上述只是看到被修改的文件，但如果能看看具体修改了什么内容就好了，<code>git diff</code> 可以实现这个功能</p>
<p><img src="https://i.loli.net/2020/08/31/nVd3hGKLJy6f7zH.png" alt="image-20200831194816455"></p>
<p>可以看到修改的详细细节（红色为修改前的内容，绿色为修改后的内容）。向下箭头可以下拉文本，<code>q</code>退出查看 （quit）</p>
<p>这样就可以放心的添加（add）到仓库的暂存区，并提交（commit）到仓库分支</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">git add .</span><br><span class="line">git commit -m 20/8/31/commit1</span><br></pre></td></tr></tbody></table></figure>
<h4 id="小结">小结</h4>
<ul>
<li>要随时掌握工作区的状态，使用<code>git status</code>命令。</li>
<li>如果<code>git status</code>告诉你有文件被修改过，用<code>git diff</code>可以查看修改内容。</li>
</ul>
<h3 id="版本回退">版本回退</h3>
<p>每当文件修改到一定程度的时候，就可以“保存一个快照”，这个快照在Git中被称为<code>commit</code>。一旦你把文件改乱了，或者误删了文件，还可以从最近的一个<code>commit</code>恢复，然后继续工作，而不是把几个月的工作成果全部丢失。</p>
<p>在Git中，我们用<code>git log</code>命令查看：</p>
<p><img src="https://i.loli.net/2020/08/31/4lC7ufP6ZmbUvWT.png" alt="image-20200831233324006" style="zoom:80%;"></p>
<p><code>git log</code>命令显示从最近到最远的提交日志，每一次<code>commit</code>很详细</p>
<p>可以加上<code>--pretty=oneline</code>参数，来简化显示。推荐使用</p>
<p><img src="https://i.loli.net/2020/08/31/xNXAn7Pt8rT2mcE.png" alt="image-20200831233340202" style="zoom:80%;"></p>
<p>其中前面编号类似<code>012214236e...</code>的是<code>commit id</code>（版本号），是一个<code>SHA1</code>计算出来的一个非常大的数字，用十六进制表示</p>
<p>每个人的编号不一样，因为Git是分布式的版本控制系统，多人在同一个版本库里工作，如果大家都用1，2，3……作为版本号，那肯定就冲突了。</p>
<blockquote>
<p><a href="https://1024tools.com/hash" target="_blank" rel="noopener">Hash在线计算、md5计算、sha1计算、sha256计算、sha512计算</a></p>
</blockquote>
<h4 id="回退到历史版本">回退到历史版本</h4>
<p>这样我们就可以进行回退操作</p>
<p>首先，Git必须知道当前版本是哪个版本。</p>
<p>在Git中，用<code>HEAD</code>表示当前版本，也就是最新的提交<code>012214236e...</code>，上一个版本就是<code>HEAD^</code>，上上一个版本就是<code>HEAD^^</code>，当然往上100个版本写100个<code>^</code>比较容易数不过来，所以写成<code>HEAD~100</code>。</p>
<p>我们可以使用<code>git reset</code>命令：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">git reset --hard HEAD^ <span class="comment">#回退到上一版本</span></span><br></pre></td></tr></tbody></table></figure>
<p><img src="https://i.loli.net/2020/08/31/k37ptdoMJxGcufR.png" alt="image-20200831233416378"></p>
<p>结果显示出现在是<code>ca41b0a</code>，也就是上一次<code>commit</code>的版本。我们成功回退版本！</p>
<p>当我们再查看日志的时候，发现已经没有<code>20/8/31/commit1</code>版本了</p>
<p><img src="https://i.loli.net/2020/08/31/xWIlbtTwazUJNdO.png" alt="image-20200831233446124"></p>
<hr>
<h4 id="还原到最新版本">还原到最新版本</h4>
<p>如果想要再还原到<code>20/8/31/commit1</code>版本呢？</p>
<p>也是可以的，只要<strong><code>上面的命令行窗口还没有被关掉</code></strong>，就可以顺着往上找，找到那个<code>20/8/31/commit1</code>版本的<code>commit id</code>是<code>012214236e...</code>，于是就可以指定回到未来的某个版本：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">git reset --hard 0221423</span><br></pre></td></tr></tbody></table></figure>
<p><img src="https://i.loli.net/2020/08/31/9pK7UsnRrv1loSD.png" alt="image-20200831233507305"></p>
<p>版本号没必要写全，前几位就可以了，Git会自动去找。当然也不能只写前一两位，因为Git可能会找到多个版本号，就无法确定是哪一个了。</p>
<p>这样就实现了还原到最后<code>commit</code>版本</p>
<p>Git的版本回退速度非常快，因为Git在内部有个指向当前版本的<code>HEAD</code>指针，当你回退版本的时候，Git仅仅是把HEAD从指向历史版本，再将工作区的文件更新即可</p>
<p>如果回退到了某个版本，关掉了命令行窗口，后悔想恢复到新版本但是找不到新版本的<code>commit id</code>怎么办？</p>
<p>在Git中，总是有后悔药可以吃的。Git提供了一个命令<code>git reflog</code>用来记录你的每一次命令：</p>
<p><img src="https://i.loli.net/2020/09/01/Ly4MDnv6WAEwQlV.png" alt="image-20200901000008019"></p>
<p>知道<code>commit_id</code>，还原版本就十分滴完美！</p>
<blockquote>
<p><strong>注！！！</strong></p>
<p>如果从历史版本回到最后的版本，也只能还原到最后<code>commit</code>后的版本。</p>
<p>我才开始<code>commit</code>了版本A，之后又写了一部分内容 B(未<code>commit</code>)。还原到了A-1版本，之后又想还原到A+B版本，操作完之后发现还原后的没有B部分，也就是我只能还原到A。</p>
<p>原因就是我在最后一次<code>commit</code>就是A，而写完B之后，没有<code>commit</code> ，于是无法还原。 （多多<code>commit</code>，</p>
<p>，还原需谨慎。我是真的折腾）<span class="github-emoji" style="display:inline;vertical-align:middle;color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f62d.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f62d.png?v8">😭</span></p>
</blockquote>
<h4 id="小结-1">小结</h4>
<ul>
<li><code>HEAD</code>指向的版本就是当前版本，因此，Git允许我们在版本的历史之间穿梭，使用命令<code>git reset --hard commit_id</code>。 （commit_id也写成HEAD^）</li>
<li>穿梭前，用<code>git log</code>可以查看提交历史，以便确定要回退到哪个版本。</li>
<li>要重返未来，用<code>git reflog</code>查看命令历史，以便确定要回到未来的哪个版本。</li>
</ul>
<h4 id="参考">参考</h4>
<blockquote>
<p><a href="http://www.ruanyifeng.com/blog/2015/12/git-cheat-sheet.html" target="_blank" rel="noopener">常用git命令清单-阮一峰</a></p>
<p><a href="http://www.ruanyifeng.com/blog/2012/08/how_to_read_diff.html" target="_blank" rel="noopener">读懂diff-阮一峰</a></p>
<p><a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000" target="_blank" rel="noopener">git教程-廖雪峰</a></p>
<p><a href="http://www.runoob.com/git/git-install-setup.html" target="_blank" rel="noopener">git教程-菜鸟教程</a></p>
<p><a href="https://git-scm.com/book/zh/v2" target="_blank" rel="noopener">gitbook</a></p>
<p><a href="http://gitbook.liuhui998.com/index." target="_blank" rel="noopener">Git Community Book</a></p>
<p><a href="https://juejin.im/post/6844903586023866375" target="_blank" rel="noopener">从只会git add .的菜鸟到掌握git基本功能</a></p>
</blockquote>
<h3 id="工作区和暂存区">工作区和暂存区</h3>
<h4 id="工作区working-directory">工作区（Working Directory）</h4>
<p>就是在电脑里能看到的目录，比如我的<code>mynlog</code>文件夹就是一个工作区：</p>
<p><img src="https://i.loli.net/2020/09/01/lp9hvTzLtVuMPG5.png" alt="image-20200901000937480" style="zoom:80%;"></p>
<h4 id="版本库repository">版本库（Repository）</h4>
<p>也就是本地仓库</p>
<p>工作区有一个隐藏目录<code>.git</code>，这个不算工作区，而是Git的版本库。（选择<code>隐藏文件可见</code>就可以看到）</p>
<p>Git的版本库里存了很多东西，其中最重要的就是称为stage（或者叫<code>index</code>）的暂存区，还有Git为我们自动创建的第一个分支<code>master</code>，以及指向<code>master</code>的一个指针叫<code>HEAD</code>。</p>
<p><img src="https://i.loli.net/2020/09/01/wBe5iWuajDJKxdV.png" alt="image-20200901001300425" style="zoom:80%;"></p>
<p><img src="https://i.loli.net/2020/09/01/KywEFn2dtMJeBkq.png" alt="image-20200901001406385" style="zoom:80%;"></p>
<p>前面讲了我们把文件往Git版本库里添加的时候，是分两步执行的：</p>
<p>第一步是用<code>git add</code>把文件添加进去，实际上就是把文件修改添加到暂存区(<code>index</code>)；</p>
<p>第二步是用<code>git commit</code>提交更改，实际上就是把暂存区的所有内容提交到当前分支(<code>master</code>)。</p>
<p>因为我们创建Git版本库时，Git自动为我们创建了唯一一个<code>master</code>分支，所以，现在，<code>git commit</code>就是往<code>master</code>分支上提交更改。</p>
<p>你可以简单理解为，<strong>需要提交的文件修改通通放到暂存区，然后，一次性提交暂存区的所有修改。</strong> 也就是可以多次<code>git add .</code> ,之后再一次性<code>git commit</code></p>
<p>我对文件进行修改之后，<code>git status</code> 显示如下：</p>
<p><img src="https://i.loli.net/2020/09/01/qZ4JcAzCrBOQng5.png" alt="image-20200901003431025"></p>
<p>这是对文件进行了修改，但是未添加（add）到暂存区和提交（commit）到仓库分支。 并且出现了之前从未提交的文件（四张png图片）</p>
<p>然后<code>git add .</code>,再查看目前的状态 <code>git status</code></p>
<p><img src="https://i.loli.net/2020/09/01/leMQO9F83N5DxUq.png" alt="image-20200901003844459"></p>
<p>出现了绿色的<code>new file</code>字样和<code>modified</code>，代表已添加到缓存区。</p>
<p>现在，暂存区的状态就变成这样了（原文是添加的readme和LICENSE文件）：</p>
<p><img src="https://i.loli.net/2020/09/01/MHQiJkB674jcAE5.png" alt="image-20200901003951252"></p>
<p>所以，<code>git add</code>命令实际上就是把要提交的所有修改放到暂存区（index），然后，执行<code>git commit</code>就可以一次性把暂存区的所有修改提交到分支。</p>
<p><img src="https://i.loli.net/2020/09/01/1uzidT9knarwNMU.png" alt="image-20200901004202394"></p>
<p>这时候再 <code>git status</code>，则是干净的</p>
<p>现在版本库变成了这样，暂存区就没有任何内容了：</p>
<p><img src="https://i.loli.net/2020/09/01/kCXlv3FiurZNbIO.jpg" alt="git-stage-after-commit"></p>
<h4 id="小结-2">小结</h4>
<p>了解工作区和暂存区的概念，并通过例子加强<code>git status</code> 、<code>git add</code>、<code>git commit</code>的理解</p>
<p>如果不用<code>git add</code>到暂存区，那就不会加入到<code>commit</code>中。也就是说<code>commit</code>只会提交暂存区里的内容</p>
<h3 id="撤销修改">撤销修改</h3>
<h4 id="在工作区撤销修改">在工作区撤销修改</h4>
<p>在工作区写的内容想要撤销，当然可以手动删除。同时还有另外的一种方法</p>
<p><code>git status</code> 查看一下状态</p>
<p><img src="https://i.loli.net/2020/09/01/qT7BN95PkQeSumd.jpg" alt="img"></p>
<p>根据git提示，可以知道如下信息：</p>
<ol type="1">
<li><code>changes not staged for commit</code>：表示没有更改添加到暂存区，也就是对于当前的修改还没有进行<code>add</code>操作</li>
</ol>
<p><img src="https://i.loli.net/2020/09/01/NfZ2vXuB4etLHxF.jpg" alt="img"></p>
<ol start="2" type="1">
<li><p>可以看到修改的部分是<code>2020-08-31-git 进阶.md</code>文件，不能显示中文，所以用编码表示</p></li>
<li><p>同时<code>next</code>文件也做了修改。这个每次都有提示，猜想应该是因为next是我<code>clone</code>下来的文件，所以存在<code>.git</code>文件，将<code>.git</code>文件删除就ok了</p></li>
<li><p>提示显示，<code>git checkout -- file</code>可以丢弃工作区（work directory）的修改</p></li>
</ol>
<h5 id="git-checkout----file">git checkout -- file</h5>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">git checkout -- <span class="built_in">source</span>/_posts/2020-08-31-git进阶.md （注意--不要遗漏，同时后面有一个空格）</span><br><span class="line"><span class="comment"># git checkout -- .  这种写法也是可以的，表示全部撤销</span></span><br></pre></td></tr></tbody></table></figure>
<p>命令<code>git checkout -- filename</code>意思就是，把<code>filename</code>文件在工作区的修改全部撤销，这里有两种情况：</p>
<ul>
<li>一种是文件自修改后还没有被放到暂存区，现在，撤销修改就回到和版本库一模一样的状态；</li>
<li>一种是文件已经添加到暂存区后，又作了修改，现在，撤销修改就回到添加到暂存区后的状态。</li>
</ul>
<p>总之，就是让这个文件回到最近一次<code>git commit</code>或<code>git add</code>时的状态。</p>
<p><code>git checkout</code>其实是<strong>用版本库里的版本替换工作区的版本</strong>，无论工作区是修改还是删除，都可以“一键还原”。</p>
<h5 id="注">注</h5>
<ul>
<li>文件必须写当前git bash 下的完整路径，可以参考<code>git status</code>下的modified部分路径名称，如上的<code>source/_posts/</code></li>
<li>文件名必须写中文（就是正常的文件名），不能按照modified部分的编码后的名称</li>
</ul>
<p><img src="https://i.loli.net/2020/09/01/2l6Nxsh14bdpAY8.jpg" alt="img"></p>
<p>这是错误过程，可以看到最后一次没有提示，表示成功撤销修改</p>
<p>打开git进阶文件可以看到内容已经撤销</p>
<h4 id="添加到暂存区后的撤销">添加到暂存区后的撤销</h4>
<p>如果在工作区已经修改，并且添加到暂存区了，在<code>commit</code>之前，发现了这个问题。用<code>git status</code>查看一下，修改只是添加到了暂存区，还没有提交：</p>
<p><img src="https://i.loli.net/2020/09/01/CIASBwaTMEY3Gh1.png" alt="添加到暂存区前"></p>
<p><img src="https://i.loli.net/2020/09/01/T8jefu37XvFnZMJ.png" alt="添加到暂存区后"></p>
<ul>
<li>在添加到暂存区后，可以看到在<code>changes to be committed</code> 部分，添加的部分已经变成绿色，等待被<code>commit</code>提交</li>
<li>根据git提示，用命令<code>git reset HEAD &lt;file&gt;</code>可以把暂存区的修改撤销掉（<code>unstage</code>），重新放回工作区：</li>
</ul>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">git reset HEAD .</span><br></pre></td></tr></tbody></table></figure>
<blockquote>
<p><code>git reset</code>命令既可以回退版本，也可以把暂存区的修改回退到工作区。当我们用<code>HEAD</code>时，表示最新的版本。</p>
</blockquote>
<p>撤销到工作区的内容可以根据上述内容撤销其修改</p>
<h4 id="提交到版本库后的撤销">提交到版本库后的撤销</h4>
<p>前提是<strong>还没有把自己的本地版本库推送到远程</strong>。</p>
<p>可以利用上述的<code>版本回退</code>功能</p>
<h4 id="小结-3">小结</h4>
<ul>
<li>场景1：当你改乱了<code>工作区</code>某个文件的内容，想直接丢弃工作区的修改时，用命令<code>git checkout -- file</code>。</li>
<li>场景2：当你不但改乱了工作区某个文件的内容，还<code>添加到了暂存区</code>时，想丢弃修改，分两步，第一步用命令<code>git reset HEAD &lt;file&gt;</code>，就回到了场景1，第二步按场景1操作，用命令<code>git checkout -- file</code>。</li>
<li>场景3：已经提交了不合适的修改到版本库时，想要撤销本次提交，参考<code>版本回退</code>，不过前提是没有推送到远程库。</li>
</ul>
<h3 id="远程仓库">远程仓库</h3>
<h4 id="section"></h4>
<p>已经在本地创建了一个Git仓库后，又想在GitHub创建一个Git仓库，并且让这两个仓库进行远程同步，这样，GitHub上的仓库既可以作为备份，又可以让其他人通过该仓库来协作。</p>
<p>首先，登陆GitHub，然后，在右上角找到“Create a new repo”按钮，创建一个新的仓库</p>
<p>在Repository name填入<code>shijian</code>，其他保持默认设置，点击“Create repository”按钮，就成功地创建了一个新的Git仓库。出现以下界面：</p>
<p><img src="https://i.loli.net/2020/09/01/vMnFmb9aQU6xuXp.png" alt="image-20200901142716798"></p>
<p>复制仓库的SSH链接</p>
<p>根据提示，可以返回到需要上传的文件夹目录下，右键选择<code>git bash</code></p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">git init  <span class="comment">#创建.git隐藏文件，用于本地仓库</span></span><br><span class="line"></span><br><span class="line">git remote add origin git@github.com:OopsAaron/shijian.git <span class="comment">#关联本地仓库和github远程仓库</span></span><br></pre></td></tr></tbody></table></figure>
<p>添加后，<strong>远程库的名字就是<code>origin</code></strong>，这是Git默认的叫法，也可以改成别的，但是<code>origin</code>这个名字一看就知道是远程库。</p>
<p>接下来就是git的基本三样操作，添加提交并推送</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">git add . <span class="comment">#添加当前文件夹下的所有文件</span></span><br><span class="line"></span><br><span class="line">git commit -m <span class="string">"first commit "</span> <span class="comment"># 引号内是本次的提交说明 </span></span><br><span class="line"></span><br><span class="line">git push -u origin master <span class="comment"># 提交本地分支到远程分支</span></span><br><span class="line">		(若出现failed to push som refs to， 则执行git pull origin master，</span><br><span class="line">		将远程服务器github上的master拉下来，再重新push)</span><br></pre></td></tr></tbody></table></figure>
<p>把本地库的内容推送到远程，用<code>git push</code>命令，实际上是把当前分支<code>master</code>推送到远程。这时候在github界面就可以看到推送的文件</p>
<blockquote>
<p>第一次push的时候可以添加参数 <code>-u</code> ，之后可以不添加</p>
<p>由于远程库是空的，我们第一次推送<code>master</code>分支时，加上了<code>-u</code>参数，Git不但会把本地的<code>master</code>分支内容推送的远程新的<code>master</code>分支，还会把本地的<code>master</code>分支和远程的<code>master</code>分支关联起来，在以后的推送或者拉取时就可以简化命令。</p>
</blockquote>
<h4 id="section-1"></h4>
<h3 id="分支">分支</h3>
<p>分支暂时用不到，就没有学习</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-08-30-阅读论文</title>
    <url>/2020/08/30/2020-08-30-%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87/</url>
    <content><![CDATA[<p>一篇论文至少要看三遍。 第一遍，仔细阅读论文中的标题、摘要和关键词。 第二遍，阅读文中的导言、结论以及图表，快速扫描一下论文剩下的内容。 这一步主要是要把握论文中的关键信息，不光是导言和结论，还包括文章中任何小结论的总结，文中涉及的补充信息都跳过。 第三遍，阅读论文的整个部分，但是要跳过任何可能陌生看不懂的数学公式，技术术语。</p>
<p>不过，如果你需要对这个专业领域有一个「深入」的理解，那就必须要搞懂那些公式术语了。</p>
<blockquote>
<p>参考</p>
<p><a href="https://mp.weixin.qq.com/s?__biz=MzA5ODEzMjIyMA==&amp;mid=2247501483&amp;idx=1&amp;sn=9b21f8e62fa2b4b33045900a1e721d30&amp;chksm=9094cf38a7e3462ed5901bd8b0b8ebf99a892b31d75aa6eaf8b3c8cc7698b1d708fd0891ab3e&amp;mpshare=1&amp;scene=1&amp;srcid=08304BRBOlTyXb4qDBM5SGTj&amp;sharer_sharetime=1598771438122&amp;sharer_shareid=2c9f868695c34cf5ff5f7a42eab3d2ed&amp;key=9b9af4fa8e2c96d71311b901f9c755ada338c70880cf596dfe4bc2b5ca69cfb094a0a310cf713fb50591ef0933e5f438e73110d797ab7406eeefa5dd5f4c460076a3e94537447c235df683c3eb24a048c7472ad2cdef063ec759505ebb6902987eb9a08ca55be656525ace69f39c8cdbedf7f2b71fa3d2c7c2c4b0dd2e660589&amp;ascene=1&amp;uin=ODEyNzQwMTM5&amp;devicetype=Windows+10+x64&amp;version=62090529&amp;lang=zh_CN&amp;exportkey=A4y9BBn%2B1GAGlkX0mhK71n0%3D&amp;pass_ticket=tS9Bcx3H%2FQ34yaxv%2F0nHTttU4aZeBoKDlw2k4Zwl5JMpqZkqPjEwcrpqIlAybtka" target="_blank" rel="noopener">沈向阳：读论文的三个层次</a></p>
<p><a href="https://www.youtube.com/watch?v=Du7qLsToW-o&amp;t=443s" target="_blank" rel="noopener">youtube视频，沈向阳读论文</a></p>
<p><a href="https://mp.weixin.qq.com/s?subscene=19&amp;__biz=MzIzNjc1NzUzMw==&amp;mid=2247546863&amp;idx=2&amp;sn=275577791d4cee894bd874eedc846f88&amp;chksm=e8d0809ddfa7098b90f2d601d59c1ea11162180387dd9677a96c03e666c1b1e05f8e719d989e&amp;scene=7&amp;ascene=1&amp;devicetype=Windows+10+x64&amp;version=62090529&amp;nettype=cmnet&amp;abtest_cookie=AAACAA%3D%3D&amp;lang=zh_CN&amp;exportkey=Ax%2BhWVV2Xr753%2BtDF%2BAIKRw%3D&amp;pass_ticket=sT%2F05g2Sqp72CoAfTsiZ8TDrxTKg0f%2FTh968brMSrSyOqE%2F1GuTq0PTOveYYBqof&amp;wx_header=0&amp;key=573aef4c1f9b4b5fc4e631a99eb0547e4182bf3ee5dd048cf4487f739b93c9b004f67e751713dea6880a5a922c03ceb30730558ff6be83d973abec53f6fb592491c98a1e205921d9c380c59d6f30c92ea2b1836956318f54b99e962b4d620a7ca074f2e317b259e495570360cc981c43758194fb5e38587a176b8af431cca351&amp;uin=ODEyNzQwMTM5" target="_blank" rel="noopener">吴恩达教你如何读论文：绘制进度表格，论文至少看三遍，还要问自己问题</a></p>
</blockquote>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
  </entry>
  <entry>
    <title>2020-08-27-edge还原到旧版本问题</title>
    <url>/2020/08/27/2020-08-27-edge%E8%BF%98%E5%8E%9F%E5%88%B0%E6%97%A7%E7%89%88%E6%9C%AC%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h3 id="前言">前言</h3>
<p>win10自动更新edge，但是新版的edge用的是Chromium内核，新功能添加不少，也全部支持chrome的插件，但是对pdf的支持不友好，和chrome一个德行<span class="github-emoji" style="display:inline;vertical-align:middle;color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f611.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f611.png?v8">😑</span>。导致我在旧版本edge阅读论文时做的笔记在新版edge体验感极差，于是想着回退到旧版本 （edge不就是用来阅读论文的 ）<span class="github-emoji" style="display:inline;vertical-align:middle;color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f606.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f606.png?v8">😆</span></p>
<h3 id="版本回退">版本回退</h3>
<p>百度后发现将新版本的edge删除，就可以自己回退到旧版本的edge</p>
<p>geek<code>强制删除</code>新版edge之后，系统回退到旧版edge了。但是同时发现原来的一些设置消失了，系统不太稳定，可能是强制删除了一些系统配置文件。不太清楚具体原因，待解决</p>
<p>导致如下两个功能消失</p>
<ul>
<li>在开始栏中不显示安装的软件</li>
<li>在文件夹右键不能使用<code>发送到</code>功能</li>
</ul>
<p>目前发现这两个不能使用的功能。可能还存在其它故障。平时经常通过<code>开始栏</code>打开软件，不显示之后有点麻烦</p>
<h3 id="解决">解决</h3>
<p>用<code>listary</code>软件快速搜索软件名称，（双击ctrl键打开搜索框），然后添加到Rolan中，如图所示</p>
<p><img src="https://i.loli.net/2020/09/04/Kbk3ERwsMJ9AhlP.png" alt="image-20200904145340602" style="zoom:80%;"></p>
<h3 id="小结">小结</h3>
<ul>
<li>下次还是少用geek强制删除吧，乖乖在<code>程序与功能</code>中卸载删除吧，有可能涉及系统配置文件的就不要轻易删除</li>
<li>等待新版edge友好支持论文阅读</li>
</ul>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <tags>
        <tag>故障排除</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-08-25-chrome插件</title>
    <url>/2020/08/25/2020-08-25-chrome%E6%8F%92%E4%BB%B6/</url>
    <content><![CDATA[<h3 id="下载提速"><strong>下载提速</strong></h3>
<ul>
<li><h4 id="使用场景"><strong>使用场景</strong></h4></li>
</ul>
<p>Chrome的下载速度，有时候确实是慢得可以跟某网盘相媲美了，甚至赶不上某些国产浏览器。</p>
<p>这是因为，Google为了兼容所有的电脑性能和带宽，在Chrome中采取的是保守<strong>单线程下载机制</strong>，这就导致很多时候下载速度非常慢了。</p>
<p><img src="https://i.loli.net/2020/08/25/5ZMgPIUfc2YndtE.png" alt="img"></p>
<p>不过，很多人都不知道的是，Chrome其实也是自带多线程下载功能的。所谓多线程下载，就是可以同时对资源建立多个连接，提升下载速度。</p>
<p>只是这个功能是默认关闭的，需要用户手动去开启。</p>
<ul>
<li><h4 id="使用方法"><strong>使用方法</strong></h4></li>
</ul>
<p>在浏览器地址栏输入以下网址并回车：</p>
<p><strong>chrome://flags/#enable-parallel-downloading</strong></p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/D1XlU0QfU3GnNfPmPtO4D3rncDTK3kFcCxQMtjnyMUqI5hTIZydfXEDTnp06YjKEBIbdlnvUoFj3ht3ibXUatiaw/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
<p>在Parallel downloading的后面选项里，把「default」改为「Enabled」，并按照提示重启浏览器。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/D1XlU0QfU3GnNfPmPtO4D3rncDTK3kFcYca3x5SEBJpOky2icdUADwP04jUYiaib6WvUQZ9XlSNHdeiach2RMydRGg/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
<p>这样就可以开启多线程下载了，经过实际测试，下载速度至少提高了三倍左右（也有可能下载速度飙升一段时间又跌回去）。</p>
<h3 id="link-to-text-fragment">Link to Text Fragment</h3>
<p>实际上就是带锚点功能的网页分享工具。</p>
<p>所谓锚文本，简单来说就像是关键词的定位，将关键字指向指向另一个页面的链接就是锚文本。这个工具则可以让你将网页上选中的文本片段生成为一个锚文本。</p>
<p><strong>当你点击这个锚文本时，就会直接跳转到该网页对应标记的锚点上了。</strong></p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_png/D1XlU0QfU3FtLAG0sobAP0xrYk6LJk6m3AU0icjVgSjiavYp3msxibjM7D9U6PXFbzm4wUeZ6OkaFibhPXLFeIBLOQ/640?wx_fmt=png&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>
<h4 id="使用方法-1"><strong>使用方法</strong></h4>
<h5 id="生成锚文本"><strong>生成锚文本</strong></h5>
<p>鼠标划词选中文本，在右键菜单中选择【Copy Link to Text Fragment】，然后可以看到该文本被黄色标记。</p>
<p><img src="https://i.loli.net/2020/08/25/ns29PiDENtvJp3R.gif" alt="img"></p>
<h5 id="打开锚文本"><strong>打开锚文本</strong></h5>
<p>此时，锚文本已经自动生成并复制到你的剪贴板上，你可以将它发送给需要分享的好友，或者在浏览器中打开，另存为书签。</p>
<p>可以看到，在浏览器内打开这个锚文本，网页会自动定位到我们做了锚点的文本部分，再也不需要我们自行阅读查找，非常方便。</p>
<p><img src="https://mmbiz.qpic.cn/mmbiz_gif/D1XlU0QfU3FtLAG0sobAP0xrYk6LJk6m5F5vhCQoN8IeaxDibdMzqk2jFVjhDDhGMJdY3ZHpibCicN5yWbsRoN9Bg/640?wx_fmt=gif&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1" alt="img"></p>
<p>需要注意的是，这个锚文本也<strong>仅限在安装了Link to Text Fragment插件的浏览器上打开</strong>，若没有安装则不会跳转对应位置。</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>chrome</category>
      </categories>
      <tags>
        <tag>chrome</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-08-23-google搜索的高效使用</title>
    <url>/2020/08/23/2020-08-23-google%E6%90%9C%E7%B4%A2%E7%9A%84%E9%AB%98%E6%95%88%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<h3 id="section">“”</h3>
<p>以整个短语作为搜索关键词，而不是拆开成每个词。</p>
<p>表示完全匹配，结果中必须出现与搜索文本完全相同的内容。</p>
<h3 id="a--b">A -B</h3>
<p>搜索包含A但不包含B的结果（请注意A后面的<strong>空格不能省略</strong>）</p>
<p><img src="https://i.loli.net/2020/08/23/pjDWIreKGtJU7Oa.png" alt="image-20200823173525379" style="zoom: 67%;"></p>
<p>当加上 <code>-poweredge</code> 时，就可以屏蔽掉机架式服务器关键字中所有含有poweredge的内容</p>
<p><img src="https://i.loli.net/2020/08/23/k2vbVtdCMBYzJfQ.png" alt="image-20200823173614387" style="zoom:67%;"></p>
<h3 id="filetype">filetype</h3>
<p>搜索对应类型的文件。例如：<code>时间简史 filetype:pdf</code>，即为搜索包含关键字时间简史的pdf文件。（请注意<strong>使用英文的冒号</strong>） （一般不加filetype也可以）</p>
<p><img src="https://i.loli.net/2020/08/23/UBJHv4YmfiGRhuy.png" alt="image-20200823174000514" style="zoom:67%;"></p>
<h3 id="site">site</h3>
<p>在某个网站内搜索，比如：site:<a href="https://link.zhihu.com/?target=http%3A//pan.baidu.com">http://pan.baidu.com</a> 特别好用，用来搜百度云里的资源。再如：</p>
<p>在我们实验室网站查找<code>招生</code>关键字，则 <code>招生 site:http://www.ubinec.org/</code>，十分便捷。</p>
<p>（直接招生 site:ubinec.org/ 也可以 ，中间不要加空格 ）</p>
<p><img src="https://i.loli.net/2020/08/24/aypbHTDAnF79CIi.png" alt="image-20200824160653772" style="zoom:67%;"></p>
<h3 id="section-1">*</h3>
<p>很多时候想搜一个东西但是不确定具体名字，可以用星号代替忘了的字，可以代替多个字</p>
<h3 id="define">define</h3>
<p><strong>当字典或快速查找意思</strong>，如[define:right]，还能看到单词在书籍中出现频率的年代变化，词源等；</p>
<p><img src="https://i.loli.net/2020/08/24/skUBwAFtVMm6OWJ.png" alt="image-20200824162755266" style="zoom:67%;"></p>
<h3 id="section-2">~</h3>
<p>同时搜索近义词。如搜“higher education” 和 “university”</p>
<h3 id="or-或逻辑">OR (或)逻辑</h3>
<p>通过<em>OR</em> 搜索, 可以得到和两个关键词分别相关的结果, 而不仅仅是和两个关键词都同时相关的结果.</p>
<p><img src="E:\myBlog\source\_posts\image-20200824193744947.png" alt="image-20200824193744947" style="zoom:67%;"></p>
<p><img src="E:\myBlog\source\_posts\image-20200824193731306.png" alt="image-20200824193731306" style="zoom:67%;"></p>
<h3 id="限定年份">限定年份</h3>
<ol type="1">
<li>在google工具选项中可以选择时间</li>
</ol>
<p><img src="E:\myBlog\source\_posts\image-20200824194504266.png" alt="image-20200824194504266" style="zoom:67%;"></p>
<ol start="2" type="1">
<li><code>世界杯 2010..2014</code></li>
</ol>
<h3 id="参考">参考</h3>
<p><img src="https://i.loli.net/2020/08/24/uqRyUOdYnGJbxHN.jpg" alt="preview"></p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
  </entry>
  <entry>
    <title>2020-08-19-期望、方差、协方差及相关系数的基本运算</title>
    <url>/2020/08/19/2020-08-19-%E6%9C%9F%E6%9C%9B%E3%80%81%E6%96%B9%E5%B7%AE%E3%80%81%E5%8D%8F%E6%96%B9%E5%B7%AE%E5%8F%8A%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0%E7%9A%84%E5%9F%BA%E6%9C%AC%E8%BF%90%E7%AE%97/</url>
    <content><![CDATA[<p>链接</p>
<p>https://blog.csdn.net/touristman5/article/details/56281887</p>
<p>https://developer.aliyun.com/article/65262</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
  </entry>
  <entry>
    <title>2020-08-19-解读卡尔曼滤波[第二部分]</title>
    <url>/2020/08/19/2020-08-19-%E8%A7%A3%E8%AF%BB%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2-%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86/</url>
    <content><![CDATA[<p>在开始之前，我想解释几个基本术语，如方差（variance）、标准差（standard deviation）、估计值（estimate）、准确度（accuracy）、精度（precision）、平均值（mean）和期望值（expected value）。</p>
<p>我想本教程的许多读者都熟悉基本统计学知识。但是，在本教程的开头，我承诺提供理解卡尔曼滤波器操作所需的必要背景知识。如果您熟悉这个主题，可以跳过它。</p>
<ul>
<li><strong>平均值与期望值</strong></li>
</ul>
<p>虽然<strong>平均值(mean)</strong>与<strong>期望值（expected value）</strong>是密切相关的术语。但是，它们是不同的。</p>
<p>例如，假设有五种不同的硬币——两个5美分的硬币和三个10美分的硬币，我们可以通过平均硬币的价值来轻松计算硬币的平均值。</p>
<p><img src="https://i.loli.net/2020/08/19/JxTbfDa2QOrz4oj.png" alt="image-20200819144207768"></p>
<p>上述结果不能被定义为期望值，因为系统状态（硬币值）没有被隐藏（想要表达的是确定的，此处没有任何不确定性），我们已经使用了所有的population（所有5枚硬币）来计算平均值。</p>
<p><strong>译者补充：因为很多同学经常混淆平均值与期望值的概念，因此，我在此特别解释一下。在解释两个概念之前，先说一下“大数法则”。</strong></p>
<ul>
<li>先说一下大数法则：</li>
</ul>
<p><img src="https://i.loli.net/2020/08/19/cHPovqVb2RFOS3r.jpg" alt="img"></p>
<ul>
<li>思考一下为什么会用到期望值？</li>
</ul>
<p><img src="https://i.loli.net/2020/08/19/F6K9M4aPrJxgnOS.jpg" alt="img"></p>
<p><img src="https://i.loli.net/2020/08/19/1QKexZgmNF6STPt.jpg" alt="img"></p>
<p>期望值也就是每个数*对应的概率值，再求和</p>
<p><strong>译者补充完毕。</strong></p>
<p>现在假设同一个人的五个不同的体重测量值：79.8kg，80kg，80.1kg，79.8kg和80.2kg。</p>
<p>由于秤的随机测量误差，称重测量值不同。 我们不知道准确的重量值是多少，因为它是一个<strong>隐藏变量Hidden Variable</strong>。 但是，我们可以通过平均尺度测量来估计重量。 （准确测量值不可知）</p>
<p><img src="https://i.loli.net/2020/08/19/PjARubvWoLmEV5U.png" alt="image-20200819145014158"></p>
<p>估计的结果是体重的期望值。</p>
<p>平均数经常使用希腊字母：<strong>μ</strong></p>
<p>期望值使用字母：<strong>E</strong></p>
<ul>
<li><strong>方差与标准差</strong></li>
</ul>
<p>方差用来度量随机变量与其期望值（即随机变量的期望值）之间的离散程度。</p>
<p>标准差是方差的平方根。标准差： <img src="https://www.zhihu.com/equation?tex=%5Csigma" alt="[公式]"> ，方差： <img src="https://www.zhihu.com/equation?tex=%5Csigma%5E%7B2%7D" alt="[公式]"></p>
<p>例如，我们想比较两个高中篮球队的身高。下表提供了两支球队的球员身高及其平均值。</p>
<p><img src="https://pic4.zhimg.com/80/v2-d6b2fea25722ffe4774167c3ca530177_1440w.png" alt="img"></p>
<p>如我们所见，两队的平均身高是一样的。现在让我们检查一下高度变化height variance。</p>
<p>由于方差用来度量随机变量与其期望值（即随机变量的期望值）之间的离散程度，我们想知道数据集偏离其平均值的情况。我们可以通过从每个变量中减去平均值来计算每个变量与平均值之间的距离。</p>
<p>我们将用x表示高度，用希腊字母μ表示高度的平均值。每个变量与平均值的距离为：</p>
<p><img src="https://pic1.zhimg.com/80/v2-8aeb370b55ec20c81cd6fb0ea1581a60_1440w.jpg" alt="img"></p>
<p>下表给出了每个变量与平均值之间的距离。</p>
<p><img src="https://picb.zhimg.com/80/v2-47a3cbd7c92a22bc2c1b532557d90609_1440w.png" alt="img"></p>
<p>下表给出了每个变量与平均值的平方距离。</p>
<p>有些值是负数。为了消除负值影响，让我们将高度与平均值的距离平方：</p>
<p><img src="https://picb.zhimg.com/80/v2-2bcc387a3d7e0da6267b04936c845c17_1440w.jpg" alt="img"></p>
<p><img src="https://pic4.zhimg.com/80/v2-759cb19ebbc545066259cfefb22237fb_1440w.png" alt="img"></p>
<p>为了计算数据集的离散程度，我们需要从中找出所有平方距离的平均值：</p>
<p><img src="https://pic2.zhimg.com/80/v2-0d382cddfdce473dfc744a382782c5ac_1440w.jpg" alt="img"></p>
<p>A队的方差是：</p>
<p><img src="https://pic3.zhimg.com/80/v2-9cc05c08921aab36b395d5b42134c911_1440w.png" alt="img"></p>
<p>B队的方差是：</p>
<p><img src="https://pic2.zhimg.com/80/v2-2b935f764b16ebf4b3420fd0f25574a5_1440w.png" alt="img"></p>
<p>我们可以看出，虽然两队的平均值相同，但A队的身高分布值高于B队的身高分布值，这意味着A队在控球员、中锋和后卫等不同位置有不同的球员，而B队球员则技能相差无几。</p>
<p>方差的单位是平方的；查看标准差更方便。正如我已经提到的，标准差是方差的平方根。</p>
<p><img src="https://pic1.zhimg.com/80/v2-4e5b9f3338566969fe0523fa06731489_1440w.jpg" alt="img"></p>
<p>A队运动员身高的标准差为0.12米。</p>
<p>B队运动员身高的标准差为0.036米。</p>
<p>进一步的，现在，假设我们要计算所有高中篮球运动员的平均值和方差。这是一项非常艰巨的任务，我们需要收集所有高中运动员的数据。</p>
<p>但是，我们可以通过选择一个大的数据集并对这个数据集进行计算来估计参与者的平均值和方差。（样本估计全局）</p>
<p>随机选取的100名选手的数据集足以进行准确的估计。</p>
<p>然而，当我们<strong>估计方差</strong>时，方差计算公式略有不同。我们不用N因子归一化，而是用N - 1因子归一化:</p>
<p><img src="https://pic1.zhimg.com/80/v2-9045e0012dc9592019009cca6c64f97f_1440w.jpg" alt="img"></p>
<p>你可以在以下资源中看到这个方程的数学证明：<a href="https://link.zhihu.com/?target=http%3A//www.visiondummy.com/2014/03/divide-variance-n-1/">http://www.visiondummy.com/2014/03/divide-variance-n-1/</a></p>
<hr>
<ul>
<li><strong>正态分布</strong></li>
</ul>
<p>事实证明，许多自然现象服从正态分布。继续以篮球运动员身高为例，如果随机选取运动员，构建大数据集，绘制身高VS.身高（heights vs. heights）的频率曲线图，得到“钟形”曲线，如下图所示:</p>
<p><img src="https://pic4.zhimg.com/80/v2-ca75549d80903118ac9a6ac08b58debc_1440w.jpg" alt="img"></p>
<p>正如你所看到的，这条曲线关于平均值（平均值是1.9米）对称。平均值附近值的频率高于远处值的频率。</p>
<p>高度的标准差等于0.2米。68.26%的值在平均值的一个标准差内。如下图所示，68.26%的值介于1.7米和2.1米之间（绿色区域占曲线下总面积的68.26%）。</p>
<p><img src="https://pic2.zhimg.com/80/v2-0f51f3a38d61049e8dc5dd18e363c114_1440w.jpg" alt="img"></p>
<p>95.44%的值在距离平均值的两个标准差内。</p>
<p>99.74%的值在距离平均值的三个标准差内。</p>
<p>正态分布，也称为高斯分布（它以数学家Carl Friedrich Gauss的名字命名），由以下方程描述：</p>
<p><img src="https://pic4.zhimg.com/80/v2-e4aee9ac01b76d9d688929e3d07ae69e_1440w.jpg" alt="img"></p>
<p>通常，测量误差是正态分布的，因此<code>卡尔曼滤波器设计基于测量误差是正态分布的假设。</code></p>
<hr>
<ul>
<li><strong>估计、准确度与精度</strong></li>
</ul>
<p><strong>— 估计（Estimate）：</strong>评估系统的隐藏状态。飞机的真实位置对观察者来说是隐藏的。我们可以用雷达等传感器来估计飞机的位置。采用多传感器和先进的估计跟踪算法（如卡尔曼滤波），可以显著提高估计精度。每一个测量或计算参数都是一个估计值。</p>
<p><strong>— 准确度（Accuracy）：</strong>表明测量值与真实值的接近程度。</p>
<p><strong>— 精度（Precision）：</strong>描述同一参数的许多 度量值中有多少可变性。准确度和精度是估算的基础。</p>
<p>下图说明了准确度和精度。</p>
<p><img src="https://pic3.zhimg.com/80/v2-7a1dfe3f5186ade70b8937099fb180a8_1440w.jpg" alt="img"></p>
<p><strong>高精度系统的测量方差较低</strong>（即不确定度/离散程度/变化程度较低），而低精度系统的测量方差较大（即不确定度/离散程度/变化程度较高）。方差是由随机测量误差产生的。</p>
<p>低精度系统被称为偏差系统，因为它们的测量具有内置的系统误差（偏差）。</p>
<p>通过<strong>平均或平滑测量</strong>可以显著降低方差的影响。例如，如果我们使用一个具有随机测量误差的温度计来测量温度，我们可以进行多次测量并对测量的值进行平均。由于误差是随机的，所以有些测量值会高于真实值，而另一些测量值会低于真实值。我们做的测量越多，估计就越接近。</p>
<p>另一方面，如果温度计有偏差，估计将包括一个恒定的系统误差。</p>
<p>本教程中的所有示例都假定系统是无偏差的。</p>
<p><img src="E:\myBlog\source_posts\image-20200819194609812.png" alt="image-20200819194609812"></p>
<p>极小化性能指标： 最优解</p>
<p><img src="E:\myBlog\source_posts\image-20200819195511498.png" alt="image-20200819195511498"></p>
<p>J就是选择能够令方差 <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%7B%28%7BX_%7Bi%7D%7D%28%5Ctheta_%7Bhat%7D%29-%5Cmu%29%5E2%7D" alt="[公式]"> 最小的的参数。 X^就是最优解</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>研究方向</category>
      </categories>
      <tags>
        <tag>卡尔曼滤波</tag>
        <tag>RKN</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-08-18-解决hexo发布文章报错</title>
    <url>/2020/08/18/2020-08-18-%E8%A7%A3%E5%86%B3hexo%E5%8F%91%E5%B8%83%E6%96%87%E7%AB%A0%E6%8A%A5%E9%94%99/</url>
    <content><![CDATA[<h3 id="前言">前言</h3>
<p>在进行<code>hexo s -g</code> 发布文章时，出现如下错误</p>
<p><img src="https://i.loli.net/2020/08/18/p17vthyDgEBlukC.png" alt="image-20200818023730784"></p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>故障排除</category>
      </categories>
      <tags>
        <tag>故障排除</tag>
        <tag>next</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-08-18-解决图片caption出现多次</title>
    <url>/2020/08/18/2020-08-18-%E8%A7%A3%E5%86%B3%E5%9B%BE%E7%89%87caption%E5%87%BA%E7%8E%B0%E5%A4%9A%E6%AC%A1/</url>
    <content><![CDATA[<p>大部分参考自<a href="https://wylu.github.io/posts/7bd83fc5/" target="_blank" rel="noopener">Hexo NexT 图片caption出现多次</a></p>
<p>在使用 Hexo + NexT 搭建个人博客的过程中一直有个问题没有解决，直到今天才找到了解决方法。问题就是在展示同一张图片中，caption出现了两次，如图：</p>
<p><a href="https://cdn.jsdelivr.net/gh/wylu/cdn/post/Tool/Hexo/Hexo-NexT-图片caption出现多次/multiple-captions.png" target="_blank" rel="noopener"><img src="https://cdn.jsdelivr.net/gh/wylu/cdn/post/Tool/Hexo/Hexo-NexT-图片caption出现多次/multiple-captions.png" alt="multiple-captions"></a></p>
<h3 id="问题分析">问题分析</h3>
<p>图片正下方的 image-caption 是 NexT 给 fancybox 加上的；而图片左下方的 figcaption 是因为使用了 hexo-renderer-pandoc Markdown 渲染器导致的，hexo-renderer-pandoc 将 Markdown 文件渲染成 HTML 时，会对图片进行渲染，然后生成一个 figcaption 的标签。</p>
<p>很多人可能不会有这样的问题，因为 Hexo 默认的 Markdown 渲染器是 hexo-renderer-marked，hexo-renderer-marked 渲染图片时不会生成 figcaption。</p>
<p>如果你使用的是 hexo-renderer-marked 渲染器，就不会有这样的问题，但是相信很多人都是因为需要使用 mathjax，所以都将默认的 Hexo 默认的 Markdown 渲染器换成了 hexo-renderer-pandoc，hexo-renderer-pandoc 功能强大（依赖与 pandoc 自身强大的功能），它对数学公式的渲染简直可以说是吊打 hexo-renderer-marked，这也是我一直使用它的原因。</p>
<p>所以为了在使用 hexo-renderer-pandoc 的同时，把图片 caption 出现了两次的问题解决，我提过 issue，查阅了许多资料，终于找到了解决的方法。</p>
<h3 id="解决方法">解决方法</h3>
<p>编辑站点配置文件 <code>_config.yml</code>，添加如下内容：</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">pandoc:</span><br><span class="line">  extensions:</span><br><span class="line">    - '-implicit_figures'</span><br></pre></td></tr></tbody></table></figure>
<p>执行下列命令重新生成站点，展示效果如下：</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">$ hexo clean &amp;&amp; hexo g &amp;&amp; hexo s -o</span><br></pre></td></tr></tbody></table></figure>
<p><a href="https://cdn.jsdelivr.net/gh/wylu/cdn/post/Tool/Hexo/Hexo-NexT-图片caption出现多次/single-caption.png" target="_blank" rel="noopener"><img src="https://cdn.jsdelivr.net/gh/wylu/cdn/post/Tool/Hexo/Hexo-NexT-图片caption出现多次/single-caption.png" alt="single-caption"></a></p>
<h3 id="隐藏-fancybox-的-caption">隐藏 fancybox 的 caption</h3>
<p>以 NexT v7.7.0 为例，通过查看 hexo-theme-next/source/js/utils.js 源码，发现 NexT 在使用 fancybox 时，如果图片 title 或 alt 属性不为空时，就会 fancybox 添加一个子标签展示图片的 title 或 alt 属性值。</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">var imageTitle = $image.attr('title') || $image.attr('alt');</span><br><span class="line">if (imageTitle) {</span><br><span class="line">  $imageWrapLink.append(`&lt;p class="image-caption"&gt;${imageTitle}&lt;/p&gt;`);</span><br><span class="line">  // Make sure img title tag will show correctly in fancybox</span><br><span class="line">  $imageWrapLink.attr('title', imageTitle).attr('data-caption', imageTitle);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>如果想通过配置支持选择是否展示 caption，可以参考下方的方法（在 NexT v7.7.0 已测试过），其实不管 NexT 的版本如何，解决方法的思路基本是一致的。</p>
<p>首先修改主题配置文件 <code>_config.yml</code>，找到 fancybox 的配置，将 fancybox 的配置改成如下所示内容：</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line"># FancyBox is a tool that offers a nice and elegant way to add zooming functionality for images.</span><br><span class="line"># For more information: https://fancyapps.com/fancybox</span><br><span class="line">fancybox: </span><br><span class="line">  enable: true</span><br><span class="line">  caption: false</span><br></pre></td></tr></tbody></table></figure>
<p>其中，enable 控制是否启用 fancybox，而 caption 控制是否展示 caption (当然只有在 enable 为 true 时，caption 配置才有效)，如果你不启用 fancybox 自然也不会有 caption。</p>
<p>然后，编辑 hexo-theme-next/source/js/utils.js 文件，将上面的代码修改成如下内容：</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">var imageTitle = $image.attr('title') || $image.attr('alt');</span><br><span class="line">if (imageTitle) {</span><br><span class="line">  if (CONFIG.fancybox.caption) {</span><br><span class="line">    $imageWrapLink.append(`&lt;p class="image-caption"&gt;${imageTitle}&lt;/p&gt;`);</span><br><span class="line">  }</span><br><span class="line">  // Make sure img title tag will show correctly in fancybox</span><br><span class="line">  $imageWrapLink.attr('title', imageTitle).attr('data-caption', imageTitle);</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<p>接着，编辑 hexo-theme-next/source/js/next-boot.js 文件，将 <code>CONFIG.fancybox &amp;&amp; NexT.utils.wrapImageWithFancyBox();</code> 替换成如下内容：</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">/**</span><br><span class="line"> * Register JS handlers by condition option.</span><br><span class="line"> * Need to add config option in Front-End at 'layout/_partials/head.swig' file.</span><br><span class="line"> */</span><br><span class="line">CONFIG.fancybox.enable &amp;&amp; NexT.utils.wrapImageWithFancyBox();</span><br></pre></td></tr></tbody></table></figure>
<p>相信你可以发现，我们这里将 <code>CONFIG.fancybox</code> 替换成 <code>CONFIG.fancybox.enable</code>，正是因为我们自定义的配置是通过 fancybox 下的 enable 的值来确定是否启用的。另外从源码上方的注释可以看到，CONFIG 下的配置项需要在前端文件 'layout/_partials/head.swig' （实际上该文件在'layout/_partials/head/head.swig'）中加上。</p>
<p>所以最后，我们需要在 <code>layout/_partials/head/head.swig</code> 中修改一下上面我们所使用 <code>CONFIG.fancybox.caption</code> 配置。参照其它配置，这里需要将 <code>fancybox:</code>，修改成如下内容：</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">fancybox: {{ theme.fancybox | json }}</span><br></pre></td></tr></tbody></table></figure>
<p>重新生成，效果如下：</p>
<p><a href="https://cdn.jsdelivr.net/gh/wylu/cdn/post/Tool/Hexo/Hexo-NexT-图片caption出现多次/no-caption.png" target="_blank" rel="noopener"><img src="https://cdn.jsdelivr.net/gh/wylu/cdn/post/Tool/Hexo/Hexo-NexT-%E5%9B%BE%E7%89%87caption%E5%87%BA%E7%8E%B0%E5%A4%9A%E6%AC%A1/no-caption.png" alt="no-caption"></a></p>
<blockquote>
<h3 id="references">References</h3>
<p>https://github.com/wzpan/hexo-renderer-pandoc/issues/34</p>
<p>https://github.com/theme-next/hexo-theme-next/issues/857</p>
</blockquote>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>故障排除</category>
      </categories>
      <tags>
        <tag>故障排除</tag>
        <tag>next</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-08-14-解读卡尔曼滤波[第一部分]</title>
    <url>/2020/08/14/2020-08-14-%E8%A7%A3%E8%AF%BB%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2%5B%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%5D/</url>
    <content><![CDATA[<h3 id="关于卡尔曼滤波">关于卡尔曼滤波</h3>
<p>大多数现代系统都搭载上数量众多的传感器，它们通过传感器返回的一系列测量数据来估算一些有用的信息。例如，我们生活上的GPS接收器就是提供位置和速度的装置，它估算的位置和速度就是我们需要的有用数据，而不同时刻的卫星数据就是一系列的测量数据。</p>
<p><strong>对于一个跟踪和控制系统来说，其中最大的问题就是在存在不确定性的前提下提供一个准确的有用信息。</strong>回到刚刚的例子，GPS接收器测量的卫星数据充满不确定性，这些不确定性往往取决于外部环境的变化，其中包括热噪声，大气层影响，卫星位置的轻微改变，GPS的内部时钟准确性等等。</p>
<p>而卡尔曼滤波就是众多常用且重要的估算算法。因为卡尔曼滤波器在进行预估时是默认假设输入数据是不准确的。与此同时，卡尔曼滤波是根据上一次系统的预估值来预估下一次系统的状态。</p>
<p>这种类型的滤波器是卡尔曼首次公开发表的，因此被命名为卡尔曼滤波器。在1960年，卡尔曼发布了一篇描述离散数据线性滤波问题的递归解的问题的论文。</p>
<p>现在，卡尔曼滤波器常常用于雷达跟踪系统，位置和导航系统，控制系统，计算机图形等等领域。</p>
<h3 id="一个预测例子">一个预测例子</h3>
<p>在介绍卡尔曼滤波之前，让我们先来了解一下预测算法。</p>
<p>我们用一个雷达跟踪系统作为例子。</p>
<p><img src="https://www.kalmanfilter.net/img/Overview/tracking_radar.png" alt="Tracking Radar"></p>
<p>雷达跟踪系统向目标方向发射一个笔尖型射束用于追踪目标。假设发射周期为5S，因此，雷达系统会在每5秒的时间后通过向目标方向发射专用的跟踪射束来定位目标。</p>
<p>在发射射束之后，雷达系统会估算当前目标的位置和速度。与此同时，雷达系统也会预测下一个发射束应该发送到哪一个位置。</p>
<p>通过牛顿运动方程，我们能很容易计算出目标在下一个发射周期的位置。</p>
<p><img src="https://i.loli.net/2020/08/19/AIj2cnWB7HYlMzQ.png" alt="image-20200819142535510"></p>
<p>将上诉公式映射到<strong>三维空间</strong>，我们可以将牛顿运动方程作为系统的方程：</p>
<p><img src="https://i.loli.net/2020/08/19/ZnDtdf9ENB7xkga.png" alt="image-20200819142610853"></p>
<p>这些目标参数 [x,y,z,vx,vy,vz,ax,ay,az][x,y,z,vx,vy,vz,ax,ay,az] 被称为 <strong>系统状态</strong>. 通过当前系统状态代入到系统方程中，我们可以得到目标的下一个系统状态。</p>
<p>上面的方程被称为 <strong>动态模型</strong> (或者<strong>空间状态模型</strong>). 动态模型是一种描述输入和输出关系的方法。</p>
<p>回到我们的例子,我们知道当我们有了当前系统状态和掌握系统的动态模型之后，我们就能很容易地预测出目标的下一个状态。</p>
<p>然而并不是这样的。首先,雷达系统的测量数值不是完全可靠，它包含随机误差(或者这类型的不确定性)。这些随机错误的大小取决于很多因素，例如雷达自身的准确性，发射光束的宽度，返回信号强弱等等。这些测量误差被称为<strong>测量噪声</strong>。</p>
<p>此外, 因为有很多外部因素会做成干扰，目标运动并不是完全按着运动方程。例如：风向，空气流动，驾驶策略等等。这个动态模型误差被称为 <strong>处理噪声</strong>。</p>
<p>因为测量噪声和处理噪声的存在，这个根据上诉系统方程估算出来的目标位置会真实的目标位置相差很大。假若这样，雷达系统会向错误的方向发射跟踪射束并且丢失目标。</p>
<p>为了提高雷达跟踪系统的表现，这就需要<strong>能够将处理噪声和测量噪声考虑进来的预测算法</strong>。</p>
<p>对于此类算法，应用得最广泛无疑是<strong>卡尔曼滤波</strong>. (处理模型中的噪声)</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>研究方向</category>
      </categories>
      <tags>
        <tag>卡尔曼滤波</tag>
        <tag>RKN</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-08-12-更换hexo渲染器支持Latex</title>
    <url>/2020/08/12/2020-08-12-%E6%9B%B4%E6%8D%A2hexo%E6%B8%B2%E6%9F%93%E5%99%A8%E6%94%AF%E6%8C%81Latex/</url>
    <content><![CDATA[<h3 id="前言">前言</h3>
<blockquote>
<p><code>LATEX</code> 是一种基于 <code>TEX</code> 的排版系统，利用这种格式，可以迅速生成复杂表格和数学公式等，对于我们写博客帮助十分大。</p>
</blockquote>
<h3 id="初级">初级</h3>
<h4 id="版本">版本</h4>
<p>我使用的是hexo + Next ，版本号如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">Hexo: <span class="number">4.2</span><span class="number">.1</span>  <span class="comment">#在 ~\package.json中查看</span></span><br><span class="line">NexT: <span class="number">7.8</span><span class="number">.0</span>  <span class="comment"># 在~\themes\next\package.json中查看</span></span><br></pre></td></tr></tbody></table></figure>
<h4 id="mathjax-插件">MathJax 插件</h4>
<p>渲染数学公式需要MathJax插件，有些 Hexo 主题自带 MathJax 插件，例如 <a href="http://theme-next.iissnan.com/" target="_blank" rel="noopener">NexT</a>只需启用该插件即可</p>
<p>如果没有的话，可以手动安装：</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">npm install hexo-math --save</span><br></pre></td></tr></tbody></table></figure>
<h4 id="启用">启用</h4>
<p>NexT 主题的 MathJax 插件默认是禁用的，打开主题配置文件，将<code>mathjax</code>的<code>enable</code> 的值改为 <code>true</code> 即可启用 <code>MathJax</code></p>
<p>注意 <code>per_page</code> 上面的注释，注释表明了，MathJax 只渲染在文件前端注明 <code>mathjax: true</code> 字段的文章，</p>
<p>所以为了以后在每一个新建的文件都默认带有<code>mathjax: true</code> ，可以在<code>~\scaffolds\post.md</code>中修改文章头部，添加<code>mathjax: true</code> 即可</p>
<h4 id="效果">效果</h4>
<p>行内公式：</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">这是一个行内公式：$sin^2\theta + cos^2\theta = 1$</span><br></pre></td></tr></tbody></table></figure>
<p>效果：</p>
<p>这是一个行内公式：<span class="math inline">\(sin^2\theta + cos^2\theta = 1\)</span></p>
<p>整行公式：</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">$$sin^2\theta + cos^2\theta = 1$$</span><br></pre></td></tr></tbody></table></figure>
<p>效果：</p>
<p><span class="math display">\[sin^2\theta + cos^2\theta = 1\]</span></p>
<h4 id="获取latex公式mathpix-snipping-tool">获取Latex公式：Mathpix Snipping Tool</h4>
<p>如果要写Latex公式的话，需要掌握很多Latex语法，操作起来比较麻烦，学习成本也高。再加上平时我都是直接copy所读论文中的公式，于是我使用了<code>Mathpix Snipping Tool</code> 软件。</p>
<p><code>Mathpix Snipping Tool</code> ： 通过对所要获取的公式进行截图，可以得到公式的Latex表达形式，复制到博客中即可。操作简单高效。使用方法不再赘述，网上资源很多。</p>
<h3 id="高级">高级</h3>
<h4 id="危渲染复杂latex数学公式出现问题">危：渲染复杂LaTeX数学公式出现问题</h4>
<p>发现一个问题就是编辑好的LaTex公式可以在 Markdown 编辑器（Typora）中显示出来，但部署之后，公式出现无法被渲染</p>
<p>之后通过Google之后，发现问题的一些源头</p>
<blockquote>
<p>将<code>MathJax</code>改为true后发现，<strong>只能渲染部分简单的公式</strong>，对于稍微复杂一点的，特别是有下划线 ' _ ' 符号的公式，几乎都无法被渲染。</p>
<p>hexo默认使用marked.js去解析我们写的markdown，比如一些符号，_代表斜体，会被处理为*标签，比如x_i在开始被渲染的时候，处理为xi，比如__init__会被处理成<strong>init。</strong>*</p>
<p>Hexo 对 Markdown 文件的处理实际上分为两个步骤：</p>
<ol type="1">
<li>Hexo 中的 Markdown 引擎把 Markdown 变为 html 文件</li>
<li>MathJax 负责解释 html 的数学公式</li>
</ol>
</blockquote>
<p>所以现有的hexo渲染器是无法解决当前的问题，所以就要更换渲染器</p>
<h4 id="下载pandoc">下载pandoc</h4>
<p>打开powershell，输入以下命令行</p>
<figure class="highlight shell"><table><tbody><tr><td class="code"><pre><span class="line">pip install Pandoc</span><br></pre></td></tr></tbody></table></figure>
<h4 id="安装-hexo-renderer-pandoc">安装 hexo-renderer-pandoc</h4>
<p>在blog文件夹下打开git bash</p>
<figure class="highlight shell"><table><tbody><tr><td class="code"><pre><span class="line">npm uninstall hexo-renderer-marked --save #卸载旧版本</span><br><span class="line"><span class="meta">#</span><span class="bash">因为之前为了支持emoji，我已经将hexo-renderer-marked换成了hexo-renderer-markdown-it，所以我卸载后者</span></span><br><span class="line">npm install hexo-renderer-pandoc --save #安装新版本</span><br></pre></td></tr></tbody></table></figure>
<h4 id="更新部署">更新部署</h4>
<p>可以看到对于复杂的公式也是支持的~</p>
<p><img src="https://i.loli.net/2020/08/13/cXzadkQeCvAIwfB.png" alt="image-20200813003737055"></p>
<h3 id="注意事项">注意事项</h3>
<p>如果你使用这款 Pandoc renderer，那么书写 Markdown 时候需要遵循 <a href="https://pandoc.org/MANUAL.html#pandocs-markdown" target="_blank" rel="noopener">Pandoc 对 Markdown 的规定</a>。</p>
<p>有一些比较明显的需要注意的事项：正常的文字后面如果跟的是<a href="https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet#lists" target="_blank" rel="noopener"><code>list</code></a>, <a href="https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet#tables" target="_blank" rel="noopener"><code>table</code></a>或者<a href="https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet#blockquotes" target="_blank" rel="noopener"><code>quotation</code></a>，文字后面需要空一行，如果不空行，这些环境将不能被 Pandoc renderer 正常渲染。</p>
<h3 id="参考">参考</h3>
<blockquote>
<p>Hexo渲染Latex出现的问题： https://zhuanlan.zhihu.com/p/35988761</p>
</blockquote>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>Latex</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-08-12-hexo添加emoji表情</title>
    <url>/2020/08/12/2020-08-12-hexo%E6%B7%BB%E5%8A%A0emoji%E8%A1%A8%E6%83%85/</url>
    <content><![CDATA[<h3 id="前言">前言</h3>
<p>markdown支持在文本中使用emoji，在Typora中可以很方便地使用表情。例如输入 <code>:star:</code> ,可以显示出:star:表情，即表情的<code>aliases</code> 编码格式。但是在部署到网站的时候，<span class="github-emoji" style="display:inline;vertical-align:middle;color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8">⭐</span>却渲染不出来，我寻找了很久的解决方案，终于解决 <span class="github-emoji" style="display:inline;vertical-align:middle;color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f606.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f606.png?v8">😆</span></p>
<h3 id="更换hexo渲染器">更换hexo渲染器</h3>
<p>我的hexo版本是version 4.2.1, 可以在在根目录下 packge.json 文件里面看到使用hexo初始化的结果。</p>
<p>将markdown 变成html的转换器叫做<code>markdown渲染器</code>.在Hexo中默认的markdown渲染器 使用的是<a href="https://github.com/hexojs/hexo-renderer-marked" target="_blank" rel="noopener">hexo-renderer-marked</a>,是Hexo版本，这个渲染器不支持插件扩展。另外一个 markdown 渲染器 <a href="https://github.com/celsomiranda/hexo-renderer-markdown-it" target="_blank" rel="noopener">hexo-renderer-markdown-it</a>，这个支持插件配置，可以使用 <a href="https://github.com/markdown-it/markdown-it-emoji" target="_blank" rel="noopener">markwon-it-emoji</a>插件来支持emoji。</p>
<p>解决方案：将原来的 <code>marked</code> 渲染器换成 <code>markdown-it</code>渲染器。</p>
<h4 id="安装新的渲染器">安装新的渲染器</h4>
<p>首先进入博客目录,卸载hexo默认的<code>marked</code>渲染器，安装<code>markdown-it</code>渲染器，运行的命令如：</p>
<figure class="highlight shell"><table><tbody><tr><td class="code"><pre><span class="line">cd Documents/blog</span><br><span class="line">npm un hexo-renderer-marked --save  #卸载旧的渲染器</span><br><span class="line">npm i hexo-renderer-markdown-it --save #暗转新的渲染器</span><br></pre></td></tr></tbody></table></figure>
<p>之后安装<code>markdown-it-emoji</code>插件：</p>
<figure class="highlight shell"><table><tbody><tr><td class="code"><pre><span class="line">npm install markdown-it-emoji --save</span><br></pre></td></tr></tbody></table></figure>
<h4 id="编辑站点配置文件">编辑站点配置文件</h4>
<p>这里的站点配置文件是指位于博客根目录下的 <code>_config.yml</code>，编辑它，然后在末尾添加如下内容：</p>
<figure class="highlight yml"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># Markdown-it config</span></span><br><span class="line"><span class="comment">## Docs: https://github.com/celsomiranda/hexo-renderer-markdown-it/wiki</span></span><br><span class="line"><span class="attr">markdown:</span></span><br><span class="line">  <span class="attr">render:</span></span><br><span class="line">    <span class="attr">html:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">xhtmlOut:</span> <span class="literal">false</span></span><br><span class="line">    <span class="attr">breaks:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">linkify:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">typographer:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">quotes:</span> <span class="string">'“”‘’'</span></span><br><span class="line">  <span class="attr">plugins:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">markdown-it-abbr</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">markdown-it-footnote</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">markdown-it-ins</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">markdown-it-sub</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">markdown-it-sup</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">markdown-it-emoji</span>  <span class="comment"># add emoji</span></span><br><span class="line">  <span class="attr">anchors:</span></span><br><span class="line">    <span class="attr">level:</span> <span class="number">2</span></span><br><span class="line">    <span class="attr">collisionSuffix:</span> <span class="string">'v'</span></span><br><span class="line">    <span class="attr">permalink:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">permalinkClass:</span> <span class="string">header-anchor</span></span><br><span class="line">    <span class="attr">permalinkSymbol:</span> <span class="string">¶</span></span><br></pre></td></tr></tbody></table></figure>
<p>上面的是<code>hexo-renderer-markdown-it</code>的所有选项的配置，详细的每一项配置说明，需要到<a href="https://github.com/celsomiranda/hexo-renderer-markdown-it/wiki/Advanced-Configuration" target="_blank" rel="noopener">Advanced Configuration</a>中查看。</p>
<p>这个时候就可以用表情的<code>aliases</code> 编码格式啦</p>
<p>如果觉得表情渲染的不好看，那么可以安装<a href="https://github.com/twitter/twemoji" target="_blank" rel="noopener">twemoji</a>，对表情进行优化。但是我对现在的渲染感到满意，就没有继续安装。</p>
<h4 id="查找emoji">查找emoji</h4>
<p>表情的<code>aliases</code> 编码可以参考<a href="https://www.webfx.com/tools/emoji-cheat-sheet/" target="_blank" rel="noopener">emoji-cheat-sheet</a>，表情很全，可以找到每个表情的表示，运用到自己的文章里</p>
<h3 id="unicode编码方案">Unicode编码方案</h3>
<p>如果不更换hexo渲染器，那么可以使用表情的<code>Unicode</code>表达方式。不过不推荐此方式，感觉过于麻烦</p>
<p>语法： <code>&amp;#xCODE ;</code></p>
<p>其中<code>CODE</code>是每个表情的编码方式，可以通过 <a href="https://link.zhihu.com/?target=https%3A//apps.timwhitlock.info/emoji/tables/unicode%23block-4-enclosed-characters">Emoji Unicode Tables</a>查询得到</p>
<p><strong>例子：</strong> 查到了 表情对应的 <strong>Unicode</strong> 编码为 <code>U+1F34E</code>，则与此表情对应的 <code>CODE</code> 为 <code>1F34E</code> (舍弃前面的 <strong>U+</strong>)。输入markdown文档内即可</p>
<h3 id="后续">后续</h3>
<p>因为要读论文，然而在论文中会出现很多的数学公式，这时候需要运用Latex，原始的hexo渲染器<a href="https://github.com/hexojs/hexo-renderer-marked" target="_blank" rel="noopener">hexo-renderer-marked</a>对渲染不了公式，在为了能够添加emoji而更换的新渲染器 <a href="https://github.com/celsomiranda/hexo-renderer-markdown-it" target="_blank" rel="noopener">hexo-renderer-markdown-it</a>还是无法渲染Latex公式<span class="github-emoji" style="display:inline;vertical-align:middle;color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f610.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f610.png?v8">😐</span></p>
<p>于是我又准备更换hexo渲染器，来让新的渲染器支持数学公式，于是我就更换了 <a href="https://github.com/wzpan/hexo-renderer-pandoc" target="_blank" rel="noopener">hexo-renderer-pandoc</a>，支持Mathjax语法，十分靠谱，然而问题来了，那就是不支持emoji了 <span class="github-emoji" style="display:inline;vertical-align:middle;color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f610.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f610.png?v8">😐</span></p>
<p>在我准备在两者中舍弃一个，或者用emoji的Unicode编码来代替渲染器的时候，我发现了一个插件，就尝试在现有的渲染器基础上添加了一个hexo插件 <a href="https://github.com/crimx/hexo-filter-github-emojis" target="_blank" rel="noopener">hexo-filter-github-emojis</a> ，发现此插件可以有效支持emoji表情，于是两全其美啦<span class="github-emoji" style="display:inline;vertical-align:middle;color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f606.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f606.png?v8">😆</span></p>
<hr>
<p>下面是插件的使用说明</p>
<h4 id="安装插件">安装插件</h4>
<p>使用以下命令安装 <a href="https://github.com/crimx/hexo-filter-github-emojis" target="_blank" rel="noopener">hexo-filter-github-emojis</a> 插件：</p>
<figure class="highlight shell"><table><tbody><tr><td class="code"><pre><span class="line">npm install hexo-filter-github-emojis --save</span><br></pre></td></tr></tbody></table></figure>
<h4 id="启用插件">启用插件</h4>
<p>向站点配置文件 <code>hexo_root\_config.yml</code> 中添加如下设置：</p>
<figure class="highlight yml"><table><tbody><tr><td class="code"><pre><span class="line"><span class="attr">githubEmojis:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">className:</span> <span class="string">github-emoji</span></span><br><span class="line">  <span class="attr">unicode:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">styles:</span></span><br><span class="line">    <span class="attr">display:</span> <span class="string">inline</span></span><br><span class="line">    <span class="attr">vertical-align:</span> <span class="string">middle</span> <span class="comment"># Freemind适用</span></span><br><span class="line">  <span class="attr">localEmojis:</span></span><br></pre></td></tr></tbody></table></figure>
<p>具体的每个配置项含义参见 <a href="https://github.com/crimx/hexo-filter-github-emojis" target="_blank" rel="noopener">说明文档</a>。</p>
<h4 id="使用方法">使用方法</h4>
<p>和上述使用方法一样，很方便！ <span class="github-emoji" style="display:inline;vertical-align:middle;color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f308.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f308.png?v8">🌈</span></p>
<h3 id="参考">参考</h3>
<blockquote>
<p>hexo中添加表情： https://www.cnblogs.com/fsong/p/5929773.html</p>
<p>hexo 使用emoji： https://spacefan.github.io/2018/06/30/hexo-emoji/</p>
</blockquote>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>故障排除</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-08-12-新页面添加友链</title>
    <url>/2020/08/12/2020-08-12-%E6%96%B0%E9%A1%B5%E9%9D%A2%E6%B7%BB%E5%8A%A0%E5%8F%8B%E9%93%BE/</url>
    <content><![CDATA[<h3 id="前言">前言</h3>
<p>NexT 主题自带的友情链接的位置是在侧栏的 Social Link 中，位置不太明显，而且容量比较小，不美观。因此可以自定义一个特定的页面，单独显示友情链接</p>
<h3 id="新建links.swig-文件">新建<code>links.swig</code> 文件</h3>
<p>首先，在 <code>~/themes/next/layout/</code> 目录下新建一个 <code>links.swig</code> 文件，并写入以下内容：</p>
<!-- 所在目录：~/themes/next/layout/ -->
<figure class="highlight html"><table><tbody><tr><td class="code"><pre><span class="line">{% block content %}</span><br><span class="line">  {######################}</span><br><span class="line">  {###  LINKS BLOCK   ###}</span><br><span class="line">  {######################}</span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"links"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">style</span>&gt;</span></span><br><span class="line"><span class="css">        <span class="selector-class">.links-content</span>{</span></span><br><span class="line"><span class="css">            <span class="selector-tag">margin-top</span><span class="selector-pseudo">:1rem</span>;</span></span><br><span class="line">        }</span><br><span class="line">        </span><br><span class="line"><span class="css">        <span class="selector-class">.link-navigation</span><span class="selector-pseudo">::after</span> {</span></span><br><span class="line">            content: " ";</span><br><span class="line">            display: block;</span><br><span class="line">            clear: both;</span><br><span class="line">        }</span><br><span class="line">        </span><br><span class="line"><span class="css">        <span class="selector-class">.card</span> {</span></span><br><span class="line">            width: 240px;</span><br><span class="line">            font-size: 1rem;</span><br><span class="line">            padding: 10px 20px;</span><br><span class="line">            border-radius: 4px;</span><br><span class="line"><span class="css">            <span class="selector-tag">transition-duration</span>: 0<span class="selector-class">.15s</span>;</span></span><br><span class="line">            margin-bottom: 1rem;</span><br><span class="line"><span class="css">            <span class="selector-tag">display</span><span class="selector-pseudo">:flex</span>;</span></span><br><span class="line">        }</span><br><span class="line"><span class="css">        <span class="keyword">@media</span> (<span class="attribute">max-width:</span> <span class="number">767px</span>) {</span></span><br><span class="line"><span class="css">			<span class="selector-class">.card</span><span class="selector-pseudo">:nth-child(odd)</span> {</span></span><br><span class="line">                float: left;</span><br><span class="line">            }</span><br><span class="line"><span class="css">            <span class="selector-class">.card</span><span class="selector-pseudo">:nth-child(even)</span> {</span></span><br><span class="line">                float: left !important;</span><br><span class="line">            }</span><br><span class="line">		}</span><br><span class="line">		</span><br><span class="line"><span class="css">        <span class="selector-class">.card</span><span class="selector-pseudo">:nth-child(odd)</span> {</span></span><br><span class="line">            float: left;</span><br><span class="line">        }</span><br><span class="line"><span class="css">        <span class="selector-class">.card</span><span class="selector-pseudo">:nth-child(even)</span> {</span></span><br><span class="line">            float: right;</span><br><span class="line">        }</span><br><span class="line"><span class="css">        <span class="selector-class">.card</span><span class="selector-pseudo">:hover</span> {</span></span><br><span class="line"><span class="css">            <span class="selector-tag">transform</span>: <span class="selector-tag">scale</span>(1<span class="selector-class">.1</span>);</span></span><br><span class="line"><span class="css">            <span class="selector-tag">box-shadow</span>: 0 2<span class="selector-tag">px</span> 6<span class="selector-tag">px</span> 0 <span class="selector-tag">rgba</span>(0, 0, 0, 0<span class="selector-class">.12</span>), 0 0 6<span class="selector-tag">px</span> 0 <span class="selector-tag">rgba</span>(0, 0, 0, 0<span class="selector-class">.04</span>);</span></span><br><span class="line">        }</span><br><span class="line"><span class="css">        <span class="selector-class">.card</span> <span class="selector-tag">a</span> {</span></span><br><span class="line"><span class="css">            <span class="selector-tag">border</span><span class="selector-pseudo">:none</span>; </span></span><br><span class="line">        }</span><br><span class="line"><span class="css">        <span class="selector-class">.card</span> <span class="selector-class">.ava</span> {</span></span><br><span class="line">            width: 3rem!important;</span><br><span class="line">            height: 3rem!important;</span><br><span class="line"><span class="css">            <span class="selector-tag">margin</span><span class="selector-pseudo">:0</span>!<span class="selector-tag">important</span>;</span></span><br><span class="line">            margin-right: 1em!important;</span><br><span class="line"><span class="css">            <span class="selector-tag">border-radius</span><span class="selector-pseudo">:4px</span>;</span></span><br><span class="line">            </span><br><span class="line">        }</span><br><span class="line"><span class="css">        <span class="selector-class">.card</span> <span class="selector-class">.card-header</span> {</span></span><br><span class="line">            font-style: italic;</span><br><span class="line">            overflow: hidden;</span><br><span class="line">            width: 100%;</span><br><span class="line">        }</span><br><span class="line"><span class="css">        <span class="selector-class">.card</span> <span class="selector-class">.card-header</span> <span class="selector-tag">a</span> {</span></span><br><span class="line">            font-style: normal;</span><br><span class="line"><span class="css">            <span class="selector-tag">color</span>: <span class="selector-id">#2bbc8a</span>;</span></span><br><span class="line">            font-weight: bold;</span><br><span class="line">            text-decoration: none;</span><br><span class="line">        }</span><br><span class="line"><span class="css">        <span class="selector-class">.card</span> <span class="selector-class">.card-header</span> <span class="selector-tag">a</span><span class="selector-pseudo">:hover</span> {</span></span><br><span class="line"><span class="css">            <span class="selector-tag">color</span>: <span class="selector-id">#a166ab</span>;</span></span><br><span class="line">            text-decoration: none;</span><br><span class="line">        }</span><br><span class="line"><span class="css">        <span class="selector-class">.card</span> <span class="selector-class">.card-header</span> <span class="selector-class">.info</span> {</span></span><br><span class="line"><span class="css">            <span class="selector-tag">font-style</span><span class="selector-pseudo">:normal</span>;</span></span><br><span class="line"><span class="css">            <span class="selector-tag">color</span>:<span class="selector-id">#a3a3a3</span>;</span></span><br><span class="line"><span class="css">            <span class="selector-tag">font-size</span><span class="selector-pseudo">:14px</span>;</span></span><br><span class="line">            min-width: 0;</span><br><span class="line">            overflow: hidden;</span><br><span class="line">            white-space: nowrap;</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line"><span class="css">        <span class="selector-tag">span</span><span class="selector-class">.focus-links</span> {</span></span><br><span class="line">            font-style: normal;</span><br><span class="line">            margin-left: 10px;</span><br><span class="line">            position: unset;</span><br><span class="line">            left: 0;</span><br><span class="line">            padding: 0 7px 0 5px;</span><br><span class="line">            font-size: 11px;</span><br><span class="line"><span class="css">            <span class="selector-tag">border-color</span>: <span class="selector-id">#42c02e</span>;</span></span><br><span class="line">            border-radius: 40px;</span><br><span class="line">            line-height: 24px;</span><br><span class="line">            height: 22px;</span><br><span class="line"><span class="css">            <span class="selector-tag">color</span>: <span class="selector-id">#fff</span> !<span class="selector-tag">important</span>;</span></span><br><span class="line"><span class="css">            <span class="selector-tag">background-color</span>: <span class="selector-id">#42c02e</span>;</span></span><br><span class="line">            display: inline-block;</span><br><span class="line">        }</span><br><span class="line"><span class="css">        <span class="selector-tag">span</span><span class="selector-class">.focus-links</span><span class="selector-pseudo">:hover</span>{</span></span><br><span class="line"><span class="css">            <span class="selector-tag">background-color</span>: <span class="selector-id">#318024</span>;</span></span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line"><span class="css">        <span class="selector-class">.friends-btn</span>{</span></span><br><span class="line">            text-align: center;</span><br><span class="line"><span class="css">            <span class="selector-tag">color</span>: <span class="selector-id">#555</span>!<span class="selector-tag">important</span>;</span></span><br><span class="line"><span class="css">            <span class="selector-tag">background-color</span>: <span class="selector-id">#fff</span>;</span></span><br><span class="line">            border-radius: 3px;</span><br><span class="line">            font-size: 15px;</span><br><span class="line"><span class="css">            <span class="selector-tag">box-shadow</span>: <span class="selector-tag">inset</span> 0 0 10<span class="selector-tag">px</span> 0 <span class="selector-tag">rgba</span>(0,0,0,<span class="selector-class">.35</span>);</span></span><br><span class="line">            border: none!important;</span><br><span class="line">            transition-property: unset;</span><br><span class="line">            padding: 0 15px;</span><br><span class="line">            margin: inherit;</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line"><span class="css">        <span class="selector-class">.friends-btn</span><span class="selector-pseudo">:hover</span>{</span></span><br><span class="line">            color: rgb(255, 255, 255) !important;</span><br><span class="line">            border-radius: 3px;</span><br><span class="line">            font-size: 15px;</span><br><span class="line"><span class="css">            <span class="selector-tag">box-shadow</span>: <span class="selector-tag">inset</span> 0<span class="selector-tag">px</span> 0<span class="selector-tag">px</span> 10<span class="selector-tag">px</span> 0<span class="selector-tag">px</span> <span class="selector-tag">rgba</span>(0, 0, 0, 0<span class="selector-class">.35</span>);</span></span><br><span class="line"><span class="css">            <span class="selector-tag">background-image</span>: <span class="selector-tag">linear-gradient</span>(90<span class="selector-tag">deg</span>, <span class="selector-id">#a166ab</span> 0%, <span class="selector-id">#ef4e7b</span> 25%, <span class="selector-id">#f37055</span> 50%, <span class="selector-id">#ef4e7b</span> 75%, <span class="selector-id">#a166ab</span> 100%);</span></span><br><span class="line">            margin: inherit;</span><br><span class="line">        }</span><br><span class="line">    <span class="tag">&lt;/<span class="name">style</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"links-content"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"link-navigation"</span>&gt;</span></span><br><span class="line"></span><br><span class="line">            {% for link in theme.mylinks %}</span><br><span class="line">            </span><br><span class="line">                <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"card"</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">img</span> <span class="attr">class</span>=<span class="string">"ava"</span> <span class="attr">src</span>=<span class="string">"{{ link.avatar }}"</span>/&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"card-header"</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">div</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"{{ link.site }}"</span> <span class="attr">target</span>=<span class="string">"_blank"</span>&gt;</span> {{ link.nickname }}<span class="tag">&lt;/<span class="name">a</span>&gt;</span> <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"{{ link.site }}"</span>&gt;</span><span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">"focus-links"</span>&gt;</span>关注<span class="tag">&lt;/<span class="name">span</span>&gt;</span><span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"info"</span>&gt;</span>{{ link.info }}<span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">            </span><br><span class="line">            {% endfor %}</span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">        {{ page.content }}</span><br><span class="line">        <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">     {##########################}</span><br><span class="line">  {###   END LINKS BLOCK  ###}</span><br><span class="line">  {##########################}</span><br><span class="line">{% endblock %}</span><br></pre></td></tr></tbody></table></figure>
<p>可以根据喜好自己更改样式</p>
<h3 id="修改page.swig文件">修改<code>page.swig</code>文件</h3>
<p>将代码行前<code>+</code>的代码添加到文件中</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">{% extends '_layout.swig' %}</span><br><span class="line">{% import '_macro/sidebar.swig' as sidebar_template with context %}</span><br><span class="line"></span><br><span class="line">  {% block title %}</span><br><span class="line">    {%- set page_title_suffix = ' | ' + title %}</span><br><span class="line"></span><br><span class="line">{%- if page.type === 'categories' and not page.title %}</span><br><span class="line">  {{- __('title.category') + page_title_suffix }}</span><br><span class="line">{%- elif page.type === 'tags' and not page.title %}</span><br><span class="line">  {{- __('title.tag') + page_title_suffix }}</span><br><span class="line"></span><br><span class="line">+ {%- elif page.type === 'links' and not page.title %}</span><br><span class="line">+	{{- __('title.links') + page_title_suffix }}</span><br><span class="line">{%- elif page.type === 'schedule' and not page.title %}</span><br><span class="line">  {{- __('title.schedule') + page_title_suffix }}</span><br><span class="line">{%- else %}</span><br><span class="line">  {{- page.title + page_title_suffix }}</span><br><span class="line">{%- endif %}</span><br><span class="line">{% endblock %}</span><br><span class="line"></span><br><span class="line">{% block content %}</span><br><span class="line"></span><br><span class="line">  &lt;div class="posts-expand"&gt;</span><br><span class="line">    {##################}</span><br><span class="line">    {### PAGE BLOCK ###}</span><br><span class="line">    {##################}</span><br><span class="line">    &lt;div class="post-block" lang="{{ page.lang or page.language or config.language }}"&gt;</span><br><span class="line">      {% include '_partials/page/page-header.swig' %}</span><br><span class="line">      {#################}</span><br><span class="line">      {### PAGE BODY ###}</span><br><span class="line">      {#################}</span><br><span class="line">      &lt;div class="post-body{%- if page.direction and page.direction.toLowerCase() === 'rtl' %} rtl{%- endif %}"&gt;</span><br><span class="line">        {%- if page.type === 'tags' %}</span><br><span class="line">          &lt;div class="tag-cloud"&gt;</span><br><span class="line">            &lt;div class="tag-cloud-title"&gt;</span><br><span class="line">              {{ _p('counter.tag_cloud', site.tags.length) }}</span><br><span class="line">            &lt;/div&gt;</span><br><span class="line">            &lt;div class="tag-cloud-tags"&gt;</span><br><span class="line">              {{ tagcloud({min_font: theme.tagcloud.min, max_font: theme.tagcloud.max, amount: theme.tagcloud.amount, color: true, start_color: theme.tagcloud.start, end_color: theme.tagcloud.end}) }}</span><br><span class="line">            &lt;/div&gt;</span><br><span class="line">          &lt;/div&gt;</span><br><span class="line">        {% elif page.type === 'categories' %}</span><br><span class="line">          &lt;div class="category-all-page"&gt;</span><br><span class="line">            &lt;div class="category-all-title"&gt;</span><br><span class="line">              {{ _p('counter.categories', site.categories.length) }}</span><br><span class="line">            &lt;/div&gt;</span><br><span class="line">            &lt;div class="category-all"&gt;</span><br><span class="line">              {{ list_categories() }}</span><br><span class="line">            &lt;/div&gt;</span><br><span class="line">          &lt;/div&gt;</span><br><span class="line">+       {% elif page.type === 'links' %}</span><br><span class="line">+         {% include 'links.swig' %}</span><br><span class="line">        {% elif page.type === 'schedule' %}</span><br><span class="line">          &lt;div class="event-list"&gt;</span><br><span class="line">          &lt;/div&gt;</span><br><span class="line">          {% include '_scripts/pages/schedule.swig' %}</span><br><span class="line">        {% else %}</span><br><span class="line">          {{ page.content }}</span><br><span class="line">        {%- endif %}</span><br><span class="line">      &lt;/div&gt;</span><br><span class="line">      {#####################}</span><br><span class="line">      {### END PAGE BODY ###}</span><br><span class="line">      {#####################}</span><br><span class="line">    &lt;/div&gt;</span><br><span class="line">    {% include '_partials/page/breadcrumb.swig' %}</span><br><span class="line">    {######################}</span><br><span class="line">    {### END PAGE BLOCK ###}</span><br><span class="line">    {######################}</span><br><span class="line">  &lt;/div&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">{% endblock %}</span><br><span class="line"></span><br><span class="line">{% block sidebar %}</span><br><span class="line">  {{ sidebar_template.render(true) }}</span><br><span class="line">{% endblock %}</span><br></pre></td></tr></tbody></table></figure>
<h3 id="新建page界面">新建page界面</h3>
<p>新建名为links的page，具体可以参考我的另外一篇博客<a href="%5Bhttps://lisijian.cn/2020/08/08/2020-08-08-hexo%E6%96%B0%E5%BB%BApage/%5D(https://lisijian.cn/2020/08/08/2020-08-08-hexo新建page/)">2020-08-08-hexo新建page</a></p>
<p>注意： 在<code>links</code> 文件夹，打开其中的 <code>index.md</code> 文件，在标题头中写入 <code>type = "links"</code> 这个属性头，如下：</p>
<figure class="highlight yml"><table><tbody><tr><td class="code"><pre><span class="line"><span class="attr">title:</span> <span class="string">友情链接</span></span><br><span class="line"><span class="attr">date:</span> <span class="number">2020</span><span class="number">-08</span><span class="number">-10</span> <span class="number">13</span><span class="string">:08:43</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">"links"</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="修改主题配置文件">修改主题配置文件</h3>
<p>在主题配置文件 <code>~/themes/next/_config.yml</code> 文件中按照以下格式添加友链：</p>
<figure class="highlight yml"><table><tbody><tr><td class="code"><pre><span class="line"><span class="attr">mylinks:</span></span><br><span class="line"></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">nickname:</span> <span class="comment"># 昵称</span></span><br><span class="line">    <span class="attr">avatar:</span> <span class="comment"># 头像地址</span></span><br><span class="line">    <span class="attr">site:</span> <span class="comment">#友链地址</span></span><br><span class="line">    <span class="attr">info:</span> <span class="comment">#相关说明</span></span><br><span class="line"></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">nickname:</span> <span class="comment"># 昵称</span></span><br><span class="line">    <span class="attr">avatar:</span> <span class="comment"># 头像地址</span></span><br><span class="line">    <span class="attr">site:</span> <span class="comment">#友链地址</span></span><br><span class="line">    <span class="attr">info:</span> <span class="comment">#相关说明</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="参考">参考</h3>
<blockquote>
<p>Hexo-NexT 主题个性优化 https://guanqr.com/tech/website/hexo-theme-next-customization/</p>
</blockquote>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>next</category>
      </categories>
      <tags>
        <tag>next</tag>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-08-11-杂记</title>
    <url>/2020/08/11/2020-08-11-%E6%9D%82%E8%AE%B0/</url>
    <content><![CDATA[<h3 id="section">2020-08-20</h3>
<p>研究生阶段想要重点去研究透彻某个知识点，还是要去系统学习。不要只是依赖博客</p>
<p>博客缺点</p>
<ol type="1">
<li>一般不成体系，比较片面</li>
<li>不管是翻译外国博客还是自己的总结，由于博主本身的能力，导致在写的时候，都会出现一定的误差，可信度不抵论文和书籍</li>
<li>讲的不深入</li>
</ol>
<p>和师兄讨论了卡尔曼滤波的内容，师兄针对我的问题也给了很好的建议，让我有方向去继续。发现一直以来我对卡尔曼滤波理解的太浅显，不深刻，理解只是停留结合例子理解卡尔曼滤波那五个公式，知道计算过程，但是没有去深入理解来源以及公式的意义，变量的含义等等，没有真正转化为自己的东西 因为卡尔曼滤波是RKN的核心基础，所以必须要深入理解，这样才能更好地运用卡尔曼滤波，也更好地理解模型。 因为融合到transformer中，也要讲清楚为什么融合之后效果好，或者为什么不好，只有将本质讲清楚，去理论分析的时候才有信服力。避免只是简单的拼接。</p>
<p>为什么这么做，这么做的好处。公式间的逻辑关系， 买了本卡尔曼滤波的书，意义和含义，背景和理论公式一步一步推导</p>
<h3 id="section-1">2020-09-09</h3>
<p>前两天给代码修理bug，总是出错，很玄乎。纠结了很久，看到自己的努力但是还是没有进展，这样很让人暴躁又绝望</p>
<p>今天一早来到实验室，看似难以解决的问题，很快解决掉了</p>
<p>一时解决不掉的困难很多的，该干嘛干嘛，保持好饮食和睡眠。第二天一早趁着脑子清醒的时候，直接面对问题，找到问题的根本所在，那些看似要压倒自己的困难，看似无法解决的问题，可能会很快解决</p>
<h3 id="section-2">2020-09-19</h3>
<p>我自己也发现，关于师兄分配的任务总是做的很慢，无法按时完成任务。是我平时的习惯和思维方式导致的。总结了一下几点：</p>
<p>（1）自己基础不好，也没有投入太多的精力去学习。 所以在阅读和调试代码的时候，就相当于在接触新知识，对于别人很熟悉的流程和知识自己却要投入很多的精力和时间去慢慢积累。不过这也是我必经的路程，做好总结，多接触代码，相信我自己会做的更好的。</p>
<p>（2）对于未知的东西有种恐惧感，会让我不想去做，也不敢去做，没学过的没接触过的，不知道结果的事情会让我恐惧。所以对于结果未知的实验的调试，我更希望去刷结果确定的优化方法数学题目。就像我来到一个陌生的地方，对所有路都很陌生，但是还是让我在天黑之前找到一件东西。前几天在调试的transformer-tensorflow版本的实验，因为调试没接触过的实验，很多流程很陌生，总是一个bug修复，又会遇到另一个bug，即使最终调试通了，结果也不一定好。所以时间和精力成本是无法预估的，这种不确定性很让我头疼。</p>
<p>其实还是主要是对于代码的不了解，如果足够了解，知道代码逻辑，那么恐惧就会减少很多，甚至还会很享受这种挑战的过程。就比如在调试transformer-ptorch版本的代码时，因为熟悉代码，即使代码出错，即使实验结果不好，但还是充满热情，乐此不疲。最终任务完成也是不错的。</p>
<p>其实我是很容易找到热情的人，很容易满足，在生活中也是对大多数外在物质条件都是要求不高的。所以哪怕在任务中给我一点点希望，那么我也会保持相当的热情。解决不了，那就明天再来。学会克服恐惧，直面恐惧</p>
<p>所以最近在跑实验跑的接近绝望的时候，才觉得学数学、做数学题是一件多么让人愉悦的事情。</p>
<p>（3）拖延症。我会经常拖延事情，直到最后要截止了，才匆匆应付。之前的两周一次的报告，因为在家的原因，自律性很差，往往最后两天才开始想这两周要学点啥，实验需要有啥进展，这样自然只能是匆匆应付，做的不好。</p>
<p>这是从小到大养成的习惯，总是将事情拖到最后，最后肯定会完成的不好。以后对于重要的事情一定要提前规划 ，提前安排，提前做，最好提前完成。</p>
<p>（4）容易分心。我发现自己的一个缺点就是无法专注的去做一件事情，去处理一个任务。就比如我在学习transformer并阅读代码的时候，过了一会又开起了小差，觉得不确定性分析很有趣，就开始看相关知识，这样很快半天时间就过去了。好不容易吃完饭回来学了一会transformer，又开始捯饬起来自己的博客了。也不是不想学transformer，至少自己容易去分心干一些自己觉得有趣的事情，但这样会分割学习特定知识一整块的时间。</p>
<p>以后遇到感兴趣的事情，可以先记录下来，留着以后自己觉得累的时候，或者想放松的时候再去做，也算是奖赏自己的吧</p>
<p>（5） 会做很多无效操作。比如点击很多鼠标，查看很多微信等等，有时候静不下心来吧。深呼吸，或者出去放松一下，找朋友聊聊天转移一下注意力，再来解决可能思路就出来了。</p>
<p>（6） 思路没有清晰，没有想到比较好的解决办法的时候，就开始去做。我这样做往往会得到一个不太好的结果。还是知行合一好一些吧</p>
<h3 id="section-3">2020-09-30</h3>
<p>这一阵的很难熬，结果出现不了，实验复现不了，每天都在怀疑自己的能力。明明感觉很简单的事情，自己却要折腾很久。一度怀疑自己不适合科研，没这个能力</p>
<p>想想真的很奇妙，昨晚和奕飞鼎哥撸串还对自己的实验绝望，没啥动力继续下去了，今天竟然灵光一闪，就解决了，看似绝望，实际上可能只是自己被绕在圈里，透透气，散散心，防空一下，再坚持一下说不定转机就出现了。</p>
<p>对于参考资料和博客，不用图多，有时候我会打开多个博客标签，然而对于每一个标签我都一扫而过，感觉很浮躁，没有去认真阅读。对于优秀的博客，一定要好好阅读</p>
<p>前一阵很迷茫，觉得我一事无成， 干啥啥不行。都23岁了，不能挣一分钱，还是为应试考试忙碌。爸妈都不容易，。最近也是比之前直觉拼多了。多希望能够有所作为啊，至少在同龄人中做到不错的位置。</p>
<p>对未来有些迷茫，想继续读博，但是最近感觉做科研有些吃力，我可能不适合科研这条路。</p>
<p>现在还有些侥幸，在开学前建立了自己的博客，可以随时记录自己的想法以及技术总结。现在在折腾这些的话，一定是没有时间和精力了。</p>
<p>这次是真的可以放松一下。</p>
<p>专注着把一件事做好，不要一件事做一半，又去做了另一件事。这样容易分心。</p>
<h3 id="section-4">2020-10-15</h3>
<p>在可参考的资料太多时，大致浏览一下，找一个最适合的作为最基本的参考点，其它资料作为辅助。这样可以避免资料太多，无处下手，导致思维混乱。</p>
<p>或者想要研究透这个知识点，还是看相关的论文，这样质量会很高，避免陷入无从下手的地步</p>
<h3 id="section-5">2020-11-1</h3>
<p>多去学习师兄们做研究的思考方式与思路。</p>
<p>比如在初涉及某个领域的时候，假如是小样本学习领域</p>
<p>1.先要考虑小样本学习主要做了哪些方向，解决哪些问题 （分类、目标检测）</p>
<p>2.用了哪些方法去解决上述的问题（传统方法、最新方法）</p>
<p>3.关注并借鉴与自己的领域比较相关的论文，自己所了解的模型是否可以应用到小样本学习应用上来</p>
<p>4.当前的解决方案存在什么问题，如何改进，改进的可行性有多大</p>
<blockquote>
<p>上述方法实际上就是细化问题，以此来逐步明确自己的目标</p>
<p>多去学习师兄们做研究的思考方式与思路</p>
</blockquote>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>杂记</category>
      </categories>
      <tags>
        <tag>杂记</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-08-09-常用chrome快捷键</title>
    <url>/2020/08/09/2020-08-09-%E5%B8%B8%E7%94%A8chrome%E5%BF%AB%E6%8D%B7%E9%94%AE/</url>
    <content><![CDATA[<p>chrome是我日常使用的浏览器，平时也会使用快捷键来提高效率。chrome的快捷键真的很好使，可以</p>
<p>摆脱很多不必要的鼠标点击，键盘直接搞定。总结一下我常用的快捷键。</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">描述</th>
<th style="text-align: center;">快捷键</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">打开新窗口</td>
<td style="text-align: center;">Ctrl + n</td>
</tr>
<tr class="even">
<td style="text-align: center;">在隐身模式下打开新窗口</td>
<td style="text-align: center;">Ctrl + Shift + n</td>
</tr>
<tr class="odd">
<td style="text-align: center;">-----</td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">打开新的标签页，并跳转到该标签页</td>
<td style="text-align: center;">Ctrl + t</td>
</tr>
<tr class="odd">
<td style="text-align: center;">恢复已关闭的标签页</td>
<td style="text-align: center;">Ctrl + Shift + t</td>
</tr>
<tr class="even">
<td style="text-align: center;">跳转到下一个标签页</td>
<td style="text-align: center;">Ctrl + Tab</td>
</tr>
<tr class="odd">
<td style="text-align: center;">跳转到上一个标签页</td>
<td style="text-align: center;">Ctrl + Shift + Tab</td>
</tr>
<tr class="even">
<td style="text-align: center;">关闭当前标签页</td>
<td style="text-align: center;">Ctrl + w</td>
</tr>
<tr class="odd">
<td style="text-align: center;">------</td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">打开当前标签页浏览记录中的上一个页面</td>
<td style="text-align: center;">alt ＋左箭头</td>
</tr>
<tr class="odd">
<td style="text-align: center;">打开当前标签页浏览记录中的下一个页面</td>
<td style="text-align: center;">alt ＋右箭头</td>
</tr>
<tr class="even">
<td style="text-align: center;">------</td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: center;">保存当前标签页为书签</td>
<td style="text-align: center;">Ctrl + d</td>
</tr>
<tr class="even">
<td style="text-align: center;">将所有打开的标签页以书签的形式保存在新文件夹中</td>
<td style="text-align: center;">Ctrl + Shift + d</td>
</tr>
<tr class="odd">
<td style="text-align: center;">------</td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;">跳转到与查找栏中搜索字词相匹配的下一条内容</td>
<td style="text-align: center;">Ctrl + g</td>
</tr>
<tr class="odd">
<td style="text-align: center;">跳转到与查找栏中搜索字词相匹配的上一条内容</td>
<td style="text-align: center;">Ctrl + Shift + g</td>
</tr>
<tr class="even">
<td style="text-align: center;">浏览下一个可点击项</td>
<td style="text-align: center;">Tab</td>
</tr>
<tr class="odd">
<td style="text-align: center;">浏览上一个可点击项</td>
<td style="text-align: center;">Shift + Tab</td>
</tr>
<tr class="even">
<td style="text-align: center;">选中浏览器地址栏</td>
<td style="text-align: center;">ctrl+L</td>
</tr>
</tbody>
</table>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>技术</tag>
        <tag>chrome</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-08-08-hexo新建page</title>
    <url>/2020/08/08/2020-08-08-hexo%E6%96%B0%E5%BB%BApage/</url>
    <content><![CDATA[<h3 id="前言">前言</h3>
<p>在博客中需要一些个性化设置，添加一些page等 ，记录下我的操作</p>
<h3 id="添加page-界面">添加page 界面</h3>
<p>我想要添加一个“一句话感想”的page，于是可以这样操作</p>
<p>step 1.hexo新建新的page界面</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">hexo new page onesentence  <span class="comment"># onesentence 是新建page的名称 （最好是英文名）</span></span><br></pre></td></tr></tbody></table></figure>
<p>这时候在博客的source文件夹里会有一个onesentence的文件夹，并且里面生成了一个index.md文件，用于写一句话感想的内容</p>
<p>step 2.在主题的配置文件 _config.yml 文件中的 menu 中进行匹配，如下图，添加一个onesentence项，<code>/onesentence</code>表示挂接到上述的新建文件夹里，</p>
<p>在这里也可以设置图标，在fontawesome网站里找，我找了一个保龄球<span class="github-emoji" style="display:inline;vertical-align:middle;color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f3b3.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3b3.png?v8">🎳</span>的图标，和page主题没啥联系，就是看着顺眼 <span class="github-emoji" style="display:inline;vertical-align:middle;color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f606.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f606.png?v8">😆</span></p>
<p>此时<code>hexo s -g</code> 就可以看到已经有了这个界面，不过是英文的文件名，所以此时还要设置一下此文件名的中文名映射</p>
<p><img src="https://i.loli.net/2020/08/08/jiUOEvuzWnT7Kmp.png" alt="image-20200808192748174"></p>
<p>step 3. 打开**themes*，我用的是zh-CN，打开此文件，在menu下添加<code>onesentence: 一句话</code>，即可完成中文映射，</p>
<p>此时 hexo s -g ,就可以在本地服务器的侧边栏部分看到新添加的“一句话”page</p>
<p><img src="https://i.loli.net/2020/08/08/cCymB2KXMh1OPN5.png" alt="image-20200808193652117"></p>
<p><img src="https://i.loli.net/2020/08/08/ZrQkcR8IPspHMdi.png" alt="image-20200808194045722" style="zoom: 67%;"></p>
<p>step 4. 编辑“一句话”页面下的md文件，部署就能看到内容</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>next</category>
      </categories>
      <tags>
        <tag>next</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-08-08-next缺少custom.styl的问题</title>
    <url>/2020/08/08/2020-08-08-%E7%BC%BA%E5%B0%91custom-styl%E7%9A%84%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h3 id="前言">前言</h3>
<p>在 next7.x 版本中没有custom.styl文件。如果我们想要在博客中添加自己的css样式，可以在此文件中添加，下面介绍一下</p>
<h3 id="操作">操作</h3>
<p>step1 ：添加custom.styl文件</p>
<p>文件路径：<code>~\themes\next\source\css</code> ,添加<code>_custom</code>文件夹。然后在<code>_custom</code>中创建<code>custom.styl</code>文件。我们自己的样式就可以在此文件中添加</p>
<p>step2： 添加引用</p>
<p>在<code>~\themes\next\source\css</code>中的<code>main.styl</code>文件末尾加入引用即可</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">//My Layer</span><br><span class="line">@import "_custom/custom.styl";</span><br></pre></td></tr></tbody></table></figure>
<p>step3： 添加样式</p>
<p>用vscode打开<code>custom.styl</code>，博客背景以及前页的不透明度等等，就可以更换样式了。</p>
<p>对于网页的组件，F12打开调试界面，就可以知道每个组件的名称等信息，便于更改样式</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>next</category>
      </categories>
      <tags>
        <tag>故障排除</tag>
        <tag>next</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-08-08-vim常见操作</title>
    <url>/2020/08/08/2020-08-08-vim%E5%B8%B8%E8%A7%81%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<h3 id="常规操作">常规操作</h3>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">vim a.txt  <span class="comment"># 创建a.txt文件并进入编辑状态 。 如果a.txt 已经存在，则直接进入编辑状态</span></span><br><span class="line"></span><br><span class="line"><span class="number">1.</span> 按下i键，下端显示 –INSERT–。可以进行插入，输入文本 </span><br><span class="line"></span><br><span class="line"><span class="number">2.</span> 输入了之后 按Esc键退出编辑状态</span><br><span class="line"></span><br><span class="line"><span class="number">3.</span> 键入 :wq! 强制保存文件并退出  <span class="comment"># !是强制执行，注：有些文件设置了只读，一般不是修改文件的，但是如果你是`							文件的owner或者root的话，通过wq!还是能保存文件退出。:wq不可以</span></span><br><span class="line"></span><br><span class="line">   :w 在编辑的过程中保存文件,相当于word中的ctrl+s    </span><br><span class="line"></span><br><span class="line">   :wq 保存文件并退出 <span class="comment">#一般使用这个命令退出</span></span><br></pre></td></tr></tbody></table></figure>
<blockquote>
<p>注：以<strong><code>:</code></strong>和<strong><code>/</code></strong>开头的命令都有历史纪录，可以首先键入:或/然后按<strong>上下箭头</strong>来选择某个历史命令</p>
</blockquote>
<h3 id="vim模式">Vim模式</h3>
<p>(都是在英文输入环境下操作)</p>
<ul>
<li><strong>Normal</strong> 模式：进入Vim后的一般模式。</li>
<li><strong>Insert</strong> 模式：按下<code>i</code>键后进入插入模式，可以修改文档。</li>
<li><strong>Visual</strong> 模式：按下<code>v</code>键后进入选择模式，可以选择文档内容。</li>
</ul>
<h3 id="vim打开和切换文件">Vim打开和切换文件</h3>
<ul>
<li><code>:ls</code>显示打开的文件，可以使用<code>:bn</code>在文件间切换( n也可以换成<code>:ls</code>里给出的文件序号 )。</li>
<li>在终端<code>vim -o file1 file2 ...</code>可以打开多个文件(横向分隔屏幕)。</li>
<li>终端<code>vim -O file1 file2 ...</code>可以打开多个文件(纵向分隔屏幕)。 <span class="github-emoji" style="display:inline;vertical-align:middle;color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8">⭐</span></li>
<li><code>Ctrl</code>+<code>w</code>+<code>方向键</code>在窗口间切换光标</li>
</ul>
<h3 id="vim退出">Vim退出</h3>
<ul>
<li><p><code>:q</code>：退出。</p></li>
<li><p><code>:q!</code>：强制退出，放弃所有修改。</p></li>
<li><p><code>:wq</code>：保存修改并退出。<span class="github-emoji" style="display:inline;vertical-align:middle;color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8">⭐</span></p></li>
</ul>
<h3 id="常用快捷键">常用快捷键</h3>
<ul>
<li><p><code>gg</code>到文档首行，<code>G</code>（shift+g）到文档结尾。</p></li>
<li><p><code>pageUp</code>下一页，<code>pageDown</code>上一页。</p></li>
<li><p><code>ctrl + d</code> 向下翻半页(down)， <code>ctrl + u</code> 向上翻半页(up) <span class="github-emoji" style="display:inline;vertical-align:middle;color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8">⭐</span></p></li>
<li><p><code>H</code>将光标移动到屏幕首行，<code>M</code>将光标移动到屏幕中间行，<code>L</code>将光标移动到屏幕最后一行。</p></li>
<li><p><code>q:</code>显示<strong>命令行历史记录</strong>（显示开头为:的历史命令行）窗口，可以选择命令行执行。若是<code>q/</code>,则会显示开头为/的历史命令行</p></li>
<li><p><code>u</code> 撤销 (undo) <span class="github-emoji" style="display:inline;vertical-align:middle;color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8">⭐</span></p></li>
<li><p><code>w</code> 下一个单词 word</p></li>
<li><p><code>b</code> 前一个单词 behind</p></li>
<li><p><code>e</code> 本单词末尾 end</p></li>
<li><p><code>:set nu</code> 显示行号 (number ) 　</p></li>
<li><p><code>:set nonu</code> 隐藏行号 ( number)</p></li>
<li><p><code>:98</code>跳转到第98行。</p></li>
<li><p><code>:5,10d</code> //回车后，第5~10行被删除</p></li>
<li><p><code>:5,$d</code> //回车后，第5~最后一行被删除</p></li>
<li><p><code>:5,10y</code> //回车后，第5~10行被复制</p></li>
</ul>
<h3 id="复制粘贴">复制粘贴</h3>
<ul>
<li>在<strong>Visual</strong>模式下选择文档内容后按<code>y</code>键，复制被选择内容。主要用于<strong>多行文字</strong>（复制完之后vim自动退出Visual模式）</li>
<li>在<strong>Visual</strong>模式下选择文档内容后按<code>d</code>删除</li>
<li>按<code>p</code>键粘贴，注意粘贴从<strong>紧跟光标后的那个字符</strong>之后才开始。（不需要进入Visual模式）</li>
<li><code>yy</code>复制当前行，<code>dd</code>删除(剪贴)当前行。 用于<strong>一行文字</strong></li>
<li><code>:5,10y</code> //回车后，第5~10行被复制</li>
</ul>
<p>如果在vim外的其它文件里复制内容到vim里，则无法使用<code>p</code>进行粘贴，此时右键粘贴即可（无需进入inset模式）</p>
<h3 id="查找">查找</h3>
<ul>
<li>在<strong>Normal</strong>模式下，按<code>/</code>进入查找模式，输入<code>/word</code>后回车，高亮显示所有文档<code>word</code>，按<code>n</code>跳到下一个<code>word</code>,按<code>N</code>跳到上一个。（默认大小写敏感）</li>
<li>若输入<code>/word\c</code>代表大小写不敏感查找，<code>\C</code>代表大小写敏感。</li>
<li>在<strong>Normal</strong>模式下按<code>q</code>+<code>/</code>显示<strong>查找历史记录</strong>窗口。</li>
<li>如果一个词很长，键入麻烦，可以将光标移动到该词上，按<code>*</code>键即可以该单词进行搜索，相当于/搜索。</li>
</ul>
<p><img src="https://i.loli.net/2020/08/11/GZszjJB9uIUMFTq.png" alt="img"></p>
<h3 id="参考">参考</h3>
<blockquote>
<p>vim的常用操作 https://www.cnblogs.com/doseoer/p/6241443.html</p>
</blockquote>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>vim</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-08-07-transformerXL解读</title>
    <url>/2020/08/07/2020-08-07-transformerXL%E8%A7%A3%E8%AF%BB/</url>
    <content><![CDATA[<p>Transformer最大的问题：在语言建模时的设置受到固定长度上下文的限制。</p>
<p>本文提出的Transformer-XL，使学习不再仅仅依赖于定长，且不破坏时间的相关性。</p>
<p>Transformer-XL包含segment-level 循环机制和positional编码框架。不仅可以捕捉长时依赖，还可以解决上下文断片问题 fragmentation problem。可以学到比RNNs长80%的依赖，比vanilla Transformers长450%。在长短序列上都取得了更好的结果。与vanilla Transformer相比，Transformer-XL的另一个优势是它可以被用于单词级和字符级的语言建模。</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
  </entry>
  <entry>
    <title>2020-08-06-pytorch函数之nn.Linear</title>
    <url>/2020/08/06/2020-08-06-pytorch%E5%87%BD%E6%95%B0%E4%B9%8BLinear/</url>
    <content><![CDATA[<h1 id="section"></h1>
<p>class torch.nn.Linear（in_features，out_features，bias = True ）</p>
<p>对传入数据应用线性变换：y = A x+ b</p>
<p>参数：</p>
<p>in_features - 每个输入样本的大小</p>
<p>out_features - 每个输出样本的大小</p>
<p>bias - 如果设置为False，则图层不会学习附加偏差。默认值：True</p>
<p>代码：</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">m = nn.Linear(20, 30)</span><br><span class="line"></span><br><span class="line">input = autograd.Variable(torch.randn(128, 20))</span><br><span class="line"></span><br><span class="line">output = m(input)</span><br><span class="line"></span><br><span class="line">print(output.size())</span><br></pre></td></tr></tbody></table></figure>
<p>输出：</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">torch.Size([128, 30])</span><br></pre></td></tr></tbody></table></figure>
<p>分析:</p>
<p>output.size()=矩阵size(128,20)*矩阵size（20,30）=(128,30)</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-08-05-BPE算法</title>
    <url>/2020/08/05/2020-08-05-%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<h3 id="总说"><strong>总说</strong></h3>
<p>BPE，（byte pair encoder）字节对编码，也可以叫做digram coding双字母组合编码，<code>主要目的是为了数据压缩</code>，算法描述为<code>字符串里频率最常见的一对字符被一个没有在这个字符中出现的字符代替的层层迭代过程</code>。具体在下面描述。</p>
<h3 id="算法">算法</h3>
<ol type="1">
<li>准备足够大的训练语料</li>
<li>确定期望的<strong>subword词表大小</strong></li>
<li>将单词拆分为字符序列并在<strong>末尾添加后缀“ &lt;/ w&gt;”</strong>，统计单词频率。 本阶段的subword的粒度是字符。 例如，“ low”的频率为5，那么我们将其改写为“ l o w &lt;/ w&gt;”：5</li>
<li>统计每一个连续字节对的出现频率，选择最高频者合并成新的subword</li>
<li>重复第4步直到达到第2步设定的subword词表大小或下一个最高频的字节对出现频率为1</li>
</ol>
<p>停止符""的意义在于表示subword是词后缀。举例来说："st"字词不加""可以出现在词首如"st ar"，加了""表明改字词位于词尾，如"wide st"，二者意义截然不同。</p>
<p>每次合并后词表可能出现3种变化：</p>
<ul>
<li>+1，表明加入合并后的新字词，同时原来的2个子词还保留（2个字词不是完全同时连续出现）</li>
<li>+0，表明加入合并后的新字词，同时原来的2个子词中一个保留，一个被消解（一个字词完全随着另一个字词的出现而紧跟着出现）</li>
<li>-1，表明加入合并后的新字词，同时原来的2个子词都被消解（2个字词同时连续出现）</li>
</ul>
<p>实际上，随着合并的次数增加，词表大小通常先增加后减小。</p>
<h4 id="例子1"><strong>例子1</strong></h4>
<p>输入：</p>
<figure class="highlight"><table><tbody><tr><td class="code"><pre><span class="line">{'l o w &lt;/w&gt;': 5, 'l o w e r &lt;/w&gt;': 2, 'n e w e s t &lt;/w&gt;': 6, 'w i d e s t &lt;/w&gt;': 3}</span><br></pre></td></tr></tbody></table></figure>
<p>Iter 1, 最高频连续字节对"e"和"s"出现了6+3=9次，合并成"es"。输出：</p>
<figure class="highlight"><table><tbody><tr><td class="code"><pre><span class="line">{'l o w &lt;/w&gt;': 5, 'l o w e r &lt;/w&gt;': 2, 'n e w es t &lt;/w&gt;': 6, 'w i d es t &lt;/w&gt;': 3}</span><br></pre></td></tr></tbody></table></figure>
<p>Iter 2, 最高频连续字节对"es"和"t"出现了6+3=9次, 合并成"est"。输出：</p>
<figure class="highlight"><table><tbody><tr><td class="code"><pre><span class="line">{'l o w &lt;/w&gt;': 5, 'l o w e r &lt;/w&gt;': 2, 'n e w est &lt;/w&gt;': 6, 'w i d est &lt;/w&gt;': 3}</span><br></pre></td></tr></tbody></table></figure>
<p>Iter 3, 以此类推，最高频连续字节对为"est"和"" 输出：</p>
<figure class="highlight"><table><tbody><tr><td class="code"><pre><span class="line">{'l o w &lt;/w&gt;': 5, 'l o w e r &lt;/w&gt;': 2, 'n e w est&lt;/w&gt;': 6, 'w i d est&lt;/w&gt;': 3}</span><br></pre></td></tr></tbody></table></figure>
<p>……</p>
<p>Iter n, 继续迭代<strong>直到达到预设的subword词表大小或下一个最高频的字节对出现频率为1</strong>。</p>
<h3 id="bpe实现">BPE实现</h3>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re, collections</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_stats</span><span class="params">(vocab)</span>:</span></span><br><span class="line">    pairs = collections.defaultdict(int)</span><br><span class="line">    <span class="keyword">for</span> word, freq <span class="keyword">in</span> vocab.items():</span><br><span class="line">        symbols = word.split()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(symbols)<span class="number">-1</span>):</span><br><span class="line">            pairs[symbols[i],symbols[i+<span class="number">1</span>]] += freq</span><br><span class="line">    <span class="keyword">return</span> pairs</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge_vocab</span><span class="params">(pair, v_in)</span>:</span></span><br><span class="line">    v_out = {}</span><br><span class="line">    bigram = re.escape(<span class="string">' '</span>.join(pair))</span><br><span class="line">    p = re.compile(<span class="string">r'(?&lt;!\S)'</span> + bigram + <span class="string">r'(?!\S)'</span>)</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> v_in:</span><br><span class="line">        w_out = p.sub(<span class="string">''</span>.join(pair), word)</span><br><span class="line">        v_out[w_out] = v_in[word]</span><br><span class="line">    <span class="keyword">return</span> v_out</span><br><span class="line"></span><br><span class="line">vocab = {<span class="string">'l o w &lt;/w&gt;'</span>: <span class="number">5</span>, <span class="string">'l o w e r &lt;/w&gt;'</span>: <span class="number">2</span>, <span class="string">'n e w e s t &lt;/w&gt;'</span>: <span class="number">6</span>, <span class="string">'w i d e s t &lt;/w&gt;'</span>: <span class="number">3</span>}</span><br><span class="line">num_merges = <span class="number">1000</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_merges):</span><br><span class="line">    pairs = get_stats(vocab)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> pairs:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    best = max(pairs, key=pairs.get)</span><br><span class="line">    vocab = merge_vocab(best, vocab)</span><br><span class="line">    print(best)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print output</span></span><br><span class="line"><span class="comment"># ('e', 's')</span></span><br><span class="line"><span class="comment"># ('es', 't')</span></span><br><span class="line"><span class="comment"># ('est', '&lt;/w&gt;')</span></span><br><span class="line"><span class="comment"># ('l', 'o')</span></span><br><span class="line"><span class="comment"># ('lo', 'w')</span></span><br><span class="line"><span class="comment"># ('n', 'e')</span></span><br><span class="line"><span class="comment"># ('ne', 'w')</span></span><br><span class="line"><span class="comment"># ('new', 'est&lt;/w&gt;')</span></span><br><span class="line"><span class="comment"># ('low', '&lt;/w&gt;')</span></span><br><span class="line"><span class="comment"># ('w', 'i')</span></span><br><span class="line"><span class="comment"># ('wi', 'd')</span></span><br><span class="line"><span class="comment"># ('wid', 'est&lt;/w&gt;')</span></span><br><span class="line"><span class="comment"># ('low', 'e')</span></span><br><span class="line"><span class="comment"># ('lowe', 'r')</span></span><br><span class="line"><span class="comment"># ('lower', '&lt;/w&gt;')</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="编码和解码">编码和解码</h3>
<ul>
<li><h4 id="编码">编码</h4></li>
</ul>
<p>在之前的算法中，我们已经得到了<strong>subword词表</strong>，<strong>对该词表按照子词长度由大到小排序</strong>。编码时，<strong>对于每个单词，遍历排好序的子词词表寻找是否有token是当前单词的子字符串，如果有，则该token是表示单词的tokens之一</strong>。</p>
<p>我们从最长的token迭代到最短的token，尝试将每个单词中的子字符串替换为token。 最终，我们将迭代所有tokens，并将所有子字符串替换为tokens。 如果仍然有子字符串没被替换但所有token都已迭代完毕，则将剩余的子词替换为特殊token，如<unk>。</unk></p>
<h4 id="例子2">例子2</h4>
<p>用得到subword词表去表示含有多个单词的句子</p>
<figure class="highlight"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 给定单词序列</span></span><br><span class="line">[“the&lt;/w&gt;”, “highest&lt;/w&gt;”, “mountain&lt;/w&gt;”]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设已有排好序的subword词表</span></span><br><span class="line">[“errrr&lt;/w&gt;”, “tain&lt;/w&gt;”, “moun”, “est&lt;/w&gt;”, “high”, “the&lt;/w&gt;”, “a&lt;/w&gt;”]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 迭代结果</span></span><br><span class="line">"the&lt;/w&gt;" -&gt; ["the&lt;/w&gt;"]</span><br><span class="line">"highest&lt;/w&gt;" -&gt; ["high", "est&lt;/w&gt;"]</span><br><span class="line">"mountain&lt;/w&gt;" -&gt; ["moun", "tain&lt;/w&gt;"]</span><br></pre></td></tr></tbody></table></figure>
<p>编码的计算量很大。 在实践中，我们可以pre-tokenize所有单词，并在词典中保存单词tokenize的结果。 如果我们看到字典中不存在的未知单词。 我们应用上述编码方法对单词进行tokenize，然后将新单词的tokenization添加到字典中备用。</p>
<ul>
<li><h4 id="解码">解码</h4></li>
</ul>
<p><strong>将所有的tokens拼在一起</strong>。</p>
<figure class="highlight"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 编码序列</span></span><br><span class="line">[“the&lt;/w&gt;”, “high”, “est&lt;/w&gt;”, “moun”, “tain&lt;/w&gt;”]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解码序列</span></span><br><span class="line">“the&lt;/w&gt; highest&lt;/w&gt; mountain&lt;/w&gt;”</span><br></pre></td></tr></tbody></table></figure>
<h4 id="例子3">例子3</h4>
<p>比如我们想编码：</p>
<p>aaabdaaabac</p>
<p>我们会发现这里的aa出现的词数最高（我们这里只看两个字符的频率），那么用这里没有的字符Z来替代aa：</p>
<p>ZabdZabac</p>
<p>Z=aa</p>
<p>此时，又发现ab出现的频率最高，那么同样的，Y来代替ab：</p>
<p>ZYdZYac</p>
<p>Y=ab</p>
<p>Z=aa</p>
<p>同样的，ZY出现的频率大，我们用X来替代ZY：</p>
<p>XdXac</p>
<p>X=ZY</p>
<p>Y=ab</p>
<p>Z=aa</p>
<p>最后，连续两个字符的频率都为1了，也就结束了。就是这么简单。</p>
<p>解码的时候，就按照相反的顺序更新替换即可。</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-08-04-博客优化以及问题解决</title>
    <url>/2020/08/04/2020-08-04-%E5%8D%9A%E5%AE%A2%E4%BC%98%E5%8C%96%E4%BB%A5%E5%8F%8A%E8%A7%A3%E5%86%B3%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h3 id="修改git-bash的默认打开工作路径">修改Git Bash的默认打开工作路径</h3>
<p>我每次想在我的博客文件夹里进入git bash，必须要打开文件夹才能进入，操作繁琐，于是在桌面建立git bash 快捷方式，并将git bash 的默认打开路径更改为我的博客文件夹下，这样点击图标，即能进入本地git仓库</p>
<p>1.找到git bash，右键属性，可以看到目标栏及起始位置栏。</p>
<p><img src="https://i.loli.net/2020/08/04/tPL1uzsVApn5FvC.png" alt="img" style="zoom: 80%;"></p>
<p>将目标栏中的 --cd-to-home 去掉；将起始位置中填写为本地git仓库的路径，即可完成操作。如下图所示，博客文件夹位置在<code>E:\myBlog</code></p>
<p><img src="https://i.loli.net/2020/08/04/Vw7Kg3U2OIZQ6R9.png" alt="image-20200804181206322" style="zoom: 50%;"></p>
<p>注： 若在文件夹里进入 git bash，则然后按下<code>shift+F10</code> （激活右键菜单栏），再按<code>s</code>跳转到git bash，最后按下<code>enter</code>即可</p>
<h3 id="博客打开网站和更新不完全">博客打开网站和更新不完全</h3>
<p>在这几天在本地文件夹更新完配置文件对博客进行个性化设置时，使用<code>localhost:4000</code>访问本地blog可以正常显示更改后的样式，但是在登录网站域名就会出现不一致的现象，有时会响应速度慢，延时高，甚至连接超时。</p>
<p>在整个过程中一直没发现问题，因为本地localhost和网站不一致就不能理解。后来才发现，我的hexo命令写错了。本应该是hexo clean ，我错写为hexo clear，导致不能轻触缓存，所以在网站中不能及时更新显示。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">hexo clean <span class="comment"># 清除缓存，网页正常情况下可以忽略此命令</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="博客无法连接">博客无法连接</h3>
<p>本地服务器可以正常显示，但是博客连接不上</p>
<p>如何可以ping 通，则代表不是域名方面的问题，应该就是服务器的问题，可能是部署在github上，所以会有点慢，后续准备买一个阿里云的服务器。</p>
<p>解决：</p>
<p>1.博客正在加载， 等一段时间刷新</p>
<p>2.如果还是不行，则清理chrome的cookie缓存再刷新即可， 可以解决问题，但是操作麻烦。</p>
<p><img src="https://i.loli.net/2020/08/10/Iwpk2yb5OAjuoKf.png" alt="image-20200810152929992"></p>
<p>3.清除特定网站下的缓存：</p>
<p>打开开发者工具（F12），选择 Network——Disable cache 。需要清除某网站缓存时 F12 打开开发者工具就会自动清除这个网站的缓存，而不必清除所有网站的缓存了。</p>
<p>4.如果在文章标题中使用了当天的日期，可能无法及时得到页面更新。因为Github使用了格林尼治标准时间，也就是UTC。中国是东八时区，UTC+8，对于hexo来说是一个未来的时间，所以新的Posts不会被渲染。</p>
<p>在hexo配置文件<code>_config.yml</code>中设置<code>timezone: Asia/Shanghai</code> (有效解决问题) <span class="github-emoji" style="display:inline;vertical-align:middle;color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8">⭐</span></p>
<p>参考</p>
<blockquote>
<p>博客无法更新post文章 https://www.jianshu.com/p/b73c28e77760</p>
</blockquote>
<h3 id="clone的时候无法clone-next的内容">clone的时候无法clone next的内容</h3>
<h4 id="问题">问题</h4>
<p>在使用git将myblog文件夹的博客内容push到github上之后，发现对于<code>./theme/next</code>中的内容无法上传</p>
<h4 id="原因">原因</h4>
<p>next当时是clone别人的仓库，在我的myblog文件夹里本身就有一个<code>.git</code>的隐藏文件，然而我在博客文件夹里又引用了next的git仓库，所以导致上述问题</p>
<blockquote>
<p>任意文件夹中，用 <code>git init</code> 命令初始化仓库，即可在此文件夹下创建 <code>.git</code> 文件夹（<code>.</code>打头为隐藏文件夹，所以平时可能看不到）。这个文件夹之外的部分叫做工作区（Working Directory），<code>.git</code> 文件夹我们称做 Git仓库 (Git Repository)。</p>
</blockquote>
<h4 id="解决">解决</h4>
<p>1.将themes/next这个文件里的.git文件删除，这样next文件就相当于是一个普通文件，可以上传到myblog仓库里。但是当next有大更新的时候不会提示</p>
<p>2.保留next 的.git，将next下的<code>_config.yml</code>和其它修改的文件单独复制出来（主要是<code>_config.yml</code>），备份一下到./source/文件夹下，将这些文件作为普通文件上传。这样next的.git不用删除，同时next文件夹下的内容不会上传。</p>
<h3 id="添加自启动项">添加自启动项</h3>
<p>添加开机自启动项，在文件管理器的路径栏输入</p>
<p><code>%USERPROFILE%\AppData\Roaming\Microsoft\Windows\Start Menu\Programs\Startup</code></p>
<p>然后将要添加的软件快捷方式复制到里面即可,如下：</p>
<p><strong><img src="https://i.loli.net/2020/08/31/dqOpCylYLNQ8EoV.png" alt="image-20200831170347078"></strong></p>
<h3 id="anaconda-prompt找不到">anaconda prompt找不到</h3>
<h4 id="操作">操作</h4>
<p>在开始栏里面找不到anaconda prompt的内容，自己在anaconda3文件夹里也没看到anaconda prompt的启动项。在powershell里使用conda，功能并不完整（如无法实现基本的<code>conda activate</code>）， 于是考虑将其添加到开始栏里，这样以后操作方便一些</p>
<p>step 1 :打开cmd, 进入到anaconda3的安装目录，下方是我的目录，于是操作如下：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> C:\Users\Administrator\anaconda3</span><br></pre></td></tr></tbody></table></figure>
<p><img src="E:\myBlog\source_posts\image-20200902003632825.png" alt="image-20200902003632825"></p>
<p>step 2 : 进入到Anaconda的安装目录后，输入：</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">python .\Lib\_nsis.py mkmenus</span><br></pre></td></tr></tbody></table></figure>
<p>通过看文件夹目录树可以发现<code>anaconda prompt</code> 应该存在于<code>.\Lib\_nsis.py</code>文件里，用python运行 ，添加到开始栏里</p>
<p>step 3 打开左下角的开始栏，可以发现出现了<code>anaconda prompt</code></p>
<h4 id="参考">参考</h4>
<blockquote>
<p><a href="https://www.py.cn/tools/anaconda/16426.html" target="_blank" rel="noopener">anaconda prompt找不到怎么解决？</a></p>
</blockquote>
<h4 id="参考-1">参考</h4>
<blockquote>
<p><a href="https://www.py.cn/tools/anaconda/16426.html" target="_blank" rel="noopener">anaconda prompt找不到怎么解决？</a></p>
</blockquote>
<h3 id="新建模板papershare以及预设置">新建模板papershare以及预设置</h3>
<p>因为平时读论文比较多，所以会在博客里面分享不少论文。我的论文分享会有一些习惯，比如会按照论文简介、背景、问题、解决、模型、实验、总结的思路去分享论文。如果每次新建博客都要重写每个小标题的话会很麻烦，于是就重新建一个模板，将这些预设置写到模板里。如下是步骤：</p>
<ol type="1">
<li><p>在<code>myBlog\scaffolds</code>文件里，新建一个md文件，名称为papershare.md，作为模板</p></li>
<li><p>打开文件，写入预设置，如下</p>
<p><img src="E:\myBlog\source\_posts\image-20201111152534109.png" alt="image-20201111152534109" style="zoom: 50%;"></p></li>
<li><p>新建文件，利用新建的papershare模板，这样就可以了</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">hexo new papershare 2020-11-11-论文分享</span><br></pre></td></tr></tbody></table></figure></li>
</ol>
<h4 id="注">注：</h4>
<p>在写头部配置的时候，有时会出现错误</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">can not read a block mapping entry; a multiline key may not be an implicit key at line 2, column 5:</span><br></pre></td></tr></tbody></table></figure>
<p>一般解决如下：</p>
<p>1.在每个“：”后面都要加上空格</p>
<p>2.尽量用英文标点符号</p>
<p>要严格按照yaml语法来写文章头部的配置才行。总之很玄学就对了</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>博客</category>
      </categories>
      <tags>
        <tag>故障排除</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-08-01-hexo+next个性化设置</title>
    <url>/2020/08/01/2020-08-10-hexo-next%E4%B8%AA%E6%80%A7%E5%8C%96%E8%AE%BE%E7%BD%AE/</url>
    <content><![CDATA[<p>一些基本的个性化设置可以参考其它博客，本文只记录在我完成</p>
<h3 id="修改文章底部的那个带号的标签">修改文章底部的那个带#号的标签</h3>
<p>在原本next自带的标签格式如下所示：</p>
<p><img src="https://i.loli.net/2020/08/10/GLtxuMpYd7WXmaV.png" alt="image-20200810180543537"></p>
<p>前面的<code>#</code>不太好看，在这里可以添加<code>font awesome</code>的<code>icon</code>，个性化标签显示</p>
<p>修改模板 <code>/themes/next/layout/_macro/post.swig</code>，搜索 <code>rel="tag"</code>，将<code>rel="tag"&gt;</code>换成<code>rel="tag"&lt;i class="fa fa-tag"&gt;&lt;/i&gt;</code> ，其中"fa fa-tag"可以根据<code>font awesome</code>里自己选择喜欢的<code>icon</code></p>
<p>因为在代码中不需要<code>tag_indicate</code>，所以可以将部分代码删去，如图中红框部分</p>
<p><img src="https://i.loli.net/2020/08/10/2zhlEdQjqyTcPRo.png" alt="image-20200810181951506"></p>
<p>个性化后如下所示：</p>
<p><img src="https://i.loli.net/2020/08/10/tuvBmHFRZ8JMk3S.png" alt="image-20200810180207065"></p>
<h3 id="hexo-文章加密">hexo 文章加密</h3>
<blockquote>
<p><a href="https://vic.kim/2019/05/27/Hexo文章加密/" target="_blank" rel="noopener">https://vic.kim/2019/05/27/Hexo%E6%96%87%E7%AB%A0%E5%8A%A0%E5%AF%86/</a></p>
</blockquote>
<h3 id="在每篇文章末尾添加本文结束标记">在每篇文章末尾添加“本文结束”标记</h3>
<p>修改模板 <code>/themes/next/layout/_macro/post.swig</code>，在<code></code>代码行中添加<code>&lt;div style="text-align:center;color: #ccc;font-size:14px;"&gt;-------------本文结束&lt;i class="fa fa-paw"&gt;&lt;/i&gt;感谢您的阅读-------------&lt;/div&gt;</code>即可完成设置，如下所示，红框内是添加内容</p>
<p><img src="https://i.loli.net/2020/08/10/TQEoIcbKLGaC5dr.png" alt="image-20200810182801218"></p>
<p>个性化如下所示：</p>
<p><img src="https://i.loli.net/2020/08/10/dCvbzpGBhs6excl.png" alt="image-20200810183022935"></p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
  </entry>
  <entry>
    <title>2020-08-01-teacher-foring以及解决</title>
    <url>/2020/08/01/2020-08-01-teacher-foring%E4%BB%A5%E5%8F%8A%E8%A7%A3%E5%86%B3/</url>
    <content><![CDATA[<p>https://zhuanlan.zhihu.com/p/93030328 链接</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
  </entry>
  <entry>
    <title>2020-07-30-pytorch使用手册</title>
    <url>/2020/07/30/2020-07-30-pytorch%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/</url>
    <content><![CDATA[<p>python中对于对象的拷贝分为浅拷贝(copy)和深拷贝(deepcopy)两种方式。其中浅拷贝由“=”完成。而深拷贝由copy模块中deepcopy()函数担任。</p>
<p>浅拷贝和深拷贝的区别是：浅拷贝只是将原对象在内存中引用地址拷贝过来了。让新的对象指向这个地址。而深拷贝是将这个对象的所有内容遍历拷贝过来了，相当于跟原来没关系了，所以如果你这时候修改原来对象的值跟他没关系了，不会随之更改。</p>
<h3 id="浅拷贝的使用">1.浅拷贝"="的使用</h3>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">#1.使用=复制不可变对象的值，以及复制以后修改其值后的变化。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">val1 = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">val2 = val1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(<span class="string">"val1 is :{0},val2 is :{1}"</span>.format(val1,val2))<span class="comment">#val1 is :1000,val2 is :1000</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(id(val1),id(val2))  <span class="comment">#34052192 34052192</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#这时候修改val1的值，尽管val2指向val1.但因为val1是不可变类型，修改其值，会重新给新值分配内存，然后指向他。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">val1 += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(val1,id(val1),val2,id(val2)) <span class="comment">#1001 10131616 1000 10131568  值不一样，内存地址也不一样了</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#1.使用=复制可变对象的值，以及复制以后修改其值后的变化。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ls1 =[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ls2 = ls1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(id(ls1),id(ls2)) <span class="comment">#43702792 43702792 直接使用=复制变量，内存地址一样，值也一样。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(ls1,ls2) <span class="comment">#[1, 2, 3, 4] [1, 2, 3, 4]直接使用=复制变量，内存地址一样，值也一样。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#这时候修改可变对的值,因为其值可变，所以只需要在原内存地址上修改即可。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ls1.append(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(id(ls1),id(ls2)) <span class="comment">#可变对象修改其值，内存引用不变</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(ls1,ls2) <span class="comment">#[1, 2, 3, 4, 5] [1, 2, 3, 4, 5] 因为两个变量的内存指向一样，所以值也一样。</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="深拷贝copy.deepcopy函数">2.深拷贝：copy.deepcopy()函数</h3>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">#1.使用copy.deepcopy()拷贝不可变对象的值，以及复制以后修改其值后的变化。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">val1 = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">val2 = copy.deepcopy(val1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(<span class="string">"val1 is :{0},val2 is :{1}"</span>.format(val1,val2))<span class="comment">#val1 is :1000,val2 is :1000</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(id(val1),id(val2))  <span class="comment">#33717408 33717408 对于不可变对象，深度拷贝内存地址没有修改。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">val1 += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(val1,id(val1),val2,id(val2)) <span class="comment">#1001 33717904 1000 33717408</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#1.使用copy.deepcopy()复制可变对象的值，以及复制以后修改其值后的变化。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ls1 =[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ls2 = copy.deepcopy(ls1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(id(ls1),id(ls2)) <span class="comment">#34628472 34628712 注意对于可变对象深度拷贝后内存地址都修改了。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(ls1,ls2) <span class="comment">#[1, 2, 3, 4] [1, 2, 3, 4]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ls1.append(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(id(ls1),id(ls2)) <span class="comment">#34628472 34628712</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(ls1,ls2) <span class="comment">#[1, 2, 3, 4, 5] [1, 2, 3, 4] #注意这个时候ls2的值没有随着ls1修改。</span></span><br></pre></td></tr></tbody></table></figure>
<p>总结：其实对于浅拷贝和深拷贝来说，如果拷贝对象都是不可变对象的话，那么两者效果是一样的。如果是可变对象的话，“=”拷贝的方式，只是拷贝了内存中的地址引用，两个对象的地址引用一样，所以两个对象的值会随着一方的修改而修改。而对于deepcopy()来说，如果是可变对象的话，那么拷贝内容后新对象的内存地址也会重新分配，跟原来的内存地址不一样了。所以两者任意修改变量的内容不会对另一方造成影响。</p>
<h3 id="注意一个特殊的copy跟深浅拷贝都有区别慎用">3.注意一个特殊的copy(),跟深浅拷贝都有区别，慎用。</h3>
<ol type="1">
<li>copy.copy对于可变类型，会进行浅拷贝</li>
<li>copy.copy对于不可变类型，不会拷贝，仅仅是指向</li>
</ol>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="number">1.</span>使用copy()拷贝不可变对象</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">val1 = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">val2 = copy.copy(val1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(val1,val2)<span class="comment">##1000 1000</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(id(val1),id(val2))<span class="comment">#8551568 8551568</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="number">2.</span>使用copy（）拷贝可变对象</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ls1 =[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ls2 = copy.copy(ls1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ls1.append(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(ls1,ls2)  <span class="comment">#[1, 2, 3, 4, 5] [1, 2, 3, 4]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">看上去copy()函数效果和deepcopy()效果一样，可变对象拷贝后值也没有随着一个对象的修改而修改。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">然后真实情况真是这样嘛？请看下面的案例，同样是拷贝可变对象。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">origin = [<span class="number">1</span>, <span class="number">2</span>, [<span class="number">3</span>, <span class="number">4</span>]]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cop1 = copy.copy(origin)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cop2 = copy.deepcopy(origin)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">origin[<span class="number">2</span>][<span class="number">0</span>] = <span class="string">"hey!"</span>  <span class="comment">#修改数据源的值</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(cop1,cop2) <span class="comment">#[1, 2, ['hey!', 4]] [1, 2, [3, 4]]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">很显然这时copy（）函数拷贝的值随着原对象的值修改了，而deepcopy()的值没有随着原对象的值修改。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">主要是因为deepcopy会将复杂对象的每一层复制一个单独的个体出来对于copy（）函数要慎用，慎用。</span><br></pre></td></tr></tbody></table></figure>
<p>神经网络的典型处理如下所示：</p>
<ol type="1">
<li>定义可学习参数的网络结构（堆叠各层和层的设计）； 2. 数据集输入； 3. 对输入进行处理（由定义的网络层进行处理）,主要体现在网络的前向传播； 4. 计算loss ，由Loss层计算； 5. 反向传播求梯度； 6. 根据梯度改变参数值,最简单的实现方式（SGD）为: weight = weight - learning_rate * gradient</li>
</ol>
<p>下面是利用PyTorch定义深度网络层（Op）示例：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FeatureL2Norm</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        super(FeatureL2Norm, self).__init__()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, feature)</span>:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        epsilon = <span class="number">1e-6</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#        print(feature.size())</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#        print(torch.pow(torch.sum(torch.pow(feature,2),1)+epsilon,0.5).size())</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        norm = torch.pow(torch.sum(torch.pow(feature,<span class="number">2</span>),<span class="number">1</span>)+epsilon,<span class="number">0.5</span>).unsqueeze(<span class="number">1</span>).expand_as(feature)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> torch.div(feature,norm)</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FeatureRegression</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, output_dim=<span class="number">6</span>, use_cuda=True)</span>:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        super(FeatureRegression, self).__init__()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.conv = nn.Sequential(</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            nn.Conv2d(<span class="number">225</span>, <span class="number">128</span>, kernel_size=<span class="number">7</span>, padding=<span class="number">0</span>),</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            nn.BatchNorm2d(<span class="number">128</span>),</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            nn.Conv2d(<span class="number">128</span>, <span class="number">64</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">0</span>),</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.linear = nn.Linear(<span class="number">64</span> * <span class="number">5</span> * <span class="number">5</span>, output_dim)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> use_cuda:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            self.conv.cuda()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            self.linear.cuda()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        x = self.conv(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        x = self.linear(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></tbody></table></figure>
<p>由上例代码可以看到，不论是在定义网络结构还是定义网络层的操作（Op），均需要定义forward函数，下面看一下<a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">PyTorch官网</a>对PyTorch的forward方法的描述：</p>
<p><img src="https://img-blog.csdnimg.cn/20181114105426553.PNG" alt="img"></p>
<p>那么调用forward方法的具体流程是什么样的呢？<a href="https://blog.csdn.net/u012436149/article/details/70145598" target="_blank" rel="noopener">具体流程是这样的：</a></p>
<p>以一个Module为例： <strong>1. 调用module的call方法 2. module的call里面调用module的forward方法 3. forward里面如果碰到Module的子类，回到第1步，如果碰到的是Function的子类，继续往下 4. 调用Function的call方法 5. Function的call方法调用了Function的forward方法。 6. Function的forward返回值 7. module的forward返回值 8. 在module的call进行forward_hook操作，然后返回值。</strong></p>
<p>上述中“调用module的call方法”是指nn.Module 的__call__方法。定义__call__方法的类可以当作函数调用，具体参考Python的面向对象编程。也就是说，当把定义的网络模型model当作函数调用的时候就自动调用定义的网络模型的forward方法。nn.Module 的__call__方法部分源码如下所示：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, *input, **kwargs)</span>:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   result = self.forward(*input, **kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   <span class="keyword">for</span> hook <span class="keyword">in</span> self._forward_hooks.values():</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">       <span class="comment">#将注册的hook拿出来用</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">       hook_result = hook(self, input, result)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   ...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   <span class="keyword">return</span> result</span><br></pre></td></tr></tbody></table></figure>
<p>可以看到，当执行model(x)的时候，底层自动调用forward方法计算结果。具体示例如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LeNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        super(LeNet, self).__init__()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	layer1 = nn.Sequential()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	layer1.add_module(<span class="string">'conv1'</span>, nn.Conv(<span class="number">1</span>, <span class="number">6</span>, <span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	layer1.add_moudle(<span class="string">'pool1'</span>, nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	self.layer1 = layer1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	layer2 = nn.Sequential()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	layer2.add_module(<span class="string">'conv2'</span>, nn.Conv(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	layer2.add_moudle(<span class="string">'pool2'</span>, nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	self.layer2 = layer2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	layer3 = nn.Sequential()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	layer3.add_module(<span class="string">'fc1'</span>, nn.Linear(<span class="number">400</span>, <span class="number">120</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	layer3.add_moudle(<span class="string">'fc2'</span>, nn.Linear(<span class="number">120</span>, <span class="number">84</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	layer3.add_moudle(<span class="string">'fc3'</span>, nn.Linear(<span class="number">84</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	self.layer3 = layer3</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	x = self.layer1(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	x = self.layer2(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	x = self.layer3(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> x</span><br></pre></td></tr></tbody></table></figure>
<h3 id="model-lenet-y-modelx"><strong>model = LeNet() y = model(x)</strong></h3>
<p>如上则调用网络模型定义的forward方法。</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-07-30-小问题解决</title>
    <url>/2020/07/30/2020-07-30-%E5%B0%8F%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/</url>
    <content><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">
  <div class="hbe-input-container">
  <input type="password" id="hbePass" placeholder="" />
    <label for="hbePass">输入密码，查看文章</label>
    <div class="bottom-line"></div>
  </div>
  <script id="hbeData" type="hbeData" data-hmacdigest="2ed6df1630f2254a3ff91f603e97b1bc7a3826437a6125203239866012ef4f7a">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</script>
</div>
<script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
  </entry>
  <entry>
    <title>2020-07-28-pytorch安装</title>
    <url>/2020/07/28/2020-07-28-pytorch%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[<h3 id="anaconda安装配置">Anaconda安装配置</h3>
<p>由于墙的问题，用conda安装Pytorch过程中会连接失败，这是因为Anaconda.org的服务器在国外。在这里可以用清华TUNA镜像源，包含Anaconda仓库的镜像，将其加入conda的配置，配置如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">#添加Anaconda的TUNA镜像</span></span><br><span class="line"></span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span><br><span class="line"></span><br><span class="line"><span class="comment">#TUNA的help中镜像地址加有引号，需要去掉</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#设置搜索时显示通道地址</span></span><br><span class="line"></span><br><span class="line">conda config --set show_channel_urls yes</span><br></pre></td></tr></tbody></table></figure>
<p>执行完上述命令后，会生成~/.condarc文件，记录着对conda的配置，直接手动创建、编辑该文件是相同的效果。</p>
<p>https://www.jianshu.com/p/39819bcb889f</p>
<p>https://blog.csdn.net/weixin_39278265/article/details/84782550</p>
<p>镜像源内容理解一下？？？</p>
<p>conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/ 好用！</p>
<p>清华镜像源好像用不了了</p>
<p>为啥要使用镜像源？ 国内有啥比较靠谱的anaconda镜像源</p>
<p>https://www.py.cn/tools/anaconda/16426.html anaconda 问题</p>
<h3 id="anaconda-中的镜像源的基本操作">Anaconda 中的镜像源的基本操作</h3>
<h4 id="显示原来的镜像源">显示原来的镜像源</h4>
<figure class="highlight shell"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> conda config --show channels <span class="comment"># 在conda配置中只是显示channels项</span></span></span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">channels:</span><br><span class="line">  - https://pypi.doubanio.com/simple/ <span class="comment">#自己添加配置的镜像源</span></span><br><span class="line">  - defaults   <span class="comment">#默认的channel</span></span><br></pre></td></tr></tbody></table></figure>
<h4 id="添加新镜像源">添加新镜像源</h4>
<figure class="highlight shell"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ <span class="comment">#后面的是镜像源地址，来自清华的镜像源</span></span></span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">channels: <span class="comment">#添加后的channels项</span></span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span><br><span class="line">  - https://pypi.doubanio.com/simple/</span><br><span class="line">  - defaults</span><br></pre></td></tr></tbody></table></figure>
<h4 id="删除旧镜像源">删除旧镜像源</h4>
<figure class="highlight shell"><table><tbody><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> conda config --remove channels https://pypi.doubanio.com/simple/</span></span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight cpp"><table><tbody><tr><td class="code"><pre><span class="line">channels:</span><br><span class="line">  - https:<span class="comment">//mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span></span><br><span class="line">  - defaults</span><br></pre></td></tr></tbody></table></figure>
<p><img src="E:\myBlog\source_posts\image-20200901232602677.png" alt="image-20200901232602677"></p>
<h3 id="pytorch安装">Pytorch安装</h3>
<p>在这里的安装，我采用conda安装：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">conda install pytorch torchvision -c soumith</span><br></pre></td></tr></tbody></table></figure>
<h3 id="测试">测试</h3>
<p>进入python模式下，看能否导入torch成功：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">python</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch</span><br></pre></td></tr></tbody></table></figure>
<p>conda install pytorch=0.3.0</p>
<p>conda install torchvision==0.2.1</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-07-27-linux的session解析</title>
    <url>/2020/07/27/2020-07-27-linux%E7%9A%84session%E8%A7%A3%E6%9E%90/</url>
    <content><![CDATA[<h3 id="前言">前言</h3>
<p>我们使用ssh连接服务器时，ssh的窗口突然断开了连接，那么在服务器上跑的程序就也跟着断掉了，之前所有跑的数据也将丢失，这样将会浪费我们大量的时间。</p>
<h3 id="为什么ssh一旦断开我们的进程也将会被杀掉">为什么ssh一旦断开我们的进程也将会被杀掉？</h3>
<p>元凶：SIGHUP 信号</p>
<p>让我们来看看为什么关掉窗口/断开连接会使得正在运行的程序死掉。</p>
<p>在Linux/Unix中，有这样几个概念：</p>
<p>进程组（process group）：一个或多个进程的集合，每一个进程组有唯一一个进程组ID，即进程组长进程的ID。</p>
<p>会话期（session）：一个或多个进程组的集合，有唯一一个会话期首进程（session leader）。会话期ID为首进程的ID。</p>
<p>会话期可以有一个单独的控制终端（controlling terminal）。与控制终端连接的会话期首进程叫做控制进程（controlling process）。当前与终端交互的进程称为前台进程组。其余进程组称为后台进程组。</p>
<p>根据POSIX.1定义：</p>
<p>挂断信号（SIGHUP）默认的动作是终止程序。</p>
<p>当终端接口检测到网络连接断开，将挂断信号发送给控制进程（会话期首进程）。</p>
<p>如果会话期首进程终止，则该信号发送到该会话期前台进程组。</p>
<p>一个进程退出导致一个孤儿进程组中产生时，如果任意一个孤儿进程组进程处于STOP状态，发送SIGHUP和SIGCONT信号到该进程组中所有进程。</p>
<p>因此当网络断开或终端窗口关闭后，控制进程收到SIGHUP信号退出，会导致该会话期内其他进程退出。</p>
<p><strong>这里我认为我们的进程被杀掉也就是因为ssh与服务器之间的通信断掉了，这个通信断掉之后linux程序就默认将该连接下的所有进程都杀掉</strong></p>
<h3 id="session-是什么">session 是什么？</h3>
<p>我们常见的 Linux session 一般是指 shell session。Shell session 是终端中当前的状态，在终端中只能有一个 session。<code>当我们打开一个新的终端时，总会创建一个新的 shell session。</code></p>
<p>就进程间的关系来说，session 由一个或多个进程组组成。一般情况下，来自单个登录的所有进程都属于同一个 session。我们可以通过下图来理解进程、进程组和 session 之间的关系：</p>
<p><img src="https://img2018.cnblogs.com/blog/952033/202001/952033-20200103182042686-2100862807.png" alt="img"></p>
<p><code>会话是由会话中的第一个进程创建的，一般情况下是打开终端时创建的 shell 进程。</code>该进程也叫 session 的领头进程。Session 中领头进程的 PID 也就是 session 的 SID。我们可以通过下面的命令查看 SID：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">$ ps -o pid,ppid,pgid,sid,tty,comm</span><br></pre></td></tr></tbody></table></figure>
<p><img src="https://img2018.cnblogs.com/blog/952033/202001/952033-20200103182117962-99639442.png" alt="img"></p>
<p>Session 中的每个进程组被称为一个 job，有一个 job 会成为 session 的前台 job(foreground)，其它的 job 则是后台 job(background)。每个 session 连接一个控制终端(control terminal)，控制终端中的输入被发送给前台 job，从前台 job 产生的输出也被发送到控制终端上。同时由控制终端产生的信号，比如 ctrl + z 等都会传递给前台 job。</p>
<p>一般情况下 session 和终端是一对一的关系，当我们打开多个终端窗口时，实际上就创建了多个 session。</p>
<p><code>Session 的意义在于多个工作(job)在一个终端中运行，其中的一个为前台 job，它直接接收该终端的输入并把结果输出到该终端。其它的 job 则在后台运行。</code></p>
<h3 id="session-的诞生与消亡">session 的诞生与消亡</h3>
<p>通常，新的 session 由系统登录程序创建，session 中的领头进程是运行用户登录 shell 的进程。<code>新创建的每个进程都会属于一个进程组，当创建一个进程时，它和父进程在同一个进程组、session 中。</code></p>
<p>将进程放入不同 session 的惟一方法是使用 setsid 函数使其成为新 session 的领头进程。这还会将 session 领头进程放入一个新的进程组中。</p>
<p><code>当 session 中的所有进程都结束时 session 也就消亡了</code>。如下两种：</p>
<p>1.实际使用中比如网络断开了，session 肯定是要消亡的。</p>
<p>2.正常的消亡，比如让 session 的领头进程退出。</p>
<p>一般情况下 session 的领头进程是 shell 进程，如果它处于前台，我们可以使用 <code>exit 命令或者是 ctrl + d</code> 让它退出。或者我们可以直接通过 kill 命令杀死 session 的领头进程。这里面的原理是：当系统检测到挂断(hangup)条件时，内核中的驱动会将 SIGHUP 信号发送到整个 session。通常情况下，这会杀死 session 中的所有进程。</p>
<p>session 与终端的关系 如果 session 关联的是伪终端，这个伪终端本身就是随着 session 的建立而创建的，session 结束，那么这个伪终端也会被销毁。 如果 session 关联的是 tty1-6，tty 则不会被销毁。因为该终端设备是在系统初始化的时候创建的，并不是依赖该会话建立的，所以当 session 退出，tty 仍然存在。只是 init 系统在 session 结束后，会重启 getty 来监听这个 tty。</p>
<h3 id="nohup">nohup</h3>
<p><code>如果我们在 session 中执行了 nohup 等类似的命令，当 session 消亡时，相关的进程并不会随着 session 结束，原因是这些进程不再受 SIGHUP 信号的影响。</code>比如我们执行下面的命令：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">$ nohup sleep <span class="number">1000</span> &gt;/dev/null <span class="number">2</span>&gt;&amp;<span class="number">1</span> &amp;</span><br></pre></td></tr></tbody></table></figure>
<p><img src="https://img2018.cnblogs.com/blog/952033/202001/952033-20200103182343352-1366915632.png" alt="img"></p>
<p>此时 sleep 进程的 sid 和其它进程是相同的，还可以通过 pstree 命令看到进程间的父子关系：</p>
<p><img src="https://img2018.cnblogs.com/blog/952033/202001/952033-20200103182417115-817556079.png" alt="img"></p>
<p><code>如果我们退出当前 session 的领头进程(bash)，sleep 进程并不会退出，这样我们就可以放心的等待该进程运行结果了。</code> nohup 并不改变进程的 sid，同时也说明在这种情况中，虽然 session 的领头进程退出了，但是 session 依然没有被销毁(至少 sid 还在被引用)。重新建立连接，通过下面的命令查看 sleep 进程的信息，发现进程的 sid 依然是 7837：</p>
<p><img src="https://img2018.cnblogs.com/blog/952033/202001/952033-20200103182448160-376880623.png" alt="img"></p>
<p>但是<code>此时的 sleep 已经被系统的 1 号进程 systemd 收养了</code>：</p>
<p><img src="https://img2018.cnblogs.com/blog/952033/202001/952033-20200103182521953-1574746082.png" alt="img"></p>
<h3 id="参考">参考</h3>
<blockquote>
<p>https://www.cnblogs.com/sparkdev/p/12146305.html</p>
</blockquote>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-07-26-picgo上传失败原因</title>
    <url>/2020/07/26/2020-07-26-picgo%E4%B8%8A%E4%BC%A0%E5%A4%B1%E8%B4%A5%E5%8E%9F%E5%9B%A0/</url>
    <content><![CDATA[<h3 id="前言">前言</h3>
<p>在使用后PicGo上传图片至github时总是会显示上传失败，以下是我的解决方案经验，我的情况是可以上传图片，但是偶尔会失败</p>
<h3 id="case-1-检查服务及端口配置">case 1 检查服务及端口配置</h3>
<p>择相应的选项“设置server”，以下操作</p>
<p><img src="https://i.loli.net/2020/07/27/lf7Bw2jDUqgIxMk.png"></p>
<p>我们可以选择将开关先关闭，然后打开，确定后再重启软件，一般会成功。</p>
<p>或者修改端口号： 如修改为36688</p>
<p>记住，如果不行，那就直接关闭软件，然后等2分钟后，再打开picgo软件就可以上传成功了。</p>
<h3 id="case-2-查看日志">case 2 查看日志</h3>
<p>找到“设置日志文件”，然后打开日志文件，检查相应的日志，了解上传失败的原因。</p>
<p><img src="https://i.loli.net/2020/07/27/421CU9GzgKkauxt.png"></p>
<h3 id="参考">参考</h3>
<p>https://www.shopee6.com/web/web-tutorial/picgo-github-fail.html</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>杂</category>
      </categories>
      <tags>
        <tag>杂</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-07-26-tensorflow函数解析</title>
    <url>/2020/07/26/2020-07-26-tensorflow%E5%87%BD%E6%95%B0%E8%A7%A3%E6%9E%90/</url>
    <content><![CDATA[<p>tf.manul</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-07-26-numpy函数解析</title>
    <url>/2020/07/26/2020-07-26-numpy%E5%87%BD%E6%95%B0%E8%A7%A3%E6%9E%90/</url>
    <content><![CDATA[<h3 id="newaxis用法">newaxis用法</h3>
<p>newaxis表示增加一个新的坐标轴</p>
<ul>
<li>x[:, np.newaxis] ，放在后面，会给列上增加维度</li>
<li>x[np.newaxis, :] ，放在前面，会给行上增加维度</li>
</ul>
<p><strong>用途：</strong> 通常用它将一维的数据转换成一个矩阵，与代码后面的权重矩阵进行相乘。</p>
<h4 id="第一个程序">第一个程序</h4>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">a = np.array([1,2,3])</span><br><span class="line">print (a.shape,'\n',a)</span><br></pre></td></tr></tbody></table></figure>
<p>结果为：</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">(3,)</span><br><span class="line">[1 2 3]</span><br></pre></td></tr></tbody></table></figure>
<h4 id="第二个程序">第二个程序</h4>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">a = np.array([1,2,3])[:,np.newaxis]</span><br><span class="line">print (a.shape,'\n',a)</span><br></pre></td></tr></tbody></table></figure>
<p>结果为：</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">(3, 1)</span><br><span class="line">[[1]</span><br><span class="line">[2]</span><br><span class="line">[3]]</span><br></pre></td></tr></tbody></table></figure>
<p>和第一个程序相比，a的shape为（3，）现在为（3，1）变为二维数组了，之前为[1,2,3]，现在变为</p>
<p>[[1] [2] [3]]</p>
<h4 id="第三个程序">第三个程序</h4>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">a = np.array([1,2,3])[np.newaxis,:]</span><br><span class="line">print (a.shape,'\n',a)</span><br></pre></td></tr></tbody></table></figure>
<p>输出结果为：</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">(1, 3)</span><br><span class="line">[[1 2 3]]</span><br></pre></td></tr></tbody></table></figure>
<p>输出结果为：</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">(1, 3)</span><br><span class="line">[[1 2 3]]</span><br></pre></td></tr></tbody></table></figure>
<h4 id="总结">总结</h4>
<p>np.newaxis的作用就是在原来的数组上增加一个维度。[np.newaxis,:]这个地方np.newaxis放的位置有关，第二个程序放在[:,]的后面，相当于在原来的后面增加一个维度，所以变为(3,1)，而第三个则放在前面，则为(1,3)。<code>加到哪一维，那一维就为1</code></p>
<h3 id="concatenate用法">concatenate用法</h3>
<p>-用于进行数组拼接</p>
<p>函数定义：</p>
<p><code>numpy.concatenate</code>((a1, a2, ...), axis=0, out=None)</p>
<ul>
<li>axis=0: 合并行</li>
<li>axis=1: 合并列</li>
</ul>
<p>例子如下：</p>
<figure class="highlight csharp"><table><tbody><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; a=np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line"> &gt;&gt;&gt; b=np.array([[<span class="number">11</span>,<span class="number">21</span>,<span class="number">31</span>],[<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]])</span><br><span class="line"> <span class="meta"># 合并行</span></span><br><span class="line"> &gt;&gt;&gt; np.concatenate((a,b,c),axis=<span class="number">0</span>)  <span class="meta"># 默认情况下，axis=0可以不写</span></span><br><span class="line"> array([[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">  [<span class="meta"> 4,  5,  6</span>],</span><br><span class="line">  [<span class="meta">11, 21, 31</span>],</span><br><span class="line">  [<span class="meta"> 7,  8,  9</span>]])</span><br><span class="line"> <span class="meta"># 合并列</span></span><br><span class="line"> &gt;&gt;&gt; np.concatenate((a,b),axis=<span class="number">1</span>) </span><br><span class="line">  array([[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>, <span class="number">11</span>, <span class="number">21</span>, <span class="number">31</span>],</span><br><span class="line">  [<span class="meta"> 4,  5,  6,  7,  8,  9</span>]])</span><br></pre></td></tr></tbody></table></figure>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-07-26-python切片疑惑解析</title>
    <url>/2020/07/26/2020-07-26-python%E5%88%87%E7%89%87%E7%96%91%E6%83%91%E8%A7%A3%E6%9E%90/</url>
    <content><![CDATA[<p>Python中对于数组和列表进行切片操作是很频繁的，我主要简单总结一下常用集中索引化方式</p>
<h3 id="一维-nm--1-1">一维： [ : n]、[m : ] 、[-1]、[::-1]</h3>
<p><strong>[m : ]</strong> ：代表列表中的第m项到最后一项 （从0开始）</p>
<p><strong>[ : n]</strong> ：代表列表中的第0项到第n-1项 （含左不含右）</p>
<p><strong>[-1]：</strong>取最后一个元素</p>
<p><strong>[::-1]</strong>：取从后向前（相反）的元素 （倒序）</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">X = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>] )</span><br><span class="line">print(X.shape)</span><br><span class="line">print(<span class="string">'\n'</span>)</span><br><span class="line">print(X[<span class="number">3</span>:])</span><br><span class="line">print(X[:<span class="number">7</span>])</span><br><span class="line">print(X[::<span class="number">-1</span>][:<span class="number">3</span>]) <span class="comment"># 在进行了[::-1]之后得到倒序数组，再取[：3]</span></span><br></pre></td></tr></tbody></table></figure>
<p>结果如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">(<span class="number">8</span>,)</span><br><span class="line">[<span class="number">4</span> <span class="number">5</span> <span class="number">6</span> <span class="number">7</span> <span class="number">8</span>]</span><br><span class="line">[<span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span> <span class="number">7</span>]</span><br><span class="line">[<span class="number">8</span>,<span class="number">7</span>,<span class="number">6</span>]</span><br></pre></td></tr></tbody></table></figure>
<h3 id="二维-x0-x1-x-mn">二维： X[:,0] 、X[:,1] 、 X[:, m:n]</h3>
<p>X[:,0]是numpy中数组的一种写法，表示对一个二维数组，取该二维数组第一维中的所有数据，第二维中取第0个数据，直观来说</p>
<p>X[:,0]：取第二维（所有行）的第0个数据,就是<code>第0列</code></p>
<p>X[:,1] ：取第二维（所有行）的第1个数据，就是<code>第1列</code></p>
<p>X[:, m:n]，即取所有数据的<code>第m到n-1列数据，含左不含右</code></p>
<p>示例如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">X = np.array([[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>],[<span class="number">9</span>,<span class="number">10</span>,<span class="number">11</span>],[<span class="number">12</span>,<span class="number">13</span>,<span class="number">14</span>],[<span class="number">15</span>,<span class="number">16</span>,<span class="number">17</span>],[<span class="number">18</span>,<span class="number">19</span>,<span class="number">20</span>]])</span><br><span class="line">print(X.shape)</span><br><span class="line">print(<span class="string">'\n'</span>)</span><br><span class="line"><span class="keyword">print</span> (X[:,<span class="number">1</span>:<span class="number">3</span>])</span><br></pre></td></tr></tbody></table></figure>
<p>结果如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">(<span class="number">7</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">[[ <span class="number">1</span>  <span class="number">2</span>]</span><br><span class="line"> [ <span class="number">4</span>  <span class="number">5</span>]</span><br><span class="line"> [ <span class="number">7</span>  <span class="number">8</span>]</span><br><span class="line"> [<span class="number">10</span> <span class="number">11</span>]</span><br><span class="line"> [<span class="number">13</span> <span class="number">14</span>]</span><br><span class="line"> [<span class="number">16</span> <span class="number">17</span>]</span><br><span class="line"> [<span class="number">19</span> <span class="number">20</span>]]</span><br></pre></td></tr></tbody></table></figure>
<h3 id="三维-x0x1xmn">三维 X[:,:,0]、X[:,:,1]、X[:,:,m:n]</h3>
<p>类比于二维，原理相同</p>
<p>X[:,:,0]：取第三维矩阵中第0列的所有数据</p>
<p>X[:,:,1]：取第三维矩阵中第1列的所有数据</p>
<p>X[:,:,m:n]：取第三维矩阵中第m列到第n-1列的所有数据</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">注：shape（<span class="number">9</span>,<span class="number">5</span>,<span class="number">2</span>）指的是最外层有<span class="number">9</span>个括号，每个括号里嵌套<span class="number">5</span>个括号，在<span class="number">5</span>个括号里又每个有<span class="number">2</span>个元素</span><br><span class="line"></span><br><span class="line">判断的时候先先出数组的shape，根据shape进行判断</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># !usr/bin/env python</span></span><br><span class="line"><span class="comment"># encoding:utf-8</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">simple_test</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    简单的小实验</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    data_list = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>], [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>], [<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>], [<span class="number">6</span>, <span class="number">7</span>, <span class="number">9</span>], [<span class="number">0</span>, <span class="number">4</span>, <span class="number">7</span>], [<span class="number">4</span>, <span class="number">6</span>, <span class="number">0</span>],[<span class="number">2</span>, <span class="number">9</span>, <span class="number">1</span>], [<span class="number">5</span>, <span class="number">8</span>, <span class="number">7</span>], [<span class="number">9</span>, <span class="number">7</span>, <span class="number">8</span>], [<span class="number">3</span>, <span class="number">7</span>, <span class="number">9</span>]]</span><br><span class="line">    <span class="comment"># data_list.toarray()</span></span><br><span class="line">    data_list = np.array(data_list)</span><br><span class="line">    print(<span class="string">'X[:,0]结果输出为：'</span>)</span><br><span class="line">    print( data_list[:, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    print(  <span class="string">'X[:,1]结果输出为：'</span>)</span><br><span class="line">    print( data_list[:, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    print(<span class="string">'X[:,m:n]结果输出为：'</span>)</span><br><span class="line"></span><br><span class="line">    print( data_list[:, <span class="number">0</span>:<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    data_list = [[[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">7</span>, <span class="number">9</span>], [<span class="number">4</span>, <span class="number">0</span>]], [[<span class="number">1</span>, <span class="number">4</span>], [<span class="number">1</span>, <span class="number">5</span>], [<span class="number">3</span>, <span class="number">6</span>], [<span class="number">8</span>, <span class="number">9</span>], [<span class="number">5</span>, <span class="number">0</span>]],</span><br><span class="line">                 [[<span class="number">8</span>, <span class="number">2</span>], [<span class="number">1</span>, <span class="number">8</span>], [<span class="number">3</span>, <span class="number">5</span>], [<span class="number">7</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">6</span>]],</span><br><span class="line">                 [[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">5</span>], [<span class="number">7</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]], [[<span class="number">9</span>, <span class="number">2</span>], [<span class="number">1</span>, <span class="number">3</span>], [<span class="number">3</span>, <span class="number">5</span>], [<span class="number">7</span>, <span class="number">67</span>], [<span class="number">4</span>, <span class="number">4</span>]],</span><br><span class="line">                 [[<span class="number">8</span>, <span class="number">2</span>], [<span class="number">1</span>, <span class="number">9</span>], [<span class="number">3</span>, <span class="number">43</span>], [<span class="number">7</span>, <span class="number">3</span>], [<span class="number">43</span>, <span class="number">0</span>]],</span><br><span class="line">                 [[<span class="number">1</span>, <span class="number">22</span>], [<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">42</span>], [<span class="number">7</span>, <span class="number">29</span>], [<span class="number">4</span>, <span class="number">20</span>]], [[<span class="number">1</span>, <span class="number">5</span>], [<span class="number">1</span>, <span class="number">20</span>], [<span class="number">3</span>, <span class="number">24</span>], [<span class="number">17</span>, <span class="number">9</span>], [<span class="number">4</span>, <span class="number">10</span>]],</span><br><span class="line">                 [[<span class="number">11</span>, <span class="number">2</span>], [<span class="number">1</span>, <span class="number">110</span>], [<span class="number">3</span>, <span class="number">14</span>], [<span class="number">7</span>, <span class="number">4</span>], [<span class="number">4</span>, <span class="number">2</span>]]]</span><br><span class="line">    data_list = np.array(data_list)</span><br><span class="line">    print(data_list.shape)</span><br><span class="line">    print(<span class="string">'X[:,:,0]结果输出为：'</span>)</span><br><span class="line"></span><br><span class="line">    print( data_list[:, :, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    print(<span class="string">'X[:,:,1]结果输出为：'</span>)</span><br><span class="line"></span><br><span class="line">    print(data_list[:, :, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    print(<span class="string">'X[:,:,m:n]结果输出为：'</span>)</span><br><span class="line"></span><br><span class="line">    print( data_list[:, :, <span class="number">0</span>:<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    simple_test()</span><br></pre></td></tr></tbody></table></figure>
<p>部分结如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">X[:,:,<span class="number">0</span>]结果输出为：</span><br><span class="line">[[ <span class="number">1</span>  <span class="number">1</span>  <span class="number">3</span>  <span class="number">7</span>  <span class="number">4</span>]</span><br><span class="line"> [ <span class="number">1</span>  <span class="number">1</span>  <span class="number">3</span>  <span class="number">8</span>  <span class="number">5</span>]</span><br><span class="line"> [ <span class="number">8</span>  <span class="number">1</span>  <span class="number">3</span>  <span class="number">7</span>  <span class="number">4</span>]</span><br><span class="line"> [ <span class="number">1</span>  <span class="number">1</span>  <span class="number">3</span>  <span class="number">7</span>  <span class="number">7</span>]</span><br><span class="line"> [ <span class="number">9</span>  <span class="number">1</span>  <span class="number">3</span>  <span class="number">7</span>  <span class="number">4</span>]</span><br><span class="line"> [ <span class="number">8</span>  <span class="number">1</span>  <span class="number">3</span>  <span class="number">7</span> <span class="number">43</span>]</span><br><span class="line"> [ <span class="number">1</span>  <span class="number">1</span>  <span class="number">3</span>  <span class="number">7</span>  <span class="number">4</span>]</span><br><span class="line"> [ <span class="number">1</span>  <span class="number">1</span>  <span class="number">3</span> <span class="number">17</span>  <span class="number">4</span>]</span><br><span class="line"> [<span class="number">11</span>  <span class="number">1</span>  <span class="number">3</span>  <span class="number">7</span>  <span class="number">4</span>]]</span><br><span class="line">X[:,:,<span class="number">1</span>]结果输出为：</span><br><span class="line">[[  <span class="number">2</span>   <span class="number">0</span>   <span class="number">4</span>   <span class="number">9</span>   <span class="number">0</span>]</span><br><span class="line"> [  <span class="number">4</span>   <span class="number">5</span>   <span class="number">6</span>   <span class="number">9</span>   <span class="number">0</span>]</span><br><span class="line"> [  <span class="number">2</span>   <span class="number">8</span>   <span class="number">5</span>   <span class="number">3</span>   <span class="number">6</span>]</span><br><span class="line"> [  <span class="number">1</span>   <span class="number">2</span>   <span class="number">5</span>   <span class="number">6</span>   <span class="number">8</span>]</span><br><span class="line"> [  <span class="number">2</span>   <span class="number">3</span>   <span class="number">5</span>  <span class="number">67</span>   <span class="number">4</span>]</span><br><span class="line"> [  <span class="number">2</span>   <span class="number">9</span>  <span class="number">43</span>   <span class="number">3</span>   <span class="number">0</span>]</span><br><span class="line"> [ <span class="number">22</span>   <span class="number">2</span>  <span class="number">42</span>  <span class="number">29</span>  <span class="number">20</span>]</span><br><span class="line"> [  <span class="number">5</span>  <span class="number">20</span>  <span class="number">24</span>   <span class="number">9</span>  <span class="number">10</span>]</span><br><span class="line"> [  <span class="number">2</span> <span class="number">110</span>  <span class="number">14</span>   <span class="number">4</span>   <span class="number">2</span>]]</span><br></pre></td></tr></tbody></table></figure>
<h3 id="startendstep">[start：end：step]</h3>
<p>start:开始索引；end:结束索引；step:步长（步长为正时，从左到右索引，正序取值；步长为负时，从右到左索引，倒序取值）</p>
<p>[::2] 步长为2</p>
<p>[3:7:2] 第3个元素开始，第6个元素结束，步长为2</p>
<p>参考</p>
<p>https://blog.csdn.net/Together_CZ/article/details/79593952</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-07-25-transformer代码</title>
    <url>/2020/07/25/2020-07-25-transformer%E4%BB%A3%E7%A0%81/</url>
    <content><![CDATA[<h1 id="tensorflow-2.0-教程-transformer">TensorFlow 2.0 教程-Transformer</h1>
<p>这里我们将实现一个Transformer模型，将葡萄牙语翻译为英语。Transformer的核心思想是self-attention--通过关注序列不同位置的内容获取句子的表示。</p>
<p>Transformer的一些优点：</p>
<ul>
<li>不受限于数据的时间/空间关系</li>
<li>可以并行计算</li>
<li>远距离token的相互影响不需要通过很长的时间步或很深的卷积层</li>
<li>可以学习远程依赖</li>
</ul>
<p>Transformer的缺点：</p>
<ul>
<li>对于时间序列，输出需要根据整个历史，而不是当前状态和输入，可能造成效率较低</li>
<li>如果想要获取时间空间信息，需要额外的位置编码</li>
</ul>
<p>In [1]:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">from __future__ import absolute_import, division, print_function, unicode_literals</span><br><span class="line"># 安装tfds pip install tfds-nightly==1.0.2.dev201904090105</span><br><span class="line">import tensorflow_datasets as tfds</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras.layers as layers</span><br><span class="line"></span><br><span class="line">import time</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">print(tf.__version__)</span><br><span class="line">2.0.0-alpha0</span><br></pre></td></tr></tbody></table></figure>
<h2 id="数据输入pipeline">1.数据输入pipeline</h2>
<p>我们将使用到Portugese-English翻译数据集。</p>
<p>该数据集包含大约50000个训练样例，1100个验证示例和2000个测试示例。</p>
<p>tfds.load :加载数据集.1.下载数据并将其另存为tfrecord文件. 2.加载tfrecord并创建tf.data.Dataset. metadata: 元数据 用于描述数据的数据，比如数码照片的EXIF信息，它就是一种用来描述数码图片的元数据</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,</span><br><span class="line">                              as_supervised=True)</span><br></pre></td></tr></tbody></table></figure>
<p>将数据转化为subwords格式</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">train_examples, val_examples = examples['train'], examples['validation']</span><br></pre></td></tr></tbody></table></figure>
<p>把单词用英语和葡萄牙语分别进行编码</p>
<p>2**13： 2的3次方大小的词汇表大小</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(</span><br><span class="line">(en.numpy() for pt, en in train_examples), target_vocab_size=2**13)</span><br><span class="line">tokenizer_pt = tfds.features.text.SubwordTextEncoder.build_from_corpus(</span><br><span class="line">(pt.numpy() for pt, en in train_examples), target_vocab_size=2**13)</span><br></pre></td></tr></tbody></table></figure>
<p>token转化测试</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">sample_str = 'hello world, tensorflow 2'</span><br><span class="line">tokenized_str = tokenizer_en.encode(sample_str)</span><br><span class="line">print(tokenized_str)</span><br><span class="line">original_str = tokenizer_en.decode(tokenized_str)</span><br><span class="line">print(original_str)</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">[3222, 439, 150, 7345, 1378, 2824, 2370, 7881]</span><br><span class="line">hello world, tensorflow 2</span><br></pre></td></tr></tbody></table></figure>
<p>添加start、end的token表示</p>
<p>lang1：用于葡萄牙语翻译的开始</p>
<p>lang2：用于英语翻译完成的结束</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">def encode(lang1, lang2):</span><br><span class="line">    lang1 = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(</span><br><span class="line">        lang1.numpy()) + [tokenizer_pt.vocab_size+1]</span><br><span class="line">    lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(</span><br><span class="line">        lang2.numpy()) + [tokenizer_en.vocab_size+1]</span><br><span class="line">    return lang1, lang2</span><br></pre></td></tr></tbody></table></figure>
<p>过滤长度超过40的数据</p>
<p>tf.logical_and ： 与运算</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">MAX_LENGTH=40</span><br><span class="line">def filter_long_sent(x, y, max_length=MAX_LENGTH):</span><br><span class="line">    return tf.logical_and(tf.size(x) &lt;= max_length,</span><br><span class="line">                         tf.size(y) &lt;= max_length)</span><br></pre></td></tr></tbody></table></figure>
<p>将python运算，转换为tensorflow运算节点。</p>
<p>这是一个可以把 TensorFlow 和 Python 原生代码无缝衔接起来的函数，有了它，你就可以在 TensorFlow 里面自由的实现你想要的功能，而不用考虑 TensorFlow 有没有实现它的 API</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">def tf_encode(pt, en):</span><br><span class="line">    return tf.py_function(encode, [pt, en], [tf.int64, tf.int64])</span><br></pre></td></tr></tbody></table></figure>
<h3 id="构造数据集">构造数据集</h3>
<p>In [9]:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">BUFFER_SIZE = 20000</span><br><span class="line">BATCH_SIZE = 64</span><br><span class="line"></span><br><span class="line"># 使用.map()运行相关图操作</span><br><span class="line">train_dataset = train_examples.map(tf_encode)</span><br><span class="line"># 过滤过长的数据</span><br><span class="line">train_dataset = train_dataset.filter(filter_long_sent) # 这个函数为什么不加（） ？？？</span><br><span class="line"># 使用缓存数据加速读入</span><br><span class="line">train_dataset = train_dataset.cache()</span><br><span class="line"># 打乱并获取批数据</span><br><span class="line">train_dataset = train_dataset.padded_batch(</span><br><span class="line">BATCH_SIZE, padded_shapes=([40], [40]))  # 填充为最大长度-90</span><br><span class="line"># 设置预取数据</span><br><span class="line">train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)</span><br><span class="line"></span><br><span class="line"># 验证集数据</span><br><span class="line">val_dataset = val_examples.map(tf_encode)</span><br><span class="line">val_dataset = val_dataset.filter(filter_long_sent).padded_batch(</span><br><span class="line">BATCH_SIZE, padded_shapes=([40], [40]))</span><br></pre></td></tr></tbody></table></figure>
<p>In [10]:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">de_batch, en_batch = next(iter(train_dataset))</span><br><span class="line">de_batch, en_batch</span><br></pre></td></tr></tbody></table></figure>
<p>Out[10]:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">(&lt;tf.Tensor: id=311363, shape=(64, 40), dtype=int64, numpy=</span><br><span class="line"> array([[8214,  116,   84, ...,    0,    0,    0],</span><br><span class="line">        [8214,    7,  261, ...,    0,    0,    0],</span><br><span class="line">        [8214,  155,   39, ...,    0,    0,    0],</span><br><span class="line">        ...,</span><br><span class="line">        [8214,  639,  590, ...,    0,    0,    0],</span><br><span class="line">        [8214,  204, 3441, ...,    0,    0,    0],</span><br><span class="line">        [8214,   27,   13, ...,    0,    0,    0]])&gt;,</span><br><span class="line"> &lt;tf.Tensor: id=311364, shape=(64, 40), dtype=int64, numpy=</span><br><span class="line"> array([[8087,   83,  145, ...,    0,    0,    0],</span><br><span class="line">        [8087, 4670, 1783, ...,    0,    0,    0],</span><br><span class="line">        [8087,  169,   56, ...,    0,    0,    0],</span><br><span class="line">        ...,</span><br><span class="line">        [8087,  174,   79, ...,    0,    0,    0],</span><br><span class="line">        [8087,   11,   16, ...,    0,    0,    0],</span><br><span class="line">        [8087,    4,   12, ...,    0,    0,    0]])&gt;)</span><br></pre></td></tr></tbody></table></figure>
<h2 id="位置嵌入">2.位置嵌入</h2>
<p>将位置编码矢量添加得到词嵌入，相同位置的词嵌入将会更接近，但并不能直接编码相对位置</p>
<p>基于角度的位置编码方法如下：</p>
<p><img src="https://render.githubusercontent.com/render/math?math=%5CLarge%7BPE_%7B%28pos%2C%202i%29%7D%20%3D%20sin%28pos%20%2F%2010000%5E%7B2i%20%2F%20d_%7Bmodel%7D%7D%29%7D%20%24%24%24%24%5CLarge%7BPE_%7B%28pos%2C%202i%2B1%29%7D%20%3D%20cos%28pos%20%2F%2010000%5E%7B2i%20%2F%20d_%7Bmodel%7D%7D%29%7D&amp;mode=display" alt="\Large{PE_{(pos, 2i)} = sin(pos / 10000^{2i / d_{model}})} \Large{PE_{(pos, 2i+1)} = cos(pos / 10000^{2i / d_{model}})} "></p>
<p>得到角度</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">def get_angles(pos, i, d_model):</span><br><span class="line">    # 这里的i等价与上面公式中的2i和2i+1</span><br><span class="line">    angle_rates = 1 / np.power(10000, (2*(i // 2))/ np.float32(d_model))</span><br><span class="line">    return pos * angle_rates</span><br></pre></td></tr></tbody></table></figure>
<p>In [12]:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">def positional_encoding(position, d_model):</span><br><span class="line">	#numpy.arange(start, stop, step, dtype = None) np.arange(position) = [0,...,position-1]</span><br><span class="line">    angle_rads = get_angles(np.arange(position)[:, np.newaxis],   # shape(position,1)</span><br><span class="line">                           np.arange(d_model)[np.newaxis,:],  # shape(1,d_model)</span><br><span class="line">                           d_model)</span><br><span class="line">    # 第2i项使用sin</span><br><span class="line">    sines = np.sin(angle_rads[:, 0::2])  #从0开始，步长为2</span><br><span class="line">    # 第2i+1项使用cos</span><br><span class="line">    cones = np.cos(angle_rads[:, 1::2]) #从1开始，步长为2</span><br><span class="line">    pos_encoding = np.concatenate([sines, cones], axis=-1)</span><br><span class="line">    pos_encoding = pos_encoding[np.newaxis, ...]</span><br><span class="line">    </span><br><span class="line">    return tf.cast(pos_encoding, dtype=tf.float32)</span><br></pre></td></tr></tbody></table></figure>
<p>获得位置嵌入编码</p>
<p>position： 一句话中某个token的位置（0-50）， 如果一句话不够50个单词，那么就需要padding</p>
<p>depth ： 对每一个token进行编码 d_model：512维</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">pos_encoding = positional_encoding(50, 512)</span><br><span class="line">print(pos_encoding.shape)</span><br><span class="line"></span><br><span class="line">plt.pcolormesh(pos_encoding[0], cmap='RdBu')</span><br><span class="line">plt.xlabel('Depth')</span><br><span class="line">plt.xlim((0, 512))</span><br><span class="line">plt.ylabel('Position')</span><br><span class="line">plt.colorbar()</span><br><span class="line">plt.show() # 在这里左右边分别为原来2i 和 2i+1的特征</span><br><span class="line">(1, 50, 512)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="data:image/png;base64,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%0A" alt="img"></p>
<h2 id="掩码">3.掩码</h2>
<p>为了避免输入中padding的token对句子语义的影响，需要将<code>padding位mark掉，原来为0的padding项的mark输出为1</code></p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">def create_padding_mark(seq):</span><br><span class="line">    # 获取为0的padding项</span><br><span class="line">    seq = tf.cast(tf.math.equal(seq, 0), tf.float32) #将seq的编码逐个与0比较，相等为1不等为0</span><br><span class="line">    </span><br><span class="line">    # 扩充维度以便用于attention矩阵</span><br><span class="line">    return seq[:, np.newaxis, np.newaxis, :] # (batch_size,1,1,seq_len)</span><br><span class="line"></span><br><span class="line"># mark 测试</span><br><span class="line">create_padding_mark([[1,2,0,0,3],[3,4,5,0,0]])</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: id=311819, shape=(2, 1, 1, 5), dtype=float32, numpy=</span><br><span class="line">array([[[[0., 0., 1., 1., 0.]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">       [[[0., 0., 0., 1., 1.]]]], dtype=float32)&gt;</span><br></pre></td></tr></tbody></table></figure>
<p>look-ahead mask 用于对未预测的token进行掩码 这意味着要预测第三个单词，只会使用第一个和第二个单词。 要预测第四个单词，仅使用第一个，第二个和第三个单词，依此类推。 ？？？？？？</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">def create_look_ahead_mark(size):</span><br><span class="line">    # 1 - 对角线和取下三角的全部对角线（-1-&gt;全部）</span><br><span class="line">    # 这样就可以构造出每个时刻未预测token的掩码</span><br><span class="line">    mark = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)</span><br><span class="line">    return mark  # (seq_len, seq_len)</span><br></pre></td></tr></tbody></table></figure>
<p>In [16]:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line"># x = tf.random.uniform((1,3))</span><br><span class="line">temp = create_look_ahead_mark(3)</span><br><span class="line">print(temp)</span><br><span class="line">tf.Tensor(</span><br><span class="line">[[0. 1. 1.]</span><br><span class="line"> [0. 0. 1.]</span><br><span class="line"> [0. 0. 0.]], shape=(3, 3), dtype=float32)</span><br></pre></td></tr></tbody></table></figure>
<h2 id="scaled-dot-product-attention">4.Scaled dot product attention</h2>
<p><img src="https://camo.githubusercontent.com/22371733ed7ffa9065a60e80d3b83b10d832072a/68747470733a2f2f7777772e74656e736f72666c6f772e6f72672f696d616765732f7475746f7269616c732f7472616e73666f726d65722f7363616c65645f617474656e74696f6e2e706e67" alt="img">进行attention计算的时候有3个输入 Q (query), K (key), V (value)。计算公式如下：<img src="https://render.githubusercontent.com/render/math?math=%5CLarge%7BAttention%28Q%2C%20K%2C%20V%29%20%3D%20softmax_k%28%5Cfrac%7BQK%5ET%7D%7B%5Csqrt%7Bd_k%7D%7D%29%20V%7D&amp;mode=display" alt="\Large{Attention(Q, K, V) = softmax_k(\frac{QK^T}{\sqrt{d_k}}) V} "></p>
<p>点积注意力通过深度d_k的平方根进行缩放,因为较大的深度会使点积变大，由于使用softmax，会使梯度变小。 例如，考虑Q和K的均值为0且方差为1.它们的矩阵乘法的均值为0，方差为dk。我们使用dk的根用于缩放（而不是任何其他数字），因为Q和K的matmul应该具有0的均值和1的方差。</p>
<p>在这里我们将<code>被掩码的token乘以-1e9(表示负无穷)</code>,这样softmax之后就为0,不对其他token产生影响。</p>
<p>In [17]:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">def scaled_dot_product_attention(q, k, v, mask):</span><br><span class="line">    # query key 相乘获取匹配关系</span><br><span class="line">    matmul_qk = tf.matmul(q, k, transpose_b=True)  # transpose_b=True表示b项在运算前要进行转置运算</span><br><span class="line">    </span><br><span class="line">    # 使用dk进行缩放</span><br><span class="line">    dk = tf.cast(tf.shape(k)[-1], tf.float32)</span><br><span class="line">    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)</span><br><span class="line">    </span><br><span class="line">    # 掩码</span><br><span class="line">    if mask is not None:</span><br><span class="line">        scaled_attention_logits += (mask * -1e9)</span><br><span class="line">        </span><br><span class="line">    # 通过softmax获取attention权重</span><br><span class="line">    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)</span><br><span class="line">    </span><br><span class="line">    # attention 乘上value</span><br><span class="line">    output = tf.matmul(attention_weights, v) # （.., seq_len_v, depth）</span><br><span class="line">    </span><br><span class="line">    return output, attention_weights</span><br></pre></td></tr></tbody></table></figure>
<p>使用attention获取需要关注的语义</p>
<p>In [18]:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">def print_out(q, k, v):</span><br><span class="line">    temp_out, temp_att = scaled_dot_product_attention(</span><br><span class="line">    q, k, v, None)</span><br><span class="line">    print('attention weight:')</span><br><span class="line">    print(temp_att)</span><br><span class="line">    print('output:')</span><br><span class="line">    print(temp_out)</span><br></pre></td></tr></tbody></table></figure>
<p>attention测试</p>
<p>In [19]:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line"># 显示为numpy类型</span><br><span class="line">np.set_printoptions(suppress=True)</span><br><span class="line"></span><br><span class="line">temp_k = tf.constant([[10,0,0],</span><br><span class="line">                      [0,10,0],</span><br><span class="line">                      [0,0,10],</span><br><span class="line">                      [0,0,10]], dtype=tf.float32)  # (4, 3)</span><br><span class="line"></span><br><span class="line">temp_v = tf.constant([[   1,0],</span><br><span class="line">                      [  10,0],</span><br><span class="line">                      [ 100,5],</span><br><span class="line">                      [1000,6]], dtype=tf.float32)  # (4, 3)</span><br><span class="line"># 关注第2个key, 返回对应的value</span><br><span class="line">temp_q = tf.constant([[0,10,0]], dtype=tf.float32)</span><br><span class="line">print_out(temp_q, temp_k, temp_v)</span><br><span class="line">attention weight:</span><br><span class="line">tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)</span><br><span class="line">output:</span><br><span class="line">tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)</span><br></pre></td></tr></tbody></table></figure>
<p>In [20]:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line"># 关注重复的key(第3、4个), 返回对应的value（平均）</span><br><span class="line">temp_q = tf.constant([[0,0,10]], dtype=tf.float32)</span><br><span class="line">print_out(temp_q, temp_k, temp_v)</span><br><span class="line">attention weight:</span><br><span class="line">tf.Tensor([[0.  0.  0.5 0.5]], shape=(1, 4), dtype=float32)</span><br><span class="line">output:</span><br><span class="line">tf.Tensor([[550.    5.5]], shape=(1, 2), dtype=float32)</span><br></pre></td></tr></tbody></table></figure>
<p>In [21]:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line"># 关注第1、2个key, 返回对应的value（平均）</span><br><span class="line">temp_q = tf.constant([[10,10,0]], dtype=tf.float32)</span><br><span class="line">print_out(temp_q, temp_k, temp_v)</span><br><span class="line">attention weight:</span><br><span class="line">tf.Tensor([[0.5 0.5 0.  0. ]], shape=(1, 4), dtype=float32)</span><br><span class="line">output:</span><br><span class="line">tf.Tensor([[5.5 0. ]], shape=(1, 2), dtype=float32)</span><br></pre></td></tr></tbody></table></figure>
<p>In [22]:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line"># 依次放入每个query</span><br><span class="line">temp_q = tf.constant([[0, 0, 10], [0, 10, 0], [10, 10, 0]], dtype=tf.float32)  # (3, 3)</span><br><span class="line">print_out(temp_q, temp_k, temp_v)</span><br><span class="line">attention weight:</span><br><span class="line">tf.Tensor(</span><br><span class="line">[[0.  0.  0.5 0.5]</span><br><span class="line"> [0.  1.  0.  0. ]</span><br><span class="line"> [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)</span><br><span class="line">output:</span><br><span class="line">tf.Tensor(</span><br><span class="line">[[550.    5.5]</span><br><span class="line"> [ 10.    0. ]</span><br><span class="line"> [  5.5   0. ]], shape=(3, 2), dtype=float32)</span><br></pre></td></tr></tbody></table></figure>
<h2 id="mutil-head-attention">5.Mutil-Head Attention</h2>
<p><img src="https://camo.githubusercontent.com/5464abc4535579fdb88afb7117617f6f23ff50af/68747470733a2f2f7777772e74656e736f72666c6f772e6f72672f696d616765732f7475746f7269616c732f7472616e73666f726d65722f6d756c74695f686561645f617474656e74696f6e2e706e67" alt="img"></p>
<p>mutil-head attention包含3部分：</p>
<ul>
<li>线性层与分头</li>
<li>缩放点积注意力</li>
<li>头连接</li>
<li>末尾线性层</li>
</ul>
<p>每个多头注意块有三个输入; Q（查询），K（密钥），V（值）。 它们<code>通过第一层线性层并分成多个头</code>。</p>
<p>注意:点积注意力时需要使用mask， 多头输出需要使用tf.transpose调整各维度。</p>
<p>Q，K和V不是一个单独的注意头，而是分成多个头，因为<code>它允许模型共同参与来自不同表征空间的不同信息。 在拆分之后，每个头部具有降低的维度，总计算成本与具有全维度的单个头部注意力相同。</code></p>
<p>python 中<code>assert断言</code>是声明其布尔值必须为真的判定，如果发生异常就说明表达式为假。可以理解assert断言语句为<strong>raise-if-not</strong>，用来测试表示式，其返回值为假，则会抛出AssertError并且包含错误信息。如果它为真，就不做任何事</p>
<p>"//"不管两者出现任何数，都以整除结果为准，不对小数部分进行处理，直接抛弃，也就是整除法</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line"># 构造mutil head attention层</span><br><span class="line">class MutilHeadAttention(tf.keras.layers.Layer):</span><br><span class="line">    def __init__(self, d_model, num_heads):</span><br><span class="line">        super(MutilHeadAttention, self).__init__()</span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        </span><br><span class="line">        # d_model 必须可以正确分为各个头</span><br><span class="line">        assert d_model % num_heads == 0</span><br><span class="line">        # 分头后的维度</span><br><span class="line">        self.depth = d_model // num_heads</span><br><span class="line">        </span><br><span class="line">        self.wq = tf.keras.layers.Dense(d_model)</span><br><span class="line">        self.wk = tf.keras.layers.Dense(d_model)</span><br><span class="line">        self.wv = tf.keras.layers.Dense(d_model)</span><br><span class="line">        </span><br><span class="line">        self.dense = tf.keras.layers.Dense(d_model)</span><br><span class="line">        </span><br><span class="line">    def split_heads(self, x, batch_size):</span><br><span class="line">        # 分头, 将头个数的维度 放到 seq_len 前面</span><br><span class="line">        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))</span><br><span class="line">        return tf.transpose(x, perm=[0, 2, 1, 3])</span><br><span class="line">    </span><br><span class="line">    def call(self, v, k, q, mask):</span><br><span class="line">        batch_size = tf.shape(q)[0]</span><br><span class="line">        </span><br><span class="line">        # 分头前的前向网络，获取q、k、v语义</span><br><span class="line">        q = self.wq(q)  # (batch_size, seq_len, d_model)</span><br><span class="line">        k = self.wk(k)</span><br><span class="line">        v = self.wv(v)</span><br><span class="line">        </span><br><span class="line">        # 分头</span><br><span class="line">        q = self.split_heads(q, batch_size) # (batch_size, num_heads, seq_len_q, depth)</span><br><span class="line">        k = self.split_heads(k, batch_size)</span><br><span class="line">        v = self.split_heads(v, batch_size)</span><br><span class="line">        # scaled_attention.shape == (batch_size, num_heads, seq_len_v, depth)</span><br><span class="line">        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)</span><br><span class="line">        </span><br><span class="line">        # 通过缩放点积注意力层</span><br><span class="line">        scaled_attention, attention_weights = scaled_dot_product_attention(</span><br><span class="line">        q, k, v, mask)</span><br><span class="line">        # 把多头维度后移</span><br><span class="line">        scaled_attention = tf.transpose(scaled_attention, [0, 2, 1, 3]) # (batch_size, seq_len_v, num_heads, depth)</span><br><span class="line"></span><br><span class="line">        # 合并多头</span><br><span class="line">        concat_attention = tf.reshape(scaled_attention, </span><br><span class="line">                                      (batch_size, -1, self.d_model))</span><br><span class="line">        </span><br><span class="line">        # 全连接重塑</span><br><span class="line">        output = self.dense(concat_attention)</span><br><span class="line">        return output, attention_weights</span><br></pre></td></tr></tbody></table></figure>
<p>测试多头attention</p>
<p>In [24]:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">temp_mha = MutilHeadAttention(d_model=512, num_heads=8)</span><br><span class="line">y = tf.random.uniform((1, 60, 512))</span><br><span class="line">output, att = temp_mha(y, k=y, q=y, mask=None)</span><br><span class="line">print(output.shape, att.shape)</span><br><span class="line">(1, 60, 512) (1, 8, 60, 60)</span><br></pre></td></tr></tbody></table></figure>
<p>point wise前向网络</p>
<p>In [25]:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">def point_wise_feed_forward_network(d_model, diff):</span><br><span class="line">    return tf.keras.Sequential([</span><br><span class="line">        tf.keras.layers.Dense(diff, activation='relu'),</span><br><span class="line">        tf.keras.layers.Dense(d_model)</span><br><span class="line">    ])</span><br></pre></td></tr></tbody></table></figure>
<p>In [26]:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">sample_fnn = point_wise_feed_forward_network(512, 2048)</span><br><span class="line">sample_fnn(tf.random.uniform((64, 50, 512))).shape</span><br></pre></td></tr></tbody></table></figure>
<p>Out[26]:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">TensorShape([64, 50, 512])</span><br></pre></td></tr></tbody></table></figure>
<h2 id="编码器和解码器">6.编码器和解码器</h2>
<p><img src="https://camo.githubusercontent.com/ce5d9e0b508ffa8c84c943258c677b7576f35757/68747470733a2f2f7777772e74656e736f72666c6f772e6f72672f696d616765732f7475746f7269616c732f7472616e73666f726d65722f7472616e73666f726d65722e706e67" alt="img"></p>
<ul>
<li>通过N个编码器层，为序列中的每个字/令牌生成输出。</li>
<li>解码器连接编码器的输出和它自己的输入（自我注意）以预测下一个字。</li>
</ul>
<h3 id="编码层">编码层</h3>
<p>每个编码层包含以下子层</p>
<ul>
<li>Multi-head attention（带掩码）</li>
<li>Point wise feed forward networks</li>
</ul>
<p>每个子层中都有残差连接，并最后通过一个正则化层。残差连接有助于避免深度网络中的梯度消失问题。 每个子层输出是LayerNorm(x + Sublayer(x))，规范化是在d_model维的向量上。Transformer一共有n个编码层。</p>
<p>In [27]:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">class LayerNormalization(tf.keras.layers.Layer):</span><br><span class="line">    def __init__(self, epsilon=1e-6, **kwargs):</span><br><span class="line">        self.eps = epsilon</span><br><span class="line">        super(LayerNormalization, self).__init__(**kwargs)</span><br><span class="line">    def build(self, input_shape):</span><br><span class="line">        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:],</span><br><span class="line">                                     initializer=tf.ones_initializer(), trainable=True)</span><br><span class="line">        self.beta = self.add_weight(name='beta', shape=input_shape[-1:],</span><br><span class="line">                                    initializer=tf.zeros_initializer(), trainable=True)</span><br><span class="line">        super(LayerNormalization, self).build(input_shape)</span><br><span class="line">    def call(self, x):</span><br><span class="line">        mean = tf.keras.backend.mean(x, axis=-1, keepdims=True)</span><br><span class="line">        std = tf.keras.backend.std(x, axis=-1, keepdims=True)</span><br><span class="line">        return self.gamma * (x - mean) / (std + self.eps) + self.beta</span><br><span class="line">    def compute_output_shape(self, input_shape):</span><br><span class="line">        return input_shape</span><br></pre></td></tr></tbody></table></figure>
<p>In [28]:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">class EncoderLayer(tf.keras.layers.Layer):</span><br><span class="line">    def __init__(self, d_model, n_heads, ddf, dropout_rate=0.1):</span><br><span class="line">        super(EncoderLayer, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.mha = MutilHeadAttention(d_model, n_heads)</span><br><span class="line">        self.ffn = point_wise_feed_forward_network(d_model, ddf)</span><br><span class="line">        </span><br><span class="line">        self.layernorm1 = LayerNormalization(epsilon=1e-6)</span><br><span class="line">        self.layernorm2 = LayerNormalization(epsilon=1e-6)</span><br><span class="line">        </span><br><span class="line">        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)</span><br><span class="line">        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)</span><br><span class="line">        </span><br><span class="line">    def call(self, inputs, training, mask):</span><br><span class="line">        # 多头注意力网络</span><br><span class="line">        att_output, _ = self.mha(inputs, inputs, inputs, mask)</span><br><span class="line">        att_output = self.dropout1(att_output, training=training)</span><br><span class="line">        out1 = self.layernorm1(inputs + att_output)  # (batch_size, input_seq_len, d_model)</span><br><span class="line">        # 前向网络</span><br><span class="line">        ffn_output = self.ffn(out1)</span><br><span class="line">        ffn_output = self.dropout2(ffn_output, training=training)</span><br><span class="line">        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)</span><br><span class="line">        return out2</span><br></pre></td></tr></tbody></table></figure>
<p>encoder层测试</p>
<p>In [29]:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">sample_encoder_layer = EncoderLayer(512, 8, 2048)</span><br><span class="line">sample_encoder_layer_output = sample_encoder_layer(</span><br><span class="line">tf.random.uniform((64, 43, 512)), False, None)</span><br><span class="line"></span><br><span class="line">sample_encoder_layer_output.shape</span><br></pre></td></tr></tbody></table></figure>
<p>Out[29]:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">TensorShape([64, 43, 512])</span><br></pre></td></tr></tbody></table></figure>
<h3 id="解码层">解码层</h3>
<p>每个编码层包含以下子层：</p>
<ul>
<li>Masked muti-head attention（带padding掩码和look-ahead掩码）</li>
<li>Muti-head attention（带padding掩码）value和key来自encoder输出，query来自Masked muti-head attention层输出</li>
<li>Point wise feed forward network</li>
</ul>
<p>每个子层中都有残差连接，并最后通过一个正则化层。残差连接有助于避免深度网络中的梯度消失问题。</p>
<p>每个子层输出是LayerNorm(x + Sublayer(x))，规范化是在d_model维的向量上。Transformer一共有n个解码层。</p>
<p>当Q从解码器的第一个注意块接收输出，并且K接收编码器输出时，注意权重表示基于编码器输出给予解码器输入的重要性。 换句话说，解码器通过查看编码器输出并自我关注其自己的输出来预测下一个字。</p>
<p>ps：因为padding在后面所以look-ahead掩码同时掩padding</p>
<p>In [30]:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">class DecoderLayer(tf.keras.layers.Layer):</span><br><span class="line">    def __init__(self, d_model, num_heads, dff, drop_rate=0.1):</span><br><span class="line">        super(DecoderLayer, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.mha1 = MutilHeadAttention(d_model, num_heads)</span><br><span class="line">        self.mha2 = MutilHeadAttention(d_model, num_heads)</span><br><span class="line">        </span><br><span class="line">        self.ffn = point_wise_feed_forward_network(d_model, dff)</span><br><span class="line">        </span><br><span class="line">        self.layernorm1 = LayerNormalization(epsilon=1e-6)</span><br><span class="line">        self.layernorm2 = LayerNormalization(epsilon=1e-6)</span><br><span class="line">        self.layernorm3 = LayerNormalization(epsilon=1e-6)</span><br><span class="line">        </span><br><span class="line">        self.dropout1 = layers.Dropout(drop_rate)</span><br><span class="line">        self.dropout2 = layers.Dropout(drop_rate)</span><br><span class="line">        self.dropout3 = layers.Dropout(drop_rate)</span><br><span class="line">        </span><br><span class="line">    def call(self,inputs, encode_out, training, </span><br><span class="line">             look_ahead_mask, padding_mask):</span><br><span class="line">        # masked muti-head attention</span><br><span class="line">        att1, att_weight1 = self.mha1(inputs, inputs, inputs,look_ahead_mask)</span><br><span class="line">        att1 = self.dropout1(att1, training=training)</span><br><span class="line">        out1 = self.layernorm1(inputs + att1)</span><br><span class="line">        # muti-head attention</span><br><span class="line">        att2, att_weight2 = self.mha2(encode_out, encode_out, inputs, padding_mask)</span><br><span class="line">        att2 = self.dropout2(att2, training=training)</span><br><span class="line">        out2 = self.layernorm2(out1 + att2)</span><br><span class="line">        </span><br><span class="line">        ffn_out = self.ffn(out2)</span><br><span class="line">        ffn_out = self.dropout3(ffn_out, training=training)</span><br><span class="line">        out3 = self.layernorm3(out2 + ffn_out)</span><br><span class="line">        </span><br><span class="line">        return out3, att_weight1, att_weight2</span><br></pre></td></tr></tbody></table></figure>
<p>测试解码层</p>
<p>In [31]:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">sample_decoder_layer = DecoderLayer(512, 8, 2048)</span><br><span class="line"></span><br><span class="line">sample_decoder_layer_output, _, _ = sample_decoder_layer(</span><br><span class="line">tf.random.uniform((64, 50, 512)), sample_encoder_layer_output,</span><br><span class="line">    False, None, None)</span><br><span class="line">sample_decoder_layer_output.shape</span><br></pre></td></tr></tbody></table></figure>
<p>Out[31]:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">TensorShape([64, 50, 512])</span><br></pre></td></tr></tbody></table></figure>
<h3 id="编码器">编码器</h3>
<p>编码器包含：</p>
<ul>
<li>Input Embedding</li>
<li>Positional Embedding</li>
<li>N个编码层</li>
</ul>
<p>In [32]:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">class Encoder(layers.Layer):</span><br><span class="line">    def __init__(self, n_layers, d_model, n_heads, ddf,</span><br><span class="line">                input_vocab_size, max_seq_len, drop_rate=0.1):</span><br><span class="line">        super(Encoder, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.n_layers = n_layers</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        </span><br><span class="line">        self.embedding = layers.Embedding(input_vocab_size, d_model)</span><br><span class="line">        self.pos_embedding = positional_encoding(max_seq_len, d_model)</span><br><span class="line">        </span><br><span class="line">        self.encode_layer = [EncoderLayer(d_model, n_heads, ddf, drop_rate)</span><br><span class="line">                            for _ in range(n_layers)]</span><br><span class="line">        </span><br><span class="line">        self.dropout = layers.Dropout(drop_rate)</span><br><span class="line">    def call(self, inputs, training, mark):</span><br><span class="line">        </span><br><span class="line">        seq_len = inputs.shape[1]</span><br><span class="line">        word_emb = self.embedding(inputs)</span><br><span class="line">        word_emb *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))</span><br><span class="line">        emb = word_emb + self.pos_embedding[:,:seq_len,:]</span><br><span class="line">        x = self.dropout(emb, training=training)</span><br><span class="line">        for i in range(self.n_layers):</span><br><span class="line">            x = self.encode_layer[i](x, training, mark)</span><br><span class="line">        </span><br><span class="line">        return x</span><br></pre></td></tr></tbody></table></figure>
<p>编码器测试</p>
<p>In [33]:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">sample_encoder = Encoder(2, 512, 8, 1024, 5000, 200)</span><br><span class="line">sample_encoder_output = sample_encoder(tf.random.uniform((64, 120)),</span><br><span class="line">                                      False, None)</span><br><span class="line">sample_encoder_output.shape</span><br></pre></td></tr></tbody></table></figure>
<p>Out[33]:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">TensorShape([64, 120, 512])</span><br></pre></td></tr></tbody></table></figure>
<h3 id="解码器">解码器</h3>
<p>解码器包含以下部分：1、输出嵌入；2、位置编码；3、n个解码层</p>
<p>输出嵌入和位置编码叠加后输入解码器，解码器最后的输出送给一个全连接</p>
<p>In [34]:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line"># import pdb</span><br><span class="line"># pdb.set_trace()</span><br><span class="line">class Decoder(layers.Layer):</span><br><span class="line">    def __init__(self, n_layers, d_model, n_heads, ddf,</span><br><span class="line">                target_vocab_size, max_seq_len, drop_rate=0.1):</span><br><span class="line">        super(Decoder, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.n_layers = n_layers</span><br><span class="line">        </span><br><span class="line">        self.embedding = layers.Embedding(target_vocab_size, d_model)</span><br><span class="line">        self.pos_embedding = positional_encoding(max_seq_len, d_model)</span><br><span class="line">        </span><br><span class="line">        self.decoder_layers= [DecoderLayer(d_model, n_heads, ddf, drop_rate)</span><br><span class="line">                             for _ in range(n_layers)]</span><br><span class="line">        </span><br><span class="line">        self.dropout = layers.Dropout(drop_rate)</span><br><span class="line">        </span><br><span class="line">    def call(self, inputs, encoder_out,training,</span><br><span class="line">             look_ahead_mark, padding_mark):</span><br><span class="line">    </span><br><span class="line">        seq_len = tf.shape(inputs)[1]</span><br><span class="line">        attention_weights = {}</span><br><span class="line">        h = self.embedding(inputs)</span><br><span class="line">        h *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))</span><br><span class="line">        h += self.pos_embedding[:,:seq_len,:]</span><br><span class="line">        </span><br><span class="line">        h = self.dropout(h, training=training)</span><br><span class="line">#         print('--------------------\n',h, h.shape)</span><br><span class="line">        # 叠加解码层</span><br><span class="line">        for i in range(self.n_layers):</span><br><span class="line">            h, att_w1, att_w2 = self.decoder_layers[i](h, encoder_out,</span><br><span class="line">                                                   training, look_ahead_mark,</span><br><span class="line">                                                   padding_mark)</span><br><span class="line">            attention_weights['decoder_layer{}_att_w1'.format(i+1)] = att_w1</span><br><span class="line">            attention_weights['decoder_layer{}_att_w2'.format(i+1)] = att_w2</span><br><span class="line">        </span><br><span class="line">        return h, attention_weights</span><br></pre></td></tr></tbody></table></figure>
<p>解码器测试</p>
<p>In [35]:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">sample_decoder = Decoder(2, 512,8,1024,5000, 200)</span><br><span class="line">sample_decoder_output, attn = sample_decoder(tf.random.uniform((64, 100)),</span><br><span class="line">                                            sample_encoder_output, False,</span><br><span class="line">                                            None, None)</span><br><span class="line">sample_decoder_output.shape, attn['decoder_layer1_att_w1'].shape</span><br></pre></td></tr></tbody></table></figure>
<p>Out[35]:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">(TensorShape([64, 100, 512]), TensorShape([64, 8, 100, 100]))</span><br></pre></td></tr></tbody></table></figure>
<h3 id="创建transformer">创建Transformer</h3>
<p>Transformer包含编码器、解码器和最后的线性层，解码层的输出经过线性层后得到Transformer的输出</p>
<p>In [36]:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">class Transformer(tf.keras.Model):</span><br><span class="line">    def __init__(self, n_layers, d_model, n_heads, diff,</span><br><span class="line">                input_vocab_size, target_vocab_size,</span><br><span class="line">                max_seq_len, drop_rate=0.1):</span><br><span class="line">        super(Transformer, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.encoder = Encoder(n_layers, d_model, n_heads,diff,</span><br><span class="line">                              input_vocab_size, max_seq_len, drop_rate)</span><br><span class="line">        </span><br><span class="line">        self.decoder = Decoder(n_layers, d_model, n_heads, diff,</span><br><span class="line">                              target_vocab_size, max_seq_len, drop_rate)</span><br><span class="line">        </span><br><span class="line">        self.final_layer = tf.keras.layers.Dense(target_vocab_size)</span><br><span class="line">    def call(self, inputs, targets, training, encode_padding_mask, </span><br><span class="line">            look_ahead_mask, decode_padding_mask):</span><br><span class="line">        </span><br><span class="line">        encode_out = self.encoder(inputs, training, encode_padding_mask)</span><br><span class="line">        print(encode_out.shape)</span><br><span class="line">        decode_out, att_weights = self.decoder(targets, encode_out, training, </span><br><span class="line">                                               look_ahead_mask, decode_padding_mask)</span><br><span class="line">        print(decode_out.shape)</span><br><span class="line">        final_out = self.final_layer(decode_out)</span><br><span class="line">        </span><br><span class="line">        return final_out, att_weights</span><br></pre></td></tr></tbody></table></figure>
<p>Transformer测试</p>
<p>In [37]:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">sample_transformer = Transformer(</span><br><span class="line">n_layers=2, d_model=512, n_heads=8, diff=1024,</span><br><span class="line">input_vocab_size=8500, target_vocab_size=8000, max_seq_len=120</span><br><span class="line">)</span><br><span class="line">temp_input = tf.random.uniform((64, 62))</span><br><span class="line">temp_target = tf.random.uniform((64, 26))</span><br><span class="line">fn_out, _ = sample_transformer(temp_input, temp_target, training=False,</span><br><span class="line">                              encode_padding_mask=None,</span><br><span class="line">                               look_ahead_mask=None,</span><br><span class="line">                               decode_padding_mask=None,</span><br><span class="line">                              )</span><br><span class="line">fn_out.shape</span><br><span class="line">(64, 62, 512)</span><br><span class="line">(64, 26, 512)</span><br></pre></td></tr></tbody></table></figure>
<p>Out[37]:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">TensorShape([64, 26, 8000])</span><br></pre></td></tr></tbody></table></figure>
<h2 id="实验设置">7.实验设置</h2>
<h3 id="设置超参">设置超参</h3>
<p>In [38]:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">num_layers = 4</span><br><span class="line">d_model = 128</span><br><span class="line">dff = 512</span><br><span class="line">num_heads = 8</span><br><span class="line"></span><br><span class="line">input_vocab_size = tokenizer_pt.vocab_size + 2</span><br><span class="line">target_vocab_size = tokenizer_en.vocab_size + 2</span><br><span class="line">max_seq_len = 40</span><br><span class="line">dropout_rate = 0.1</span><br></pre></td></tr></tbody></table></figure>
<h3 id="优化器">优化器</h3>
<p>带自定义学习率调整的Adam优化器<img src="https://render.githubusercontent.com/render/math?math=%5CLarge%7Blrate%20%3D%20d_%7Bmodel%7D%5E%7B-0.5%7D%20%2A%20min%28step%7B%5C_%7Dnum%5E%7B-0.5%7D%2C%20step%7B%5C_%7Dnum%20%2A%20warmup%7B%5C_%7Dsteps%5E%7B-1.5%7D%29%7D&amp;mode=display" alt="\Large{lrate = d_{model}^{-0.5} * min(step{\_}num^{-0.5}, step{\_}num * warmup{\_}steps^{-1.5})}"></p>
<p>In [39]:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):</span><br><span class="line">    def __init__(self, d_model, warmup_steps=4000):</span><br><span class="line">        super(CustomSchedule, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.d_model = tf.cast(d_model, tf.float32)</span><br><span class="line">        self.warmup_steps = warmup_steps</span><br><span class="line">    </span><br><span class="line">    def __call__(self, step):</span><br><span class="line">        arg1 = tf.math.rsqrt(step)</span><br><span class="line">        arg2 = step * (self.warmup_steps ** -1.5)</span><br><span class="line">        </span><br><span class="line">        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)</span><br></pre></td></tr></tbody></table></figure>
<p>In [40]:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">learing_rate = CustomSchedule(d_model)</span><br><span class="line">optimizer = tf.keras.optimizers.Adam(learing_rate, beta_1=0.9, </span><br><span class="line">                                    beta_2=0.98, epsilon=1e-9)</span><br></pre></td></tr></tbody></table></figure>
<p>In [41]:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line"># 测试</span><br><span class="line">temp_learing_rate = CustomSchedule(d_model)</span><br><span class="line">plt.plot(temp_learing_rate(tf.range(40000, dtype=tf.float32)))</span><br><span class="line">plt.xlabel('learning rate')</span><br><span class="line">plt.ylabel('train step')</span><br></pre></td></tr></tbody></table></figure>
<p>Out[41]:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">Text(0, 0.5, 'train step')</span><br></pre></td></tr></tbody></table></figure>
<p><img src="data:image/png;base64,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%0A" alt="img"></p>
<h3 id="损失和指标">损失和指标</h3>
<p>由于目标序列是填充的，因此在计算损耗时应用填充掩码很重要。 padding的掩码为0，没padding的掩码为1</p>
<p>In [42]:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,</span><br><span class="line">                                                           reduction='none')</span><br><span class="line"></span><br><span class="line">def loss_fun(y_ture, y_pred):</span><br><span class="line">    mask = tf.math.logical_not(tf.math.equal(y_ture, 0))  # 为0掩码标1</span><br><span class="line">    loss_ = loss_object(y_ture, y_pred)</span><br><span class="line">    </span><br><span class="line">    mask = tf.cast(mask, dtype=loss_.dtype)</span><br><span class="line">    loss_ *= mask</span><br><span class="line">    return tf.reduce_mean(loss_)</span><br></pre></td></tr></tbody></table></figure>
<p>In [43]:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">train_loss = tf.keras.metrics.Mean(name='train_loss')</span><br><span class="line">train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')</span><br></pre></td></tr></tbody></table></figure>
<h2 id="训练和保持模型">8、训练和保持模型</h2>
<p>In [44]:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">transformer = Transformer(num_layers, d_model, num_heads, dff,</span><br><span class="line">                          input_vocab_size, target_vocab_size,</span><br><span class="line">                          max_seq_len, dropout_rate)</span><br></pre></td></tr></tbody></table></figure>
<p>In [45]:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line"># 构建掩码</span><br><span class="line">def create_mask(inputs,targets):</span><br><span class="line">    encode_padding_mask = create_padding_mark(inputs)</span><br><span class="line">    # 这个掩码用于掩输入解码层第二层的编码层输出</span><br><span class="line">    decode_padding_mask = create_padding_mark(inputs)</span><br><span class="line">    </span><br><span class="line">    # look_ahead 掩码， 掩掉未预测的词</span><br><span class="line">    look_ahead_mask = create_look_ahead_mark(tf.shape(targets)[1])</span><br><span class="line">    # 解码层第一层得到padding掩码</span><br><span class="line">    decode_targets_padding_mask = create_padding_mark(targets)</span><br><span class="line">    </span><br><span class="line">    # 合并解码层第一层掩码</span><br><span class="line">    combine_mask = tf.maximum(decode_targets_padding_mask, look_ahead_mask)</span><br><span class="line">    </span><br><span class="line">    return encode_padding_mask, combine_mask, decode_padding_mask</span><br></pre></td></tr></tbody></table></figure>
<p>创建checkpoint管理器</p>
<p>In [46]:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">checkpoint_path = './checkpoint/train'</span><br><span class="line">ckpt = tf.train.Checkpoint(transformer=transformer,</span><br><span class="line">                          optimizer=optimizer)</span><br><span class="line"># ckpt管理器</span><br><span class="line">ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)</span><br><span class="line"></span><br><span class="line">if ckpt_manager.latest_checkpoint:</span><br><span class="line">    ckpt.restore(ckpt_manager.latest_checkpoint)</span><br><span class="line">    print('last checkpoit restore')</span><br></pre></td></tr></tbody></table></figure>
<p>target分为target_input和target real. target_input是传给解码器的输入，target_real是其左移一个位置的结果，每个target_input位置对应下一个预测的标签</p>
<p>如句子=“SOS A丛林中的狮子正在睡觉EOS”</p>
<p>target_input =“SOS丛林中的狮子正在睡觉”</p>
<p>target_real =“丛林中的狮子正在睡觉EOS”</p>
<p>transformer是个自动回归模型：它一次预测一个部分，并使用其到目前为止的输出，决定下一步做什么。</p>
<p>在训练期间使用teacher-forcing，即无论模型当前输出什么都强制将正确输出传给下一步。</p>
<p>而预测时则根据前一个的输出预测下一个词</p>
<p>为防止模型在预期输出处达到峰值，模型使用look-ahead mask</p>
<p>In [47]:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">@tf.function</span><br><span class="line">def train_step(inputs, targets):</span><br><span class="line">    tar_inp = targets[:,:-1]</span><br><span class="line">    tar_real = targets[:,1:]</span><br><span class="line">    # 构造掩码</span><br><span class="line">    encode_padding_mask, combined_mask, decode_padding_mask = create_mask(inputs, tar_inp)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    with tf.GradientTape() as tape:</span><br><span class="line">        predictions, _ = transformer(inputs, tar_inp,</span><br><span class="line">                                    True,</span><br><span class="line">                                    encode_padding_mask,</span><br><span class="line">                                    combined_mask,</span><br><span class="line">                                    decode_padding_mask)</span><br><span class="line">        loss = loss_fun(tar_real, predictions)</span><br><span class="line">    # 求梯度</span><br><span class="line">    gradients = tape.gradient(loss, transformer.trainable_variables)</span><br><span class="line">    # 反向传播</span><br><span class="line">    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))</span><br><span class="line">    </span><br><span class="line">    # 记录loss和准确率</span><br><span class="line">    train_loss(loss)</span><br><span class="line">    train_accuracy(tar_real, predictions)</span><br></pre></td></tr></tbody></table></figure>
<p>葡萄牙语用作输入语言，英语是目标语言。</p>
<p>In [48]:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">EPOCHS = 20</span><br><span class="line">for epoch in range(EPOCHS):</span><br><span class="line">    start = time.time()</span><br><span class="line">    </span><br><span class="line">    # 重置记录项</span><br><span class="line">    train_loss.reset_states()</span><br><span class="line">    train_accuracy.reset_states()</span><br><span class="line">    </span><br><span class="line">    # inputs 葡萄牙语， targets英语</span><br><span class="line">    </span><br><span class="line">    for batch, (inputs, targets) in enumerate(train_dataset):</span><br><span class="line">        # 训练</span><br><span class="line">        train_step(inputs, targets)</span><br><span class="line">        </span><br><span class="line">        if batch % 500 == 0:</span><br><span class="line">            print('epoch {}, batch {}, loss:{:.4f}, acc:{:.4f}'.format(</span><br><span class="line">            epoch+1, batch, train_loss.result(), train_accuracy.result()</span><br><span class="line">            ))</span><br><span class="line">            </span><br><span class="line">    if (epoch + 1) % 2 == 0:</span><br><span class="line">        ckpt_save_path = ckpt_manager.save()</span><br><span class="line">        print('epoch {}, save model at {}'.format(</span><br><span class="line">        epoch+1, ckpt_save_path</span><br><span class="line">        ))</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    print('epoch {}, loss:{:.4f}, acc:{:.4f}'.format(</span><br><span class="line">    epoch+1, train_loss.result(), train_accuracy.result()</span><br><span class="line">    ))</span><br><span class="line">    </span><br><span class="line">    print('time in 1 epoch:{} secs\n'.format(time.time()-start))</span><br><span class="line">(64, 40, 128)</span><br><span class="line">(64, 39, 128)</span><br><span class="line">(64, 40, 128)</span><br><span class="line">(64, 39, 128)</span><br><span class="line">epoch 1, batch 0, loss:4.0259, acc:0.0000</span><br><span class="line">epoch 1, batch 500, loss:3.4436, acc:0.0340</span><br><span class="line">(31, 40, 128)</span><br><span class="line">(31, 39, 128)</span><br><span class="line">epoch 1, loss:3.2112, acc:0.0481</span><br><span class="line">time in 1 epoch:467.3876633644104 secs</span><br><span class="line"></span><br><span class="line">epoch 2, batch 0, loss:2.4443, acc:0.0982</span><br><span class="line">epoch 2, batch 500, loss:2.3006, acc:0.1139</span><br><span class="line">epoch 2, save model at ./checkpoint/train/ckpt-1</span><br><span class="line">epoch 2, loss:2.2473, acc:0.1184</span><br><span class="line">time in 1 epoch:429.6356120109558 secs</span><br><span class="line"></span><br><span class="line">epoch 3, batch 0, loss:2.0709, acc:0.1306</span><br><span class="line">epoch 3, batch 500, loss:2.0279, acc:0.1412</span><br><span class="line">epoch 3, loss:1.9927, acc:0.1443</span><br><span class="line">time in 1 epoch:426.3838963508606 secs</span><br><span class="line"></span><br><span class="line">epoch 4, batch 0, loss:1.8720, acc:0.1571</span><br><span class="line">epoch 4, batch 500, loss:1.8020, acc:0.1678</span><br><span class="line">epoch 4, save model at ./checkpoint/train/ckpt-2</span><br><span class="line">epoch 4, loss:1.7664, acc:0.1714</span><br><span class="line">time in 1 epoch:387.37333059310913 secs</span><br><span class="line"></span><br><span class="line">epoch 5, batch 0, loss:1.6616, acc:0.1807</span><br><span class="line">epoch 5, batch 500, loss:1.5908, acc:0.1936</span><br><span class="line">epoch 5, loss:1.5610, acc:0.1961</span><br><span class="line">time in 1 epoch:389.60524225234985 secs</span><br><span class="line"></span><br><span class="line">epoch 6, batch 0, loss:1.4435, acc:0.2087</span><br><span class="line">epoch 6, batch 500, loss:1.4117, acc:0.2127</span><br><span class="line">epoch 6, save model at ./checkpoint/train/ckpt-3</span><br><span class="line">epoch 6, loss:1.3852, acc:0.2147</span><br><span class="line">time in 1 epoch:438.03212571144104 secs</span><br><span class="line"></span><br><span class="line">epoch 7, batch 0, loss:1.3070, acc:0.2183</span><br><span class="line">epoch 7, batch 500, loss:1.2383, acc:0.2320</span><br><span class="line">epoch 7, loss:1.2111, acc:0.2346</span><br><span class="line">time in 1 epoch:378.90994358062744 secs</span><br><span class="line"></span><br><span class="line">epoch 8, batch 0, loss:1.1545, acc:0.2332</span><br><span class="line">epoch 8, batch 500, loss:1.0854, acc:0.2508</span><br><span class="line">epoch 8, save model at ./checkpoint/train/ckpt-4</span><br><span class="line">epoch 8, loss:1.0658, acc:0.2525</span><br><span class="line">time in 1 epoch:377.7305886745453 secs</span><br><span class="line"></span><br><span class="line">epoch 9, batch 0, loss:1.0109, acc:0.2532</span><br><span class="line">epoch 9, batch 500, loss:0.9780, acc:0.2645</span><br><span class="line">epoch 9, loss:0.9624, acc:0.2656</span><br><span class="line">time in 1 epoch:378.3708670139313 secs</span><br><span class="line"></span><br><span class="line">epoch 10, batch 0, loss:0.9245, acc:0.2576</span><br><span class="line">epoch 10, batch 500, loss:0.8940, acc:0.2749</span><br><span class="line">epoch 10, save model at ./checkpoint/train/ckpt-5</span><br><span class="line">epoch 10, loss:0.8820, acc:0.2757</span><br><span class="line">time in 1 epoch:378.5437033176422 secs</span><br><span class="line"></span><br><span class="line">epoch 11, batch 0, loss:0.8609, acc:0.2728</span><br><span class="line">epoch 11, batch 500, loss:0.8305, acc:0.2833</span><br><span class="line">epoch 11, loss:0.8222, acc:0.2835</span><br><span class="line">time in 1 epoch:378.56798672676086 secs</span><br><span class="line"></span><br><span class="line">epoch 12, batch 0, loss:0.8031, acc:0.2821</span><br><span class="line">epoch 12, batch 500, loss:0.7770, acc:0.2905</span><br><span class="line">epoch 12, save model at ./checkpoint/train/ckpt-6</span><br><span class="line">epoch 12, loss:0.7700, acc:0.2906</span><br><span class="line">time in 1 epoch:378.8077425956726 secs</span><br><span class="line"></span><br><span class="line">epoch 13, batch 0, loss:0.7457, acc:0.2857</span><br><span class="line">epoch 13, batch 500, loss:0.7311, acc:0.2971</span><br><span class="line">epoch 13, loss:0.7257, acc:0.2969</span><br><span class="line">time in 1 epoch:378.34697437286377 secs</span><br><span class="line"></span><br><span class="line">epoch 14, batch 0, loss:0.7159, acc:0.2925</span><br><span class="line">epoch 14, batch 500, loss:0.6931, acc:0.3026</span><br><span class="line">epoch 14, save model at ./checkpoint/train/ckpt-7</span><br><span class="line">epoch 14, loss:0.6874, acc:0.3025</span><br><span class="line">time in 1 epoch:379.3904767036438 secs</span><br><span class="line"></span><br><span class="line">epoch 15, batch 0, loss:0.6885, acc:0.2905</span><br><span class="line">epoch 15, batch 500, loss:0.6594, acc:0.3072</span><br><span class="line">epoch 15, loss:0.6546, acc:0.3070</span><br><span class="line">time in 1 epoch:377.10075068473816 secs</span><br><span class="line"></span><br><span class="line">epoch 16, batch 0, loss:0.6465, acc:0.2961</span><br><span class="line">epoch 16, batch 500, loss:0.6306, acc:0.3117</span><br><span class="line">epoch 16, save model at ./checkpoint/train/ckpt-8</span><br><span class="line">epoch 16, loss:0.6257, acc:0.3115</span><br><span class="line">time in 1 epoch:379.0886535644531 secs</span><br><span class="line"></span><br><span class="line">epoch 17, batch 0, loss:0.6033, acc:0.3021</span><br><span class="line">epoch 17, batch 500, loss:0.6023, acc:0.3162</span><br><span class="line">epoch 17, loss:0.5984, acc:0.3159</span><br><span class="line">time in 1 epoch:377.6911520957947 secs</span><br><span class="line"></span><br><span class="line">epoch 18, batch 0, loss:0.5469, acc:0.3225</span><br><span class="line">epoch 18, batch 500, loss:0.5791, acc:0.3195</span><br><span class="line">epoch 18, save model at ./checkpoint/train/ckpt-9</span><br><span class="line">epoch 18, loss:0.5755, acc:0.3191</span><br><span class="line">time in 1 epoch:378.4746241569519 secs</span><br><span class="line"></span><br><span class="line">epoch 19, batch 0, loss:0.5287, acc:0.3209</span><br><span class="line">epoch 19, batch 500, loss:0.5575, acc:0.3229</span><br><span class="line">epoch 19, loss:0.5546, acc:0.3224</span><br><span class="line">time in 1 epoch:378.284138917923 secs</span><br><span class="line"></span><br><span class="line">epoch 20, batch 0, loss:0.5182, acc:0.3193</span><br><span class="line">epoch 20, batch 500, loss:0.5374, acc:0.3263</span><br><span class="line">epoch 20, save model at ./checkpoint/train/ckpt-10</span><br><span class="line">epoch 20, loss:0.5344, acc:0.3257</span><br><span class="line">time in 1 epoch:377.9467544555664 secs</span><br></pre></td></tr></tbody></table></figure>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>transformer</category>
      </categories>
      <tags>
        <tag>transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-07-23-深度学习杂记</title>
    <url>/2020/07/23/2020-07-23-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h3 id="logits">logits</h3>
<p>在tensorflow代码中，经常会出现logits，logits表示的含义就是在模型的最后一层输出后，进入到Softmax函数之前得到的n维向量。是feature的抽象。</p>
<p>logits是未归一化的概率， 一般也就是 softmax层的输入。所以logits和lables的shape一样</p>
<p>而softmax是在一个n分类问题中，输入一个n维的logits向量，输出一个n维概率向量，其物理意义是logits代表的物体属于各类的概率。是对logits进行归一化。</p>
<p><img src="https://i.loli.net/2020/08/09/W6yEXJorYvaK4pf.png" alt="image-20200723160820328" style="zoom:50%;"></p>
<p>输入softmax的Logits中最大的一维会成为输出中同样最大的一维。例如：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="number">4</span>分类问题的</span><br><span class="line">Logits = [<span class="number">1</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">0.2</span>]</span><br><span class="line">输入softmax后</span><br><span class="line">得到的 one_hot_pred = [<span class="number">0.017</span> <span class="number">0.958</span> <span class="number">0.017</span> <span class="number">0.008</span>]</span><br></pre></td></tr></tbody></table></figure>
<h3 id="beam-search">beam search</h3>
<p>在sequence2sequence模型中，beam search的方法只用在测试的情况，因为在训练过程中，每一个decoder的输出是有正确答案的，也就不需要beam search去加大输出的准确率。</p>
<p><strong>我们需要翻译中文“我是中国人”---&gt;英文“I am Chinese”</strong></p>
<p>假设我们的词表大小只有三个单词就是I am Chinese。那么如果我们的beam size为2的话，我们现在来解释,</p>
<p>如下图所示，我们在decoder的过程中，有了beam search方法后，在第一次的输出，我们选取概率最大的"I"和"am"两个单词，而不是只挑选一个概率最大的单词。</p>
<p><img src="https://i.loli.net/2020/07/24/32grlE4uLGpQPCq.png"></p>
<p>然后接下来我们要做的就是，把“I”单词作为下一个decoder的输入算一遍得到y2的输出概率分布，把“am”单词作为下一个decoder的输入算一遍也得到y2的输出概率分布。</p>
<p>比如将“I”单词作为下一个decoder的输入算一遍得到y2的输出概率分布如下：</p>
<p><img src="https://i.loli.net/2020/07/24/s6OErw5yT41uLPa.png" alt="image-20200724095714528"></p>
<p>比如将“am”单词作为下一个decoder的输入算一遍得到y2的输出概率分布如下：</p>
<p><img src="https://i.loli.net/2020/07/24/Ng1ZyrxehmpUIoC.png" alt="image-20200724095747262"></p>
<p>那么此时我们由于我们的beam size为2，也就是我们只能保留概率最大的两个序列，此时我们可以计算所有的序列概率：</p>
<p>“I I” = 0.4<em>0.3 "I am" = 0.4</em>0.6 "I sChinese" = 0.4*0.1</p>
<p>"am I" = 0.5<em>0.3 "am am" = 0.5</em>0.3 "am Chinese" = 0.5*0.4</p>
<p>我们很容易得出俩个最大概率的序列为 “I am”和“am Chinese”，然后后面会不断重复这个过程，直到遇到结束符为止。</p>
<p><strong>最终输出2个得分最高的序列。</strong></p>
<p><strong>这就是seq2seq中的beam search算法过程</strong></p>
<h3 id="tpu--tensorflow">TPU -tensorflow</h3>
<p>在神经网络学习过程中，需要进行矩阵运算，包括大量的加法和乘法，所以关键点是我们该如何快速执行大型矩阵运算，同时还需要更小的能耗。</p>
<h4 id="与cpu和gpu的对比">与CPU和GPU的对比</h4>
<p>CPU：CPU 非常灵活，硬件无法一直了解下一个计算是什么，直到它读取了软件的下一个指令。</p>
<p>​ 缺点:每一个 CPU 的算术逻辑单元（ALU，控制乘法器和加法器的组件）都只能一个接一个地执行它们，每一次都需要访问内存，限制了总体吞吐量，并需要大量的能耗。</p>
<p>GPU：在单个处理器中使用成千上万个 ALU。现代 GPU 通常在单个处理器中拥有 2500-5000 个 ALU，意味着你可以同时执行数千次乘法和加法运算。在<code>并行化</code>的应用中很好，比如神经网络的矩阵乘法。</p>
<p>​ 缺点：因为 GPU 在其 ALU 上执行更多的并行计算，它也会成比例地耗费更多的能量来访问内存，同时也因为复杂的线路而增加 GPU 的物理空间占用。</p>
<h4 id="tpu的工作特点">TPU的工作特点</h4>
<p>TPU 不能运行文本处理软件、控制火箭引擎或执行银行业务，但它们可以为神经网络处理大量的乘法和加法运算，同时 TPU 的速度非常快、能耗非常小且物理空间占用也更小。常用于<code>加速神经网络</code></p>
<p>TPU 可以在神经网络运算上达到高计算吞吐量，同时能耗和物理空间都很小。</p>
<h4 id="参考">参考</h4>
<blockquote>
<p>TPU 加速深度学习 https://www.ednchina.com/news/201809041331.html</p>
<p>PPT 解释了 TPU 的特性与定义 tpudemo.com</p>
</blockquote>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-07-22-如何有效读论文-思维导图</title>
    <url>/2020/07/22/2020-07-22-%E5%A6%82%E4%BD%95%E6%9C%89%E6%95%88%E8%AF%BB%E8%AE%BA%E6%96%87-%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE/</url>
    <content><![CDATA[<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>论文</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-07-22-每周论文分享</title>
    <url>/2020/07/22/2020-07-22-%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/</url>
    <content><![CDATA[<p><strong>对神经网络的不确定度估计</strong>涉及较多概率论的知识，而且从理论到应用的转化也涉及到使用近似量估计的问题，因此初次接触这部分知识该我带来了不小的挑战。经过几天的探究，还是走了不少弯路，虽说很多细节的理论推理还没有深究，但摸清了借助贝叶斯神经网络估计不确定度的原理和过程。你会发现，在原有神经网络上增加不确定度估计功能，<strong>从结构上的变化看似非常简单（只是损失函数，dropout层，输出等已有部分的修改），但背后有很严密的数理逻辑驱动着。</strong></p>
<h2 id="原理简述">原理简述</h2>
<p>不确定度虽分为两类，一类是模型不确定性，另一类是数据不确定性，两种不确定性所在的对象不同。<strong>利用模型预测结果的分布特点（注意是分布），可以成为测量不确定度的一种手段。</strong></p>
<p>模型预测由模型生成，在给定训练数据情况下，模型自身有后验分布（因此用不确定度衡量）。为了得到模型预测的分布特点，由贝叶斯定理，需要边缘化（marginalizing，旨在消除条件分布中某个变量的影响）模型的后验分布。</p>
<p>从理想层面，边缘化就是以积分形式穷尽所有模型的分布最后得到模型预测的分布，但现实层面由于深度网络的复杂性，有<strong>两方面不现实 - 模型表示的抽象性 &amp; 积分操作</strong>，为了落地，在应用时，理想的“积分操作”由离散的采样操作代替（因为我们最终不需要预测分布的完整描述，只需方差即可，而基于dropout的采样方式被证明可以求出方差）。</p>
<p>理想的模型分布是无法满足采样需求的，因此需引入近似模型分布的可采样的分布，新引入的分布<strong>由诸多参数决定着</strong>，要想近似原模型分布必须要进行参数调整，一种方法是最小化近似分布和原分布的KL散度指标，而这恰好是一个带有明确目标函数的优化任务。至此，<strong>对预测分布求解</strong>实现了从理论的边缘化操作到实际的优化操作（可通过反向传播）+采样的转换。</p>
<h2 id="不确定性估计">不确定性估计</h2>
<h3 id="不确定性分类">不确定性分类</h3>
<p>在贝叶斯模型中，有两类主要的不确定性：模型不确定性和数据不确定性。</p>
<ol type="1">
<li><p>模型不确定性</p>
<p>（又称认知不确定性，epistemic uncertainty）：</p>
<ul>
<li>主要代表模型参数的不确定性，<strong>这种不确定常常来自于我们对收集的用于训练数据的无知（Ignorance）</strong>，例如当数据集中缺乏某类数据时，模型中对应处理这种场景的参数的不确定性就变大了，结果就是，如果测试时给一个这类的数据，模型表现可能很差。</li>
<li>模型不确定性在现有诸多的深度神经网络中都没有被考虑，因为推理预测的后验分布过于困难，<strong>变分推理（Variational Inference）</strong>是一种流行的方法，其目的在于使用已有的数据样本，驱动简化版的分布去拟合真实的后验分布，而这个拟合指标通过变换<strong>某些无法求出的分布</strong>，从而变得可以计算。Gal等人发表的论文显示，在神经网络上的带dropout训练过程可被理解为以Bernoulli为假设分布的变分分布拟合过程。（当然，若要达到拟合效果，损失函数等结构都要调整，这篇文章也正是基于此。）</li>
<li>模型不确定性<strong>可以</strong>被消除：通过增大数据集。</li>
</ul></li>
<li><p>数据不确定性</p>
<p>（又称偶然不确定性，aleatoric uncertainty）：</p>
<ul>
<li>数据不确定性主要是<strong>观测噪音</strong>，如传感器噪音或者动作噪音。数据不确定相较模型不确定性在神经网络运用更多，通过分析输出的分布，可以评估该类不确定性。例如之前提到的Social LSTM模型，输出是二维位置高斯分布的五个参数(μ1,μ2,σ1,σ2,ρ)(μ1,μ2,σ1,σ2,ρ)，通过预测这五个参数就反映了数据不确定性。</li>
<li>数据不确定性<strong>不可以</strong>被消除。</li>
</ul></li>
<li><p>文章的一篇重要引文用了”formalize“来表示对于这两类不确定性的定义：模型不确定制定在<strong>参数的分布上</strong>，通过先验分布预设模型，在给予一些训练数据后观察这个分布的变化；数据不确定性制定在<strong>输出的分布上</strong>，通过分析不同输入对应输出的方差。根据笔者来看，文章最终在估计这两类不确定性时，只有数据不确定性按定义来的，模型不确定性的估计文章通过Gal. 等人的结论成功转移到了估计输出分布上。</p></li>
</ol>
<p>这里模型不确定性的重要性在此就不赘述了.</p>
<ul>
<li>其实,笔者之前就有一个疑惑:为什么在神经网络中模型不确定度不能按照传统模型那样去计算熵? 文献指出:主流的Deep Learning都采用最大似然估计或最大后验来训练,因此产生的往往是一个point estimation而不是uncertainty value. 具体来说, 直观来说Softmax层之后的概率向量可以用来解释模型的置信度. 但是实际上模型依然会对具有很大Softmax输出的预测表明较大的不确定性.</li>
<li>大部分不确定性估计算法都是基于Bayesian Neural Networks (可以理解为概率性的神经网络, 认为其参数服从一定的先验分布), 给定输入时就可以产生具有一定分布性质的输出用来计算模型不确定度. 但是计算非常复杂,只是理论上的保证.</li>
</ul>
<p>transformer</p>
<h3 id="regularization"><strong>3.4 Regularization</strong></h3>
<p>在训练过程中，使用了两种正则化手段：</p>
<p>第一种是 <strong>「Residual Dropout」</strong>。在每一层进行残差连接和归一化之前，先执行 dropout [6]。此外，编码器与解码器中嵌入编码与位置编码之和也应用了 dropout。对于基础模型，原文使用 <img src="https://www.zhihu.com/equation?tex=P_%7Bdrop%7D+%3D+0.1" alt="[公式]">.</p>
<p>第二种是 <strong>「Label Smoothing」</strong>。在训练过程中，使用了标签平滑策略 [7]，参数设置 <img src="https://www.zhihu.com/equation?tex=%5Cepsilon_%7Bls%7D+%3D+0.1" alt="[公式]">。这种策略会增加模型的不确定性，影响困惑度（perplexity），但会提升准确率与 BLEU 得分。</p>
<p>在模型的不确定性程度较低时人工会介入对话，这一特性使得系统会在出现差错之前咨询人工的建议，所以会极大程度地降低给出不合理回复带来的风险。</p>
<p>不确定性估计模块还能让系统以更少的数据量达到更好的性能。</p>
<h2 id="论文-4a-general-framework-for-uncertainty-estimation-in-deep-learning"><strong>论文 4：A General Framework for Uncertainty Estimation in Deep Learning</strong></h2>
<ul>
<li>作者：Antonio Loquercio、Mattia Segu、Davide Scaramuzza</li>
<li>论文地址：https://arxiv.org/pdf/1907.06890v3.pdf</li>
</ul>
<p><strong>摘要：</strong>神经网络的预测通常是不可靠的，特别是当输入的样本不在训练集的分布中，或者因为噪声而损坏的情况下。深度学习算法应当具有自动预测这种失败的能力，然而现有的不确定性预测方法需要对网络和优化流程进行调整，尤其忽略了数据中先验知识的重要性。这些方法倾向于过度简化假设，从而低估了不确定性。为了解决这些问题，研究者提出了一种新的不确定性估计框架。基于贝叶斯信念网络和蒙特卡洛采样，研究者的框架不仅能够完善模型对不同来源的不确定性预测，还可以和之前的感知噪声等数据信息相结合。研究者从理论上说明这一模型相比现有模型可以更好地捕捉不确定性。相比之前的方法，在计算机视觉和控制任务上，研究者的方法最多可以超出 23% 的表现。</p>
<p><img src="https://bbs.cvmart.net/uploads/images/201910/21/11/ns0sx4ghnM.png?imageView2/2/w/1240/h/0" alt="file"> <em>▲图 1：模型的架构。给定变量 x 作为输入，以及噪声 v^(0)，和训练好的神经网络。研究者的方法需要计算输出的置信度。</em></p>
<p><strong>推荐：</strong>对于神经网络预测结果的不确定性研究是近来关注的一个热点。本文提出了一种新颖的方法，推荐读者参考。</p>
<p>神经网络是非线性的 。所以要变为线性，再使用卡尔曼更新</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>论文</tag>
      </tags>
  </entry>
  <entry>
    <title>linux &amp; vim 常用操作-初级</title>
    <url>/2020/07/22/2020-07-22-%20linux&amp;vim%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<h2 id="前言">前言</h2>
<p>跑实验需要用服务器，远程服务器我用的是XShell + FileZilla进行连接和传输文件，然而在操控linux服务器中需要用到一些常见的linux命令，之前学过《鸟哥linux》，但是一直没有总结过，现在总结下。</p>
<h2 id="linux-常用命令">linux 常用命令</h2>
<h3 id="ls-命令">ls 命令</h3>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">ls  -a   <span class="comment"># 列出目录所有文件，包含以.开始的隐藏文件</span></span><br><span class="line"></span><br><span class="line">ls ~  <span class="comment">#进入主目录</span></span><br><span class="line"></span><br><span class="line">ls E: <span class="comment"># 在win中进入E盘</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="cd-目录名">cd [目录名]</h3>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">cd ..   <span class="comment">#返回上一级目录</span></span><br><span class="line"></span><br><span class="line">cd . /temp  <span class="comment"># 当前目录下的temp文件夹</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="pwd-命令">pwd 命令</h3>
<p>查看当前工作目录路径</p>
<h3 id="mkdir-命令">mkdir 命令</h3>
<p>用于创建文件夹</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">mkdir temp  <span class="comment">#当前工作目录下创建名为 temp的文件夹</span></span><br><span class="line"></span><br><span class="line">mkdir -p /tmp/test/t1/t   <span class="comment"># 在 tmp 目录下创建路径为 test/t1/t 的目录，若不存在，则创建</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="rm-命令">rm 命令</h3>
<p>删除一个目录中的一个或多个文件或目录</p>
<p>如果没有使用 -r 选项，则 rm 不会删除目录。如果使用 rm 来删除文件，通常仍可以将该文件恢复原状。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">rm  -i  *.log <span class="comment"># 删除全部.log文件， 删除前逐一询问</span></span><br><span class="line"></span><br><span class="line">rm a.txt <span class="comment"># 删除文件a.txt</span></span><br><span class="line"></span><br><span class="line">rm -rf test <span class="comment"># 删除test文件夹 </span></span><br><span class="line"></span><br><span class="line">​	 <span class="comment">#-r 就是向下递归，不管有多少级目录，一并删除</span></span><br><span class="line"></span><br><span class="line">​	<span class="comment">#-f 就是直接强行删除，不作任何提示的意思</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="vim-命令">vim 命令</h3>
<p>编辑文件</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">vim a.txt  <span class="comment"># 创建a.txt文件并进入编辑状态 。 如果a.txt 已经存在，则直接进入编辑状态</span></span><br><span class="line"></span><br><span class="line"><span class="number">1.</span> 按下i键，下端显示 –INSERT–。可以进行插入，输入文本 </span><br><span class="line"></span><br><span class="line"><span class="number">2.</span> 输入了之后 按Esc键退出编辑状态</span><br><span class="line"></span><br><span class="line"><span class="number">3.</span> 键入 :wq!保存退出  <span class="comment"># 注意是在英文输入状态下进行的操作</span></span><br><span class="line"></span><br><span class="line">    :w 在编辑的过程中保存文件,相当于word中的ctrl+s    </span><br><span class="line"></span><br><span class="line">   :wq 保存文件并退出</span><br></pre></td></tr></tbody></table></figure>
<h3 id="less-命令">less 命令</h3>
<p>有时实验结果较长，在终端前面的输出会无法显示，此命令可以在终端显示上屏无法查看的内容，可以<strong>分页显示</strong>。</p>
<p>用法： 在执行命令后面加上|less即可，它可以用<strong><code>PageUp</code></strong>和<strong><code>PageDown</code></strong>按键上下翻页，也可以用<strong><code>上下方向键</code></strong>一点点查看。退出按<strong><code>q</code></strong>。</p>
<p>一般显示的结果过长，都可在命令行后加上<strong>|less</strong>来分页显示</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">python main.py | less</span><br></pre></td></tr></tbody></table></figure>
<p>less也可直接查看文件</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">less test.txt</span><br></pre></td></tr></tbody></table></figure>
<h3 id="重定向">重定向</h3>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">python main.py &gt;test.txt  <span class="number">2</span>&gt;&amp;<span class="number">1</span> </span><br><span class="line"></span><br><span class="line"><span class="comment">#表示将标准输出（STDOUT）重定向到test.txt文件</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#2&gt;&amp;1 ：把标准输出和标准错误一起重定向到一个文件中。1是标准输出的文件描述符，2是标准错误的文件描述符</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="cat-命令">cat 命令</h3>
<p>可以直接查看a.txt 文件</p>
<h3 id="cp-命令">cp 命令</h3>
<p>复制文件/文件夹</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">cp [option] Source Directory</span><br></pre></td></tr></tbody></table></figure>
<p>1.如果要复制的源目录中还存在子目录，此时使用选项R递归地复制子目录。</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">cp -r test file/  <span class="comment">#将目录test复制到目录file中</span></span><br></pre></td></tr></tbody></table></figure>
<p>2..复制并重命名文件</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">cp /etc/samba/smb.conf  smb.backup  <span class="comment">#将/etc/samba/smb.conf备份到当前目录中，并将文件重命名 smb.backup</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="swp文件">.swp文件</h3>
<p>非正常关闭vi/vim编辑器时（如不小心按下<code>ctrl+Z</code>强制退出）会生成一个.swp文件。</p>
<h4 id="解决">解决</h4>
<ul>
<li>使用vim -r a.txt 来恢复文件</li>
<li>然后在提示中删除.swp文件即可，以后就不会有提示了</li>
</ul>
<h3 id="shell-bash-退出码">Shell Bash 退出码</h3>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">exit <span class="number">0</span> :也就是说调用环境就认为你的这个程序执行正确</span><br><span class="line">exit <span class="number">1</span> :一般是出错定义这个<span class="number">1</span>，也可以是其他数字，很多系统程序这个错误编号是有约定的含义的。 </span><br><span class="line">但不为<span class="number">0</span> 就表示程序运行出错。 </span><br><span class="line">exit <span class="number">127</span>: command <span class="keyword">not</span> found <span class="comment">#指令输入错误</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="ps--ef-grep-查找进程">ps -ef | grep 查找进程</h3>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">ps -ef | grep main.py  <span class="comment">#其中main.py是要查找的关键字</span></span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">ps命令将某个进程显示出来</span><br><span class="line"></span><br><span class="line">grep命令是查找</span><br><span class="line"></span><br><span class="line">中间的|是管道命令 是指ps命令与grep同时执行</span><br><span class="line"></span><br><span class="line">字段含义如下：</span><br><span class="line">UID    PID    PPID    C   STIME   TTY    TIME     CMD</span><br><span class="line"></span><br><span class="line">zzw   <span class="number">14124</span>  <span class="number">13991</span>   <span class="number">0</span>   <span class="number">00</span>:<span class="number">38</span>   pts/<span class="number">0</span>   <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span>  grep --color=auto dae</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">UID   ：程序被该 UID 所拥有</span><br><span class="line"></span><br><span class="line">PID   ：就是这个程序的 ID </span><br><span class="line"></span><br><span class="line">PPID  ：则是其上级父程序的ID</span><br><span class="line"></span><br><span class="line">C     ：CPU使用的资源百分比</span><br><span class="line"></span><br><span class="line">STIME ：系统启动时间</span><br><span class="line"></span><br><span class="line">TTY   ：登入者的终端机位置</span><br><span class="line"></span><br><span class="line">TIME  ：使用掉的CPU时间。</span><br><span class="line"></span><br><span class="line">CMD  ：所下达的是什么指令</span><br></pre></td></tr></tbody></table></figure>
<p>这里是两个shell命令通过管道进行了结合，第一个ps能够列出当前系统所有活跃的进程，然后通过grep 关键字查找就能找到带有关键字的进程。<code>找到PID</code>（PID是输出的第二列那个数字）再杀掉。</p>
<h3 id="uptime-查看系统运行时间">uptime 查看系统运行时间</h3>
<p>它依次显示：系统当前时间、系统已经运行了多长时间、目前有多少登陆用户、系统在过去的1分钟、5分钟、15分钟内的平均负载。</p>
<p><img src="E:\myBlog\source_posts\image-20200908232753725.png" alt="image-20200908232753725"></p>
<p>13:52:48 系统当前时间</p>
<p>up 12 days 21:48 ： 系统运行有12 天21小时48分钟了</p>
<h4 id="参考">参考</h4>
<blockquote>
<p><a href="https://www.jb51.net/LINUXjishu/353818.html" target="_blank" rel="noopener">Linux查看系统和进程运行时间的多种方法</a></p>
</blockquote>
<h3 id="服务器常用操作">服务器常用操作</h3>
<p>具体操作参考《服务器心得》</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
  </entry>
  <entry>
    <title>Anaconda管理虚拟环境</title>
    <url>/2020/07/21/2020-07-21-Anaconda%E7%AE%A1%E7%90%86%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/</url>
    <content><![CDATA[<h2 id="前言">前言</h2>
<p>之前一直用的是pycharm上的虚拟环境管理，后来觉得不利于管理。于是开始使用anaconda。</p>
<p>anaconda可以跨平台使用，我安装window版的anaconda，使用它管理虚拟环境。</p>
<h2 id="下载并安装">下载并安装</h2>
<p>到<a href="https://www.anaconda.com/download/" target="_blank" rel="noopener">官网</a>根据自己情况下载好anaconda版本，安装完毕，可通过以下两种方式进入anaconda操作环境</p>
<ul>
<li>1.可以增加到path环境变量，打开cmd/powershell，即可操作</li>
<li>2.打开Anaconda Prompt即可操作（推荐）</li>
</ul>
<p>在命令行输入<code>conda --version</code> ，显示出版本号，即安装成功</p>
<h2 id="anaconda管理虚拟环境">anaconda管理虚拟环境</h2>
<p>anaconda自带的是base环境，命令行前的（base）说明当前的虚拟环境是base环境 ，我们创建并管理的虚拟环境会放在<code>C:\Users\Administrator\anaconda3\envs</code> 里，</p>
<h3 id="创建新的虚拟环境">1.创建新的虚拟环境</h3>
<p>为自己的项目配置一个单独的虚拟环境</p>
<p>创建一个名字叫做python36的虚拟环境， 同时指定python的版本，如果本机内没有安装这个版本的python，就会自动下载安装。 后面<code>python==3.6</code>一般可以不添加</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">conda create -n python36  python==<span class="number">3.6</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="激活虚拟环境">2. 激活虚拟环境</h3>
<p>激活进入python36的虚拟环境。如果activate后什么参数都不加，就会进入anaconda自带的base环境</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">conda activate python36</span><br></pre></td></tr></tbody></table></figure>
<h3 id="退出虚拟环境">3.退出虚拟环境</h3>
<p>在激活新环境的时候要先退出目前的环境至base环境，然后才能activate 新环境，不然代码会bug</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">conda deactivate</span><br></pre></td></tr></tbody></table></figure>
<h3 id="查看所有的虚拟环境">4.查看所有的虚拟环境</h3>
<p>如果忘记了虚拟环境名称，可以如下命令</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">conda env list</span><br></pre></td></tr></tbody></table></figure>
<p><img src="https://i.loli.net/2020/07/22/gnUbt2eR7TZvjfy.png" alt="image-20200722093423681"></p>
<p>可以看到目前的虚拟环境状况，共6个虚拟环境。其中<code>*</code>表示当前操作的虚拟环境。</p>
<h3 id="安装第三方包">5.安装第三方包</h3>
<p>现在pyhton36的虚拟环境除了python自带的一些官方包之外是没有其他包</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">conda install requests <span class="comment">#安装requests包</span></span><br><span class="line">conda install keras==<span class="number">2.2</span><span class="number">.0</span> <span class="comment">#安装指定版本的keras</span></span><br></pre></td></tr></tbody></table></figure>
<p>安装完成之后我们输入python进入解释器并import requests包, 好使的.</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">anaconda search -t conda tensorflow <span class="comment"># 帮助找tensorflow可安装的包。 找到合适的资源（win64的版本包）  ，按照指示操作即可</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="卸载第三方包">6.卸载第三方包</h3>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">conda remove requests</span><br></pre></td></tr></tbody></table></figure>
<h3 id="查看环境的包信息">7.查看环境的包信息</h3>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">conda list <span class="comment"># 查看当前环境的包信息</span></span><br><span class="line">conda list -n python36 <span class="comment">#查看指定环境的包信息</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="卸载环境">8.卸载环境</h3>
<p>卸载test虚拟环境</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">conda remove -n test --all</span><br></pre></td></tr></tbody></table></figure>
<h3 id="conda查看tensorflow和keras的版本">9.conda查看tensorflow和keras的版本</h3>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="number">1.</span>进入python解释器</span><br><span class="line"><span class="number">2.</span><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf </span><br><span class="line"><span class="number">3.</span>tf.__version__ <span class="comment">#注意是两个下划线</span></span><br><span class="line"><span class="number">4.</span> exit（） <span class="comment">#退出python操作环境  或者 ctrl + Z</span></span><br><span class="line">(keras 同理）</span><br></pre></td></tr></tbody></table></figure>
<h3 id="虚拟环境的克隆">10.虚拟环境的克隆</h3>
<p>创建一个新的虚拟环境test， 将环境python36信息克隆到test中</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">conda create -n   test --clone  python36</span><br></pre></td></tr></tbody></table></figure>
<h3 id="导入导出环境">11.导入导出环境</h3>
<p>切换到了要导出的环境之后，使用命令将当前环境导出</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">conda env export &gt; environment.yml</span><br></pre></td></tr></tbody></table></figure>
<p>使用命令建立（导入）新的环境</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">conda env create -f environment.yml</span><br></pre></td></tr></tbody></table></figure>
<p>注： 不过由于不同的操作平台（例如从windows迁移到linux），迁移的时候会报错，找不到安装的包，因为不同平台的包的格式是不一样的，，每个版本号后面的一串字符就类似于手机的序列号，就是指示用于不同环境下的。目前我没有办法进行有效迁移</p>
<p><img src="https://i.loli.net/2020/07/22/MEFRgdAxiuzyhDZ.png" alt="image-20200722102049506"></p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">补充：anaconda所谓的创建虚拟环境其实就是安装了一个真实的python环境, </span><br><span class="line">只不过我们可以通过activate,conda等命令去随意的切换我们当前的python环境, </span><br><span class="line">用不同版本的解释器和不同的包环境去运行python脚本.</span><br><span class="line"></span><br><span class="line">conda 安装新版本python之后，会覆盖之前的版本</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">补充：conda、anaconda概念的差别</span><br><span class="line"></span><br><span class="line"><span class="number">1.</span> conda可以理解为一个工具，也是一个可执行命令，其核心功能是包管理与环境管理。</span><br><span class="line">包管理与pip的使用类似，环境管理则允许用户方便地安装不同版本的python并快速切换。</span><br><span class="line"></span><br><span class="line"><span class="number">2.</span> Anaconda是一个打包的集合，里面预装好了conda、某个版本的python、众多packages、</span><br><span class="line">科学计算工具等，也称为Python的一种发行版。</span><br></pre></td></tr></tbody></table></figure>
<h2 id="参考">参考</h2>
<p>Anaconda详细安装及使用教程： https://blog.csdn.net/ITLearnHall/article/details/81708148</p>
<p>Anaconda虚拟环境跨平台迁移： https://blog.csdn.net/lixufeng1028/article/details/80669525</p>
<p>不同tensorflow、keras、python的版本对应： https://docs.floydhub.com/guides/environments/</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
  </entry>
  <entry>
    <title>2020-07-20-git使用指南</title>
    <url>/2020/07/20/2020-07-20-git%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/</url>
    <content><![CDATA[<h3 id="利用git上传本地项目到github">利用git上传本地项目到github</h3>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">利用git上传本地项目到github</span><br><span class="line"></span><br><span class="line">1.绑定用户</span><br><span class="line"></span><br><span class="line">2.设置ssh key 并为github账号配置ssh key</span><br><span class="line"></span><br><span class="line">3.上传本地项目到github</span><br></pre></td></tr></tbody></table></figure>
<h4 id="绑定用户">1.绑定用户</h4>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">$ git config --global user.name "OopsAaron" # 注册github时的name</span><br><span class="line">$ git config --global user.email "1574468139@qq.com" # 注册github时的email</span><br></pre></td></tr></tbody></table></figure>
<h4 id="生成密钥ssh-key">2.生成密钥SSH key</h4>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa -C <span class="string">"1574468139@qq.com"</span></span><br></pre></td></tr></tbody></table></figure>
<p>此时，在根目录的.ssh文件中会生成公钥和密钥文件</p>
<p>打开<a href="https://link.zhihu.com/?target=http%3A//github.com/">github</a>，在头像下面点击<code>settings</code>，再点击<code>SSH and GPG keys</code>，新建一个SSH，名字随便。</p>
<p>git bash中输入</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">cat ~/.ssh/id_rsa.pub <span class="comment"># 复制.ssh里的公钥文件到github中</span></span><br></pre></td></tr></tbody></table></figure>
<p>将输出的内容复制到框中，点击确定保存。即建立了ssh连接</p>
<p><img src="https://i.loli.net/2020/07/20/45pTnsvKBbYPDCQ.png" alt="image-20200720163835551"></p>
<p><img src="https://i.loli.net/2020/08/10/TBqj7pxGadESw1m.png" alt="image-20200810192758491"></p>
<p>github上新建一个仓库后，在本地进行初始化本地仓库以及上传文件</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">git init #初始化仓库</span><br><span class="line"></span><br><span class="line">#复制git clone中的ssh地址</span><br><span class="line"></span><br><span class="line">git remote add origin git@github.com:OopsAaron/myBlog.git #远程连接仓库 （后部分是ssh地址）</span><br><span class="line"></span><br><span class="line">		（若出现fatal: remote origin already exists. 则执行 git remote rm origin，</span><br><span class="line"> 		 再重新执行git remote add origin git@github.com:OopsAaron/myBlog.git）</span><br></pre></td></tr></tbody></table></figure>
<p>在本地写md文件以及修改，然后进行提交到github， 并更新到网站上。</p>
<h4 id="提交到github">3.提交到github</h4>
<blockquote>
<p>提交到 github 中的myBlog文件夹中</p>
</blockquote>
<p>将现有的项目添加并上传 (在所在目录下右键git bash)</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">git add . #添加当前文件夹下的所有文件</span><br><span class="line"></span><br><span class="line">git commit -m "first commit " # 引号内是本次的提交说明 </span><br><span class="line"></span><br><span class="line">git push -u origin master # 提交本地分支到远程分支</span><br><span class="line">		(若出现failed to push som refs to， 则执行git pull origin master，</span><br><span class="line">		将远程服务器github上的master拉下来，再重新push)</span><br></pre></td></tr></tbody></table></figure>
<p>刷新github，即可看到上传的文件</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">git clone   https://github.com/raymond-zhao/cat-mall.git   ../Github/cat-mall </span><br><span class="line">#将cat-mall代码克隆到  ../Github/cat-mall 中</span><br></pre></td></tr></tbody></table></figure>
<h4 id="注">注</h4>
<p>在<code>git commit -m "first commit"</code>中，若名称<code>first commit</code>不加引号，则不能有空格</p>
<p>如下则会报错</p>
<p><img src="https://i.loli.net/2020/09/01/KJbWjRr6OsHzu4E.png" alt="image-20200901141302927"></p>
<p>不知道原因，我用<code>git status</code> 排查了一下，发现已经添加到暂存区了，所以应该是<code>commit</code>的原因</p>
<p><img src="https://i.loli.net/2020/09/01/smtWaV2eRCpbhJx.png" alt="image-20200901142150766"></p>
<p>可以看到，<code>changes to be committed</code> ，说明都在暂存区，等待提交<code>commit</code></p>
<p>将空格去掉，或者加上双引号即可成功</p>
<p><img src="https://i.loli.net/2020/09/01/1rntRV8cQGlaHfg.png" alt="image-20200901141341959"></p>
<p>之后就可以push到远程仓库中了</p>
<h4 id="参考">参考</h4>
<blockquote>
<p><a href="http://www.ruanyifeng.com/blog/2015/12/git-cheat-sheet.html" target="_blank" rel="noopener">常用git命令清单-阮一峰</a> <a href="http://www.ruanyifeng.com/blog/2012/08/how_to_read_diff.html" target="_blank" rel="noopener">读懂diff-阮一峰</a> <a href="https://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000" target="_blank" rel="noopener">git教程-廖雪峰</a> <a href="http://www.runoob.com/git/git-install-setup.html" target="_blank" rel="noopener">git教程-菜鸟教程</a> <a href="https://git-scm.com/book/zh/v2" target="_blank" rel="noopener">gitbook</a> <a href="http://gitbook.liuhui998.com/index." target="_blank" rel="noopener">Git Community Book</a></p>
<h6 id="从只会git-add-.的菜鸟到掌握git基本功能"><a href="https://juejin.im/post/6844903586023866375" target="_blank" rel="noopener">从只会git add .的菜鸟到掌握git基本功能</a></h6>
</blockquote>
<h3 id="hexo更新到网站">hexo更新到网站</h3>
<p>提交到github中的<code>OopsAaron</code>文件中托管，并更新到网站</p>
<p>在本地编写完md文件，所在目录下右键git bash</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">hexo new new_article  # 新建md文件</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">e.g.</span><br><span class="line">#打开Typora（已添加到环境变量）</span><br><span class="line">hexo new 2020-07-20-tensorflow笔记</span><br><span class="line">#Typora自动跳出新建笔记界面， 这时笔记会自带已预设好的title、description等(因为用的是hexo new 命令)</span><br><span class="line"></span><br><span class="line">注： 1.预设的标签不能空着，不用的话去掉</span><br><span class="line"> 	2. 在每一个title冒号后面要空格，再添加信息 ，不然会报错，示例如下</span><br></pre></td></tr></tbody></table></figure>
<p><img src="https://i.loli.net/2020/07/21/AeMFUSKwfWGkjpr.png" alt="image-20200721232830843"></p>
<h4 id="一般命令">一般命令</h4>
<figure class="highlight shell"><table><tbody><tr><td class="code"><pre><span class="line">hexo clean # </span><br><span class="line"></span><br><span class="line">hexo g  # ==hexo generate 生成静态网页至public目录 </span><br><span class="line"></span><br><span class="line">hexo s # ==hexo server 可以在本地预览效果 http://localhost:4000/   ctrl+C 退出预览</span><br><span class="line"></span><br><span class="line">hexo d # ==hexo deploy 部署到github上，并可以看到发布的文章</span><br><span class="line"></span><br><span class="line">hexo help # 查看帮助</span><br><span class="line"></span><br><span class="line">hexo version # 查看版本</span><br></pre></td></tr></tbody></table></figure>
<h4 id="组合命令">组合命令</h4>
<figure class="highlight shell"><table><tbody><tr><td class="code"><pre><span class="line">hexo s -g  # 生成静态网页并本地预览</span><br><span class="line"></span><br><span class="line">hexo d -g  # 生成并上传</span><br><span class="line"></span><br><span class="line">hexo clean &amp;&amp; hexo d -g &amp;&amp; hexo s  #一步到位</span><br></pre></td></tr></tbody></table></figure>
<h3 id="hexo高级操作">hexo高级操作</h3>
<p>hexo的根目录结构如下所示</p>
<p><img src="https://i.loli.net/2020/07/22/5EWcOhQ8MBCem4s.png" alt="image-20200722181143933"></p>
<h4 id="config.yml">_config.yml</h4>
<p>网站配置信息，也就是本文所称的<strong>站点配置文件</strong>，可以在此配置大部分的参数。</p>
<h4 id="scaffolds">scaffolds</h4>
<p>模版文件夹。新建文章时，Hexo 会根据 scaffold 来建立文件。</p>
<p>Hexo的模板是指新建的markdown文件中默认填充的内容。例如，在使用<code>hexo new 文章名</code>时，默认生成的md文件会包含如下内容：</p>
<p><img src="https://i.loli.net/2020/07/22/H6JKBeIwhcTkN5z.png" alt="image-20200722182540918"></p>
<p>默认内容就在scaffold/post.md中保存</p>
<p>假如对每篇博客我都需要添加分类<code>categories</code>，每次都手动添加太麻烦，我希望每次默认生成都有<code>categories:</code>，那么就可以在scaffold/post.md中添加categories</p>
<p>保存后，每次新建一篇文章时都会包含post.md中的内容。</p>
<p>当然，你也可以在scaffolds文件夹下创建自己的博客模板，我创建一个名为<code>blog</code>的模板：</p>
<p><img src="https://i.loli.net/2020/07/22/oLMuDQOmg6xaBYU.png" alt="image-20200722182806115"></p>
<p>通过如下命令调用我创建的blog模板新建文章，在<code>_posts</code>中生成md文件，并且是以blog.md为模板的</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">hexo new blog 2020-07-22-测试blog</span><br></pre></td></tr></tbody></table></figure>
<h4 id="public">public</h4>
<p>该文件夹中的内容将被最终push到github仓库中。</p>
<h4 id="source">source</h4>
<p>资源文件夹是存放用户资源的地方。除<code>_posts</code> 文件夹之外，开头命名为 _ (下划线)的文件 / 文件夹和隐藏的文件将会被忽略。Markdown 和 HTML 文件会被解析并放到 public 文件夹，而其他文件（如刚刚生成的about文件夹）会被拷贝到 public 文件夹。</p>
<h4 id="为github仓库添加readme">为github仓库添加readme</h4>
<p>既然 source 文件夹中的内容将全部被推送到 public 文件夹，public 文件夹中的内容又将被最终push到github仓库中，那么如果我们想要为github仓库添加readme.md，只要在 source 文件夹中创建就好了：</p>
<p>部署到github，就有readme了，但我们发现，README.md已经被解析为README.html，显示的全是html代码，并不是我们想要的文档格式的内容</p>
<p>为了解决这个问题，我们回到source文件夹，将<code>README.md</code>重命名为<code>README.MDOWN</code>，再部署到github即可</p>
<p>source文件夹中，.md会被解析为html，并放到 public 文件夹被push到github，但.MDWN不会被解析。</p>
<h4 id="themes">themes</h4>
<p>主题文件夹，下载的主题都保存在此文件夹下。Hexo 会根据主题来生成静态页面。</p>
<h4 id="参考-1">参考</h4>
<blockquote>
<p>Hexo+Github博客搭建： https://zhuanlan.zhihu.com/p/35668237 git上传文件： https://blog.csdn.net/sinat_20177327/article/details/76062030 git版本管理工具详细教程： https://www.cnblogs.com/cuixiaoying/p/11821797.html</p>
</blockquote>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer原理</title>
    <url>/2020/07/20/2020-07-20-%20Transformer%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<p>本文转自国外一篇讲解transformer的优秀博客<a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">《The Illustrated Transformer》</a>，阅读完之后对于transformer的理解深入了很多，正文如下：</p>
<p>In the <a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" target="_blank" rel="noopener">previous post, we looked at Attention</a> – a ubiquitous method in modern deep learning models. <code>Attention is a concept that helped improve the performance of neural machine translation applications</code>. In this post, we will look at <strong>The Transformer</strong> – a model that uses attention to boost the speed with which these models can be trained. The Transformers outperforms the Google Neural Machine Translation model in specific tasks. <code>The biggest benefit, however, comes from how The Transformer lends itself to parallelization.</code> It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their <a href="https://cloud.google.com/tpu/" target="_blank" rel="noopener">Cloud TPU</a> offering. So let’s try to break the model apart and look at how it functions.</p>
<p>The Transformer was proposed in the paper <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention is All You Need</a>. A TensorFlow implementation of it is available as a part of the <a href="https://github.com/tensorflow/tensor2tensor" target="_blank" rel="noopener">Tensor2Tensor</a> package. Harvard’s NLP group created a <a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">guide annotating the paper with PyTorch implementation</a>. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.</p>
<h2 id="a-high-level-look">A High-Level Look</h2>
<p>Let’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.</p>
<p><img src="https://i.loli.net/2020/07/24/MJXH3BE5vi4wFao.png" alt="img"></p>
<p>Popping open that Optimus Prime goodness, we see an encoding component, a decoding component, and connections between them.</p>
<p><img src="https://i.loli.net/2020/07/26/nvqhT56NEYVf9J7.png" alt="img"></p>
<p><code>The encoding component is a stack of encoders</code> (the paper stacks six of them on top of each other – there’s nothing magical about the number six, one can definitely experiment with other arrangements). <code>The decoding component is a stack of decoders of the same number.</code></p>
<p><img src="https://i.loli.net/2020/07/25/2zSYCOeodvq1mMV.png" alt="img"></p>
<p>The encoders are all identical in structure (yet they do not share weights). Each one is broken down into two sub-layers:</p>
<p><img src="https://i.loli.net/2020/07/25/5Aqnfeo9iPQs1Oh.png" alt="img"></p>
<p><code>The encoder’s inputs first flow through a self-attention layer –</code> a layer that helps the encoder look at other words in the input sentence as it encodes a specific word. We’ll look closer at self-attention later in the post.</p>
<p><code>The outputs of the self-attention layer are fed to a feed-forward neural network</code>. The exact same feed-forward network is independently applied to each position.</p>
<p><code>The decoder has both those layers, but between them is an attention layer that helps the decoder focus on relevant parts of the input sentence</code> (similar what attention does in <a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" target="_blank" rel="noopener">seq2seq models</a>).</p>
<p><img src="https://i.loli.net/2020/07/25/Kmc68dxUuqfQRk4.png" alt="img"></p>
<h2 id="bringing-the-tensors-into-the-picture">Bringing The Tensors Into The Picture</h2>
<p>Now that we’ve seen the major components of the model, let’s start to look at the various vectors/tensors and how they flow between these components to turn the input of a trained model into an output.</p>
<p>As is the case in NLP applications in general, <code>we begin by turning each input word into a vector using an</code> <a href="https://medium.com/deeper-learning/glossary-of-deep-learning-word-embedding-f90c3cec34ca" target="_blank" rel="noopener">embedding algorithm</a>.</p>
<p><img src="https://i.loli.net/2020/07/25/jkPRN5E2cDTfL4p.png" alt="img"> <code>Each word is embedded into a vector of size 512</code>. We'll represent those vectors with these simple boxes.</p>
<p>The embedding only happens in the bottom-most encoder. <code>The abstraction that is common to all the encoders is that they receive a list of vectors each of the size 512 – In the bottom encoder that would be the word embeddings, but in other encoders, it would be the output of the encoder that’s directly below.</code> The size of this list is hyperparameter we can set – basically it would be the length of the longest sentence in our training dataset.</p>
<p>After embedding the words in our input sequence, each of them flows through each of the two layers of the encoder.</p>
<p><img src="https://i.loli.net/2020/07/25/Aj4sHXynq1lOa2x.png" alt="img"></p>
<p>Here we begin to see <code>one key property of the Transformer, which is that the word in each position flows through its own path in the encoder</code>. <code>There are dependencies between these paths</code> in the self-attention layer. The feed-forward layer does not have those dependencies, however, and <code>thus the various paths can be executed in parallel while flowing through the feed-forward layer.</code></p>
<p>Next, we’ll switch up the example to a shorter sentence and we’ll look at what happens in each sub-layer of the encoder.</p>
<h2 id="now-were-encoding">Now We’re Encoding!</h2>
<p>As we’ve mentioned already, an encoder receives a list of vectors as input. It processes this list by passing these vectors into a ‘self-attention’ layer, then into a feed-forward neural network, then sends out the output upwards to the next encoder.</p>
<p><img src="https://i.loli.net/2020/07/25/e34cngFjsyxNvSq.png" alt="img"> The word at each position passes through a self-attention process. Then, they each pass through a feed-forward neural network -- the exact same network with each vector flowing through it separately.</p>
<h2 id="self-attention-at-a-high-level">Self-Attention at a High Level</h2>
<p>Don’t be fooled by me throwing around the word “self-attention” like it’s a concept everyone should be familiar with. I had personally never came across the concept until reading the Attention is All You Need paper. Let us distill how it works.</p>
<p>Say the following sentence is an input sentence we want to translate:</p>
<p>”<code>The animal didn't cross the street because it was too tired</code>”</p>
<p>What does “it” in this sentence refer to? Is it referring to the street or to the animal? It’s a simple question to a human, but not as simple to an algorithm.</p>
<p><code>When the model is processing the word “it”, self-attention allows it to associate “it” with “animal”.</code></p>
<p><code>As the model processes each word (each position in the input sequence), self attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word.</code></p>
<p>If you’re familiar with RNNs, think of how maintaining a hidden state allows an RNN to incorporate its representation of previous words/vectors it has processed with the current one it’s processing. Self-attention is the method the Transformer uses to bake the “understanding” of other relevant words into the one we’re currently processing.</p>
<p><img src="https://i.loli.net/2020/07/25/VB6jDHY9PuLrShx.png" alt="img"> <code>As we are encoding the word "it" in encoder #5 (the top encoder in the stack), part of the attention mechanism was focusing on "The Animal", and baked a part of its representation into the encoding of "it".</code></p>
<p>Be sure to check out the <a href="https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb" target="_blank" rel="noopener">Tensor2Tensor notebook</a> where you can load a Transformer model, and examine it using this interactive visualization.</p>
<h2 id="self-attention-in-detail">Self-Attention in Detail</h2>
<p>Let’s first look at <code>how to calculate self-attention using vectors</code>, then proceed to look at how it’s actually implemented – using matrices.</p>
<p>The <strong>first step</strong> in calculating self-attention is to <code>create three vectors from each of the encoder’s input vectors</code> (in this case, the embedding of each word). So <code>for each word, we create a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that we trained during the training process.</code></p>
<p>Notice that these new vectors are smaller in dimension than the embedding vector. <code>Their dimensionality is 64</code>, while the embedding and encoder input/output vectors have dimensionality of 512. <code>They don’t HAVE to be smaller, this is an architecture choice to make the computation of multiheaded attention (mostly) constant.</code></p>
<p><img src="https://i.loli.net/2020/07/25/aCuE9FmWZyOn2Xs.png" alt="img"> Multiplying x1 by the WQ weight matrix produces q1, the "query" vector associated with that word. We end up creating a "query", a "key", and a "value" projection of each word in the input sentence.</p>
<p>What are the “query”, “key”, and “value” vectors?</p>
<p>They’re abstractions that are useful for calculating and thinking about attention. Once you proceed with reading how attention is calculated below, you’ll know pretty much all you need to know about the role each of these vectors plays.</p>
<p><code>The **second step** in calculating self-attention is to calculate a score</code>. Say we’re calculating the self-attention for the first word in this example, “Thinking”. We need to score each word of the input sentence against this word. <code>The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position.</code></p>
<p>The score is calculated by taking the dot product of the query vector with the key vector of the respective word we’re scoring. So if we’re processing the self-attention for the word in position #1, the first score would be the dot product of q1 and k1. The second score would be the dot product of q1 and k2.</p>
<p><img src="https://i.loli.net/2020/07/25/xgtf9K4cJkRpbV2.png" alt="img"></p>
<p><code>The **third and forth steps** are to divide the scores by 8</code> (the square root of the dimension of the <code>key vectors</code> used in the paper – 64. This leads to having more stable gradients. There could be other possible values here, but this is the default), <code>then pass the result through a softmax operation. Softmax normalizes the scores so they’re all positive and add up to 1.</code></p>
<ul>
<li></li>
</ul>
<p><img src="https://i.loli.net/2020/07/25/u7HQAF28ci3oPpq.png" alt="img"></p>
<p>This softmax score determines how much each word will be expressed at this position. <code>Clearly the word at this position will have the highest softmax score, but sometimes it’s useful to attend to another word that is relevant to the current word.</code></p>
<p><code>The **fifth step** is to multiply each value vector by the softmax score</code> (in preparation to sum them up). The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers like 0.001, for example).</p>
<p><code>The **sixth step** is to sum up the weighted value vectors.</code> This produces the output of the self-attention layer at this position (for the first word).</p>
<p><img src="https://i.loli.net/2020/07/25/SNa4Km1EvVpyXIr.png" alt="img"></p>
<p>That concludes the self-attention calculation. <code>The resulting vector is one we can send along to the feed-forward neural network</code>. In the actual implementation, however, <code>this calculation is done in matrix form for faster processing. So let’s look at that now that we’ve seen the intuition of the calculation on the word level.</code></p>
<h2 id="matrix-calculation-of-self-attention">Matrix Calculation of Self-Attention</h2>
<p><strong>The first step</strong> is to calculate the Query, Key, and Value matrices. We do that by packing our embeddings into a matrix X, and multiplying it by the weight matrices we’ve trained (WQ, WK, WV).</p>
<p><img src="https://i.loli.net/2020/07/25/jAwgJ62muho8nvI.png" alt="img"> Every row in the X matrix corresponds to a word in the input sentence. We again see the difference in size of the embedding vector (512, or 4 boxes in the figure), and the q/k/v vectors (64, or 3 boxes in the figure)</p>
<p><strong>Finally</strong>, since we’re dealing with matrices, we can condense steps two through six in one formula to calculate the outputs of the self-attention layer.</p>
<p><img src="https://i.loli.net/2020/07/25/PCcj7Rlpn9XHaMF.png" alt="img"> The self-attention calculation in matrix form</p>
<h2 id="the-beast-with-many-heads">The Beast With Many Heads</h2>
<p><code>The paper further refined the self-attention layer by adding a mechanism called “multi-headed” attention. This improves the performance of the attention layer in two ways:</code></p>
<ol type="1">
<li><code>It expands the model’s ability to focus on different positions</code>. Yes, <code>in the example above, z1 contains a little bit of every other encoding, but it could be dominated by the the actual word itself.</code> It would be useful if we’re translating a sentence like “The animal didn’t cross the street because it was too tired”, we would want to know which word “it” refers to.</li>
<li><code>It gives the attention layer multiple “representation subspaces</code>”. As we’ll see next, <code>with multi-headed attention we have not only one, but multiple sets of Query/Key/Value weight matrices</code> (the Transformer uses eight attention heads, so we end up with eight sets for each encoder/decoder). <code>Each of these sets is randomly initialized. Then, after training, each set is used to project the input embeddings (or vectors from lower encoders/decoders) into a different representation subspace.</code></li>
</ol>
<p><img src="https://i.loli.net/2020/07/25/7UB4w8oijMvCfyq.png" alt="img"> With multi-headed attention, <code>we maintain separate Q/K/V weight matrices for each head resulting in different Q/K/V matrices.</code> As we did before, we multiply X by the WQ/WK/WV matrices to produce Q/K/V matrices.</p>
<p>If we do the same self-attention calculation <code>we outlined above, just eight different times with different weight matrices, we end up with eight different Z matrices</code></p>
<p><img src="https://i.loli.net/2020/07/25/p9WTsRgd2ye7F8I.png" alt="img"></p>
<p><code>This leaves us with a bit of a challenge.</code> The feed-forward layer is not expecting eight matrices – <code>it’s expecting a single matrix (a vector for each word). So we need a way to condense these eight down into a single matrix.</code></p>
<p>How do we do that? <code>We concat the matrices then multiple them by an additional weights matrix WO.</code></p>
<p><img src="https://i.loli.net/2020/07/25/i7bU1pIfDqgZuAx.png" alt="img"></p>
<p>That’s pretty much all there is to multi-headed self-attention. It’s quite a handful of matrices, I realize. Let me try to put them all in one visual so we can look at them in one place</p>
<p><img src="https://i.loli.net/2020/07/25/YIVPms5cNDj2Mpq.png" alt="img"></p>
<p>Now that we have touched upon attention heads, let’s revisit our example from before to see <code>where the different attention heads are focusing as we encode the word “it” in our example sentence:</code></p>
<p><img src="https://i.loli.net/2020/07/25/nXZ4OMkluRFjwJH.png" alt="img"> As we encode the word "it", <code>one attention head is focusing most on "the animal", while another is focusing on "tired" -- in a sense, the model's representation of the word "it" bakes in some of the representation of both "animal" and "tired".</code></p>
<p>If we add all the attention heads to the picture, however, things can be harder to interpret:</p>
<p><img src="https://i.loli.net/2020/07/25/jk2hUsJGM6qxRwD.png" alt="img"></p>
<h2 id="representing-the-order-of-the-sequence-using-positional-encoding">Representing The Order of The Sequence Using <code>Positional Encoding</code></h2>
<p>One thing that’s missing from the model as we have described it so far is a way to account for the order of the words in the input sequence.</p>
<p>To address this, the transformer adds a vector to each input embedding. <code>These vectors follow a specific pattern that the model learns, which helps it determine the position of each word, or the distance between different words in the sequence.</code> The intuition here is that adding these values to the embeddings provides meaningful distances between the embedding vectors once they’re projected into Q/K/V vectors and during dot-product attention.</p>
<p><img src="https://i.loli.net/2020/07/25/ypGiMmaYjeIo5ct.png" alt="img"> To give the model a sense of the order of the words, we add positional encoding vectors -- the values of which follow a specific pattern.</p>
<p>If we assumed the <code>embedding has a dimensionality of 4,</code> the actual positional encodings would look like this:</p>
<p><img src="https://i.loli.net/2020/07/25/u3Kn2rIF68DWmtX.png" alt="img"> A real example of positional encoding with a toy embedding size of 4</p>
<p>What might this pattern look like?</p>
<p>In the following figure, <code>each row corresponds the a positional encoding of a vector.</code> So the <code>first row would be the vector we’d add to the embedding of the first word in an input sequence</code>. <code>Each row contains 512 values – each with a value between 1 and -1. We’ve color-coded them so the pattern is visible.</code></p>
<p><img src="https://i.loli.net/2020/07/25/XTrgBuPAKbU58lL.png" alt="img"> A real example of <code>positional encoding for 20 words (rows) with an embedding size of 512 (columns)</code>. You can see that it appears split in half down the center. <code>That's because the values of the left half are generated by one function (which uses sine), and the right half is generated by another function (which uses cosine).</code> They're then concatenated to form each of the positional encoding vectors.</p>
<p>The formula for positional encoding is described in the paper (section 3.5). You can see the code for generating positional encodings in <a href="https://github.com/tensorflow/tensor2tensor/blob/23bd23b9830059fbc349381b70d9429b5c40a139/tensor2tensor/layers/common_attention.py" target="_blank" rel="noopener"><code>get_timing_signal_1d()</code></a>. <code>This is not the only possible method for positional encoding. It, however, gives the advantage of being able to scale to unseen lengths of sequences</code> (e.g. if our trained model is asked to translate a sentence longer than any of those in our training set).</p>
<p><strong>July 2020 Update:</strong> The positional encoding shown above is from the Tranformer2Transformer implementation of the Transformer. The method shown in the paper is slightly different in that it doesn’t directly concatenate, but interweaves the two signals. The following figure shows what that looks like. <a href="https://github.com/jalammar/jalammar.github.io/blob/master/notebookes/transformer/transformer_positional_encoding_graph.ipynb" target="_blank" rel="noopener">Here’s the code to generate it</a>:</p>
<p><img src="https://i.loli.net/2020/07/25/9V4sImeBjPaXQpE.png" alt="img"></p>
<h2 id="the-residuals">The <code>Residuals</code></h2>
<p>One detail in the architecture of the encoder that we need to mention before moving on, is that each sub-layer (self-attention, ffnn) in each encoder has a residual connection around it, and is followed by a <a href="https://arxiv.org/abs/1607.06450" target="_blank" rel="noopener">layer-normalization</a> step.</p>
<p><img src="https://i.loli.net/2020/07/25/uteZNE9q7wxIo6Y.png" alt="img"></p>
<p>If we’re to visualize the vectors and the layer-norm operation associated with self attention, it would look like this:</p>
<p><img src="https://i.loli.net/2020/07/25/2GZuRSainMsYfHQ.png" alt="img"></p>
<p>This goes for the sub-layers of the decoder as well. If we’re to think of a Transformer of 2 stacked encoders and decoders, it would look something like this:</p>
<p><img src="https://i.loli.net/2020/07/25/9RHz2i8nZhyMPdt.png" alt="img"></p>
<h2 id="the-decoder-side">The Decoder Side</h2>
<p>Now that we’ve covered most of the concepts on the encoder side, we basically know how the components of decoders work as well. But let’s take a look at how they work together.</p>
<p>The encoder start by processing the input sequence. <code>The output of the top encoder is then transformed into a set of attention vectors K and V. These are to be used by each decoder in its “encoder-decoder attention” layer which helps the decoder focus on appropriate places in the input sequence:</code></p>
<p><img src="https://i.loli.net/2020/07/26/2MZ3SophkRgD9eJ.gif" alt="MQfaxEpcKV8AGDv"></p>
<p>After finishing the encoding phase, we begin the decoding phase. <code>Each step in the decoding phase outputs an element from the output sequence (the English translation sentence in this case).</code></p>
<p>The following steps repeat the process until a <code>special symbol</code> is reached indicating the transformer decoder has completed its output. <code>The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did</code>. And just like we did with the encoder inputs, <code>we embed and add positional encoding</code> to those decoder inputs to indicate the position of each word.</p>
<p><img src="https://jalammar.github.io/images/t/transformer_decoding_2.gif" alt="img"></p>
<p><img src="https://i.loli.net/2020/07/26/lwq4T3gIaEZHdu5.png" alt="image-20200726205210899"></p>
<p>The self attention layers in the decoder operate in a slightly different way than the one in the encoder:</p>
<p><code>In the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence</code>. This is done by <code>masking future positions (</code>setting them to <code>-inf</code>) before the softmax step in the self-attention calculation.</p>
<p><code>The “Encoder-Decoder Attention” layer works just like multiheaded self-attention</code>, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack.</p>
<h2 id="the-final-linear-and-softmax-layer">The Final Linear and Softmax Layer</h2>
<p><code>The decoder stack outputs a vector of floats</code>. How do we turn that into a word? That’s the job of the final Linear layer which is followed by a Softmax Layer.</p>
<p><code>The Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector.</code></p>
<p><code>Let’s assume that our model knows 10,000 unique English words (our model’s “output vocabulary”) that it’s learned from its training dataset. This would make the logits vector 10,000 cells wide – each cell corresponding to the score of a unique word</code>. That is how we interpret the output of the model followed by the Linear layer.</p>
<p><code>The softmax layer then turns those scores into probabilities</code> (all positive, all add up to 1.0). <code>The cell with the highest probability is chosen</code>, and the <code>word associated with it is produced as the output for this time step.</code></p>
<p><img src="https://i.loli.net/2020/07/25/w62DtS8vVYpkQqg.png" alt="img"> This figure starts from the bottom with the vector produced as the output of the decoder stack. It is then turned into an output word.</p>
<h2 id="recap-of-training">Recap Of Training</h2>
<p>Now that we’ve covered the entire forward-pass process through a trained Transformer, it would be useful to glance at the intuition of training the model.</p>
<p><code>During training, an untrained model would go through the exact same forward pass. But since we are training it on a labeled training dataset, we can compare its output with the actual correct output.</code></p>
<p>To visualize this, let’s assume our output vocabulary only contains six words(“a”, “am”, “i”, “thanks”, “student”, and “<eos>” (short for ‘end of sentence’)).</eos></p>
<p><img src="https://i.loli.net/2020/07/25/BgKxbL57uJGiZ4E.png" alt="img"> <code>The output vocabulary of our model is created in the preprocessing phase before we even begin training.</code></p>
<p>Once we define our output vocabulary, we can use a vector of the same width to indicate each word in our vocabulary. This also known as <code>one-hot encoding</code>. So for example, we can indicate the word “am” using the following vector:</p>
<p><img src="https://i.loli.net/2020/07/25/ot8s4Gl6LNC3pdk.png" alt="img"> Example: one-hot encoding of our output vocabulary</p>
<p>Following this recap, let’s discuss the model’s loss function – the metric we are optimizing during the training phase to lead up to a trained and hopefully amazingly accurate model.</p>
<h2 id="the-loss-function">The <code>Loss Function</code></h2>
<p>Say we are training our model. Say it’s our first step in the training phase, and we’re training it on a simple example – translating “merci” into “thanks”.</p>
<p>What this means, is that we want the output to be a probability distribution indicating the word “thanks”. But since this model is not yet trained, that’s unlikely to happen just yet.</p>
<p><img src="https://i.loli.net/2020/07/25/qcKFVhUSmiNIkTJ.png" alt="img"> <code>Since the model's parameters (weights) are all initialized randomly, the (untrained) model produces a probability distribution with arbitrary values for each cell/word.</code> We can compare it with the actual output, then tweak all the model's weights using <code>backpropagation</code> to make the output closer to the desired output.</p>
<p>How do you compare two probability distributions? We simply subtract one from the other. For more details, look at <a href="https://colah.github.io/posts/2015-09-Visual-Information/" target="_blank" rel="noopener">cross-entropy</a> and <a href="https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained" target="_blank" rel="noopener">Kullback–Leibler divergence</a>.</p>
<p>But note that this is an oversimplified example. More realistically, we’ll use a sentence longer than one word. For example – input: “je suis étudiant” and expected output: “i am a student”. What this really means, is that we want our model to successively output probability distributions where:</p>
<ul>
<li><code>Each probability distribution is represented by a vector of width vocab_size</code> (6 in our toy example, but more realistically a number like 30,000 or 50,000)</li>
<li>The first probability distribution has the highest probability at the cell associated with the word “i”</li>
<li>The second probability distribution has the highest probability at the cell associated with the word “am”</li>
<li>And so on, until the fifth output distribution indicates ‘<code>&lt;end of sentence&gt;</code>’ symbol, which also has a cell associated with it from the 10,000 element vocabulary.</li>
</ul>
<p><img src="https://i.loli.net/2020/07/25/VKGpQ8MqYTnrROs.png" alt="img"> The targeted probability distributions we'll train our model against in the training example for one sample sentence.</p>
<p>After training the model for enough time on a large enough dataset, we would <code>hope the produced probability distributions would look like this:</code></p>
<p><img src="https://jalammar.github.io/images/t/output_trained_model_probability_distributions.png" alt="img"> Hopefully upon training, the model would output the right translation we expect. Of course it's no real indication if this phrase was part of the training dataset (see: <a href="https://www.youtube.com/watch?v=TIgfjmp-4BA" target="_blank" rel="noopener">cross validation</a>). Notice that every position gets a little bit of probability even if it's unlikely to be the output of that time step -- that's a very useful property of softmax which helps the training process.</p>
<p>Now, <code>because the model produces the outputs one at a time, we can assume that the model is selecting the word with the highest probability from that probability distribution and throwing away the rest.</code> That’s one way to do it (called <code>greedy decoding</code>). Another way to do it would be to hold on to, say, the top two words (say, ‘I’ and ‘a’ for example), then in the next step, run the model twice: once assuming the first output position was the word ‘I’, and another time assuming the first output position was the word ‘a’, and whichever version produced less error considering both positions #1 and #2 is kept. We repeat this for positions #2 and #3…etc. This method is called “<code>beam search</code>”, where in our example, beam_size was two (meaning that at all times, two partial hypotheses (unfinished translations) are kept in memory), and top_beams is also two (meaning we’ll return two translations). These are both hyperparameters that you can experiment with.</p>
<h2 id="go-forth-and-transform">Go Forth And Transform</h2>
<p>I hope you’ve found this a useful place to start to break the ice with the major concepts of the Transformer. If you want to go deeper, I’d suggest these next steps:</p>
<ul>
<li>Read the <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need</a> paper, the Transformer blog post (<a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html" target="_blank" rel="noopener">Transformer: A Novel Neural Network Architecture for Language Understanding</a>), and the <a href="https://ai.googleblog.com/2017/06/accelerating-deep-learning-research.html" target="_blank" rel="noopener">Tensor2Tensor announcement</a>.</li>
<li>Watch <a href="https://www.youtube.com/watch?v=rBCqOTEfxvg" target="_blank" rel="noopener">Łukasz Kaiser’s talk</a> walking through the model and its details</li>
<li>Play with the <a href="https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb" target="_blank" rel="noopener">Jupyter Notebook provided as part of the Tensor2Tensor repo</a></li>
<li>Explore the <a href="https://github.com/tensorflow/tensor2tensor" target="_blank" rel="noopener">Tensor2Tensor repo</a>.</li>
</ul>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>]]></content>
      <categories>
        <category>DeepLearning</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
      </tags>
  </entry>
</search>
