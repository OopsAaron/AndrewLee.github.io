<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>2020-07-28-transformer解读-pytorch版本</title>
    <url>/2020/07/28/2020-07-28-transformer-pytorch/</url>
    <content><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>最近几天都在阅读哈佛pytorch实现transformer的代码，代码风格很好，很值得参考和研读。和实验室师兄又在一起讨论了几次，代码思路和实现过程基本都了解了，对于原论文 <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">“Attention is All You Need”</a> 中关于transformer模型的理解又深入了许多。果然要想了解模型，还是要好好研读实现代码。以便于后面自己模型的研究。</p>
<p>本篇是对实现代码的注释，加上了自己的理解，也会有一些函数的介绍扩充。</p>
<h4 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h4><blockquote>
<p>解读的是哈佛的一篇transformer的pytorch版本实现</p>
<p><a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">http://nlp.seas.harvard.edu/2018/04/03/attention.html</a></p>
<p>参考另一篇博客</p>
<p><a href="http://fancyerii.github.io/2019/03/09/transformer-codes/" target="_blank" rel="noopener">http://fancyerii.github.io/2019/03/09/transformer-codes/</a></p>
<p>Transformer注解及PyTorch实现（上）</p>
<p><a href="https://www.jiqizhixin.com/articles/2018-11-06-10" target="_blank" rel="noopener">https://www.jiqizhixin.com/articles/2018-11-06-10</a></p>
<p>Transformer注解及PyTorch实现（下）</p>
<p><a href="https://www.jiqizhixin.com/articles/2018-11-06-18" target="_blank" rel="noopener">https://www.jiqizhixin.com/articles/2018-11-06-18</a></p>
<p>训练过程中的 Mask实现</p>
<p><a href="https://www.cnblogs.com/wevolf/p/12484972.html" target="_blank" rel="noopener">https://www.cnblogs.com/wevolf/p/12484972.html</a></p>
</blockquote>
<h3 id="The-Annotated-Transformer"><a href="#The-Annotated-Transformer" class="headerlink" title="The Annotated Transformer"></a>The Annotated Transformer</h3><p><img src="https://i.loli.net/2020/07/28/NUAyXWJ5DzHmjuv.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># !pip install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl numpy matplotlib spacy torchtext seaborn </span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> math, copy, time</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn</span><br><span class="line">seaborn.set_context(context=<span class="string">"talk"</span>)</span><br></pre></td></tr></table></figure>





<p>Transformer使用了Self-Attention机制，它在编码每一词的时候都能够注意(attend to)整个句子，从而可以解决长距离依赖的问题，同时计算Self-Attention可以用矩阵乘法一次计算所有的时刻，因此可以充分利用计算资源(CPU/GPU上的矩阵运算都是充分优化和高度并行的)。</p>
<h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>Most competitive neural sequence transduction models have an encoder-decoder structure <a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="noopener">(cite)</a>. Here, <code>the encoder maps an input sequence of symbol representations (x1,…,xn)(x1,…,xn) to a sequence of continuous representations z=(z1,…,zn)z=(z1,…,zn). Given z, the decoder then generates an output sequence (y1,…,ym)(y1,…,ym) of symbols one element at a time.</code> At each step the model is auto-regressive <a href="https://arxiv.org/abs/1308.0850" target="_blank" rel="noopener">(cite)</a>, consuming the previously generated symbols as additional input when generating the next.</p>
<p><strong>EncoderDecoder定义了一种通用的Encoder-Decoder架构</strong>，具体的Encoder、Decoder、src_embed、target_embed和generator都是构造函数传入的参数。这样我们<strong>做实验更换不同的组件就会更加方便</strong>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderDecoder</span><span class="params">(nn.Module)</span>:</span> <span class="comment">#定义的是整个模型 ，不包括generator</span></span><br><span class="line">    </span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">   标准的Encoder-Decoder架构。这是很多模型的基础</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    class里， init函数是实例化一个对象的时候用于初始化对象用的</span></span><br><span class="line"><span class="string">    forward函数是在执行调用对象的时候使用， 需要传入正确的参数 </span></span><br><span class="line"><span class="string">    在执行时候调用__call__方法，然后再call里再调用forward</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, encoder, decoder, src_embed, tgt_embed, generator)</span>:</span></span><br><span class="line">        super(EncoderDecoder, self).__init__()</span><br><span class="line">        <span class="comment"># encoder和decoder都是构造的时候传入的，这样会非常灵活</span></span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line">        <span class="comment"># 源语言和目标语言的embedding，包括embedding层和position encode层</span></span><br><span class="line">        self.src_embed = src_embed <span class="comment">#源数据集的嵌入</span></span><br><span class="line">        self.tgt_embed = tgt_embed <span class="comment">#目标数据集的嵌入，作为decoder的输入</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        generator后面会讲到，就是根据Decoder的隐状态输出当前时刻的词</span></span><br><span class="line"><span class="string">	    基本的实现就是隐状态输入一个全连接层，全连接层的输出大小是词的个数</span></span><br><span class="line"><span class="string">		然后接一个softmax变成概率</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.generator = generator</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, src, tgt, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="comment">#首先调用encode方法对输入进行编码，然后调用decode方法解码</span></span><br><span class="line">        <span class="keyword">return</span> self.decode(self.encode(src, src_mask), src_mask,</span><br><span class="line">                            tgt, tgt_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, src, src_mask)</span>:</span></span><br><span class="line">        <span class="comment"># 调用encoder来进行编码，传入的参数embedding的src和src_mask</span></span><br><span class="line">        <span class="keyword">return</span> self.encoder(self.src_embed(src), src_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span><span class="params">(self, memory, src_mask, tgt, tgt_mask)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask) <span class="comment">#目标是输入的一部分</span></span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span>  <span class="comment">#decoder后面的linear+softmax</span></span><br><span class="line">    <span class="comment"># 根据Decoder的隐状态输出一个词</span></span><br><span class="line">	<span class="comment"># d_model是Decoder输出的大小，vocab是词典大小 （数据语料有多少词 ）</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, vocab)</span>:</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        self.proj = nn.Linear(d_model, vocab) <span class="comment">#全连接，作为softmax的输入。</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(self.proj(x), dim=<span class="number">-1</span>) <span class="comment">#softmax的log值</span></span><br></pre></td></tr></table></figure>



<p>注：<code>Generator返回的是softmax的log值</code>。在PyTorch里为了计算交叉熵损失，有两种方法。第一种方法是使用<strong>nn.CrossEntropyLoss()</strong>，一种是使用<strong>NLLLoss()</strong>。很多开源代码里第二种更常见，</p>
<p>我们先看CrossEntropyLoss，它就是计算交叉熵损失函数，比如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">x = torch.randn(<span class="number">1</span>, <span class="number">5</span>)</span><br><span class="line">y = torch.empty(<span class="number">1</span>, dtype=torch.long).random_(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">loss = criterion(x, y)</span><br></pre></td></tr></table></figure>

<p>比如上面的代码，假设是5分类问题，x表示模型的输出logits(batch=1)，而y是真实分类的下标(0-4)。实际的计算过程为：<img src="https://i.loli.net/2020/08/06/KyPspa4Cqef6m8Q.png" alt="image-20200806000621448" style="zoom: 67%;" /></p>
<p>比如logits是[0,1,2,3,4]，真实分类是3，那么上式就是：</p>
<img src="https://i.loli.net/2020/08/06/i7mfUWAeHE5P1zd.png" alt="image-20200806000641945" style="zoom:67%;" />

<p>因此我们也可以使用NLLLoss()配合F.log_softmax函数(或者nn.LogSoftmax，这不是一个函数而是一个Module了)来实现一样的效果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">m = nn.LogSoftmax(dim=<span class="number">1</span>)</span><br><span class="line">criterion = nn.NLLLoss()</span><br><span class="line">x = torch.randn(<span class="number">1</span>, <span class="number">5</span>)</span><br><span class="line">y = torch.empty(<span class="number">1</span>, dtype=torch.long).random_(<span class="number">5</span>)</span><br><span class="line">loss = criterion(m(x), y)</span><br></pre></td></tr></table></figure>

<p>NLLLoss(Negative Log Likelihood Loss)是计算负log似然损失。它输入的x是log_softmax之后的结果(长度为5的数组)，y是真实分类(0-4)，输出就是x[y]。因此上面的代码为：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">criterion(m(x), y)=m(x)[y]</span><br></pre></td></tr></table></figure>



<p>The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.</p>
<p><img src="https://i.loli.net/2020/07/28/P3fSgRhrmFtlpxY.png" alt="png"></p>
<h3 id="Encoder-and-Decoder-Stacks"><a href="#Encoder-and-Decoder-Stacks" class="headerlink" title="Encoder and Decoder Stacks"></a>Encoder and Decoder Stacks</h3><h4 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h4><p>Encoder和Decoder都是由N个相同结构的Layer堆积(stack)而成。<strong>因此我们首先定义clones函数，用于克隆相同的SubLayer。</strong></p>
<p>这里使用了<strong>nn.ModuleList</strong>，ModuleList就像一个普通的Python的List，我们可以使用下标来访问它，它的好处是传入的ModuleList的所有Module都会注册的PyTorch里，这样Optimizer就能找到这里面的参数，从而能够用梯度下降更新这些参数。但是nn.ModuleList并不是Module(的子类)，因此它没有forward等方法，我们通常把它放到某个Module里。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clones</span><span class="params">(module, N)</span>:</span>  <span class="comment">#克隆N层，是个层数的列表。 copy.deepcopy是深复制， 一个改变不会影响另一个</span></span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="keyword">for</span> _ <span class="keyword">in</span> range(N)]) <span class="comment">#复制N=6层</span></span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span><span class="params">(nn.Module)</span>:</span>  <span class="comment">#定义编码器 </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#Encoder是N个EncoderLayer的stack</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layer, N)</span>:</span> <span class="comment"># 根据make_model定义，layer = encoderlayer （sublayer）</span></span><br><span class="line">        super(Encoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N) <span class="comment">#编码器有6层编码层，根据上述函数的定义，module=layer</span></span><br><span class="line">        self.norm = LayerNorm(layer.size) <span class="comment">#调用下面的LayerNorm。 分开定义是因为 LayerNorm = 2* layer</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask)</span>:</span> </span><br><span class="line">      	 <span class="comment">#逐层进行处理</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers: <span class="comment"># x 在每一层中传递</span></span><br><span class="line">            x = layer(x, mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x) <span class="comment">#最终encoder的返回值</span></span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LayerNorm</span><span class="params">(nn.Module)</span>:</span> <span class="comment">#add &amp; norm部分  作为每一个子层的输出</span></span><br><span class="line">    <span class="string">"Construct a layernorm module (See citation for details)."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, features, eps=<span class="number">1e-6</span>)</span>:</span> <span class="comment">#feature = layer.size layer的形状</span></span><br><span class="line">        super(LayerNorm, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.a_2 = nn.Parameter(torch.ones(features))  <span class="comment">#将后面的tensor转换为可优化的参数</span></span><br><span class="line">        self.b_2 = nn.Parameter(torch.zeros(features))</span><br><span class="line">        self.eps = eps <span class="comment">#很小的值</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span> <span class="comment"># 平均值和标准差</span></span><br><span class="line">        mean = x.mean(<span class="number">-1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        std = x.std(<span class="number">-1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self.a_2 * (x - mean) / (std + self.eps) + self.b_2 <span class="comment">#输出</span></span><br></pre></td></tr></table></figure>

<p><strong>不管是Self-Attention还是全连接层，都首先是LayerNorm，然后是Self-Attention/Dense，然后是Dropout，最好是残差连接。这里面有很多可以重用的代码，我们把它封装成SublayerConnection。</strong></p>
<hr>
<p>That is, <code>the output of each sub-layer is LayerNorm(x+Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself.</code> We apply dropout <a href="http://jmlr.org/papers/v15/srivastava14a.html" target="_blank" rel="noopener">(cite)</a> to the output of each sub-layer, before it is added to the sub-layer input and normalized.</p>
<p>To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension <code>dmodel=512</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SublayerConnection</span><span class="params">(nn.Module)</span>:</span> <span class="comment">#每一个编码层中的两个子层之间的连接</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">	LayerNorm + sublayer(Self-Attenion/Dense) + dropout + 残差连接</span></span><br><span class="line"><span class="string">	为了简单，把LayerNorm放到了前面，这和原始论文稍有不同，原始论文LayerNorm在最后。</span></span><br><span class="line"><span class="string">	"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, dropout)</span>:</span></span><br><span class="line">        super(SublayerConnection, self).__init__()</span><br><span class="line">        self.norm = LayerNorm(size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, sublayer)</span>:</span></span><br><span class="line">       <span class="comment">#sublayer是传入的参数，参考DecoderLayer，它可以当成函数调用，这个函数的有一个输入参数</span></span><br><span class="line">        <span class="keyword">return</span> x + self.dropout(sublayer(self.norm(x))) <span class="comment">#调用layernorm ，正则化之后再相加</span></span><br></pre></td></tr></table></figure>

<p>这个类会构造LayerNorm和Dropout，但是Self-Attention或者Dense并不在这里构造，还是放在了EncoderLayer里，在forward的时候由EncoderLayer传入。这样的好处是更加通用，比如Decoder也是类似的需要在Self-Attention、Attention或者Dense前面后加上LayerNorm和Dropout以及残差连接，我们就可以复用代码。但是这里要求传入的sublayer可以使用一个参数来调用的函数(或者有<strong>call</strong>)。</p>
<hr>
<p>forward调用sublayer[0] (这是SublayerConnection对象)的<strong>call</strong>方法，最终会调到它的forward方法，而这个方法需要两个参数，<strong>一个是输入Tensor，一个是一个callable，并且这个callable可以用一个参数来调用</strong>。而<strong>self_attn函数需要4个参数(Query的输入,Key的输入,Value的输入和Mask)</strong>，因此这里我们使用lambda的技巧把它变成一个参数x的函数(mask可以看成已知的数)。</p>
<p>  Callable 类型是可以被执行调用操作的类型。包含自定义函数等。自定义的函数比如使用def、lambda所定义的函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderLayer</span><span class="params">(nn.Module)</span>:</span> <span class="comment">#每一个编码层</span></span><br><span class="line">    <span class="string">"Encoder is made up of self-attn and feed forward (defined below)"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, self_attn, feed_forward, dropout)</span>:</span></span><br><span class="line">        super(EncoderLayer, self).__init__()</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">2</span>) <span class="comment">#每一层有2子层</span></span><br><span class="line">        self.size = size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask)</span>:</span></span><br><span class="line">      <span class="comment">#attention层，括号里面是参数。接收来自attention的输出</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">     lambda : atten()SublayerConnection里是作为sublayer出现的，而它的参数是norm(x),norm(x)的输出是一个向量x，</span></span><br><span class="line"><span class="string">   所以atten的参数是只有一个x， 而在muitihead里面，k、q、v在函数里是要被重新根据x计算的</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, mask)) </span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">1</span>](x, self.feed_forward) <span class="comment">#x是atten+norm之后的输出，再ff输出</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    可以理解为</span></span><br><span class="line"><span class="string">    z = lambda y: self.self_attn(y, y, y, mask)</span></span><br><span class="line"><span class="string">	x = self.sublayer[0](x, z)</span></span><br><span class="line"><span class="string">    """</span></span><br></pre></td></tr></table></figure>



<h4 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h4><p>The decoder is also composed of a stack of <code>N=6</code> identical layers.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Generic N layer decoder with masking."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layer, N)</span>:</span></span><br><span class="line">        super(Decoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">     <span class="comment">#memory: 编码器的输出 x是输入</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, memory, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, memory, src_mask, tgt_mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderLayer</span><span class="params">(nn.Module)</span>:</span> <span class="comment">#每一层解码层</span></span><br><span class="line">    <span class="string">"Decoder is made of self-attn, src-attn, and feed forward (defined below)"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, self_attn, src_attn, feed_forward, dropout)</span>:</span></span><br><span class="line">        super(DecoderLayer, self).__init__()</span><br><span class="line">        self.size = size</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.src_attn = src_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">3</span>) <span class="comment">#每一层有3个子层</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, memory, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="string">"Follow Figure 1 (right) for connections."</span></span><br><span class="line">        m = memory</span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, tgt_mask)) <span class="comment">#第一子层</span></span><br><span class="line">        x = self.sublayer[<span class="number">1</span>](x, <span class="keyword">lambda</span> x: self.src_attn(x, m, m, src_mask)) <span class="comment">#第二子层 </span></span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">2</span>](x, self.feed_forward) <span class="comment">#第三子层</span></span><br></pre></td></tr></table></figure>

<p><strong>src-attn和self-attn的实现是一样的，只不过使用的Query，Key和Value的输入不同。</strong>普通的Attention(src-attn)的Query是下层输入进来的(来自self-attn的输出)，Key和Value是Encoder最后一层的输出memory；而Self-Attention的Query，Key和Value都是来自下层输入进来的。</p>
<hr>
<p>Decoder和Encoder有一个关键的不同：Decoder在解码第t个时刻的时候只能使用<strong>1…t时刻</strong>的输入，而不能使用t+1时刻及其之后的输入。因此我们需要一个函数来产生一个Mask矩阵，所以代码如下：</p>
<p>注意： t时刻包括t时刻的输入</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">subsequent_mask</span><span class="params">(size)</span>:</span>  <span class="comment">#将i后面的mask掉</span></span><br><span class="line">    <span class="string">"Mask out subsequent positions."</span></span><br><span class="line">    attn_shape = (<span class="number">1</span>, size, size)</span><br><span class="line">    subsequent_mask = np.triu(np.ones(attn_shape), k=<span class="number">1</span>).astype(<span class="string">'uint8'</span>) <span class="comment">#triu 上三角</span></span><br><span class="line">    <span class="keyword">return</span> torch.from_numpy(subsequent_mask) == <span class="number">0</span> <span class="comment">#将numpy格式转换为tensor格式，判断是否为0， 输出布尔值</span></span><br></pre></td></tr></table></figure>





<p><img src="https://i.loli.net/2020/08/08/7brnPfDJxsLBtvh.png" alt="png"></p>
<p>它的输出：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print(subsequent_mask(5))</span><br><span class="line"># 输出</span><br><span class="line">  1  0  0  0  0</span><br><span class="line">  1  1  0  0  0</span><br><span class="line">  1  1  1  0  0</span><br><span class="line">  1  1  1  1  0</span><br><span class="line">  1  1  1  1  1</span><br></pre></td></tr></table></figure>

<p>我们发现它输出的是一个方阵，对角线和下面都是1。<strong>第一行只有第一列是1，它的意思是时刻1只能attend to输入1</strong>，第三行说明时刻3可以attend to {1,2,3}而不能attend to{4,5}的输入，因为在真正Decoder的时候这是属于Future的信息。代码首先使用triu产生一个上三角阵：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0 1 1 1 1</span><br><span class="line">0 0 1 1 1</span><br><span class="line">0 0 0 1 1</span><br><span class="line">0 0 0 0 1</span><br><span class="line">0 0 0 0 0</span><br></pre></td></tr></table></figure>

<p>然后需要把0变成1，把1变成0，这可以使用 matrix == 0来实现。</p>
<p>因为：布尔值True被索引求值为1，而False就等于0。</p>
<h4 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h4><p>An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.</p>
<p>We call our particular attention “<code>Scaled Dot-Product Attention</code>”. The input consists of queries and keys of dimension <code>dk</code>, and values of dimension <code>dv</code>. We compute the dot products of the query with all keys, divide each by <code>√dk</code>, and apply a softmax function to obtain the weights on the values.</p>
<p><img src="https://i.loli.net/2020/08/06/O3UNSGF7Poa1w4Q.png" alt="image-20200806015122441"></p>
<p><strong>Attention可以看成一个函数，它的输入是Query,Key,Value和Mask，输出是一个Tensor</strong>。其中输出是Value的加权平均，而权重来自Query和Key的计算。具体的计算如下图所示，计算公式为：</p>
<img src="https://i.loli.net/2020/07/28/WaSfHnNdt2L1AXU.png" alt="image-20200728212241453" style="zoom:50%;" />

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention</span><span class="params">(query, key, value, mask=None, dropout=None)</span>:</span></span><br><span class="line">    <span class="string">"Compute 'Scaled Dot Product Attention'"</span></span><br><span class="line">    d_k = query.size(<span class="number">-1</span>) <span class="comment"># query.size的最后一维</span></span><br><span class="line">    scores = torch.matmul(query, key.transpose(<span class="number">-2</span>, <span class="number">-1</span>)) / math.sqrt(d_k)</span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:<span class="comment"># 如果有mask</span></span><br><span class="line">        scores = scores.masked_fill(mask == <span class="number">0</span>, <span class="number">-1e9</span>)</span><br><span class="line">    p_attn = F.softmax(scores, dim = <span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: <span class="comment">#对p_attn进行dropout</span></span><br><span class="line">        p_attn = dropout(p_attn)</span><br><span class="line">    <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br></pre></td></tr></table></figure>

<p>我们知道, 在训练的时候, 我们是以 batch_size 为单位的, 那么就会有 padding, 一般我们取 pad == 0, 那么就会造成在 Attention 的时候, query 的值为 0, query 的值为 0, 所以我们计算的对应的 scores 的值也是 0, 那么就会导致 softmax 很可能分配给该单词一个相对不是很小的比例, 因此, 我们将 pad 对应的 score 取值为<strong>负无穷</strong>（普通的计算，score可以为负数？）, 以此来减小 pad 的影响. </p>
<p>很容易想到, 在 decoder, <strong>未预测的单词</strong>也是用 padding 的方式加入到 batch 的, 所以使用的mask 机制与 padding 时mask 的机制是相同的, 本质上都是query 的值为0, 只是 mask 矩阵不同, 我们可以根据 decoder 部分的代码发现这一点.</p>
<hr>
<p>我们使用一个<strong>实际的例子跟踪一些不同Tensor的shape</strong>，然后对照公式就很容易理解。比如<strong>Q是(30,8,33,64)，其中30是batch，8是head个数，33是序列长度，64是每个时刻的特征数（size）。K和Q的shape必须相同的，而V可以不同，但是这里的实现shape也是相同的。</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">scores = torch.matmul(query, key.transpose(<span class="number">-2</span>, <span class="number">-1</span>)) \</span><br><span class="line">/ math.sqrt(d_k)</span><br></pre></td></tr></table></figure>

<p>上面的代码实现<img src="https://i.loli.net/2020/08/06/rLCJ7VFBAsmQb4x.png" alt="image-20200806014945713" style="zoom:50%;" />，和公式里稍微不同的是，这里的Q和K都是4d的Tensor，包括batch和head维度。<strong>matmul会把query和key的最后两维进行矩阵乘法</strong>，这样效率更高，如果我们要用标准的矩阵(二维Tensor)乘法来实现，那么需要遍历batch维和head维：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_num = query.size(<span class="number">0</span>) <span class="comment"># query.size(0)返回的是0维的数</span></span><br><span class="line">head_num = query.size(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(batch_num):</span><br><span class="line">	<span class="keyword">for</span> j <span class="keyword">in</span> range(head_num):</span><br><span class="line">		scores[i,j] = torch.matmul(query[i,j], key[i,j].transpose())</span><br></pre></td></tr></table></figure>

<p>而上面的写法一次完成所有这些循环，效率更高。<strong>输出的score是(30, 8, 33, 33)</strong>，前面两维不看，那<strong>么是一个(33, 33)的attention矩阵a，aij表示时刻 i关注 j 的得分</strong>(还没有经过softmax变成概率)。</p>
<p><strong>在编码器的attention中src_mask的作用！！！</strong></p>
<p>接下来是<code>scores.masked_fill(mask == 0, -1e9)</code>，用于<strong>把mask是0的变成一个很小的数</strong>，这样后面经过softmax之后的概率就很接近零(但是理论上还是用来很少一点点未来的信息)。</p>
<blockquote>
<p>masked_fill_(mask, value)：掩码操作<br>masked_fill方法有两个参数，maske和value，mask是一个pytorch张量（Tensor），<strong>元素是布尔值，value是要填充的值</strong>，填充规则是mask中取值为True位置对应于self的相应位置用value填充。</p>
<p>注：参数mask必须与score的size相同或者两者是可广播(broadcasting-semantics)的</p>
<p>pytorch masked_fill方法简单理解 </p>
<p> <a href="https://blog.csdn.net/jianyingyao7658/article/details/103382654" target="_blank" rel="noopener">https://blog.csdn.net/jianyingyao7658/article/details/103382654</a></p>
<p>pytorch 广播语义(Broadcasting semantics) </p>
<p> <a href="https://blog.csdn.net/qq_35012749/article/details/88308657" target="_blank" rel="noopener">https://blog.csdn.net/qq_35012749/article/details/88308657</a></p>
</blockquote>
<p>这里<strong>mask是(30, 1, 1, 33)的tensor</strong>，因为8个head的mask都是一样的，所有第二维是1，masked_fill时使用broadcasting就可以了。这里是self-attention的mask，所以每个时刻都可以attend到所有其它时刻，所有第三维也是1，也使用broadcasting。如果是普通的mask，那么mask的shape是(30, 1, 33, 33)。</p>
<p>这样讲有点抽象，我们可以举一个例子，为了简单，我们假设batch=2, head=8。第一个序列长度为3，第二个为4，那么self-attention的mask为(2, 1, 1, 4)，我们可以用两个向量表示：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1 1 1 0</span><br><span class="line">1 1 1 1</span><br></pre></td></tr></table></figure>

<p>它的意思是在self-attention里，第一个序列的任一时刻可以attend to 前3个时刻(因为第4个时刻是padding的)；而第二个序列的可以attend to所有时刻的输入。而Decoder的src-attention的mask为(2, 1, 4, 4)，我们需要用2个矩阵表示：(一个序列对应一个一维src_mask（1×4），  一个序列对应一个二维的tgt_mask（4×4）)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">第一个序列的mask矩阵</span><br><span class="line">1 0 0 0</span><br><span class="line">1 1 0 0</span><br><span class="line">1 1 1 0</span><br><span class="line">1 1 1 0</span><br><span class="line"></span><br><span class="line">第二个序列的mask矩阵</span><br><span class="line">1 0 0 0</span><br><span class="line">1 1 0 0 </span><br><span class="line">1 1 1 0</span><br><span class="line">1 1 1 1</span><br></pre></td></tr></table></figure>



<p>接下来对score求softmax，把得分变成概率p_attn，如果有dropout还对p_attn进行Dropout(这也是原始论文没有的)。最后把p_attn和value相乘。p_attn是(30, 8, 33, 33)，value是(30, 8, 33, 64)，我们<strong>只看后两维，(33x33) x (33x64)最终得到33x64。</strong></p>
<hr>
<p>接下来就是输入怎么变成Q,K和V了，<strong>对于每一个Head，都使用三个矩阵WQ,WK,WV把输入转换成Q，K和V。</strong>然后<strong>分别用每一个Head进行Self-Attention的计算，最后把N个Head的输出拼接起来，最后用一个矩阵WO把输出压缩一下。</strong>具体计算过程为：</p>
<img src="https://i.loli.net/2020/08/06/1IbPcFJeK8tsHqN.png" alt="image-20200806023820900" style="zoom: 67%;" />



<p>详细结构如下图所示，输入Q，K和V经过多个线性变换后得到N(8)组Query，Key和Value，然后使用Self-Attention计算得到N个向量，然后拼接起来，<strong>最后使用一个线性变换进行降维。</strong></p>
<img src="https://i.loli.net/2020/08/08/a2gozSYGn8NOkpH.png" alt="png" style="zoom:67%;" />

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadedAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, h, d_model, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        <span class="string">"Take in model size and number of heads."</span></span><br><span class="line">        super(MultiHeadedAttention, self).__init__()</span><br><span class="line">        <span class="keyword">assert</span> d_model % h == <span class="number">0</span>  <span class="comment"># 不能整除就报错</span></span><br><span class="line">        <span class="comment"># We assume d_v always equals d_k</span></span><br><span class="line">        self.d_k = d_model // h</span><br><span class="line">        self.h = h</span><br><span class="line">        self.linears = clones(nn.Linear(d_model, d_model), <span class="number">4</span>)</span><br><span class="line">        self.attn = <span class="literal">None</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, mask=None)</span>:</span></span><br><span class="line">        <span class="string">"Implements Figure 2"</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># # 所有h个head的mask都是相同的 </span></span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>) <span class="comment">#在维度为1的位置添加一个维度，数字为1</span></span><br><span class="line">        nbatches = query.size(<span class="number">0</span>) <span class="comment">#就是有多少batch的值</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 1) 首先使用线性变换，然后把d_model分配给h个Head，每个head为d_k=d_model/h </span></span><br><span class="line">        query, key, value = \</span><br><span class="line">            [l(x).view(nbatches, <span class="number">-1</span>, self.h, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">             <span class="keyword">for</span> l, x <span class="keyword">in</span> zip(self.linears, (query, key, value))]</span><br><span class="line">           <span class="comment">#.view()表示重构张量的维度</span></span><br><span class="line">         <span class="comment">#注：因为每个Linear学习到的参数是不一样的。所以qkv三个也是不一样的</span></span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">        <span class="comment"># 2)使用attention函数计算 </span></span><br><span class="line">        x, self.attn = attention(query, key, value, mask=mask, </span><br><span class="line">                                 dropout=self.dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3) 把8个head的64维向量拼接成一个512的向量。然后再使用一个线性变换(512,521)，shape不变。 </span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous() \</span><br><span class="line">             .view(nbatches, <span class="number">-1</span>, self.h * self.d_k)</span><br><span class="line">        <span class="keyword">return</span> self.linears[<span class="number">-1</span>](x)</span><br></pre></td></tr></table></figure>



<p>我们先看构造函数，这里<strong>d_model(512)是Multi-Head的输出大小</strong>，因为有h(8)个head，因此每个head的d_k=512/8=64。接着我们构造4个(d_model ， d_model)的矩阵，后面我们会看到它的用处。最后是构造一个Dropout层。</p>
<p>然后我们来看forward方法。<strong>输入的mask是(batch, 1, time)的，因为每个head的mask都是一样的，所以先用unsqueeze(1)变成(batch, 1, 1, time)</strong>，mask我们前面已经详细分析过了。</p>
<p>接下来是<strong>根据输入query，key和value计算变换后的Multi-Head的query，key和value</strong>。这是通过下面的语句来实现的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">query, key, value = \</span><br><span class="line">		[l(x).view(nbatches, <span class="number">-1</span>, self.h, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)	</span><br><span class="line">			<span class="keyword">for</span> l, x <span class="keyword">in</span> zip(self.linears, (query, key, value))] <span class="comment"># l(x): 调用nn.Linear函数</span></span><br></pre></td></tr></table></figure>

<p><strong>zip(self.linears, (query, key, value))是把(self.linears[0],self.linears[1],self.linears[2])和(query, key, value)放到一起然后遍历。我们只看一个self.linears[0] (query)。根据构造函数的定义，self.linears[0]是一个(512, 512)的矩阵，而query是(batch, time, 512)，相乘之后得到的新query还是512(d_model)维的向量，然后用view把它变成(batch, time, 8, 64)。然后transponse成(batch, 8,time,64)，这是attention函数要求的shape。分别对应8个Head，每个Head的Query都是64维。</strong></p>
<blockquote>
<p>1.一般来说，矩阵相乘，[a,b] x [b,c] = [a,c]</p>
<p>所以不同维度要进行处理，必须降维。例如 A 矩阵 [a,b,c], B 矩阵是[c,d]</p>
<p>这个时候就需要将 A 矩阵看成是 [axb, c] 与 [c,d] 进行相乘，得到结果。</p>
<ol start="2">
<li>Linear函数l(x)，应该就是 (batch<em>time,512)*</em>(512,512)</li>
</ol>
</blockquote>
<p>Key和Value的运算完全相同，因此我们也分别得到8个Head的64维的Key和64维的Value。接下来<strong>调用attention函数，得到x和self.attn。其中x的shape是(batch, 8, time, 64)，而attn是(batch, 8, time, time)。</strong></p>
<p><strong>x.transpose(1, 2)把x变成(batch, time, 8, 64)，然后把它view成(batch, time, 512)，其实就是把最后8个64维的向量拼接成512的向量。最后使用self.linears[-1]对x进行线性变换，self.linears[-1]是(512, 512)的，因此最终的输出还是(batch, time, 512)。我们最初构造了4个(512, 512)的矩阵，前3个用于对query，key和value进行变换，而最后一个对8个head拼接后的向量再做一次变换。</strong></p>
<h4 id="A0ttention在模型中的应用"><a href="#A0ttention在模型中的应用" class="headerlink" title="A0ttention在模型中的应用"></a>A0ttention在模型中的应用</h4><p>在Transformer里，有3个地方用到了MultiHeadedAttention：</p>
<ul>
<li><p>Encoder的Self-Attention层</p>
<p><strong>query，key和value都是相同的值</strong>，来自下层的输入。Mask都是1(当然padding的不算)。</p>
</li>
<li><p>Decoder的Self-Attention层</p>
<p><strong>query，key和value都是相同的值</strong>，来自下层的输入。但是Mask使得它不能访问未来的输入。</p>
</li>
<li><p>Encoder-Decoder的普通Attention</p>
<p><strong>query来自下层的输入，而key和value相同</strong>，是Encoder最后一层的输出，而Mask都是1。</p>
</li>
</ul>
<h3 id="Position-wise-前馈网络"><a href="#Position-wise-前馈网络" class="headerlink" title="Position-wise 前馈网络"></a>Position-wise 前馈网络</h3><p>In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. <code>This consists of two linear transformations with a ReLU activation in between.</code></p>
<p>全连接层有两个线性变换以及它们之间的ReLU激活组成：</p>
<img src="https://i.loli.net/2020/08/08/PU96rciRsWxOCKp.png" alt="image-20200728231445307" style="zoom:50%;" />

<p>全连接层的输入和输出都是d_model(512)维的，中间隐单元的个数是d_ff(2048)维</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionwiseFeedForward</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Implements FFN equation."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, d_ff, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        super(PositionwiseFeedForward, self).__init__()</span><br><span class="line">        self.w_1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        self.w_2 = nn.Linear(d_ff, d_model)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.w_2(self.dropout(F.relu(self.w_1(x))))</span><br></pre></td></tr></table></figure>

<h3 id="Embeddings-和-Softmax"><a href="#Embeddings-和-Softmax" class="headerlink" title="Embeddings 和 Softmax"></a>Embeddings 和 Softmax</h3><p><strong>输入的词序列都是ID序列，我们需要Embedding</strong>。源语言和目标语言都需要Embedding，此外我们需要一个线性变换把隐变量变成输出概率，这可以通过前面的类Generator来实现。我们这里实现Embedding：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Embeddings</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, vocab)</span>:</span></span><br><span class="line">        super(Embeddings, self).__init__()</span><br><span class="line">        self.lut = nn.Embedding(vocab, d_model) <span class="comment">#将字典vocab大小映射到d_model大小</span></span><br><span class="line">        self.d_model = d_model</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.lut(x) * math.sqrt(self.d_model)</span><br></pre></td></tr></table></figure>

<p>注意的就是forward处理使用nn.Embedding对输入x进行Embedding之外，还除以了sqrt(d_model) （开方）</p>
<h3 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h3><p>位置编码的公式为：</p>
 <img src="https://i.loli.net/2020/08/08/WUpXhHsK3S1jCqn.png" alt="image-20200728232133981" style="zoom:50%;" />

<img src="https://i.loli.net/2020/08/08/XOZPy89KVi1xjTh.png" alt="image-20200728232255029" style="zoom:50%;" />

<p> where <code>pos</code> is the position and <code>i</code> is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. </p>
<p>假设输入是ID序列长度为10，<strong>如果输入Embedding之后是(10, 512)，那么位置编码的输出也是(10, 512)。</strong>上式中pos就是位置(0-9)，512维的偶数维使用sin函数，而奇数维使用cos函数。这种位置编码的好处是：PE_pos+k可以表示成 PE_pos的线性函数，这样网络就能容易的学到相对位置的关系。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">15</span>, <span class="number">5</span>))</span><br><span class="line">pe = PositionalEncoding(<span class="number">20</span>, <span class="number">0</span>)</span><br><span class="line">y = pe.forward(Variable(torch.zeros(<span class="number">1</span>, <span class="number">100</span>, <span class="number">20</span>)))</span><br><span class="line">plt.plot(np.arange(<span class="number">100</span>), y[<span class="number">0</span>, :, <span class="number">4</span>:<span class="number">8</span>].data.numpy())</span><br><span class="line">plt.legend([<span class="string">"dim %d"</span>%p <span class="keyword">for</span> p <span class="keyword">in</span> [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>]])</span><br></pre></td></tr></table></figure>

<p>图是一个示例，向量的大小d_model=20，我们这里画出来第4、5、6和7维(下标从零开始)维的图像，最大的位置是100。我们可以看到它们都是正弦(余弦)函数，而且周期越来越长。</p>
<p><img src="https://i.loli.net/2020/08/08/TfDHKvnM3emYysL.png" alt="png"></p>
<p>前面我们提到位置编码的好处是PE_pos+k可以表示成 P_Epos的线性函数，我们下面简单的验证一下。我们以第i维为例，为了简单，我们把<img src="https://i.loli.net/2020/08/06/iEoDOvKzB42N6Xe.png" alt="image-20200806104700979" style="zoom: 67%;" />记作Wi，这是一个常量。</p>
<img src="https://i.loli.net/2020/08/06/E9h2vXIDK1MAjUg.png" alt="image-20200806104725624" style="zoom:67%;" />

<p>我们发现PE_pos+k 确实可以表示成 PE_pos的线性函数。</p>
<p>In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of <code>Pdrop=0.1</code>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Implement the PE function."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, dropout, max_len=<span class="number">5000</span>)</span>:</span></span><br><span class="line">        super(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute the positional encodings once in log space.</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        <span class="comment">#之所以用log再exp,可能是考虑到数值过大溢出的问题</span></span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) *</span><br><span class="line">                             -(math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        self.register_buffer(<span class="string">'pe'</span>, pe)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = x + Variable(self.pe[:, :x.size(<span class="number">1</span>)], </span><br><span class="line">                         requires_grad=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure>

<p>代码可以参考公式，调用了<code>Module.register_buffer函数</code>。这个函数的作用是创建一个buffer，比如这里把pe保存下来。register_buffer通常用于保存一些模型参数之外的值，比如在BatchNorm中，我们需要保存running_mean(Moving Average)，它不是模型的参数(不用梯度下降)，但是模型会修改它，而且在预测的时候也要使用它。这里也是类似的，pe是一个提前计算好的常量，我们在forward要用到它。我们在构造函数里并没有把pe保存到self里，但是在forward的时候我们却可以直接使用它(self.pe)。如果我们保存(序列化)模型到磁盘的话，PyTorch框架也会帮我们保存buffer里的数据到磁盘，这样反序列化的时候能恢复它们</p>
<h3 id="完整模型"><a href="#完整模型" class="headerlink" title="完整模型"></a>完整模型</h3><blockquote>
<p>Here we <code>define a function that takes in hyperparameters and produces a full model.</code></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_model</span><span class="params">(src_vocab, tgt_vocab, N=<span class="number">6</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">               d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, h=<span class="number">8</span>, dropout=<span class="number">0.1</span>)</span>:</span> <span class="comment">#d_ff： feedforward的维度</span></span><br><span class="line">    <span class="string">"Helper: Construct a model from hyperparameters."</span></span><br><span class="line">    c = copy.deepcopy</span><br><span class="line">    attn = MultiHeadedAttention(h, d_model)</span><br><span class="line">    ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line">    position = PositionalEncoding(d_model, dropout)</span><br><span class="line">    model = EncoderDecoder(</span><br><span class="line">        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),</span><br><span class="line">        Decoder(DecoderLayer(d_model, c(attn), c(attn), </span><br><span class="line">                             c(ff), dropout), N),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),</span><br><span class="line">        Generator(d_model, tgt_vocab))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># This was important from their code. </span></span><br><span class="line">    <span class="comment"># Initialize parameters with Glorot / fan_avg. 随机初始化参数，这非常重要</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">            nn.init.xavier_uniform(p)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例: 对model简单输入参数</span></span><br><span class="line">tmp_model = make_model(<span class="number">10</span>, <span class="number">10</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<p>首先把copy.deepcopy命名为c，这样使下面的代码简洁一点。然后构造MultiHeadedAttention，PositionwiseFeedForward和PositionalEncoding对象。接着就是构造EncoderDecoder对象。它需要5个参数：Encoder、Decoder、src-embed、tgt-embed和Generator。</p>
<p>我们先看后面三个简单的参数，Generator直接构造就行了，它的作用是把模型的隐单元变成输出词的概率。而src-embed是一个Embeddings层和一个位置编码层c(position)，tgt-embed也是类似的。</p>
<p>最后我们来看Decoder(Encoder和Decoder类似的)。Decoder由N个DecoderLayer组成，而DecoderLayer需要传入self-attn, src-attn，全连接层和Dropout。因为所有的MultiHeadedAttention都是一样的，因此我们直接deepcopy就行；同理所有的PositionwiseFeedForward也是一样的网络结果，我们可以deepcopy而不要再构造一个。</p>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>This section describes the training regime for our models.</p>
<blockquote>
<p>We stop for a quick interlude to introduce some of the tools needed to train a standard encoder decoder model. First <code>we define a batch object that holds the src and target sentences for training, as well as constructing the masks.</code></p>
</blockquote>
<h4 id="Batches-和-Masking"><a href="#Batches-和-Masking" class="headerlink" title="Batches 和 Masking"></a>Batches 和 Masking</h4><p><code>mask 矩阵来自 batch</code></p>
<p><code>self.src_mask = (src != pad).unsqueeze(-2)</code> 也就是说, 源语言的 <strong>mask 矩阵的维度是 (batch_size, 1, length)</strong>, 那么为什么 <code>attn_shape = (batch_size, size, size)</code> 呢? 可以这么解释, <strong>在 encoder 阶段的 Self_Attention 阶段, 所有的 Attention 是可以同时进行的, 把所有的 Attention_result 算出来, 然后用同一个 mask vector * Attention_result 就可以了</strong>, 但是在 decoder 阶段却不能这么做, 我们需要关注的问题是:</p>
<blockquote>
<p>根据已经预测出来的单词预测下面的单词, 这一过程<strong>是序列的</strong>,</p>
<p>而我们的计算是<strong>并行</strong>的, 所以这一过程中, 必须要引入矩阵. 也就是上面的 subsequent_mask() 函数获得的矩阵.</p>
</blockquote>
<p>这个矩阵也很形象, 分别表示已经预测的单词的个数为, 1, 2, 3, 4, 5.</p>
<p>然后我们将以上过程反过来过一篇, 就很明显了, 在 batch阶段获得 mask 矩阵, 然后和 batch 一起训练, 在 encoder 与 deocder 阶段实现 mask 机制.</p>
<blockquote>
<ul>
<li><p>mask在Batch中定义，src_mask.size (30,1,10) ,  trg_mask.size(30,10,10)</p>
</li>
<li><p>然后在MultiHeadedAttention中<code>mask = mask.unsqueeze(1)</code>又扩维了，</p>
<p>其中src_mask.size(30,1,1,10) ,trg_mask.size(30,1,10,10)</p>
</li>
<li><p>src_mask.size满足attention中的维度，所以可以对score进行mask</p>
<p> src_mask还在解码器的第1子层用到，相同的原理</p>
</li>
<li><p>trg_mask在解码器的第0子层用到，满足要求</p>
</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Batch</span>:</span> <span class="comment">#定义每一个batch中的src、tgt、mask</span></span><br><span class="line">    <span class="comment">#trg = tgt: 真实的标签序列  out ： 预测的单词  </span></span><br><span class="line">    <span class="string">"Object for holding a batch of data with mask during training."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, src, trg=None, pad=<span class="number">0</span>)</span>:</span> </span><br><span class="line">        self.src = src</span><br><span class="line">        self.src_mask = (src != pad).unsqueeze(<span class="number">-2</span>) <span class="comment">#扩充维度 倒数第二维增加值为1 size=(30,1,10)</span></span><br><span class="line">        <span class="comment">#并且非零值全部赋值为1</span></span><br><span class="line">        </span><br><span class="line">         <span class="comment"># 在预测的时候是没有 tgt 的,此时为 None 此时trg是tgt的形参</span></span><br><span class="line">        <span class="keyword">if</span> trg <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.trg = trg[:, :<span class="number">-1</span>] <span class="comment">#trg.size(30,9) ，在预测中，会提前输入起始符到ys中</span></span><br><span class="line">            <span class="string">"""</span></span><br><span class="line"><span class="string">              trg.size(30,9) 这里去掉的最后一个单词, 不是真正的单词, 而是标志 '&lt;eos&gt;' , 						输入与输出都还有一个 '&lt;sos&gt;' 在句子的开头,  是decoder的输入，</span></span><br><span class="line"><span class="string">            需要进行mask，使得Self-Attention不能访问未来的输入。最后一个词不需要用到trg</span></span><br><span class="line"><span class="string">            """</span></span><br><span class="line">          </span><br><span class="line">            self.trg_y = trg[:, <span class="number">1</span>:] <span class="comment"># trg_y.size(30,9) </span></span><br><span class="line">            <span class="comment">#trg_y: 最后的结果。用于loss中的比较。 去掉开头的'&lt;sos&gt;'，是decoder的输出</span></span><br><span class="line">            self.trg_mask = \</span><br><span class="line">                self.make_std_mask(self.trg, pad)</span><br><span class="line">            self.ntokens = (self.trg_y != pad).data.sum() <span class="comment">#不为0的总数</span></span><br><span class="line">    </span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_std_mask</span><span class="params">(tgt, pad)</span>:</span> <span class="comment">#tgt_mask.size(30,9,9)</span></span><br><span class="line">        <span class="string">"Create a mask to hide padding and future words."</span></span><br><span class="line">        <span class="comment">#"创建Mask，使得我们不能attend to未来的词"</span></span><br><span class="line">        tgt_mask = (tgt != pad).unsqueeze(<span class="number">-2</span>)</span><br><span class="line">        tgt_mask = tgt_mask &amp; Variable(</span><br><span class="line">            subsequent_mask(tgt.size(<span class="number">-1</span>)).type_as(tgt_mask.data))</span><br><span class="line">        <span class="keyword">return</span> tgt_mask</span><br></pre></td></tr></table></figure>

<p>Batch构造函数的输入是src和trg，后者可以为None，因为再预测的时候是没有tgt的。</p>
<p>我们用一个例子来说明Batch的代码，这是训练阶段的一个Batch，<strong>src是(48, 20)</strong>，48是batch大小，而20是最长的句子长度，其它的不够长的都padding成20了。而<strong>trg是(48, 25)</strong>，表示翻译后的最长句子是25个词，不足的也padding过了。</p>
<p>我们首先看src_mask怎么得到，(src != pad)把src中大于0的时刻置为1，这样表示它可以attend to的范围。然后unsqueeze(-2)把src_mask变成(48/batch, 1, 20/time)。它的用法参考前面的attention函数。</p>
<p>对于训练来说(Teaching Forcing模式)，Decoder有一个输入和一个输出。<strong>比如句子”<sos> it is a good day <eos>”，输入会变成”<sos> it is a good day”，而输出为”it is a good day <eos>”。对应到代码里，self.trg就是输入，而self.trg_y就是输出。</strong>接着对输入self.trg进行mask，使得Self-Attention不能访问未来的输入。这是通过make_std_mask函数实现的，这个函数会调用我们之前详细介绍过的subsequent_mask函数。最终得到的<strong>trg_mask的shape是(48/batch, 24, 24)</strong>，表示24个时刻的Mask矩阵，这是一个对角线以及之下都是1的矩阵，前面已经介绍过了。</p>
<p>注意<strong>src_mask的shape是(batch, 1, time)</strong>，而<strong>trg_mask是(batch, time, time)</strong>。因为src_mask的每一个时刻都能attend to所有时刻(padding的除外)，一次只需要一个向量就行了，而trg_mask需要一个矩阵。</p>
<h4 id="Training-Loop"><a href="#Training-Loop" class="headerlink" title="Training Loop"></a>Training Loop</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_epoch</span><span class="params">(data_iter, model, loss_compute)</span>:</span> <span class="comment">#返回total_loss / total_tokens 。是一个数值，损失计算</span></span><br><span class="line">    <span class="comment">#遍历一个epoch的数据</span></span><br><span class="line">    <span class="string">"Standard Training and Logging Function"</span></span><br><span class="line">    start = time.time() <span class="comment">#开始时间，计算用时</span></span><br><span class="line">    total_tokens = <span class="number">0</span> </span><br><span class="line">    total_loss = <span class="number">0</span> </span><br><span class="line">    tokens = <span class="number">0</span> </span><br><span class="line">    <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(data_iter): <span class="comment">#每一步data_iter（gen_data），实例化batch数据用于学习.进行20次</span></span><br><span class="line">        <span class="comment">#gen_data返回的是20个Batch，通过enumerate实例化20个batch </span></span><br><span class="line">        out = model.forward(batch.src, batch.trg, </span><br><span class="line">                            batch.src_mask, batch.trg_mask) <span class="comment">#调用EncoderDecoder的实例化model，解码器作为输出</span></span><br><span class="line">        loss = loss_compute(out, batch.trg_y, batch.ntokens) <span class="comment">#计算出loss。 trg_y是标准值。ntokens作为norm？？</span></span><br><span class="line">        total_loss += loss <span class="comment">#loss叠加。进行20次</span></span><br><span class="line">        total_tokens += batch.ntokens </span><br><span class="line">        tokens += batch.ntokens</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">50</span> == <span class="number">1</span>: <span class="comment">#i从0开始的，当i=1的时候，进行了一次batch，所以这里计算的就是一次batch所用的时间。而要进行20次。  50是随机设置</span></span><br><span class="line">            elapsed = time.time() - start <span class="comment">#计算一共用时</span></span><br><span class="line">            print(<span class="string">"Epoch Step: %d Loss: %f Tokens per Sec: %f"</span> %</span><br><span class="line">                    (i, loss / batch.ntokens, tokens / elapsed))</span><br><span class="line">            start = time.time()</span><br><span class="line">            tokens = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> total_loss / total_tokens</span><br></pre></td></tr></table></figure>

<p>它遍历一个epoch的数据，然后调用forward，接着用loss_compute函数计算梯度，更新参数并且返回loss。这里的loss_compute是一个函数，它的输入是模型的预测out，真实的标签序列batch.trg_y和batch的词个数。实际的实现是MultiGPULossCompute类，这是一个callable。本来计算损失和更新参数比较简单，但是这里为了实现多GPU的训练，这个类就比较复杂了。</p>
<h4 id="Training-Data-和-Batching"><a href="#Training-Data-和-Batching" class="headerlink" title="Training Data 和 Batching"></a>Training Data 和 Batching</h4><p>We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding, which has a shared source-target vocabulary of about 37000 tokens. For English- French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary.</p>
<p>Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.</p>
<blockquote>
<p>We will use torch text for batching. This is discussed in more detail below. Here we create batches in a torchtext function that ensures our batch size padded to the maximum batchsize does not surpass a threshold (25000 if we have 8 gpus).</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">global</span> max_src_in_batch, max_tgt_in_batch</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batch_size_fn</span><span class="params">(new, count, sofar)</span>:</span></span><br><span class="line">    <span class="string">"Keep augmenting batch and calculate total number of tokens + padding."</span></span><br><span class="line">    <span class="keyword">global</span> max_src_in_batch, max_tgt_in_batch</span><br><span class="line">    <span class="keyword">if</span> count == <span class="number">1</span>:</span><br><span class="line">        max_src_in_batch = <span class="number">0</span></span><br><span class="line">        max_tgt_in_batch = <span class="number">0</span></span><br><span class="line">    max_src_in_batch = max(max_src_in_batch,  len(new.src))</span><br><span class="line">    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + <span class="number">2</span>)</span><br><span class="line">    src_elements = count * max_src_in_batch</span><br><span class="line">    tgt_elements = count * max_tgt_in_batch</span><br><span class="line">    <span class="keyword">return</span> max(src_elements, tgt_elements)</span><br></pre></td></tr></table></figure>

<h4 id="硬件-和-训练进度"><a href="#硬件-和-训练进度" class="headerlink" title="硬件 和 训练进度"></a>硬件 和 训练进度</h4><p>We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models, step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).</p>
<h4 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h4><p>We used the <code>Adam optimizer</code> <a href="https://arxiv.org/abs/1412.6980" target="_blank" rel="noopener">(cite)</a> with β1=0.9β1=0.9, β2=0.98β2=0.98 and ϵ=10−9ϵ=10−9. We varied the learning rate over the course of training, according to the formula: lrate=d−0.5model⋅min(step_num−0.5,step_num⋅warmup_steps−1.5)lrate=dmodel−0.5⋅min(step_num−0.5,step_num⋅warmup_steps−1.5) This corresponds to increasing the learning rate linearly for the first warmupstepswarmupsteps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmupsteps=4000warmupsteps=4000.</p>
<blockquote>
<p>Note: This part is very important. Need to train with this setup of the model.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NoamOpt</span>:</span></span><br><span class="line">    <span class="string">"Optim wrapper that implements rate."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, model_size, factor, warmup, optimizer)</span>:</span></span><br><span class="line">        self.optimizer = optimizer</span><br><span class="line">        self._step = <span class="number">0</span></span><br><span class="line">        self.warmup = warmup</span><br><span class="line">        self.factor = factor</span><br><span class="line">        self.model_size = model_size</span><br><span class="line">        self._rate = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"Update parameters and rate"</span></span><br><span class="line">        self._step += <span class="number">1</span></span><br><span class="line">        rate = self.rate()</span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> self.optimizer.param_groups:</span><br><span class="line">            p[<span class="string">'lr'</span>] = rate</span><br><span class="line">        self._rate = rate</span><br><span class="line">        self.optimizer.step()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">rate</span><span class="params">(self, step = None)</span>:</span></span><br><span class="line">        <span class="string">"Implement `lrate` above"</span></span><br><span class="line">        <span class="keyword">if</span> step <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            step = self._step</span><br><span class="line">        <span class="keyword">return</span> self.factor * \</span><br><span class="line">            (self.model_size ** (<span class="number">-0.5</span>) *</span><br><span class="line">            min(step ** (<span class="number">-0.5</span>), step * self.warmup ** (<span class="number">-1.5</span>)))</span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_std_opt</span><span class="params">(model)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> NoamOpt(model.src_embed[<span class="number">0</span>].d_model, <span class="number">2</span>, <span class="number">4000</span>,</span><br><span class="line">            torch.optim.Adam(model.parameters(), lr=<span class="number">0</span>, betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span>))</span><br></pre></td></tr></table></figure>

<blockquote>
<p>Example of the curves of this model for different model sizes and for optimization hyperparameters.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Three settings of the lrate hyperparameters.</span></span><br><span class="line">opts = [NoamOpt(<span class="number">512</span>, <span class="number">1</span>, <span class="number">4000</span>, <span class="literal">None</span>), </span><br><span class="line">        NoamOpt(<span class="number">512</span>, <span class="number">1</span>, <span class="number">8000</span>, <span class="literal">None</span>),</span><br><span class="line">        NoamOpt(<span class="number">256</span>, <span class="number">1</span>, <span class="number">4000</span>, <span class="literal">None</span>)]</span><br><span class="line">plt.plot(np.arange(<span class="number">1</span>, <span class="number">20000</span>), [[opt.rate(i) <span class="keyword">for</span> opt <span class="keyword">in</span> opts] <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">20000</span>)])</span><br><span class="line">plt.legend([<span class="string">"512:4000"</span>, <span class="string">"512:8000"</span>, <span class="string">"256:4000"</span>])</span><br><span class="line"><span class="literal">None</span></span><br></pre></td></tr></table></figure>

<p><img src="http://nlp.seas.harvard.edu/images/the-annotated-transformer_69_0.png" alt="png"></p>
<h4 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h4><h5 id="Label-Smoothing"><a href="#Label-Smoothing" class="headerlink" title="Label Smoothing"></a>Label Smoothing</h5><p>During training, we employed label smoothing of value ϵls=0.1ϵls=0.1 <a href="https://arxiv.org/abs/1512.00567" target="_blank" rel="noopener">(cite)</a>. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.</p>
<blockquote>
<p>We implement label smoothing using the KL div loss. Instead of using a one-hot target distribution, we create a distribution that has <code>confidence</code> of the correct word and the rest of the <code>smoothing</code> mass distributed throughout the vocabulary.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LabelSmoothing</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Implement label smoothing."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, padding_idx, smoothing=<span class="number">0.0</span>)</span>:</span></span><br><span class="line">        super(LabelSmoothing, self).__init__()</span><br><span class="line">        self.criterion = nn.KLDivLoss(size_average=<span class="literal">False</span>)  <span class="comment">#KL散度</span></span><br><span class="line">        self.padding_idx = padding_idx</span><br><span class="line">        self.confidence = <span class="number">1.0</span> - smoothing</span><br><span class="line">        self.smoothing = smoothing</span><br><span class="line">        self.size = size</span><br><span class="line">        self.true_dist = <span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, target)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> x.size(<span class="number">1</span>) == self.size</span><br><span class="line">        true_dist = x.data.clone()</span><br><span class="line">        true_dist.fill_(self.smoothing / (self.size - <span class="number">2</span>))</span><br><span class="line">        true_dist.scatter_(<span class="number">1</span>, target.data.unsqueeze(<span class="number">1</span>), self.confidence)</span><br><span class="line">        true_dist[:, self.padding_idx] = <span class="number">0</span></span><br><span class="line">        mask = torch.nonzero(target.data == self.padding_idx)</span><br><span class="line">        <span class="keyword">if</span> mask.dim() &gt; <span class="number">0</span>:</span><br><span class="line">            true_dist.index_fill_(<span class="number">0</span>, mask.squeeze(), <span class="number">0.0</span>)</span><br><span class="line">        self.true_dist = true_dist</span><br><span class="line">        <span class="keyword">return</span> self.criterion(x, Variable(true_dist, requires_grad=<span class="literal">False</span>))</span><br></pre></td></tr></table></figure>

<blockquote>
<p>Here we can see an example of how the mass is distributed to the words based on confidence.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Example of label smoothing.</span></span><br><span class="line">crit = LabelSmoothing(<span class="number">5</span>, <span class="number">0</span>, <span class="number">0.4</span>)</span><br><span class="line">predict = torch.FloatTensor([[<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>],</span><br><span class="line">                             [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>], </span><br><span class="line">                             [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>]])</span><br><span class="line">v = crit(Variable(predict.log()), </span><br><span class="line">         Variable(torch.LongTensor([<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>])))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show the target distributions expected by the system.</span></span><br><span class="line">plt.imshow(crit.true_dist)</span><br><span class="line"><span class="literal">None</span></span><br></pre></td></tr></table></figure>

<p><img src="http://nlp.seas.harvard.edu/images/the-annotated-transformer_74_0.png" alt="png"></p>
<blockquote>
<p>Label smoothing actually starts to penalize the model if it gets very confident about a given choice.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">crit = LabelSmoothing(<span class="number">5</span>, <span class="number">0</span>, <span class="number">0.1</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(x)</span>:</span></span><br><span class="line">    d = x + <span class="number">3</span> * <span class="number">1</span></span><br><span class="line">    predict = torch.FloatTensor([[<span class="number">0</span>, x / d, <span class="number">1</span> / d, <span class="number">1</span> / d, <span class="number">1</span> / d],</span><br><span class="line">                                 ])</span><br><span class="line">    <span class="comment">#print(predict)</span></span><br><span class="line">    <span class="keyword">return</span> crit(Variable(predict.log()),</span><br><span class="line">                 Variable(torch.LongTensor([<span class="number">1</span>]))).data[<span class="number">0</span>]</span><br><span class="line">plt.plot(np.arange(<span class="number">1</span>, <span class="number">100</span>), [loss(x) <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">100</span>)])</span><br><span class="line"><span class="literal">None</span></span><br></pre></td></tr></table></figure>

<p><img src="http://nlp.seas.harvard.edu/images/the-annotated-transformer_76_0.png" alt="png"></p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h3><p>transformer模型主要分为两大部分, 分别是编码器和解码器, 编码器负责把自然语言序列映射成为隐藏层(下图中第2步用九宫格比喻的部分), 含有自然语言序列的数学表达. 然后解码器把隐藏层再映射为自然语言序列, 从而使我们可以解决各种问题, 如情感分类, 命名实体识别, 语义关系抽取, 摘要生成, 机 器翻译等等, 下面我们简单说一下下图的每一步都做了什么:</p>
<blockquote>
<p>1.输入自然语言序列到编码器: Why do we work?(为什么要工作); </p>
<p>2.编码器输出的隐藏层, 再输入到解码器; </p>
<p>3.输入&lt;𝑠𝑡𝑎𝑟𝑡&gt;<start>(起始)符号到解码器; </p>
<p>4.得到第一个字”为”; </p>
<p>5.将得到的第一个字”为”落下来再输入到解码器; </p>
<p>6.得到第二个字”什”; </p>
<p>7.将得到的第二字再落下来, 直到解码器输出&lt;𝑒𝑛𝑑&gt;<end>(终止符), 即序列生成完成.</p>
</blockquote>
<img src="https://i.loli.net/2020/08/06/1pea3WSThisHBql.png" alt="image-20200806233205808" style="zoom:67%;" />







<p><img src="https://i.loli.net/2020/08/07/ZGa1snNULJtjFWS.png" alt="transformer"></p>
<h3 id="第一个例子"><a href="#第一个例子" class="headerlink" title="第一个例子"></a>第一个例子</h3><blockquote>
<p>We can begin by trying out a simple copy-task. Given a random set of input symbols from a small vocabulary, the goal is to generate back those same symbols.</p>
</blockquote>
<h4 id="Synthetic-Data"><a href="#Synthetic-Data" class="headerlink" title="Synthetic Data"></a>Synthetic Data</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_gen</span><span class="params">(V, batch, nbatches)</span>:</span> <span class="comment"># batch=30:一次输入多少， nbatch=20：输入多少次</span></span><br><span class="line">    <span class="string">"Generate random data for a src-tgt copy task."</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(nbatches): <span class="comment">#一共循环nbatches个，在每一个是一个batch</span></span><br><span class="line">		<span class="comment">#from_numpy ： 将numpy数据转换为tensor</span></span><br><span class="line">		<span class="comment">#注：生成返回的tensor会和ndarry共享数据，任何对tensor的操作都会影响到ndarry</span></span><br><span class="line">        data = torch.from_numpy(np.random.randint(<span class="number">1</span>, V, size=(batch, <span class="number">10</span>))) <span class="comment">#1是产生的最小值，V=11是最大值，size是形状（batch，10）。生成（batch，10）的矩阵，矩阵的每一个元素都是1~V-1之间  （取不到V）</span></span><br><span class="line">        data[:, <span class="number">0</span>] = <span class="number">1</span> <span class="comment">#取第0列的所有值？？？</span></span><br><span class="line">        <span class="comment"># Variable 就是一个存放值， 里面的值会不停的变化.  存放的是Torch 的 Tensor . 如果用一个 Variable 进行计算, 那返回的也是一个同类型的 Variable.  </span></span><br><span class="line">        <span class="comment">#requires_grad： 是否参与误差反向传播, 要不要计算梯度</span></span><br><span class="line">        src = Variable(data, requires_grad=<span class="literal">False</span>) <span class="comment">#size(batch,10)</span></span><br><span class="line">        tgt = Variable(data, requires_grad=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">yield</span> Batch(src, tgt, <span class="number">0</span>)<span class="comment">#yield就是return一个值，并且记住这个返回的位置，下次迭代就从这个位置后(下一行)开始</span></span><br><span class="line">        <span class="comment">#batch返回的是trg_mask</span></span><br></pre></td></tr></table></figure>

<h4 id="Loss-Computation"><a href="#Loss-Computation" class="headerlink" title="Loss Computation"></a>Loss Computation</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleLossCompute</span>:</span> <span class="comment">#loss计算以及更新。调用LabelSmoothing，使用KL散度</span></span><br><span class="line">    <span class="string">"A simple loss compute and train function."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, generator, criterion, opt=None)</span>:</span></span><br><span class="line">        self.generator = generator <span class="comment">#解码器后的生成函数</span></span><br><span class="line">        self.criterion = criterion <span class="comment"># LabelSmoothing（计算loss KLDivLoss KL散度）的实例化</span></span><br><span class="line">        self.opt = opt <span class="comment"># NoamOpt（优化）的实例化</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, x, y, norm)</span>:</span></span><br><span class="line">        x = self.generator(x) <span class="comment">#解码器的输出</span></span><br><span class="line">        loss = self.criterion(x.contiguous().view(<span class="number">-1</span>, x.size(<span class="number">-1</span>)), </span><br><span class="line">                              y.contiguous().view(<span class="number">-1</span>)) / norm  <span class="comment">#计算loss</span></span><br><span class="line">        loss.backward() <span class="comment">#将loss反向传播。loss是标量，根据链式法则自动计算出叶子节点的梯度值</span></span><br><span class="line">        <span class="keyword">if</span> self.opt <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: <span class="comment">#存在优化</span></span><br><span class="line">            self.opt.step() <span class="comment">#调用opt的step函数。 adam优化，，更新参数</span></span><br><span class="line">            self.opt.optimizer.zero_grad() <span class="comment">#把梯度置零，也就是把loss关于weight的导数变成0.</span></span><br><span class="line">        <span class="keyword">return</span> loss.data[<span class="number">0</span>] * norm</span><br></pre></td></tr></table></figure>

<h4 id="Greedy-Decoding"><a href="#Greedy-Decoding" class="headerlink" title="Greedy Decoding"></a>Greedy Decoding</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Train the simple copy task.</span></span><br><span class="line">V = <span class="number">11</span></span><br><span class="line">criterion = LabelSmoothing(size=V, padding_idx=<span class="number">0</span>, smoothing=<span class="number">0.0</span>) <span class="comment">#LabelSmoothing是KL散度实现的</span></span><br><span class="line">model = make_model(V, V, N=<span class="number">2</span>) <span class="comment">#src_vocab=11, tgt_vocab=11，覆盖N=2</span></span><br><span class="line"><span class="comment"># 对模型参数进行更新优化，使用Adam优化</span></span><br><span class="line">model_opt = NoamOpt(model.src_embed[<span class="number">0</span>].d_model, <span class="number">1</span>, <span class="number">400</span>,</span><br><span class="line">        torch.optim.Adam(model.parameters(), lr=<span class="number">0</span>, betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#model.eval()，pytorch会自动把BN和DropOut固定住，不会取平均，而是用训练好的值。</span></span><br><span class="line"><span class="comment">#model.train() 让model变成训练模式，此时dropout和batch normalization的操作在训练起到防止网络过拟合的问题</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">10</span>): <span class="comment">#一共10大份， model.train()打印1行，model.eval()打印1行</span></span><br><span class="line">    model.train()</span><br><span class="line">    <span class="comment">#调用run_epoch(data_iter, model, loss_compute)函数</span></span><br><span class="line">    <span class="comment">#返回total_loss / total_tokens 。返回值可以没有接收，不会报错</span></span><br><span class="line">    run_epoch(data_gen(V, <span class="number">30</span>, <span class="number">20</span>), model, </span><br><span class="line">              SimpleLossCompute(model.generator, criterion, model_opt))</span><br><span class="line">    model.eval()</span><br><span class="line">    <span class="comment">#print接收run_epoch的返回值 在输出的第三行</span></span><br><span class="line">    print(run_epoch(data_gen(V, <span class="number">30</span>, <span class="number">5</span>), model, </span><br><span class="line">                    SimpleLossCompute(model.generator, criterion, <span class="literal">None</span>)))</span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">3.023465</span> Tokens per Sec: <span class="number">403.074173</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.920030</span> Tokens per Sec: <span class="number">641.689380</span></span><br><span class="line"><span class="number">1.9274832487106324</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.940011</span> Tokens per Sec: <span class="number">432.003378</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.699767</span> Tokens per Sec: <span class="number">641.979665</span></span><br><span class="line"><span class="number">1.657595729827881</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.860276</span> Tokens per Sec: <span class="number">433.320240</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.546011</span> Tokens per Sec: <span class="number">640.537198</span></span><br><span class="line"><span class="number">1.4888023376464843</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.682198</span> Tokens per Sec: <span class="number">432.092305</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.313169</span> Tokens per Sec: <span class="number">639.441857</span></span><br><span class="line"><span class="number">1.3485562801361084</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.278768</span> Tokens per Sec: <span class="number">433.568756</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.062384</span> Tokens per Sec: <span class="number">642.542067</span></span><br><span class="line"><span class="number">0.9853351473808288</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.269471</span> Tokens per Sec: <span class="number">433.388727</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">0.590709</span> Tokens per Sec: <span class="number">642.862135</span></span><br><span class="line"><span class="number">0.5686767101287842</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">0.997076</span> Tokens per Sec: <span class="number">433.009746</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">0.343118</span> Tokens per Sec: <span class="number">642.288427</span></span><br><span class="line"><span class="number">0.34273059368133546</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">0.459483</span> Tokens per Sec: <span class="number">434.594030</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">0.290385</span> Tokens per Sec: <span class="number">642.519464</span></span><br><span class="line"><span class="number">0.2612409472465515</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">1.031042</span> Tokens per Sec: <span class="number">434.557008</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">0.437069</span> Tokens per Sec: <span class="number">643.630322</span></span><br><span class="line"><span class="number">0.4323212027549744</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">0.617165</span> Tokens per Sec: <span class="number">436.652626</span></span><br><span class="line">Epoch Step: <span class="number">1</span> Loss: <span class="number">0.258793</span> Tokens per Sec: <span class="number">644.372296</span></span><br><span class="line"><span class="number">0.27331129014492034</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>This code predicts a translation using greedy decoding for simplicity.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#预测过程</span></span><br><span class="line"><span class="comment">#预测的时候没有用tgt（标准值），而是每次解码器的输入都是ys，是预测的值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">greedy_decode</span><span class="params">(model, src, src_mask, max_len, start_symbol)</span>:</span></span><br><span class="line">    memory = model.encode(src, src_mask) <span class="comment">#memory是编码器的输出 。是一个矩阵</span></span><br><span class="line">    ys = torch.ones(<span class="number">1</span>, <span class="number">1</span>).fill_(start_symbol).type_as(src.data) <span class="comment">#填充输出开始符，和src的类型一样。对预测的句子进行初始化</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(max_len<span class="number">-1</span>): <span class="comment">#0~8 对每一个词都进行预测</span></span><br><span class="line">        out = model.decode(memory, src_mask, </span><br><span class="line">                           Variable(ys), </span><br><span class="line">                           Variable(subsequent_mask(ys.size(<span class="number">1</span>))</span><br><span class="line">                                    .type_as(src.data)))</span><br><span class="line">         <span class="comment"># ys 的维度是 batch_size * times （固定的）,   所以target_mask 矩阵必须是ys.size(1),所以是 times * times</span></span><br><span class="line">        <span class="comment"># 根据 decoder 的训练步骤, 这里的 out 输出就应该是 batch_size * (times+1) 的矩阵</span></span><br><span class="line">        </span><br><span class="line">        prob = model.generator(out[:, <span class="number">-1</span>]) <span class="comment">#generator返回的是softmax</span></span><br><span class="line">          <span class="comment"># out[:, -1] 这里是最新的一个单词的 embedding 向量</span></span><br><span class="line">        <span class="comment"># generator 就是产生最后的 vocabulary 的概率, 是一个全连接层</span></span><br><span class="line">        </span><br><span class="line">        _, next_word = torch.max(prob, dim = <span class="number">1</span>) <span class="comment"># torch.max:按维度dim 返回最大值，并且会返回索引。next_data接收											#索引</span></span><br><span class="line">        next_word = next_word.data[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 将句子拼接起来  .type_as: 将tensor强制转换为src.data 格式的</span></span><br><span class="line">        ys = torch.cat([ys, </span><br><span class="line">                        torch.ones(<span class="number">1</span>, <span class="number">1</span>).type_as(src.data).fill_(next_word)], dim=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> ys</span><br><span class="line"></span><br><span class="line">model.eval()</span><br><span class="line">src = Variable(torch.LongTensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>]]) )</span><br><span class="line">src_mask = Variable(torch.ones(<span class="number">1</span>, <span class="number">1</span>, <span class="number">10</span>) )</span><br><span class="line">print(greedy_decode(model, src, src_mask, max_len=<span class="number">10</span>, start_symbol=<span class="number">1</span>))</span><br><span class="line">    <span class="number">1</span>     <span class="number">2</span>     <span class="number">3</span>     <span class="number">4</span>     <span class="number">5</span>     <span class="number">6</span>     <span class="number">7</span>     <span class="number">8</span>     <span class="number">9</span>    <span class="number">10</span></span><br><span class="line">[torch.LongTensor of size <span class="number">1</span>x10]</span><br></pre></td></tr></table></figure>

<h3 id="真实例子"><a href="#真实例子" class="headerlink" title="真实例子"></a>真实例子</h3><blockquote>
<p>Now we consider a real-world example using the IWSLT German-English Translation task. This task is much smaller than the WMT task considered in the paper, but it illustrates the whole system. We also show how to use multi-gpu processing to make it really fast.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!pip install torchtext spacy</span></span><br><span class="line"><span class="comment">#!python -m spacy download en</span></span><br><span class="line"><span class="comment">#!python -m spacy download de</span></span><br></pre></td></tr></table></figure>

<h4 id="Data-Loading"><a href="#Data-Loading" class="headerlink" title="Data Loading"></a>Data Loading</h4><blockquote>
<p>We will load the dataset using torchtext and spacy for tokenization.</p>
<p>用torchtext来加载数据集 ， 用spacy来分词</p>
</blockquote>
<img src="https://i.loli.net/2020/08/07/teSG1hufEjF4Zkv.png" alt="image-20200807001353729" style="zoom: 67%;" />





<p>torchtext组件流程：</p>
<blockquote>
<ul>
<li>定义Field：声明如何处理数据，主要包含以下数据预处理的配置信息，比如指定分词方法，是否转成小写，起始字符，结束字符，补全字符以及词典等等</li>
<li>定义Dataset：用于得到数据集，继承自pytorch的Dataset。此时数据集里每一个样本是一个 经过 Field声明的预处理 预处理后的 wordlist</li>
<li>建立vocab：在这一步建立词汇表，词向量(word embeddings)</li>
<li>构造迭代器Iterator：: 主要是数据输出的模型的迭代器。构造迭代器，支持batch定制用来分批次训练模型。</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># For data loading.</span></span><br><span class="line"><span class="keyword">from</span> torchtext <span class="keyword">import</span> data, datasets</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="keyword">import</span> spacy</span><br><span class="line">    spacy_de = spacy.load(<span class="string">'de'</span>) <span class="comment">#加载德语语言模型</span></span><br><span class="line">    spacy_en = spacy.load(<span class="string">'en'</span>) <span class="comment">#加载英语语言模型</span></span><br><span class="line">	</span><br><span class="line">    </span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">   在文本处理的过程中，spaCy首先对文本分词，原始文本在空格处分割，类似于text.split(' ')，然后分词器（Tokenizer）从左向右依次处理token</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tokenize_de</span><span class="params">(text)</span>:</span> <span class="comment">#Tokenizer:分词器  进行德语分词  </span></span><br><span class="line">        <span class="comment">#text：输入的段落句子  tok.text：分后的token词</span></span><br><span class="line">        <span class="keyword">return</span> [tok.text <span class="keyword">for</span> tok <span class="keyword">in</span> spacy_de.tokenizer(text)]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tokenize_en</span><span class="params">(text)</span>:</span> <span class="comment"># 进行英语分词</span></span><br><span class="line">        <span class="keyword">return</span> [tok.text <span class="keyword">for</span> tok <span class="keyword">in</span> spacy_en.tokenizer(text)]</span><br><span class="line"></span><br><span class="line">    BOS_WORD = <span class="string">'&lt;s&gt;'</span>  <span class="comment">#开始符</span></span><br><span class="line">    EOS_WORD = <span class="string">'&lt;/s&gt;'</span> <span class="comment">#终止符</span></span><br><span class="line">    BLANK_WORD = <span class="string">"&lt;blank&gt;"</span> <span class="comment">#空格</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 构建Filed对象，声明如何处理数据。主要包含以下数据预处理的配置信息，比如指定分词方法，是否转成小写，		#起始字符，结束字符，补全字符以及词典等等</span></span><br><span class="line">    SRC = data.Field(tokenize=tokenize_de, pad_token=BLANK_WORD) <span class="comment">#得到源句子</span></span><br><span class="line">    TGT = data.Field(tokenize=tokenize_en, init_token = BOS_WORD,  </span><br><span class="line">                     eos_token = EOS_WORD, pad_token=BLANK_WORD)</span><br><span class="line"></span><br><span class="line">    MAX_LEN = <span class="number">100</span> <span class="comment">#最大长度</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># https://s3.amazonaws.com/opennmt-models/iwslt.pt 数据集</span></span><br><span class="line">    <span class="comment">#同时对训练集和验证集还有测试集的构建，此时数据集里每一个样本是一个 经过 Field声明的预处理 预处理后的 	#wordlist</span></span><br><span class="line">    train, val, test = datasets.IWSLT.splits(</span><br><span class="line">        exts=(<span class="string">'.de'</span>, <span class="string">'.en'</span>)   <span class="comment"># 构建数据集所需的数据集</span></span><br><span class="line">        , fields=(SRC, TGT),  <span class="comment">#如何赋值给train那三个的？？？？</span></span><br><span class="line">        filter_pred=<span class="keyword">lambda</span> x: len(vars(x)[<span class="string">'src'</span>]) &lt;= MAX_LEN <span class="keyword">and</span> </span><br><span class="line">            len(vars(x)[<span class="string">'trg'</span>]) &lt;= MAX_LEN)  <span class="comment">#源句子和目标句子长度小于100的筛选出来</span></span><br><span class="line">    </span><br><span class="line">    MIN_FREQ = <span class="number">2</span> <span class="comment">#定义最小频率</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#建立词汇表，词向量(word embeddings)。即需要给每个单词编码，然后输入模型</span></span><br><span class="line">    <span class="comment">#bulid_vocab()方法中传入用于构建词表的数据集</span></span><br><span class="line">    SRC.build_vocab(train.src, min_freq=MIN_FREQ) </span><br><span class="line">    TGT.build_vocab(train.trg, min_freq=MIN_FREQ)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#一旦运行了这些代码行，SRC.vocab.stoi将是一个词典，其词汇表中的标记作为键，而其对应的索引作为值； 	#SRC.vocab.itos将是相同的字典，其中的键和值被交换。</span></span><br></pre></td></tr></table></figure>



<blockquote>
<p>批训练对于速度来说很重要。希望批次分割非常均匀并且填充最少。 要做到这一点，我们<strong>必须修改torchtext默认的批处理函数</strong>。 这部分代码修补其默认批处理函数，以确保我们搜索足够多的句子以构建紧密批处理。  一般来说直接调用<code>BucketIterator</code> （训练用）和 <code>Iterator</code>（测试用） 即可</p>
<p><code>BucketIterator</code>和<code>Iterator</code>的区别是，BucketIterator尽可能的把长度相似的句子放在一个batch里面。</p>
</blockquote>
<h4 id="Iterators"><a href="#Iterators" class="headerlink" title="Iterators"></a>Iterators</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">定义一个迭代器，该迭代器将相似长度的示例批处理在一起。 在为每个新纪元(epoch)生产新鲜改组的批次时，最大程度地减少所需的填充量。</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyIterator</span><span class="params">(data.Iterator)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_batches</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment">#在train的时候，要进行sort，尽量减少padding</span></span><br><span class="line">        <span class="comment">#目的是自动进行shuffle和padding，并且为了训练效率期间，尽量把句子长度相似的shuffle在一起。</span></span><br><span class="line">        <span class="keyword">if</span> self.train:</span><br><span class="line">            <span class="function"><span class="keyword">def</span> <span class="title">pool</span><span class="params">(d, random_shuffler)</span>:</span></span><br><span class="line">                <span class="keyword">for</span> p <span class="keyword">in</span> data.batch(d, self.batch_size * <span class="number">100</span>):</span><br><span class="line">                    p_batch = data.batch(</span><br><span class="line">                        sorted(p, key=self.sort_key), <span class="comment">#按照词的数大小排序</span></span><br><span class="line">                        self.batch_size, self.batch_size_fn)</span><br><span class="line">                    <span class="keyword">for</span> b <span class="keyword">in</span> random_shuffler(list(p_batch)):</span><br><span class="line">                        <span class="keyword">yield</span> b <span class="comment">#b就是batch， 类比上述的gen_data函数</span></span><br><span class="line">            self.batches = pool(self.data(), self.random_shuffler) <span class="comment">#调用pool</span></span><br><span class="line">            </span><br><span class="line">         <span class="comment">#在valid+test(验证集和测试集)的时候  和上面具体区别在哪？？？？</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.batches = []</span><br><span class="line">            <span class="keyword">for</span> b <span class="keyword">in</span> data.batch(self.data(), self.batch_size,</span><br><span class="line">                                          self.batch_size_fn):</span><br><span class="line">                self.batches.append(sorted(b, key=self.sort_key))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rebatch</span><span class="params">(pad_idx, batch)</span>:</span></span><br><span class="line">    <span class="string">"Fix order in torchtext to match ours"</span></span><br><span class="line">    src, trg = batch.src.transpose(<span class="number">0</span>, <span class="number">1</span>), batch.trg.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> Batch(src, trg, pad_idx) <span class="comment">#调用上述的Batch类   pad_idx就是pad</span></span><br></pre></td></tr></table></figure>

<h4 id="Multi-GPU-Training"><a href="#Multi-GPU-Training" class="headerlink" title="Multi-GPU Training"></a>Multi-GPU Training</h4><blockquote>
<p>最后为了真正地快速训练，将使用多个GPU。 这部分代码实现了多GPU字生成，它不是Transformer特有的。 其<strong>思想是将训练时的单词生成分成块，以便在许多不同的GPU上并行处理。</strong> 我们使用PyTorch并行原语来做到这一点：</p>
<ul>
<li>replicate -复制 - 将模块拆分到不同的GPU上</li>
<li>scatter -分散 - 将批次拆分到不同的GPU上</li>
<li>parallel_apply -并行应用 - 在不同GPU上将模块应用于批处理</li>
<li>gather - 聚集 - 将分散的数据聚集到一个GPU上</li>
<li>nn.DataParallel - 一个特殊的模块包装器，在评估之前调用它们。</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Skip if not interested in multigpu.</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiGPULossCompute</span>:</span></span><br><span class="line">    <span class="string">"A multi-gpu loss compute and train function."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, generator, criterion, devices, opt=None, chunk_size=<span class="number">5</span>)</span>:</span></span><br><span class="line">        <span class="comment"># Send out to different gpus.</span></span><br><span class="line">        self.generator = generator</span><br><span class="line">        self.criterion = nn.parallel.replicate(criterion, </span><br><span class="line">                                               devices=devices)</span><br><span class="line">        self.opt = opt</span><br><span class="line">        self.devices = devices</span><br><span class="line">        self.chunk_size = chunk_size</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, out, targets, normalize)</span>:</span></span><br><span class="line">        total = <span class="number">0.0</span></span><br><span class="line">        generator = nn.parallel.replicate(self.generator, </span><br><span class="line">                                                devices=self.devices)</span><br><span class="line">        out_scatter = nn.parallel.scatter(out, </span><br><span class="line">                                          target_gpus=self.devices)</span><br><span class="line">        out_grad = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> out_scatter]</span><br><span class="line">        targets = nn.parallel.scatter(targets, </span><br><span class="line">                                      target_gpus=self.devices)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Divide generating into chunks.</span></span><br><span class="line">        chunk_size = self.chunk_size</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, out_scatter[<span class="number">0</span>].size(<span class="number">1</span>), chunk_size):</span><br><span class="line">            <span class="comment"># Predict distributions</span></span><br><span class="line">            out_column = [[Variable(o[:, i:i+chunk_size].data, </span><br><span class="line">                                    requires_grad=self.opt <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>)] </span><br><span class="line">                           <span class="keyword">for</span> o <span class="keyword">in</span> out_scatter]</span><br><span class="line">            gen = nn.parallel.parallel_apply(generator, out_column)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Compute loss. </span></span><br><span class="line">            y = [(g.contiguous().view(<span class="number">-1</span>, g.size(<span class="number">-1</span>)), </span><br><span class="line">                  t[:, i:i+chunk_size].contiguous().view(<span class="number">-1</span>)) </span><br><span class="line">                 <span class="keyword">for</span> g, t <span class="keyword">in</span> zip(gen, targets)]</span><br><span class="line">            loss = nn.parallel.parallel_apply(self.criterion, y)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Sum and normalize loss</span></span><br><span class="line">            l = nn.parallel.gather(loss, </span><br><span class="line">                                   target_device=self.devices[<span class="number">0</span>])</span><br><span class="line">            l = l.sum()[<span class="number">0</span>] / normalize</span><br><span class="line">            total += l.data[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Backprop loss to output of transformer</span></span><br><span class="line">            <span class="keyword">if</span> self.opt <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                l.backward()</span><br><span class="line">                <span class="keyword">for</span> j, l <span class="keyword">in</span> enumerate(loss):</span><br><span class="line">                    out_grad[j].append(out_column[j][<span class="number">0</span>].grad.data.clone())</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Backprop all loss through transformer.            </span></span><br><span class="line">        <span class="keyword">if</span> self.opt <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            out_grad = [Variable(torch.cat(og, dim=<span class="number">1</span>)) <span class="keyword">for</span> og <span class="keyword">in</span> out_grad]</span><br><span class="line">            o1 = out</span><br><span class="line">            o2 = nn.parallel.gather(out_grad, </span><br><span class="line">                                    target_device=self.devices[<span class="number">0</span>])</span><br><span class="line">            o1.backward(gradient=o2)</span><br><span class="line">            self.opt.step()</span><br><span class="line">            self.opt.optimizer.zero_grad()</span><br><span class="line">        <span class="keyword">return</span> total * normalize</span><br></pre></td></tr></table></figure>

<blockquote>
<p>Now we create our model, criterion, optimizer, data iterators, and paralelization</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># GPUs to use</span></span><br><span class="line">devices = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line"><span class="keyword">if</span> <span class="literal">True</span>:</span><br><span class="line">    pad_idx = TGT.vocab.stoi[<span class="string">"&lt;blank&gt;"</span>]</span><br><span class="line">    model = make_model(len(SRC.vocab), len(TGT.vocab), N=<span class="number">6</span>)</span><br><span class="line">    model.cuda()</span><br><span class="line">    criterion = LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, smoothing=<span class="number">0.1</span>)</span><br><span class="line">    criterion.cuda()</span><br><span class="line">    BATCH_SIZE = <span class="number">12000</span></span><br><span class="line">    train_iter = MyIterator(train, batch_size=BATCH_SIZE, device=<span class="number">0</span>,</span><br><span class="line">                            repeat=<span class="literal">False</span>, sort_key=<span class="keyword">lambda</span> x: (len(x.src), len(x.trg)),</span><br><span class="line">                            batch_size_fn=batch_size_fn, train=<span class="literal">True</span>)</span><br><span class="line">    valid_iter = MyIterator(val, batch_size=BATCH_SIZE, device=<span class="number">0</span>,</span><br><span class="line">                            repeat=<span class="literal">False</span>, sort_key=<span class="keyword">lambda</span> x: (len(x.src), len(x.trg)),</span><br><span class="line">                            batch_size_fn=batch_size_fn, train=<span class="literal">False</span>)</span><br><span class="line">    model_par = nn.DataParallel(model, device_ids=devices)</span><br><span class="line"><span class="literal">None</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>Now we <strong>train the model</strong>. I will play with the warmup steps a bit, but everything else uses the default parameters. On an AWS p3.8xlarge with 4 Tesla V100s, this runs at ~27,000 tokens per second with a batch size of 12,000</p>
<p>在具有4个Tesla V100 GPU的AWS p3.8xlarge机器上，每秒运行约27,000个词，批训练大小为12,000。</p>
</blockquote>
<h4 id="Training-the-System"><a href="#Training-the-System" class="headerlink" title="Training the System"></a>Training the System</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!wget https://s3.amazonaws.com/opennmt-models/iwslt.pt</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="literal">False</span>: <span class="comment"># false存在的意义在哪？？？ 使用GPU？</span></span><br><span class="line">    model_opt = NoamOpt(model.src_embed[<span class="number">0</span>].d_model, <span class="number">1</span>, <span class="number">2000</span>,</span><br><span class="line">            torch.optim.Adam(model.parameters(), lr=<span class="number">0</span>, betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span>))</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        model_par.train()</span><br><span class="line">        run_epoch((rebatch(pad_idx, b) <span class="keyword">for</span> b <span class="keyword">in</span> train_iter), </span><br><span class="line">                  model_par, </span><br><span class="line">                  MultiGPULossCompute(model.generator, criterion, </span><br><span class="line">                                      devices=devices, opt=model_opt))</span><br><span class="line">        model_par.eval()</span><br><span class="line">        loss = run_epoch((rebatch(pad_idx, b) <span class="keyword">for</span> b <span class="keyword">in</span> valid_iter), </span><br><span class="line">                          model_par, </span><br><span class="line">                          MultiGPULossCompute(model.generator, criterion, </span><br><span class="line">                          devices=devices, opt=<span class="literal">None</span>))</span><br><span class="line">        print(loss)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    model = torch.load(<span class="string">"iwslt.pt"</span>) <span class="comment">#加载所有的tensor到CPU</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>Once trained we can decode the model to produce a set of translations. Here we simply translate the first sentence in the validation set. This dataset is pretty small so the translations with greedy search are reasonably accurate.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#类比于run_epoch函数  </span></span><br><span class="line"><span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(valid_iter):</span><br><span class="line">    src = batch.src.transpose(<span class="number">0</span>, <span class="number">1</span>)[:<span class="number">1</span>]</span><br><span class="line">    src_mask = (src != SRC.vocab.stoi[<span class="string">"&lt;blank&gt;"</span>]).unsqueeze(<span class="number">-2</span>)</span><br><span class="line">    out = greedy_decode(model, src, src_mask, </span><br><span class="line">                        max_len=<span class="number">60</span>, start_symbol=TGT.vocab.stoi[<span class="string">"&lt;s&gt;"</span>])</span><br><span class="line">    print(<span class="string">"Translation:"</span>, end=<span class="string">"\t"</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, out.size(<span class="number">1</span>)):</span><br><span class="line">        sym = TGT.vocab.itos[out[<span class="number">0</span>, i]]</span><br><span class="line">        <span class="keyword">if</span> sym == <span class="string">"&lt;/s&gt;"</span>: <span class="keyword">break</span></span><br><span class="line">        print(sym, end =<span class="string">" "</span>)</span><br><span class="line">    print()</span><br><span class="line">    print(<span class="string">"Target:"</span>, end=<span class="string">"\t"</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, batch.trg.size(<span class="number">0</span>)):</span><br><span class="line">        sym = TGT.vocab.itos[batch.trg.data[i, <span class="number">0</span>]]</span><br><span class="line">        <span class="keyword">if</span> sym == <span class="string">"&lt;/s&gt;"</span>: <span class="keyword">break</span></span><br><span class="line">        print(sym, end =<span class="string">" "</span>)</span><br><span class="line">    print()</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line">Translation:	&lt;unk&gt; &lt;unk&gt; . In my language , that means , thank you very much . </span><br><span class="line">Gold:	&lt;unk&gt; &lt;unk&gt; . It means <span class="keyword">in</span> my language , thank you very much .</span><br></pre></td></tr></table></figure>

<h3 id="Additional-Components-BPE-Search-Averaging"><a href="#Additional-Components-BPE-Search-Averaging" class="headerlink" title="Additional Components: BPE, Search, Averaging"></a>Additional Components: BPE, Search, Averaging</h3><blockquote>
<p>So this mostly covers the transformer model itself. There are four aspects that we didn’t cover explicitly. We also have all these additional features implemented in <a href="https://github.com/opennmt/opennmt-py" target="_blank" rel="noopener">OpenNMT-py</a>.</p>
</blockquote>
<blockquote>
<p>1) BPE/ Word-piece: We can use a library to first preprocess the data into subword units. See Rico Sennrich’s <a href="https://github.com/rsennrich/subword-nmt" target="_blank" rel="noopener">subword- nmt</a> implementation. These models will transform the training data to look like this:</p>
</blockquote>
<p>▁Die ▁Protokoll datei ▁kann ▁ heimlich ▁per ▁E - Mail ▁oder ▁FTP ▁an ▁einen ▁bestimmte n ▁Empfänger ▁gesendet ▁werden .</p>
<blockquote>
<p>2) Shared Embeddings: When using BPE with shared vocabulary we can share the same weight vectors between the source / target / generator. See the <a href="https://arxiv.org/abs/1608.05859" target="_blank" rel="noopener">(cite)</a> for details. To add this to the model simply do this:</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="literal">False</span>:</span><br><span class="line">    model.src_embed[<span class="number">0</span>].lut.weight = model.tgt_embeddings[<span class="number">0</span>].lut.weight</span><br><span class="line">    model.generator.lut.weight = model.tgt_embed[<span class="number">0</span>].lut.weight</span><br></pre></td></tr></table></figure>

<blockquote>
<p>3) Beam Search: This is a bit too complicated to cover here. See the <a href="https://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/translate/Beam.py" target="_blank" rel="noopener">OpenNMT- py</a> for a pytorch implementation.</p>
</blockquote>
<blockquote>
<p>4) Model Averaging: The paper averages the last k checkpoints to create an ensembling effect. We can do this after the fact if we have a bunch of models:</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">average</span><span class="params">(model, models)</span>:</span></span><br><span class="line">    <span class="string">"Average models into model"</span></span><br><span class="line">    <span class="keyword">for</span> ps <span class="keyword">in</span> zip(*[m.params() <span class="keyword">for</span> m <span class="keyword">in</span> [model] + models]):</span><br><span class="line">        p[<span class="number">0</span>].copy_(torch.sum(*ps[<span class="number">1</span>:]) / len(ps[<span class="number">1</span>:]))</span><br></pre></td></tr></table></figure>

<h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p>On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.</p>
<p>On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate Pdrop = 0.1, instead of 0.3.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Image(filename=<span class="string">"images/results.png"</span>)</span><br></pre></td></tr></table></figure>

<p><img src="http://nlp.seas.harvard.edu/images/the-annotated-transformer_113_0.png" alt="png"></p>
<blockquote>
<p>The code we have written here is a version of the base model. There are fully trained version of this system available here <a href="http://opennmt.net/Models-py/" target="_blank" rel="noopener">(Example Models)</a>.</p>
<p>With the addtional extensions in the last section, the OpenNMT-py replication gets to 26.9 on EN-DE WMT. Here I have loaded in those parameters to our reimplemenation.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">!wget https://s3.amazonaws.com/opennmt-models/en-de-model.pt</span><br><span class="line">model, SRC, TGT = torch.load(<span class="string">"en-de-model.pt"</span>)</span><br><span class="line">model.eval()</span><br><span class="line">sent = <span class="string">"▁The ▁log ▁file ▁can ▁be ▁sent ▁secret ly ▁with ▁email ▁or ▁FTP ▁to ▁a ▁specified ▁receiver"</span>.split()</span><br><span class="line">src = torch.LongTensor([[SRC.stoi[w] <span class="keyword">for</span> w <span class="keyword">in</span> sent]])</span><br><span class="line">src = Variable(src)</span><br><span class="line">src_mask = (src != SRC.stoi[<span class="string">"&lt;blank&gt;"</span>]).unsqueeze(<span class="number">-2</span>)</span><br><span class="line">out = greedy_decode(model, src, src_mask, </span><br><span class="line">                    max_len=<span class="number">60</span>, start_symbol=TGT.stoi[<span class="string">"&lt;s&gt;"</span>])</span><br><span class="line">print(<span class="string">"Translation:"</span>, end=<span class="string">"\t"</span>)</span><br><span class="line">trans = <span class="string">"&lt;s&gt; "</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, out.size(<span class="number">1</span>)):</span><br><span class="line">    sym = TGT.itos[out[<span class="number">0</span>, i]]</span><br><span class="line">    <span class="keyword">if</span> sym == <span class="string">"&lt;/s&gt;"</span>: <span class="keyword">break</span></span><br><span class="line">    trans += sym + <span class="string">" "</span></span><br><span class="line">print(trans)</span><br><span class="line">Translation:	&lt;s&gt; ▁Die ▁Protokoll datei ▁kann ▁ heimlich ▁per ▁E - Mail ▁oder ▁FTP ▁an ▁einen ▁bestimmte n ▁Empfänger ▁gesendet ▁werden .</span><br></pre></td></tr></table></figure>

<h4 id="Attention-Visualization"><a href="#Attention-Visualization" class="headerlink" title="Attention Visualization"></a>Attention Visualization</h4><blockquote>
<p>Even with a greedy decoder the translation looks pretty good. We can further visualize it to see what is happening at each layer of the attention</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tgt_sent = trans.split()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">draw</span><span class="params">(data, x, y, ax)</span>:</span></span><br><span class="line">    seaborn.heatmap(data, </span><br><span class="line">                    xticklabels=x, square=<span class="literal">True</span>, yticklabels=y, vmin=<span class="number">0.0</span>, vmax=<span class="number">1.0</span>, </span><br><span class="line">                    cbar=<span class="literal">False</span>, ax=ax)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">6</span>, <span class="number">2</span>):</span><br><span class="line">    fig, axs = plt.subplots(<span class="number">1</span>,<span class="number">4</span>, figsize=(<span class="number">20</span>, <span class="number">10</span>))</span><br><span class="line">    print(<span class="string">"Encoder Layer"</span>, layer+<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> h <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">        draw(model.encoder.layers[layer].self_attn.attn[<span class="number">0</span>, h].data, </span><br><span class="line">            sent, sent <span class="keyword">if</span> h ==<span class="number">0</span> <span class="keyword">else</span> [], ax=axs[h])</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">6</span>, <span class="number">2</span>):</span><br><span class="line">    fig, axs = plt.subplots(<span class="number">1</span>,<span class="number">4</span>, figsize=(<span class="number">20</span>, <span class="number">10</span>))</span><br><span class="line">    print(<span class="string">"Decoder Self Layer"</span>, layer+<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> h <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">        draw(model.decoder.layers[layer].self_attn.attn[<span class="number">0</span>, h].data[:len(tgt_sent), :len(tgt_sent)], </span><br><span class="line">            tgt_sent, tgt_sent <span class="keyword">if</span> h ==<span class="number">0</span> <span class="keyword">else</span> [], ax=axs[h])</span><br><span class="line">    plt.show()</span><br><span class="line">    print(<span class="string">"Decoder Src Layer"</span>, layer+<span class="number">1</span>)</span><br><span class="line">    fig, axs = plt.subplots(<span class="number">1</span>,<span class="number">4</span>, figsize=(<span class="number">20</span>, <span class="number">10</span>))</span><br><span class="line">    <span class="keyword">for</span> h <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">        draw(model.decoder.layers[layer].self_attn.attn[<span class="number">0</span>, h].data[:len(tgt_sent), :len(sent)], </span><br><span class="line">            sent, tgt_sent <span class="keyword">if</span> h ==<span class="number">0</span> <span class="keyword">else</span> [], ax=axs[h])</span><br><span class="line">    plt.show()</span><br><span class="line">Encoder Layer <span class="number">2</span></span><br></pre></td></tr></table></figure>

<p><img src="http://nlp.seas.harvard.edu/images/the-annotated-transformer_119_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Encoder Layer <span class="number">4</span></span><br></pre></td></tr></table></figure>

<p><img src="http://nlp.seas.harvard.edu/images/the-annotated-transformer_119_3.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Encoder Layer <span class="number">6</span></span><br></pre></td></tr></table></figure>

<p><img src="http://nlp.seas.harvard.edu/images/the-annotated-transformer_119_5.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Decoder Self Layer <span class="number">2</span></span><br></pre></td></tr></table></figure>

<p><img src="http://nlp.seas.harvard.edu/images/the-annotated-transformer_119_7.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Decoder Src Layer <span class="number">2</span></span><br></pre></td></tr></table></figure>

<p><img src="http://nlp.seas.harvard.edu/images/the-annotated-transformer_119_9.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Decoder Self Layer <span class="number">4</span></span><br></pre></td></tr></table></figure>

<p><img src="http://nlp.seas.harvard.edu/images/the-annotated-transformer_119_11.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Decoder Src Layer <span class="number">4</span></span><br></pre></td></tr></table></figure>

<p><img src="http://nlp.seas.harvard.edu/images/the-annotated-transformer_119_13.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Decoder Self Layer <span class="number">6</span></span><br></pre></td></tr></table></figure>

<p><img src="http://nlp.seas.harvard.edu/images/the-annotated-transformer_119_15.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Decoder Src Layer <span class="number">6</span></span><br></pre></td></tr></table></figure>

<p><img src="http://nlp.seas.harvard.edu/images/the-annotated-transformer_119_17.png" alt="png"></p>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><blockquote>
<p>Hopefully this code is useful for future research. Please reach out if you have any issues. If you find this code helpful, also check out our other OpenNMT tools.</p>
</blockquote>
]]></content>
      <categories>
        <category>transformer</category>
      </categories>
      <tags>
        <tag>transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-07-25-服务器心得</title>
    <url>/2020/07/25/2020-07-25-%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%BF%83%E5%BE%97/</url>
    <content><![CDATA[<p>目前使用的是Xshell + FileZilla（连接+传输） ， MobaXterm也可用于ssh连接的备用软件。</p>
<p>MobaXterm（<a href="https://link.zhihu.com/?target=https%3A//mobaxterm.mobatek.net/">https://mobaxterm.mobatek.net/</a>）：功能很全，免费，有免安装版，支持多标签，同时自带文件传输系统，唯一的不足是对Z-moderm支持较差。</p>
<h3 id="linux后台执行命令：-amp-和nohup"><a href="#linux后台执行命令：-amp-和nohup" class="headerlink" title="linux后台执行命令：&amp;和nohup"></a>linux后台执行命令：&amp;和nohup</h3><p>在用本机Xshell连接服务器跑实验时，如果关闭本机电脑，Xshell不再运行时，那么linux终端会话就会关闭，跑的实验就会终止。有时候更希望它能够在每天的非负荷高峰时间段运行(例如凌晨)。为了使这些进程能够在后台运行，也就是说不在终端屏幕上运行，有几种选择方法可供使用。</p>
<h4 id="amp-使用"><a href="#amp-使用" class="headerlink" title="&amp;使用"></a>&amp;使用</h4><p>在执行文件的时候，可以在命令后面加上<code>&amp;</code>，这样可以实现将进程挂到后台运行。例如： <code>python main.py &amp;</code></p>
<p>在使用&amp;之后，系统会返回一个进程号PID，需要记下此进程的PID</p>
<h4 id="nohup使用"><a href="#nohup使用" class="headerlink" title="nohup使用"></a>nohup使用</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nohup python main.py &amp;</span><br></pre></td></tr></table></figure>

<p>这样执行的时候会将代码放在服务器后台执行，你的终端是看不到运行过程的，期间运行的结果（代码运行过程中打印出来的）会在一个生成的<code>nohup.out文件</code>中保存。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nohup python main.py &gt;test.log  <span class="number">2</span>&gt;&amp;<span class="number">1</span> &amp;</span><br><span class="line"><span class="comment"># nohup和&amp; 一起使用。 &amp; 放后台</span></span><br><span class="line"><span class="comment"># &gt;表示将标准输出（STDOUT）重定向到test.log文件</span></span><br><span class="line"><span class="comment">#2&gt;&amp;1 ：把标准输出和标准错误一起重定向到一个文件中。1是标准输出的文件描述符，2是标准错误的文件描述符；</span></span><br></pre></td></tr></table></figure>

<p>可以实现运行main.py ，并将输出结果打印到<code>test.log文件</code>中（如果这个文件不存在, 那就创建, 否则就覆盖）</p>
<p>要想使ssh连接断掉也可以继续后台运行，需要用<code>exit命令</code>断开，否则其它关闭行为视为断开异常（如直接关掉xsheel软件），不会后台运行</p>
<h4 id="nohup与session的关系"><a href="#nohup与session的关系" class="headerlink" title="nohup与session的关系"></a>nohup与session的关系</h4><p><code>如果我们在 session 中执行了 nohup 等类似的命令，当 session 消亡时，相关的进程并不会随着 session 结束，原因是这些进程不再受 SIGHUP 信号的影响。</code>比如我们执行下面的命令：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">$ nohup sleep <span class="number">1000</span> &gt;/dev/null <span class="number">2</span>&gt;&amp;<span class="number">1</span> &amp;</span><br></pre></td></tr></table></figure>



<p><img src="https://i.loli.net/2020/07/29/cAzPToGdLKmRZ2e.png" alt="img"></p>
<p>此时 sleep 进程的 sid 和其它进程是相同的，还可以通过 pstree 命令看到进程间的父子关系：</p>
<p><img src="https://i.loli.net/2020/07/29/8ZQDMlkmTHy1a2G.png" alt="img"></p>
<p><code>如果我们退出当前 session 的领头进程(bash)，sleep 进程并不会退出，这样我们就可以放心的等待该进程运行结果了。</code><br>nohup 并不改变进程的 sid，同时也说明在这种情况中，虽然 session 的领头进程退出了，但是 session 依然没有被销毁(至少 sid 还在被引用)。重新建立连接，通过下面的命令查看 sleep 进程的信息，发现进程的 sid 依然是 7837：</p>
<p><img src="https://i.loli.net/2020/07/29/5O83jJ2Hfsdoyil.png" alt="img"></p>
<p>但是<code>此时的 sleep 已经被系统的 1 号进程 systemd 收养了</code>：</p>
<p><img src="https://i.loli.net/2020/07/29/9quLvTHCNnES4F7.png" alt="img"></p>
<h5 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h5><blockquote>
<p><a href="https://www.cnblogs.com/sparkdev/p/12146305.html" target="_blank" rel="noopener">https://www.cnblogs.com/sparkdev/p/12146305.html</a></p>
</blockquote>
<h4 id="忘记进程"><a href="#忘记进程" class="headerlink" title="忘记进程"></a>忘记进程</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ps -ef | grep main.py  <span class="comment">#其中main.py是要查找的关键字</span></span><br><span class="line">ps -ef | grep main.py  | grep -v grep <span class="comment">#grep -v 排除进程。此时是排除grep自身的进程</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ps命令将某个进程显示出来</span><br><span class="line"></span><br><span class="line">grep命令是查找</span><br><span class="line"></span><br><span class="line">中间的|是管道命令 是指ps命令与grep同时执行</span><br><span class="line"></span><br><span class="line">字段含义如下：</span><br><span class="line">UID    PID    PPID    C   STIME   TTY    TIME     CMD</span><br><span class="line"></span><br><span class="line">zzw   <span class="number">14124</span>  <span class="number">13991</span>   <span class="number">0</span>   <span class="number">00</span>:<span class="number">38</span>   pts/<span class="number">0</span>   <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span>  grep --color=auto dae</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">UID   ：程序被该 UID 所拥有</span><br><span class="line"></span><br><span class="line">PID   ：就是这个程序的 ID </span><br><span class="line"></span><br><span class="line">PPID  ：则是其上级父程序的ID</span><br><span class="line"></span><br><span class="line">C     ：CPU使用的资源百分比</span><br><span class="line"></span><br><span class="line">STIME ：系统启动时间</span><br><span class="line"></span><br><span class="line">TTY   ：登入者的终端机位置</span><br><span class="line"></span><br><span class="line">TIME  ：使用掉的CPU时间。</span><br><span class="line"></span><br><span class="line">CMD  ：所下达的是什么指令</span><br></pre></td></tr></table></figure>



<p>这里是两个shell命令通过管道进行了结合，第一个ps能够列出当前系统所有活跃的进程，然后通过grep 关键字查找就能找到带有关键字的进程。<code>找到PID</code>（PID是输出的第二列那个数字）再杀掉。</p>
<h4 id="关闭进程"><a href="#关闭进程" class="headerlink" title="关闭进程"></a>关闭进程</h4><p><code>kill -9 PID</code>  . 用普通的ctrl+C是关不掉的</p>
<h4 id="疑问："><a href="#疑问：" class="headerlink" title="疑问："></a>疑问：</h4><p>在根指令行可以进行nohup，但是我用tmux建立会话之后，在tmux中是不可以运用notop，会弹出 <code>exit 1</code> 的错误指令？？？？</p>
<p>一种方法就是在后台运行之后，再进入tmux操作</p>
<h3 id="htop使用"><a href="#htop使用" class="headerlink" title="htop使用"></a>htop使用</h3><h4 id="功能介绍"><a href="#功能介绍" class="headerlink" title="功能介绍"></a>功能介绍</h4><p>监视内存，线程，CPU运行状态</p>
<p>htop是Linux系统下一个基本文本模式的、交互式的进程查看器，主要用于控制台或shell中，可以替代top，或者说是top的高级版。</p>
<h4 id="安装htop"><a href="#安装htop" class="headerlink" title="安装htop"></a>安装htop</h4><p> Ubuntu    <code>sudo apt-get install htop</code></p>
<h4 id="使用htop"><a href="#使用htop" class="headerlink" title="使用htop"></a>使用htop</h4><h5 id="界面概述"><a href="#界面概述" class="headerlink" title="界面概述"></a>界面概述</h5><p>安装完成后，命令行中直接敲击 htop 命令，即可进入 htop 的界面</p>
<p><a href="https://blog.xiewenlong.com/2018/12/htop/htop.png" target="_blank" rel="noopener"><img src="https://blog.xiewenlong.com/2018/12/htop/htop.png" alt="img"></a></p>
<p>各项从上至下分别说明如下：</p>
<p><a href="https://blog.xiewenlong.com/2018/12/htop/base.png" target="_blank" rel="noopener"><img src="https://blog.xiewenlong.com/2018/12/htop/base.png" alt="img"></a></p>
<p>左边部分从上至下，分别为，cpu、内存、交换分区的使用情况，右边部分为：Tasks 为进程总数，当前运行的进程数、Load average 为系统 1 分钟，5 分钟，10 分钟的平均负载情况、Uptime 为系统运行的时间。</p>
<p><a href="https://blog.xiewenlong.com/2018/12/htop/process.png" target="_blank" rel="noopener"><img src="https://blog.xiewenlong.com/2018/12/htop/process.png" alt="img"></a></p>
<p>以上各项分别为：</p>
<ul>
<li><strong>PID：</strong>进程的标识号</li>
<li><strong>USER：</strong>运行此进程的用户</li>
<li><strong>PRI：</strong>进程的优先级</li>
<li><strong>NI：</strong>进程的优先级别值，默认的为 0，可以进行调整</li>
<li><strong>VIRT：</strong>进程占用的虚拟内存值</li>
<li><strong>RES：</strong>进程占用的物理内存值</li>
<li><strong>SHR：</strong>进程占用的共享内存值</li>
<li><strong>S：</strong>进程的运行状况，R 表示正在运行、S 表示休眠，等待唤醒、Z 表示僵死状态</li>
<li><strong>%CPU：</strong>该进程占用的CPU使用率</li>
<li><strong>%MEM：</strong>该进程占用的物理内存和总内存的百分比</li>
<li><strong>TIME+：</strong>该进程启动后占用的总的 CPU 时间</li>
<li><strong>COMMAND：</strong>进程启动的启动命令名称</li>
</ul>
<h4 id="操作说明"><a href="#操作说明" class="headerlink" title="操作说明"></a>操作说明</h4><p><code>htop</code> 界面底部给出了 F1 ~ F10 按键的简单说明。</p>
<p><a href="https://blog.xiewenlong.com/2018/12/htop/bottom.png" target="_blank" rel="noopener"><img src="https://blog.xiewenlong.com/2018/12/htop/bottom.png" alt="img"></a></p>
<h5 id="标注进程条目"><a href="#标注进程条目" class="headerlink" title="标注进程条目"></a>标注进程条目</h5><p>在系统中运行着的实时进程视图中，要追踪某个进程是个大问题。因为整个列表在不停的刷新着，进程的排列顺序也在变动着。为了这个问题， <code>htop</code> 提供了一个很简单的解决方案：颜色标注。是的，你可以标注一个进程条目，它会以不同的颜色显示，因此要追踪它就变得容易了。</p>
<p>要标注某个进程条目，需要做的就是选中此条目，然后按下<code>空格</code>键。例如，在下面的截图示例中，我已经颜色标注了两个进程条目（黄色高亮显示的两行）:</p>
<p><a href="https://blog.xiewenlong.com/2018/12/htop/tag.png" target="_blank" rel="noopener"><img src="https://blog.xiewenlong.com/2018/12/htop/tag.png" alt="img"></a></p>
<h5 id="命令行选项"><a href="#命令行选项" class="headerlink" title="命令行选项"></a>命令行选项</h5><p>除了上面介绍的一些热键，<code>htop</code> 还提供了很有用的命令行选项。下面是其中一部分:</p>
<ul>
<li><code>-s 选项</code> : 按指定的列排序。例如，<code>htop -s PID</code> 命令会按 PID 列的大小排序来显示。</li>
<li><code>-u 选项</code> : 显示指定的用户的进程信息列表。例如，<code>htop -u vagrant</code> 命令会只显示出用户名为 vagrant 的相关进程。</li>
<li><code>-d 选项</code> : 设置刷新的延迟时间。例如，<code>htop -d 100</code> 命令会使输出在 1 秒后才会刷新（参数 -d 的单位是 10 微秒）。</li>
<li><code>-p 选项</code>：只显示给定的PIDs。例如， <code>htop -p PID</code></li>
</ul>
<h5 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h5><p> <strong>shift + m</strong> ： 按照内存大小排序。<br> <strong>shift + h</strong> ： 收缩线程。<br>      <strong>q</strong>       ： 退出</p>
<p><strong>上下键</strong> 或 <strong>PgUP，PgDn</strong> : 选定想要的进程，<br><strong>左右键</strong> 或 <strong>Home，End</strong> : 移动字段，当然也可以直接用鼠标选定进程；<br><strong>Space</strong>  标记/取消标记一个进程（类似 windows 按着 Ctrl 多选一样 ）。命令可以作用于多个进程，例如 “kill”，将应用于所有已标记的进程</p>
<h4 id="参考-1"><a href="#参考-1" class="headerlink" title="参考"></a>参考</h4><blockquote>
<p><a href="https://blog.xiewenlong.com/2018/12/htop/" target="_blank" rel="noopener">https://blog.xiewenlong.com/2018/12/htop/</a></p>
</blockquote>
<h3 id="nvidia-smi使用"><a href="#nvidia-smi使用" class="headerlink" title="nvidia-smi使用"></a>nvidia-smi使用</h3><p><code>nvidia-smi</code> 显示出当前GPU的所有基础信息，监控GPU状态和使用情况。命令判断哪几块GPU空闲</p>
<p><img src="https://i.loli.net/2020/07/22/u1CtU8Xgor45inh.png" alt="image-20200722153957282"></p>
<h4 id="解释相关参数含义"><a href="#解释相关参数含义" class="headerlink" title="解释相关参数含义"></a>解释相关参数含义</h4><p>GPU：本机中的GPU编号</p>
<p>Name：GPU 类型</p>
<p>Persistence-M：</p>
<p>Fan：风扇转速</p>
<p>Temp：温度，单位摄氏度</p>
<p>Perf：表征性能状态，从P0到P12，P0表示最大性能，P12表示状态最小性能</p>
<p>Pwr:Usage/Cap：能耗表示</p>
<p>Bus-Id：涉及GPU总线的相关信息；</p>
<p>Disp.A：Display Active，表示GPU的显示是否初始化</p>
<p>Memory-Usage：显存使用率</p>
<p>Volatile GPU-Util：浮动的GPU利用率</p>
<p>Uncorr. ECC：关于ECC的东西</p>
<p>Compute M.：计算模式</p>
<p>Processes 显示每块GPU上每个进程所使用的显存情况</p>
<h3 id="Tmux使用"><a href="#Tmux使用" class="headerlink" title="Tmux使用"></a>Tmux使用</h3><h4 id="安装tmux-linux"><a href="#安装tmux-linux" class="headerlink" title="安装tmux  -linux"></a>安装tmux  -linux</h4><p><code>sudo apt-get install tmux</code></p>
<h4 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h4><p>tmux采用C/S模型构建，<code>输入tmux命令就相当于开启了一个服务器</code>，此时默认将新建一个会话，然后会话中默认新建一个窗口，窗口中默认新建一个面板。会话、窗口、面板之间的联系如下：</p>
<p><img src="https://i.loli.net/2020/07/27/l4YyAISG8d1Lcp9.png" alt="image-20200726233552371"></p>
<p>一个tmux <code>session</code>（会话）可以包含多个<code>window</code>（窗口），窗口默认充满会话界面，因此这些窗口中可以运行相关性不大的任务。</p>
<p>一个<code>window</code>又可以包含多个<code>pane</code>（面板），窗口下的面板，都处于同一界面下，这些面板适合运行相关性高的任务，以便同时观察到它们的运行情况。</p>
<p>一般在一个<code>session</code>里进行新建<code>windows</code>和<code>pane</code>操作即可</p>
<p>一个session显示如图</p>
<p><img src="https://i.loli.net/2020/07/27/3bWsR57zZTUODF6.png" alt="img"></p>
<h4 id="会话"><a href="#会话" class="headerlink" title="会话"></a>会话</h4><h5 id="新建会话"><a href="#新建会话" class="headerlink" title="新建会话"></a>新建会话</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tmux  <span class="comment"># 新建一个无名称的会话 </span></span><br><span class="line"></span><br><span class="line">tmux new -s s1 <span class="comment"># 新建一个名称为s1的会话</span></span><br></pre></td></tr></table></figure>



<h5 id="断开当前会话"><a href="#断开当前会话" class="headerlink" title="断开当前会话"></a>断开当前会话</h5><p>暂时断开会话，可以进入到原始命令行界面进行操作。也可以<code>Ctrl+B  +d</code>进行断开</p>
<p>操作如 <code>tmux new -s demo</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tmux detach <span class="comment"># 断开当前会话，会话在后台运行</span></span><br></pre></td></tr></table></figure>



<h5 id="进入之前的会话"><a href="#进入之前的会话" class="headerlink" title="进入之前的会话"></a>进入之前的会话</h5><p>断开会话后，想要接着上次留下的现场继续工作，就要使用到tmux的attach命令了，语法为<code>tmux attach-session -t session-name</code>，可简写为<code>tmux a -t session-name</code> 或 <code>tmux a</code>。通常我们使用如下两种方式之一即可：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tmux a <span class="comment"># 默认进入第一个会话</span></span><br><span class="line">tmux a -t demo <span class="comment"># 进入到名称为demo的会话</span></span><br></pre></td></tr></table></figure>



<h5 id="关闭会话"><a href="#关闭会话" class="headerlink" title="关闭会话"></a>关闭会话</h5><p>会话的使命完成后，一定是要关闭的。我们可以使用tmux的kill命令</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tmux kill-session -t demo <span class="comment"># 关闭demo会话 </span></span><br><span class="line"></span><br><span class="line">tmux kill-server <span class="comment"># 关闭服务器，即关闭所有会话</span></span><br><span class="line"></span><br><span class="line">tmux kill-session -a -t s1　　<span class="comment">#关闭除s1外的所有会话</span></span><br></pre></td></tr></table></figure>

<h5 id="查看所有会话"><a href="#查看所有会话" class="headerlink" title="查看所有会话"></a>查看所有会话</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tmux ls <span class="comment"># 查看所有会话，显示会话列表</span></span><br></pre></td></tr></table></figure>

<h5 id="重命名会话S1为S2"><a href="#重命名会话S1为S2" class="headerlink" title="重命名会话S1为S2"></a>重命名会话S1为S2</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tmux rename -t s1 s2</span><br></pre></td></tr></table></figure>



<h4 id="Tmux快捷指令"><a href="#Tmux快捷指令" class="headerlink" title="Tmux快捷指令"></a>Tmux快捷指令</h4><p>tmux的所有指令，都包含同一个前缀，默认为<code>Ctrl+b</code>，输入完前缀过后，控制台激活，命令按键才能生效。</p>
<p>表一：常用指令</p>
<table>
<thead>
<tr>
<th align="center">前缀</th>
<th align="center">指令</th>
<th align="center">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>s</code></td>
<td align="center">显示会话列表用于选择并切换 （上下键选择+回车）</td>
</tr>
<tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>d</code></td>
<td align="center">断开当前会话</td>
</tr>
<tr>
<td align="center">===</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>c</code></td>
<td align="center">新建窗口（windows）</td>
</tr>
<tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>&amp;</code></td>
<td align="center">关闭当前窗口（关闭前需输入<code>y</code> or <code>n</code>确认）</td>
</tr>
<tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>w</code></td>
<td align="center">打开窗口列表，用于且切换窗口</td>
</tr>
<tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>,</code></td>
<td align="center">重命名当前窗口</td>
</tr>
<tr>
<td align="center">===</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>&quot;</code></td>
<td align="center">当前面板上下一分为二，下侧新建面板</td>
</tr>
<tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>%</code></td>
<td align="center">当前面板左右一分为二，右侧新建面板</td>
</tr>
<tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>x</code></td>
<td align="center">关闭当前面板（关闭前需输入<code>y</code> or <code>n</code>确认）</td>
</tr>
<tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>z</code></td>
<td align="center">最大化当前面板，再重复一次按键后恢复正常（v1.8版本新增）</td>
</tr>
<tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>方向键</code></td>
<td align="center">移动光标切换面板</td>
</tr>
<tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>o</code></td>
<td align="center">选择下一面板</td>
</tr>
<tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>t</code></td>
<td align="center">显示时钟</td>
</tr>
</tbody></table>
<p>表二：系统指令</p>
<table>
<thead>
<tr>
<th align="center">前缀</th>
<th align="center">指令</th>
<th align="center">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>?</code></td>
<td align="center">显示快捷键帮助文档</td>
</tr>
<tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>d</code></td>
<td align="center">断开当前会话</td>
</tr>
<tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>D</code></td>
<td align="center">选择要断开的会话</td>
</tr>
<tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>Ctrl+z</code></td>
<td align="center">挂起当前会话</td>
</tr>
<tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>r</code></td>
<td align="center">强制重载当前会话</td>
</tr>
<tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>s</code></td>
<td align="center">显示会话列表用于选择并切换</td>
</tr>
<tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>:</code></td>
<td align="center">进入命令行模式，此时可直接输入<code>ls</code>等命令</td>
</tr>
<tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>[</code></td>
<td align="center">进入复制模式，按<code>q</code>退出</td>
</tr>
<tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>]</code></td>
<td align="center">粘贴复制模式中复制的文本</td>
</tr>
<tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>~</code></td>
<td align="center">列出提示信息缓存</td>
</tr>
</tbody></table>
<p>表三：窗口（window）指令</p>
<table>
<thead>
<tr>
<th align="center">前缀</th>
<th align="center">指令</th>
<th align="center">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>c</code></td>
<td align="center">新建窗口</td>
</tr>
<tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>&amp;</code></td>
<td align="center">关闭当前窗口（关闭前需输入<code>y</code> or <code>n</code>确认）</td>
</tr>
<tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>0~9</code></td>
<td align="center">切换到指定窗口</td>
</tr>
<tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>p</code></td>
<td align="center">切换到上一窗口</td>
</tr>
<tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>n</code></td>
<td align="center">切换到下一窗口</td>
</tr>
<tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>w</code></td>
<td align="center">打开窗口列表，用于且切换窗口</td>
</tr>
<tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>,</code></td>
<td align="center">重命名当前窗口</td>
</tr>
<tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>.</code></td>
<td align="center">修改当前窗口编号（适用于窗口重新排序）</td>
</tr>
<tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>f</code></td>
<td align="center">快速定位到窗口（输入关键字匹配窗口名称）</td>
</tr>
</tbody></table>
<p>表四：面板（pane）指令</p>
<table>
<thead>
<tr>
<th align="center">前缀</th>
<th align="center">指令</th>
<th align="left">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>&quot;</code></td>
<td align="left">当前面板上下一分为二，下侧新建面板</td>
</tr>
<tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>%</code></td>
<td align="left">当前面板左右一分为二，右侧新建面板</td>
</tr>
<tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>x</code></td>
<td align="left">关闭当前面板（关闭前需输入<code>y</code> or <code>n</code>确认）</td>
</tr>
<tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>z</code></td>
<td align="left">最大化当前面板，再重复一次按键后恢复正常（v1.8版本新增）</td>
</tr>
<tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>!</code></td>
<td align="left">将当前面板移动到新的窗口打开（原窗口中存在两个及以上面板有效）</td>
</tr>
<tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>;</code></td>
<td align="left">切换到最后一次使用的面板</td>
</tr>
<tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>q</code></td>
<td align="left">显示面板编号，在编号消失前输入对应的数字可切换到相应的面板</td>
</tr>
<tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>{</code></td>
<td align="left">向前置换当前面板</td>
</tr>
<tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>}</code></td>
<td align="left">向后置换当前面板</td>
</tr>
<tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>Ctrl+o</code></td>
<td align="left">顺时针旋转当前窗口中的所有面板</td>
</tr>
<tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>方向键</code></td>
<td align="left">移动光标切换面板</td>
</tr>
<tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>o</code></td>
<td align="left">选择下一面板</td>
</tr>
<tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>空格键</code></td>
<td align="left">在自带的面板布局中循环切换</td>
</tr>
<tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>Alt+方向键</code></td>
<td align="left">以5个单元格为单位调整当前面板边缘</td>
</tr>
<tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>Ctrl+方向键</code></td>
<td align="left">以1个单元格为单位调整当前面板边缘（Mac下被系统快捷键覆盖）</td>
</tr>
<tr>
<td align="center"><code>Ctrl+b</code></td>
<td align="center"><code>t</code></td>
<td align="left">显示时钟</td>
</tr>
</tbody></table>
<h4 id="tmux用于代码后台运行"><a href="#tmux用于代码后台运行" class="headerlink" title="tmux用于代码后台运行"></a>tmux用于代码后台运行</h4><p>要想使代码后台运行，我用nohup训练模型时重定向到log文件发现日志显示不全，影响实验结果的呈现，而tmux可以解决这个问题。</p>
<p>在进入tmux，新建一个session的时候，实际上tmux在服务器创建了虚拟终端自己连自己。所以用ctrl+B +D退出tmux，并且断掉ssh连接时后台实验是一直在运行的。下次连接ssh后，再打开tmux的session即可，session不会断掉。并且不需要重定向，结果直接显示在屏幕上。</p>
<h4 id="tmux的个性化设置-–-待完成"><a href="#tmux的个性化设置-–-待完成" class="headerlink" title="tmux的个性化设置 – 待完成"></a>tmux的个性化设置 – 待完成</h4><p>比如通过写脚本，在新建session的时候就自动建多个pane，并运行命令</p>
<h4 id="参考-2"><a href="#参考-2" class="headerlink" title="参考"></a>参考</h4><blockquote>
<p><a href="http://louiszhai.github.io/2017/09/30/tmux/" target="_blank" rel="noopener">http://louiszhai.github.io/2017/09/30/tmux/</a></p>
<p><a href="https://harttle.land/2015/11/06/tmux-startup.html" target="_blank" rel="noopener">https://harttle.land/2015/11/06/tmux-startup.html</a></p>
</blockquote>
<h3 id="zsh"><a href="#zsh" class="headerlink" title="zsh"></a>zsh</h3><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sudo apt install git-core zsh</span><br></pre></td></tr></table></figure>



]]></content>
      <categories>
        <category>服务器</category>
      </categories>
      <tags>
        <tag>服务器</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-07-20-git使用指南</title>
    <url>/2020/07/20/2020-07-20-git%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/</url>
    <content><![CDATA[<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">利用git上传本地项目到github</span><br><span class="line"></span><br><span class="line">1.绑定用户</span><br><span class="line"></span><br><span class="line">2.设置ssh key 并为github账号配置ssh key</span><br><span class="line"></span><br><span class="line">3.上传本地项目到github</span><br></pre></td></tr></table></figure>



<p>1.绑定用户 </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ git config --global user.name &quot;OopsAaron&quot; # 注册github时的name</span><br><span class="line">$ git config --global user.email &quot;1574468139@qq.com&quot; # 注册github时的email</span><br></pre></td></tr></table></figure>



<p>2.生成密钥SSH key  </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa -C <span class="string">"1574468139@qq.com"</span></span><br></pre></td></tr></table></figure>

<p>此时，在根目录的.ssh文件中会生成公钥和密钥文件</p>
<p>打开<a href="https://link.zhihu.com/?target=http%3A//github.com/">github</a>，在头像下面点击<code>settings</code>，再点击<code>SSH and GPG keys</code>，新建一个SSH，名字随便。</p>
<p>git bash中输入</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat ~/.ssh/id_rsa.pub <span class="comment"># 复制.ssh里的公钥文件到github中</span></span><br></pre></td></tr></table></figure>

<p>将输出的内容复制到框中，点击确定保存。即建立了ssh连接</p>
<p><img src="https://i.loli.net/2020/07/20/45pTnsvKBbYPDCQ.png" alt="image-20200720163835551"></p>
<p>github上新建一个仓库后，在本地进行初始化本地仓库以及上传文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git init #初始化仓库</span><br><span class="line"></span><br><span class="line">#复制git clone中的ssh地址</span><br><span class="line"></span><br><span class="line">git remote add origin git@github.com:OopsAaron&#x2F;myBlog.git #远程连接仓库 （后部分是ssh地址）</span><br><span class="line"></span><br><span class="line">		（若出现fatal: remote origin already exists. 则执行 git remote rm origin，</span><br><span class="line"> 		 再重新执行git remote add origin git@github.com:OopsAaron&#x2F;myBlog.git）</span><br></pre></td></tr></table></figure>



<p>在本地写md文件以及修改，然后进行提交到github， 并更新到网站上。</p>
<p><strong>3.提交到github</strong></p>
<blockquote>
<p>提交到 github 中的myBlog文件夹中</p>
</blockquote>
<p>将现有的项目添加并上传 (在所在目录下右键git bash)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git add . #添加当前文件夹下的所有文件</span><br><span class="line"></span><br><span class="line">git commit -m &quot;first commit &quot; # 引号内是本次的提交说明 </span><br><span class="line"></span><br><span class="line">git push -u origin master # 提交本地分支到远程分支</span><br><span class="line">		(若出现failed to push som refs to， 则执行git pull origin master，</span><br><span class="line">		将远程服务器github上的master拉下来，再重新push)</span><br></pre></td></tr></table></figure>

<p>刷新github，即可看到上传的文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git clone   https:&#x2F;&#x2F;github.com&#x2F;raymond-zhao&#x2F;cat-mall.git   ..&#x2F;Github&#x2F;cat-mall </span><br><span class="line">#将cat-mall代码克隆到  ..&#x2F;Github&#x2F;cat-mall 中</span><br></pre></td></tr></table></figure>



<p><strong>hexo更新到网站</strong></p>
<p>提交到github中的<code>OopsAaron</code>文件中托管，并更新到网站</p>
<p>在本地编写完md文件，所在目录下右键git bash</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo new new_article  # 新建md文件</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">e.g.</span><br><span class="line">#打开Typora（已添加到环境变量）</span><br><span class="line">hexo new 2020-07-20-tensorflow笔记</span><br><span class="line">#Typora自动跳出新建笔记界面， 这时笔记会自带已预设好的title、description等(因为用的是hexo new 命令)</span><br><span class="line"></span><br><span class="line">注： 1.预设的标签不能空着，不用的话去掉</span><br><span class="line"> 	2. 在每一个title冒号后面要空格，再添加信息 ，不然会报错，示例如下</span><br></pre></td></tr></table></figure>



<p><img src="https://i.loli.net/2020/07/21/AeMFUSKwfWGkjpr.png" alt="image-20200721232830843"></p>
<p>一般命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo clean # </span><br><span class="line"></span><br><span class="line">hexo g  # &#x3D;&#x3D;hexo generate 生成静态网页至public目录 </span><br><span class="line"></span><br><span class="line">hexo s # &#x3D;&#x3D;hexo server 可以在本地预览效果 http:&#x2F;&#x2F;localhost:4000&#x2F;   ctrl+C 退出预览</span><br><span class="line"></span><br><span class="line">hexo d # &#x3D;&#x3D;hexo deploy 部署到github上，并可以看到发布的文章</span><br><span class="line"></span><br><span class="line">hexo help # 查看帮助</span><br><span class="line"></span><br><span class="line">hexo version # 查看版本</span><br></pre></td></tr></table></figure>



<p>组合命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo s -g  # 生成静态网页并本地预览</span><br><span class="line"></span><br><span class="line">hexo d -g  # 生成并上传</span><br></pre></td></tr></table></figure>





<p><strong>hexo高级操作</strong></p>
<p>hexo的根目录结构如下所示</p>
<p><img src="https://i.loli.net/2020/07/22/5EWcOhQ8MBCem4s.png" alt="image-20200722181143933"></p>
<h3 id="config-yml"><a href="#config-yml" class="headerlink" title="_config.yml"></a>_config.yml</h3><p>网站配置信息，也就是本文所称的<strong>站点配置文件</strong>，可以在此配置大部分的参数。</p>
<h3 id="scaffolds"><a href="#scaffolds" class="headerlink" title="scaffolds"></a>scaffolds</h3><p>模版文件夹。新建文章时，Hexo 会根据 scaffold 来建立文件。</p>
<p>Hexo的模板是指新建的markdown文件中默认填充的内容。例如，在使用<code>hexo new 文章名</code>时，默认生成的md文件会包含如下内容：</p>
<p><img src="https://i.loli.net/2020/07/22/H6JKBeIwhcTkN5z.png" alt="image-20200722182540918"></p>
<p>默认内容就在scaffold/post.md中保存</p>
<p>假如对每篇博客我都需要添加分类<code>categories</code>，每次都手动添加太麻烦，我希望每次默认生成都有<code>categories:</code>，那么就可以在scaffold/post.md中添加categories</p>
<p>保存后，每次新建一篇文章时都会包含post.md中的内容。</p>
<p>当然，你也可以在scaffolds文件夹下创建自己的博客模板，我创建一个名为<code>blog</code>的模板：</p>
<p><img src="https://i.loli.net/2020/07/22/oLMuDQOmg6xaBYU.png" alt="image-20200722182806115"></p>
<p>通过如下命令调用我创建的blog模板新建文章，在<code>_posts</code>中生成md文件，并且是以blog.md为模板的</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo new blog 2020-07-22-测试blog</span><br></pre></td></tr></table></figure>



<h3 id="public"><a href="#public" class="headerlink" title="public"></a>public</h3><p>该文件夹中的内容将被最终push到github仓库中。</p>
<h3 id="source"><a href="#source" class="headerlink" title="source"></a>source</h3><p>资源文件夹是存放用户资源的地方。除<code>_posts</code> 文件夹之外，开头命名为 _ (下划线)的文件 / 文件夹和隐藏的文件将会被忽略。Markdown 和 HTML 文件会被解析并放到 public 文件夹，而其他文件（如刚刚生成的about文件夹）会被拷贝到 public 文件夹。</p>
<h4 id="为github仓库添加readme"><a href="#为github仓库添加readme" class="headerlink" title="为github仓库添加readme"></a>为github仓库添加readme</h4><p>既然 source 文件夹中的内容将全部被推送到 public 文件夹，public 文件夹中的内容又将被最终push到github仓库中，那么如果我们想要为github仓库添加readme.md，只要在 source 文件夹中创建就好了：</p>
<p>部署到github，就有readme了，但我们发现，README.md已经被解析为README.html，显示的全是html代码，并不是我们想要的文档格式的内容</p>
<p>为了解决这个问题，我们回到source文件夹，将<code>README.md</code>重命名为<code>README.MDOWN</code>，再部署到github即可</p>
<p>source文件夹中，.md会被解析为html，并放到 public 文件夹被push到github，但.MDWN不会被解析。</p>
<h3 id="themes"><a href="#themes" class="headerlink" title="themes"></a>themes</h3><p>主题文件夹，下载的主题都保存在此文件夹下。Hexo 会根据主题来生成静态页面。</p>
<p>参考：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Hexo+Github博客搭建：  https:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;p&#x2F;35668237</span><br><span class="line">git上传文件：https:&#x2F;&#x2F;blog.csdn.net&#x2F;sinat_20177327&#x2F;article&#x2F;details&#x2F;76062030</span><br></pre></td></tr></table></figure>

]]></content>
  </entry>
  <entry>
    <title>2020-08-08-vim常见操作</title>
    <url>/2020/08/08/2020-08-08-vim%E5%B8%B8%E8%A7%81%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<h3 id="常规操作"><a href="#常规操作" class="headerlink" title="常规操作"></a>常规操作</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">vim a.txt  <span class="comment"># 创建a.txt文件并进入编辑状态 。 如果a.txt 已经存在，则直接进入编辑状态</span></span><br><span class="line"></span><br><span class="line"><span class="number">1.</span> 按下i键，下端显示 –INSERT–。可以进行插入，输入文本 </span><br><span class="line"></span><br><span class="line"><span class="number">2.</span> 输入了之后 按Esc键退出编辑状态</span><br><span class="line"></span><br><span class="line"><span class="number">3.</span> 键入 :wq! 强制保存文件并退出  <span class="comment"># !是强制执行，注：有些文件设置了只读，一般不是修改文件的，但是如果你是`							文件的owner或者root的话，通过wq!还是能保存文件退出。:wq不可以</span></span><br><span class="line"></span><br><span class="line">   :w 在编辑的过程中保存文件,相当于word中的ctrl+s    </span><br><span class="line"></span><br><span class="line">   :wq 保存文件并退出 <span class="comment">#一般使用这个命令退出</span></span><br></pre></td></tr></table></figure>



<blockquote>
<p>注：以<strong><code>:</code></strong>和<strong><code>/</code></strong>开头的命令都有历史纪录，可以首先键入:或/然后按<strong>上下箭头</strong>来选择某个历史命令</p>
</blockquote>
<h3 id="Vim模式"><a href="#Vim模式" class="headerlink" title="Vim模式"></a>Vim模式</h3><p>(都是在英文输入环境下操作)</p>
<ul>
<li><strong>Normal</strong> 模式：进入Vim后的一般模式。</li>
<li><strong>Insert</strong> 模式：按下<code>i</code>键后进入插入模式，可以修改文档。</li>
<li><strong>Visual</strong> 模式：按下<code>v</code>键后进入选择模式，可以选择文档内容。</li>
</ul>
<h3 id="Vim打开和切换文件"><a href="#Vim打开和切换文件" class="headerlink" title="Vim打开和切换文件"></a>Vim打开和切换文件</h3><ul>
<li><code>:ls</code>显示打开的文件，可以使用<code>:bn</code>在文件间切换( n也可以换成<code>:ls</code>里给出的文件序号 )。</li>
<li>在终端<code>vim -o file1 file2 ...</code>可以打开多个文件(横向分隔屏幕)。</li>
<li>终端<code>vim -O file1 file2 ...</code>可以打开多个文件(纵向分隔屏幕)。 :star:</li>
<li><code>Ctrl</code>+<code>w</code>+<code>w</code>在窗口间切换光标，第二个<code>w</code>也可以用<code>h、j、k、l</code>来光标代表移动方向。</li>
</ul>
<h3 id="Vim退出"><a href="#Vim退出" class="headerlink" title="Vim退出"></a>Vim退出</h3><ul>
<li><p><code>:q</code>：退出。</p>
</li>
<li><p><code>:q!</code>：强制退出，放弃所有修改。</p>
</li>
<li><p><code>:wq</code>：保存修改并退出。:star:</p>
</li>
</ul>
<h3 id="常用快捷键"><a href="#常用快捷键" class="headerlink" title="常用快捷键"></a>常用快捷键</h3><ul>
<li><p><code>gg</code>到文档首行，<code>G</code>（shift+g）到文档结尾。</p>
</li>
<li><p><code>pageUp</code>下一页，<code>pageDown</code>上一页。</p>
</li>
<li><p><code>ctrl + d</code>    向下翻半页(down)， <code>ctrl + u</code>    向上翻半页(up)  :star:</p>
</li>
<li><p><code>:98</code>跳转到第98行。</p>
</li>
<li><p><code>H</code>将光标移动到屏幕首行，<code>M</code>将光标移动到屏幕中间行，<code>L</code>将光标移动到屏幕最后一行。</p>
</li>
<li><p><code>q:</code>显示<strong>命令行历史记录</strong>（显示开头为:的历史命令行）窗口，可以选择命令行执行。若是<code>q/</code>,则会显示开头为/的历史命令行 </p>
</li>
<li><p><code>u</code> 撤销  (undo) :star:</p>
</li>
<li><p><code>w</code>    下一个单词     word</p>
<p><code>b</code>     前一个单词     behind</p>
</li>
<li><p><code>:set nu</code>    显示行号   (number )      　</p>
</li>
<li><p><code>:set nonu</code>   隐藏行号   ( number)</p>
</li>
<li><p><code>:5,10d</code>    //回车后，第5~10行被删除</p>
</li>
<li><p><code>:5,$d</code>    //回车后，第5~最后一行被删除</p>
</li>
</ul>
<ul>
<li></li>
</ul>
<h3 id="复制粘贴"><a href="#复制粘贴" class="headerlink" title="复制粘贴"></a>复制粘贴</h3><ul>
<li><p>在<strong>Visual</strong>模式下选择文档内容后按<code>y</code>键，复制被选择内容。主要用于<strong>多行文字</strong>（复制完之后vim自动退出Visual模式）</p>
</li>
<li><p>在<strong>Visual</strong>模式下选择文档内容后按<code>d</code>删除</p>
</li>
<li><p>按<code>p</code>键粘贴，注意粘贴从<strong>紧跟光标后的那个字符</strong>之后才开始。（不需要进入Visual模式）</p>
</li>
<li><p><code>yy</code>复制当前行，<code>dd</code>删除(剪贴)当前行。 用于<strong>一行文字</strong>、</p>
</li>
</ul>
<p>如果在vim外的其它文件里复制内容到vim里，则无法使用<code>p</code>进行粘贴，此时右键粘贴即可（无需进入inset模式）</p>
<h3 id="查找"><a href="#查找" class="headerlink" title="查找"></a>查找</h3><ul>
<li>在<strong>Normal</strong>模式下，按<code>/</code>进入查找模式，输入<code>/word</code>后回车，高亮显示所有文档<code>word</code>，按<code>n</code>跳到下一个<code>word</code>,按<code>N</code>跳到上一个。</li>
<li>若输入<code>/word\c</code>代表大小写不敏感查找，<code>\C</code>代表大小写敏感。</li>
<li>在<strong>Normal</strong>模式下按<code>q</code>+<code>/</code>显示<strong>查找历史记录</strong>窗口。</li>
<li>如果一个词很长，键入麻烦，可以将光标移动到该词上，按<code>*</code>键即可以该单词进行搜索，相当于/搜索。</li>
</ul>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><blockquote>
<p>vim的常用操作  <a href="https://www.cnblogs.com/doseoer/p/6241443.html" target="_blank" rel="noopener">https://www.cnblogs.com/doseoer/p/6241443.html</a></p>
</blockquote>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>vim,linux</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-08-08-hexo新建page</title>
    <url>/2020/08/08/2020-08-08-hexo%E6%96%B0%E5%BB%BApage/</url>
    <content><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>在博客中需要一些个性化设置，添加一些page等 ，记录下我的操作</p>
<h3 id="添加page-界面"><a href="#添加page-界面" class="headerlink" title="添加page 界面"></a>添加page 界面</h3><p>我想要添加一个“一句话感想”的page，于是可以这样操作</p>
<p>step 1.hexo新建新的page界面</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">hexo new page onesentence  <span class="comment"># onesentence 是新建page的名称 （最好是英文名）</span></span><br></pre></td></tr></table></figure>

<p>这时候在博客的source文件夹里会有一个onesentence的文件夹，并且里面生成了一个index.md文件，用于写一句话感想的内容</p>
<p>step 2.在主题的配置文件 _config.yml 文件中的 menu 中进行匹配，如下图，添加一个onesentence项，<code>/onesentence</code>表示挂接到上述的新建文件夹里，</p>
<p>在这里也可以设置图标，在fontawesome网站里找，我找了一个保龄球:bowling:的图标，和page主题没啥联系，就是看着顺眼 :laughing:</p>
<p>此时<code>hexo s -g</code> 就可以看到已经有了这个界面，不过是英文的文件名，所以此时还要设置一下此文件名的中文名映射</p>
<p><img src="https://i.loli.net/2020/08/08/jiUOEvuzWnT7Kmp.png" alt="image-20200808192748174"></p>
<p>step 3.   打开<strong>themes\next\languages</strong>，我用的是zh-CN，打开此文件，在menu下添加<code>onesentence: 一句话</code>，即可完成中文映射，</p>
<p>此时 hexo s -g ,就可以在本地服务器的侧边栏部分看到新添加的“一句话”page</p>
<p><img src="https://i.loli.net/2020/08/08/cCymB2KXMh1OPN5.png" alt="image-20200808193652117"></p>
<img src="https://i.loli.net/2020/08/08/ZrQkcR8IPspHMdi.png" alt="image-20200808194045722" style="zoom: 67%;" />



<p>step 4.  编辑“一句话”页面下的md文件，部署就能看到内容</p>
]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-08-08-next缺少custom.styl的问题</title>
    <url>/2020/08/08/2020-08-08-%E7%BC%BA%E5%B0%91custom-styl%E7%9A%84%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>在 next7.x 版本中没有custom.styl文件。如果我们想要在博客中添加自己的css样式，可以在此文件中添加，下面介绍一下</p>
<h3 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h3><p>step1 ：添加custom.styl文件</p>
<p>文件路径：<code>~\themes\next\source\css</code> ,添加<code>_custom</code>文件夹。然后在<code>_custom</code>中创建<code>custom.styl</code>文件。我们自己的样式就可以在此文件中添加</p>
<p>step2： 添加引用</p>
<p>在<code>~\themes\next\source\css</code>中的<code>main.styl</code>文件末尾加入引用即可</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;My Layer</span><br><span class="line">@import &quot;_custom&#x2F;custom.styl&quot;;</span><br></pre></td></tr></table></figure>



<p>step3： 添加样式</p>
<p>用vscode打开<code>custom.styl</code>，博客背景以及前页的不透明度等等，就可以更换样式了。</p>
<p>对于网页的组件，F12打开调试界面，就可以知道每个组件的名称等信息，便于更改样式</p>
]]></content>
      <categories>
        <category>next</category>
      </categories>
      <tags>
        <tag>排除故障</tag>
        <tag>next</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-08-07-transformerXL解读</title>
    <url>/2020/08/07/2020-08-07-transformerXL%E8%A7%A3%E8%AF%BB/</url>
    <content><![CDATA[<p> Transformer最大的问题：在语言建模时的设置受到固定长度上下文的限制。</p>
<p>本文提出的Transformer-XL，使学习不再仅仅依赖于定长，且不破坏时间的相关性。</p>
<p>Transformer-XL包含segment-level 循环机制和positional编码框架。不仅可以捕捉长时依赖，还可以解决上下文断片问题 fragmentation problem。可以学到比RNNs长80%的依赖，比vanilla Transformers长450%。在长短序列上都取得了更好的结果。与vanilla Transformer相比，Transformer-XL的另一个优势是它可以被用于单词级和字符级的语言建模。</p>
]]></content>
  </entry>
  <entry>
    <title>2020-08-06-pytorch函数之nn.Linear</title>
    <url>/2020/08/06/2020-08-06-pytorch%E5%87%BD%E6%95%B0%E4%B9%8BLinear/</url>
    <content><![CDATA[<h1 id=""><a href="#" class="headerlink" title=""></a></h1><p>class torch.nn.Linear（in_features，out_features，bias = True ）</p>
<p>对传入数据应用线性变换：y = A x+ b</p>
<p>参数：</p>
<p>in_features - 每个输入样本的大小</p>
<p>out_features - 每个输出样本的大小</p>
<p>bias - 如果设置为False，则图层不会学习附加偏差。默认值：True</p>
<p>代码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">m &#x3D; nn.Linear(20, 30)</span><br><span class="line"></span><br><span class="line">input &#x3D; autograd.Variable(torch.randn(128, 20))</span><br><span class="line"></span><br><span class="line">output &#x3D; m(input)</span><br><span class="line"></span><br><span class="line">print(output.size())</span><br></pre></td></tr></table></figure>



<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">torch.Size([128, 30])</span><br></pre></td></tr></table></figure>

<p>分析:</p>
<p>output.size()=矩阵size(128,20)*矩阵size（20,30）=(128,30)</p>
]]></content>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-08-05-BPE算法</title>
    <url>/2020/08/05/2020-08-05-%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<h3 id="总说"><a href="#总说" class="headerlink" title="总说"></a><strong>总说</strong></h3><p>BPE，（byte pair encoder）字节对编码，也可以叫做digram coding双字母组合编码，<code>主要目的是为了数据压缩</code>，算法描述为<code>字符串里频率最常见的一对字符被一个没有在这个字符中出现的字符代替的层层迭代过程</code>。具体在下面描述。</p>
<h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><ol>
<li>准备足够大的训练语料</li>
<li>确定期望的<strong>subword词表大小</strong></li>
<li>将单词拆分为字符序列并在<strong>末尾添加后缀“ &lt;/ w&gt;”</strong>，统计单词频率。 本阶段的subword的粒度是字符。 例如，“ low”的频率为5，那么我们将其改写为“ l o w &lt;/ w&gt;”：5</li>
<li>统计每一个连续字节对的出现频率，选择最高频者合并成新的subword</li>
<li>重复第4步直到达到第2步设定的subword词表大小或下一个最高频的字节对出现频率为1</li>
</ol>
<p>停止符”</w>“的意义在于表示subword是词后缀。举例来说：”st”字词不加”</w>“可以出现在词首如”st ar”，加了”</w>“表明改字词位于词尾，如”wide st</w>“，二者意义截然不同。</p>
<p>每次合并后词表可能出现3种变化：</p>
<ul>
<li>+1，表明加入合并后的新字词，同时原来的2个子词还保留（2个字词不是完全同时连续出现）</li>
<li>+0，表明加入合并后的新字词，同时原来的2个子词中一个保留，一个被消解（一个字词完全随着另一个字词的出现而紧跟着出现）</li>
<li>-1，表明加入合并后的新字词，同时原来的2个子词都被消解（2个字词同时连续出现）</li>
</ul>
<p>实际上，随着合并的次数增加，词表大小通常先增加后减小。</p>
<h4 id="例子1"><a href="#例子1" class="headerlink" title="例子1"></a><strong>例子1</strong></h4><p>输入：</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">&#123;'l o w &lt;/w&gt;': 5, 'l o w e r &lt;/w&gt;': 2, 'n e w e s t &lt;/w&gt;': 6, 'w i d e s t &lt;/w&gt;': 3&#125;</span><br></pre></td></tr></table></figure>

<p>Iter 1, 最高频连续字节对”e”和”s”出现了6+3=9次，合并成”es”。输出：</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">&#123;'l o w &lt;/w&gt;': 5, 'l o w e r &lt;/w&gt;': 2, 'n e w es t &lt;/w&gt;': 6, 'w i d es t &lt;/w&gt;': 3&#125;</span><br></pre></td></tr></table></figure>

<p>Iter 2, 最高频连续字节对”es”和”t”出现了6+3=9次, 合并成”est”。输出：</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">&#123;'l o w &lt;/w&gt;': 5, 'l o w e r &lt;/w&gt;': 2, 'n e w est &lt;/w&gt;': 6, 'w i d est &lt;/w&gt;': 3&#125;</span><br></pre></td></tr></table></figure>

<p>Iter 3, 以此类推，最高频连续字节对为”est”和”</w>“ 输出：</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">&#123;'l o w &lt;/w&gt;': 5, 'l o w e r &lt;/w&gt;': 2, 'n e w est&lt;/w&gt;': 6, 'w i d est&lt;/w&gt;': 3&#125;</span><br></pre></td></tr></table></figure>

<p>……</p>
<p>Iter n, 继续迭代<strong>直到达到预设的subword词表大小或下一个最高频的字节对出现频率为1</strong>。</p>
<h3 id="BPE实现"><a href="#BPE实现" class="headerlink" title="BPE实现"></a>BPE实现</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re, collections</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_stats</span><span class="params">(vocab)</span>:</span></span><br><span class="line">    pairs = collections.defaultdict(int)</span><br><span class="line">    <span class="keyword">for</span> word, freq <span class="keyword">in</span> vocab.items():</span><br><span class="line">        symbols = word.split()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(symbols)<span class="number">-1</span>):</span><br><span class="line">            pairs[symbols[i],symbols[i+<span class="number">1</span>]] += freq</span><br><span class="line">    <span class="keyword">return</span> pairs</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge_vocab</span><span class="params">(pair, v_in)</span>:</span></span><br><span class="line">    v_out = &#123;&#125;</span><br><span class="line">    bigram = re.escape(<span class="string">' '</span>.join(pair))</span><br><span class="line">    p = re.compile(<span class="string">r'(?&lt;!\S)'</span> + bigram + <span class="string">r'(?!\S)'</span>)</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> v_in:</span><br><span class="line">        w_out = p.sub(<span class="string">''</span>.join(pair), word)</span><br><span class="line">        v_out[w_out] = v_in[word]</span><br><span class="line">    <span class="keyword">return</span> v_out</span><br><span class="line"></span><br><span class="line">vocab = &#123;<span class="string">'l o w &lt;/w&gt;'</span>: <span class="number">5</span>, <span class="string">'l o w e r &lt;/w&gt;'</span>: <span class="number">2</span>, <span class="string">'n e w e s t &lt;/w&gt;'</span>: <span class="number">6</span>, <span class="string">'w i d e s t &lt;/w&gt;'</span>: <span class="number">3</span>&#125;</span><br><span class="line">num_merges = <span class="number">1000</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_merges):</span><br><span class="line">    pairs = get_stats(vocab)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> pairs:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    best = max(pairs, key=pairs.get)</span><br><span class="line">    vocab = merge_vocab(best, vocab)</span><br><span class="line">    print(best)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print output</span></span><br><span class="line"><span class="comment"># ('e', 's')</span></span><br><span class="line"><span class="comment"># ('es', 't')</span></span><br><span class="line"><span class="comment"># ('est', '&lt;/w&gt;')</span></span><br><span class="line"><span class="comment"># ('l', 'o')</span></span><br><span class="line"><span class="comment"># ('lo', 'w')</span></span><br><span class="line"><span class="comment"># ('n', 'e')</span></span><br><span class="line"><span class="comment"># ('ne', 'w')</span></span><br><span class="line"><span class="comment"># ('new', 'est&lt;/w&gt;')</span></span><br><span class="line"><span class="comment"># ('low', '&lt;/w&gt;')</span></span><br><span class="line"><span class="comment"># ('w', 'i')</span></span><br><span class="line"><span class="comment"># ('wi', 'd')</span></span><br><span class="line"><span class="comment"># ('wid', 'est&lt;/w&gt;')</span></span><br><span class="line"><span class="comment"># ('low', 'e')</span></span><br><span class="line"><span class="comment"># ('lowe', 'r')</span></span><br><span class="line"><span class="comment"># ('lower', '&lt;/w&gt;')</span></span><br></pre></td></tr></table></figure>



<h3 id="编码和解码"><a href="#编码和解码" class="headerlink" title="编码和解码"></a>编码和解码</h3><ul>
<li><h4 id="编码"><a href="#编码" class="headerlink" title="编码"></a>编码</h4></li>
</ul>
<p>在之前的算法中，我们已经得到了<strong>subword词表</strong>，<strong>对该词表按照子词长度由大到小排序</strong>。编码时，<strong>对于每个单词，遍历排好序的子词词表寻找是否有token是当前单词的子字符串，如果有，则该token是表示单词的tokens之一</strong>。</p>
<p>我们从最长的token迭代到最短的token，尝试将每个单词中的子字符串替换为token。 最终，我们将迭代所有tokens，并将所有子字符串替换为tokens。 如果仍然有子字符串没被替换但所有token都已迭代完毕，则将剩余的子词替换为特殊token，如<unk>。</p>
<h4 id="例子2"><a href="#例子2" class="headerlink" title="例子2"></a>例子2</h4><p>用得到subword词表去表示含有多个单词的句子</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 给定单词序列</span></span><br><span class="line">[“the&lt;/w&gt;”, “highest&lt;/w&gt;”, “mountain&lt;/w&gt;”]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设已有排好序的subword词表</span></span><br><span class="line">[“errrr&lt;/w&gt;”, “tain&lt;/w&gt;”, “moun”, “est&lt;/w&gt;”, “high”, “the&lt;/w&gt;”, “a&lt;/w&gt;”]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 迭代结果</span></span><br><span class="line">"the&lt;/w&gt;" -&gt; ["the&lt;/w&gt;"]</span><br><span class="line">"highest&lt;/w&gt;" -&gt; ["high", "est&lt;/w&gt;"]</span><br><span class="line">"mountain&lt;/w&gt;" -&gt; ["moun", "tain&lt;/w&gt;"]</span><br></pre></td></tr></table></figure>

<p>编码的计算量很大。 在实践中，我们可以pre-tokenize所有单词，并在词典中保存单词tokenize的结果。 如果我们看到字典中不存在的未知单词。 我们应用上述编码方法对单词进行tokenize，然后将新单词的tokenization添加到字典中备用。</p>
<ul>
<li><h4 id="解码"><a href="#解码" class="headerlink" title="解码"></a>解码</h4></li>
</ul>
<p><strong>将所有的tokens拼在一起</strong>。</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 编码序列</span></span><br><span class="line">[“the&lt;/w&gt;”, “high”, “est&lt;/w&gt;”, “moun”, “tain&lt;/w&gt;”]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解码序列</span></span><br><span class="line">“the&lt;/w&gt; highest&lt;/w&gt; mountain&lt;/w&gt;”</span><br></pre></td></tr></table></figure>





<h4 id="例子3"><a href="#例子3" class="headerlink" title="例子3"></a>例子3</h4><p>比如我们想编码：</p>
<p>aaabdaaabac</p>
<p>我们会发现这里的aa出现的词数最高（我们这里只看两个字符的频率），那么用这里没有的字符Z来替代aa：</p>
<p>ZabdZabac</p>
<p>Z=aa</p>
<p>此时，又发现ab出现的频率最高，那么同样的，Y来代替ab：</p>
<p>ZYdZYac</p>
<p>Y=ab</p>
<p>Z=aa</p>
<p>同样的，ZY出现的频率大，我们用X来替代ZY：</p>
<p>XdXac</p>
<p>X=ZY</p>
<p>Y=ab</p>
<p>Z=aa</p>
<p>最后，连续两个字符的频率都为1了，也就结束了。就是这么简单。</p>
<p>解码的时候，就按照相反的顺序更新替换即可。</p>
]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-08-04-博客优化以及问题解决</title>
    <url>/2020/08/04/2020-08-04-%E5%8D%9A%E5%AE%A2%E4%BC%98%E5%8C%96%E4%BB%A5%E5%8F%8A%E8%A7%A3%E5%86%B3%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h3 id="修改Git-Bash的默认打开工作路径"><a href="#修改Git-Bash的默认打开工作路径" class="headerlink" title="修改Git Bash的默认打开工作路径"></a>修改Git Bash的默认打开工作路径</h3><p>我每次想在我的博客文件夹里进入git bash，必须要打开文件夹才能进入，操作繁琐，于是在桌面建立git bash 快捷方式，并将git bash 的默认打开路径更改为我的博客文件夹下，这样点击图标，即能进入本地git仓库</p>
<p>1.找到git bash，右键属性，可以看到目标栏及起始位置栏。</p>
<img src="https://i.loli.net/2020/08/04/tPL1uzsVApn5FvC.png" alt="img" style="zoom: 80%;" />



<p>将目标栏中的 –cd-to-home 去掉；将起始位置中填写为本地git仓库的路径，即可完成操作。如下图所示，博客文件夹位置在<code>E:\myBlog</code></p>
<img src="https://i.loli.net/2020/08/04/Vw7Kg3U2OIZQ6R9.png" alt="image-20200804181206322" style="zoom: 50%;" />



<p>注： 若在文件夹里进入 git bash，则然后按下<code>shift+F10</code> （激活右键菜单栏），再按<code>s</code>跳转到git bash，最后按下<code>enter</code>即可</p>
<h3 id="博客打开网站和更新不完全"><a href="#博客打开网站和更新不完全" class="headerlink" title="博客打开网站和更新不完全"></a>博客打开网站和更新不完全</h3><p>在这几天在本地文件夹更新完配置文件对博客进行个性化设置时，使用<code>localhost:4000</code>访问本地blog可以正常显示更改后的样式，但是在登录网站域名就会出现不一致的现象，有时会响应速度慢，延时高，甚至连接超时。</p>
<p>在整个过程中一直没发现问题，因为本地localhost和网站不一致就不能理解。后来才发现，我的hexo命令写错了。本应该是hexo clean ，我错写为hexo clear，导致不能轻触缓存，所以在网站中不能及时更新显示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">hexo clean <span class="comment"># 清除缓存，网页正常情况下可以忽略此命令</span></span><br></pre></td></tr></table></figure>



<h3 id="博客无法连接"><a href="#博客无法连接" class="headerlink" title="博客无法连接"></a>博客无法连接</h3><p>本地服务器可以正常显示，但是博客连接不上</p>
<p>解决：</p>
<ol>
<li>博客正在加载， 等一段时间刷新</li>
<li>如果还是不行，则清理chrome的浏览记录、cookie即可</li>
</ol>
]]></content>
      <categories>
        <category>--</category>
      </categories>
      <tags>
        <tag>--</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-08-01-teacher-foring以及解决</title>
    <url>/2020/08/01/2020-08-01-teacher-foring%E4%BB%A5%E5%8F%8A%E8%A7%A3%E5%86%B3/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>2020-07-30-pytorch使用手册</title>
    <url>/2020/07/30/2020-07-30-pytorch%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/</url>
    <content><![CDATA[<h3 id="python中对于对象的拷贝分为浅拷贝-copy-和深拷贝-deepcopy-两种方式。其中浅拷贝由“-”完成。而深拷贝由copy模块中deepcopy-函数担任。"><a href="#python中对于对象的拷贝分为浅拷贝-copy-和深拷贝-deepcopy-两种方式。其中浅拷贝由“-”完成。而深拷贝由copy模块中deepcopy-函数担任。" class="headerlink" title="python中对于对象的拷贝分为浅拷贝(copy)和深拷贝(deepcopy)两种方式。其中浅拷贝由“=”完成。而深拷贝由copy模块中deepcopy()函数担任。"></a><strong>python中对于对象的拷贝分为浅拷贝(copy)和深拷贝(deepcopy)两种方式。其中浅拷贝由“=”完成。而深拷贝由copy模块中deepcopy()函数担任。</strong></h3><h3 id="浅拷贝和深拷贝的区别是：浅拷贝只是将原对象在内存中引用地址拷贝过来了。让新的对象指向这个地址。而深拷贝是将这个对象的所有内容遍历拷贝过来了，相当于跟原来没关系了，所以如果你这时候修改原来对象的值跟他没关系了，不会随之更改。"><a href="#浅拷贝和深拷贝的区别是：浅拷贝只是将原对象在内存中引用地址拷贝过来了。让新的对象指向这个地址。而深拷贝是将这个对象的所有内容遍历拷贝过来了，相当于跟原来没关系了，所以如果你这时候修改原来对象的值跟他没关系了，不会随之更改。" class="headerlink" title="\浅拷贝和深拷贝的区别是：浅拷贝只是将原对象在内存中引用地址拷贝过来了。让新的对象指向这个地址。而深拷贝是将这个对象的所有内容遍历拷贝过来了，相当于跟原来没关系了，所以如果你这时候修改原来对象的值跟他没关系了，不会随之更改。**"></a><strong><em>\</em>浅拷贝和深拷贝的区别是：浅拷贝只是将原对象在内存中引用地址拷贝过来了。让新的对象指向这个地址。而深拷贝是将这个对象的所有内容遍历拷贝过来了，相当于跟原来没关系了，所以如果你这时候修改原来对象的值跟他没关系了，不会随之更改。**</strong></h3><h3 id="1-浅拷贝”-”的使用"><a href="#1-浅拷贝”-”的使用" class="headerlink" title="1.浅拷贝”=”的使用"></a><strong>1.浅拷贝”=”的使用</strong></h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#1.使用=复制不可变对象的值，以及复制以后修改其值后的变化。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">val1 = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">val2 = val1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(<span class="string">"val1 is :&#123;0&#125;,val2 is :&#123;1&#125;"</span>.format(val1,val2))<span class="comment">#val1 is :1000,val2 is :1000</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(id(val1),id(val2))  <span class="comment">#34052192 34052192</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#这时候修改val1的值，尽管val2指向val1.但因为val1是不可变类型，修改其值，会重新给新值分配内存，然后指向他。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">val1 += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(val1,id(val1),val2,id(val2)) <span class="comment">#1001 10131616 1000 10131568  值不一样，内存地址也不一样了</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#1.使用=复制可变对象的值，以及复制以后修改其值后的变化。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ls1 =[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ls2 = ls1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(id(ls1),id(ls2)) <span class="comment">#43702792 43702792 直接使用=复制变量，内存地址一样，值也一样。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(ls1,ls2) <span class="comment">#[1, 2, 3, 4] [1, 2, 3, 4]直接使用=复制变量，内存地址一样，值也一样。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#这时候修改可变对的值,因为其值可变，所以只需要在原内存地址上修改即可。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ls1.append(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(id(ls1),id(ls2)) <span class="comment">#可变对象修改其值，内存引用不变</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(ls1,ls2) <span class="comment">#[1, 2, 3, 4, 5] [1, 2, 3, 4, 5] 因为两个变量的内存指向一样，所以值也一样。</span></span><br></pre></td></tr></table></figure>

<h3 id="2-深拷贝：copy-deepcopy-函数"><a href="#2-深拷贝：copy-deepcopy-函数" class="headerlink" title="2.深拷贝：copy.deepcopy()函数"></a>2.深拷贝：copy.deepcopy()函数</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#1.使用copy.deepcopy()拷贝不可变对象的值，以及复制以后修改其值后的变化。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">val1 = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">val2 = copy.deepcopy(val1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(<span class="string">"val1 is :&#123;0&#125;,val2 is :&#123;1&#125;"</span>.format(val1,val2))<span class="comment">#val1 is :1000,val2 is :1000</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(id(val1),id(val2))  <span class="comment">#33717408 33717408 对于不可变对象，深度拷贝内存地址没有修改。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">val1 += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(val1,id(val1),val2,id(val2)) <span class="comment">#1001 33717904 1000 33717408</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#1.使用copy.deepcopy()复制可变对象的值，以及复制以后修改其值后的变化。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ls1 =[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ls2 = copy.deepcopy(ls1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(id(ls1),id(ls2)) <span class="comment">#34628472 34628712 注意对于可变对象深度拷贝后内存地址都修改了。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(ls1,ls2) <span class="comment">#[1, 2, 3, 4] [1, 2, 3, 4]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ls1.append(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(id(ls1),id(ls2)) <span class="comment">#34628472 34628712</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(ls1,ls2) <span class="comment">#[1, 2, 3, 4, 5] [1, 2, 3, 4] #注意这个时候ls2的值没有随着ls1修改。</span></span><br></pre></td></tr></table></figure>

<h3 id="总结：其实对于浅拷贝和深拷贝来说，如果拷贝对象都是不可变对象的话，那么两者效果是一样的。如果是可变对象的话，“-”拷贝的方式，只是拷贝了内存中的地址引用，两个对象的地址引用一样，所以两个对象的值会随着一方的修改而修改。而对于deepcopy-来说，如果是可变对象的话，那么拷贝内容后新对象的内存地址也会重新分配，跟原来的内存地址不一样了。所以两者任意修改变量的内容不会对另一方造成影响。"><a href="#总结：其实对于浅拷贝和深拷贝来说，如果拷贝对象都是不可变对象的话，那么两者效果是一样的。如果是可变对象的话，“-”拷贝的方式，只是拷贝了内存中的地址引用，两个对象的地址引用一样，所以两个对象的值会随着一方的修改而修改。而对于deepcopy-来说，如果是可变对象的话，那么拷贝内容后新对象的内存地址也会重新分配，跟原来的内存地址不一样了。所以两者任意修改变量的内容不会对另一方造成影响。" class="headerlink" title="总结：其实对于浅拷贝和深拷贝来说，如果拷贝对象都是不可变对象的话，那么两者效果是一样的。如果是可变对象的话，“=”拷贝的方式，只是拷贝了内存中的地址引用，两个对象的地址引用一样，所以两个对象的值会随着一方的修改而修改。而对于deepcopy()来说，如果是可变对象的话，那么拷贝内容后新对象的内存地址也会重新分配，跟原来的内存地址不一样了。所以两者任意修改变量的内容不会对另一方造成影响。"></a>总结：其实对于浅拷贝和深拷贝来说，如果拷贝对象都是不可变对象的话，那么两者效果是一样的。如果是可变对象的话，“=”拷贝的方式，只是拷贝了内存中的地址引用，两个对象的地址引用一样，所以两个对象的值会随着一方的修改而修改。而对于deepcopy()来说，如果是可变对象的话，那么拷贝内容后新对象的内存地址也会重新分配，跟原来的内存地址不一样了。所以两者任意修改变量的内容不会对另一方造成影响。</h3><h3 id="3-注意一个特殊的copy-跟深浅拷贝都有区别，慎用。"><a href="#3-注意一个特殊的copy-跟深浅拷贝都有区别，慎用。" class="headerlink" title="3.注意一个特殊的copy(),跟深浅拷贝都有区别，慎用。"></a>3.注意一个特殊的copy(),跟深浅拷贝都有区别，慎用。</h3><ol>
<li>copy.copy对于可变类型，会进行浅拷贝</li>
<li>copy.copy对于不可变类型，不会拷贝，仅仅是指向</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="number">1.</span>使用copy()拷贝不可变对象</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">val1 = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">val2 = copy.copy(val1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(val1,val2)<span class="comment">##1000 1000</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(id(val1),id(val2))<span class="comment">#8551568 8551568</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="number">2.</span>使用copy（）拷贝可变对象</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ls1 =[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ls2 = copy.copy(ls1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ls1.append(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(ls1,ls2)  <span class="comment">#[1, 2, 3, 4, 5] [1, 2, 3, 4]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">看上去copy()函数效果和deepcopy()效果一样，可变对象拷贝后值也没有随着一个对象的修改而修改。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">然后真实情况真是这样嘛？请看下面的案例，同样是拷贝可变对象。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">origin = [<span class="number">1</span>, <span class="number">2</span>, [<span class="number">3</span>, <span class="number">4</span>]]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cop1 = copy.copy(origin)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cop2 = copy.deepcopy(origin)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">origin[<span class="number">2</span>][<span class="number">0</span>] = <span class="string">"hey!"</span>  <span class="comment">#修改数据源的值</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(cop1,cop2) <span class="comment">#[1, 2, ['hey!', 4]] [1, 2, [3, 4]]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">很显然这时copy（）函数拷贝的值随着原对象的值修改了，而deepcopy()的值没有随着原对象的值修改。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">主要是因为deepcopy会将复杂对象的每一层复制一个单独的个体出来对于copy（）函数要慎用，慎用。</span><br></pre></td></tr></table></figure>







<p>神经网络的典型处理如下所示：</p>
<h3 id="1-定义可学习参数的网络结构（堆叠各层和层的设计）；-2-数据集输入；-3-对输入进行处理（由定义的网络层进行处理）-主要体现在网络的前向传播；-4-计算loss-，由Loss层计算；-5-反向传播求梯度；-6-根据梯度改变参数值-最简单的实现方式（SGD）为-weight-weight-learning-rate-gradient"><a href="#1-定义可学习参数的网络结构（堆叠各层和层的设计）；-2-数据集输入；-3-对输入进行处理（由定义的网络层进行处理）-主要体现在网络的前向传播；-4-计算loss-，由Loss层计算；-5-反向传播求梯度；-6-根据梯度改变参数值-最简单的实现方式（SGD）为-weight-weight-learning-rate-gradient" class="headerlink" title="1. 定义可学习参数的网络结构（堆叠各层和层的设计）； 2. 数据集输入； 3. 对输入进行处理（由定义的网络层进行处理）,主要体现在网络的前向传播； 4. 计算loss ，由Loss层计算； 5. 反向传播求梯度； 6. 根据梯度改变参数值,最简单的实现方式（SGD）为:   weight = weight - learning_rate * gradient"></a><strong>1. 定义可学习参数的网络结构（堆叠各层和层的设计）； 2. 数据集输入； 3. 对输入进行处理（由定义的网络层进行处理）,主要体现在网络的前向传播； 4. 计算loss ，由Loss层计算； 5. 反向传播求梯度； 6. 根据梯度改变参数值,最简单的实现方式（SGD）为:</strong>   weight = weight - learning_rate * gradient</h3><p>下面是利用PyTorch定义深度网络层（Op）示例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FeatureL2Norm</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        super(FeatureL2Norm, self).__init__()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, feature)</span>:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        epsilon = <span class="number">1e-6</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#        print(feature.size())</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#        print(torch.pow(torch.sum(torch.pow(feature,2),1)+epsilon,0.5).size())</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        norm = torch.pow(torch.sum(torch.pow(feature,<span class="number">2</span>),<span class="number">1</span>)+epsilon,<span class="number">0.5</span>).unsqueeze(<span class="number">1</span>).expand_as(feature)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> torch.div(feature,norm)</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FeatureRegression</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, output_dim=<span class="number">6</span>, use_cuda=True)</span>:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        super(FeatureRegression, self).__init__()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.conv = nn.Sequential(</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            nn.Conv2d(<span class="number">225</span>, <span class="number">128</span>, kernel_size=<span class="number">7</span>, padding=<span class="number">0</span>),</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            nn.BatchNorm2d(<span class="number">128</span>),</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            nn.Conv2d(<span class="number">128</span>, <span class="number">64</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">0</span>),</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.linear = nn.Linear(<span class="number">64</span> * <span class="number">5</span> * <span class="number">5</span>, output_dim)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> use_cuda:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            self.conv.cuda()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            self.linear.cuda()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        x = self.conv(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        x = self.linear(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<p>由上例代码可以看到，不论是在定义网络结构还是定义网络层的操作（Op），均需要定义forward函数，下面看一下<a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">PyTorch官网</a>对PyTorch的forward方法的描述：</p>
<p><img src="https://img-blog.csdnimg.cn/20181114105426553.PNG" alt="img"></p>
<p>那么调用forward方法的具体流程是什么样的呢？<a href="https://blog.csdn.net/u012436149/article/details/70145598" target="_blank" rel="noopener">具体流程是这样的：</a></p>
<h3 id="以一个Module为例：-1-调用module的call方法-2-module的call里面调用module的forward方法-3-forward里面如果碰到Module的子类，回到第1步，如果碰到的是Function的子类，继续往下-4-调用Function的call方法-5-Function的call方法调用了Function的forward方法。-6-Function的forward返回值-7-module的forward返回值-8-在module的call进行forward-hook操作，然后返回值。"><a href="#以一个Module为例：-1-调用module的call方法-2-module的call里面调用module的forward方法-3-forward里面如果碰到Module的子类，回到第1步，如果碰到的是Function的子类，继续往下-4-调用Function的call方法-5-Function的call方法调用了Function的forward方法。-6-Function的forward返回值-7-module的forward返回值-8-在module的call进行forward-hook操作，然后返回值。" class="headerlink" title="以一个Module为例： 1. 调用module的call方法 2. module的call里面调用module的forward方法 3. forward里面如果碰到Module的子类，回到第1步，如果碰到的是Function的子类，继续往下 4. 调用Function的call方法 5. Function的call方法调用了Function的forward方法。 6. Function的forward返回值 7. module的forward返回值 8. 在module的call进行forward_hook操作，然后返回值。"></a>以一个Module为例： <strong>1. 调用module的call方法 2. module的call里面调用module的forward方法 3. forward里面如果碰到Module的子类，回到第1步，如果碰到的是Function的子类，继续往下 4. 调用Function的call方法 5. Function的call方法调用了Function的forward方法。 6. Function的forward返回值 7. module的forward返回值 8. 在module的call进行forward_hook操作，然后返回值。</strong></h3><p>上述中“调用module的call方法”是指nn.Module 的<strong>call</strong>方法。定义<strong>call</strong>方法的类可以当作函数调用，具体参考Python的面向对象编程。也就是说，当把定义的网络模型model当作函数调用的时候就自动调用定义的网络模型的forward方法。nn.Module 的<strong>call</strong>方法部分源码如下所示：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, *input, **kwargs)</span>:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   result = self.forward(*input, **kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   <span class="keyword">for</span> hook <span class="keyword">in</span> self._forward_hooks.values():</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">       <span class="comment">#将注册的hook拿出来用</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">       hook_result = hook(self, input, result)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   ...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>

<p>可以看到，当执行model(x)的时候，底层自动调用forward方法计算结果。具体示例如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LeNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        super(LeNet, self).__init__()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	layer1 = nn.Sequential()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	layer1.add_module(<span class="string">'conv1'</span>, nn.Conv(<span class="number">1</span>, <span class="number">6</span>, <span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	layer1.add_moudle(<span class="string">'pool1'</span>, nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	self.layer1 = layer1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	layer2 = nn.Sequential()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	layer2.add_module(<span class="string">'conv2'</span>, nn.Conv(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	layer2.add_moudle(<span class="string">'pool2'</span>, nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	self.layer2 = layer2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	layer3 = nn.Sequential()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	layer3.add_module(<span class="string">'fc1'</span>, nn.Linear(<span class="number">400</span>, <span class="number">120</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	layer3.add_moudle(<span class="string">'fc2'</span>, nn.Linear(<span class="number">120</span>, <span class="number">84</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	layer3.add_moudle(<span class="string">'fc3'</span>, nn.Linear(<span class="number">84</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	self.layer3 = layer3</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	x = self.layer1(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	x = self.layer2(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	x = self.layer3(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<h3 id="model-LeNet-y-model-x"><a href="#model-LeNet-y-model-x" class="headerlink" title="model = LeNet() y = model(x)"></a><strong>model = LeNet() y = model(x)</strong></h3><p>如上则调用网络模型定义的forward方法。</p>
]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-07-30-待写博客</title>
    <url>/2020/07/30/2020-07-30-%E5%BE%85%E5%86%99%E5%8D%9A%E5%AE%A2/</url>
    <content><![CDATA[<h3 id="pytorch-中forward的使用以及原理-–pytorch使用"><a href="#pytorch-中forward的使用以及原理-–pytorch使用" class="headerlink" title="pytorch 中forward的使用以及原理   –pytorch使用"></a>pytorch 中forward的使用以及原理   –pytorch使用</h3><p><a href="https://blog.csdn.net/u011501388/article/details/84062483" target="_blank" rel="noopener">https://blog.csdn.net/u011501388/article/details/84062483</a></p>
<h4 id="阅读代码时的问题-记录"><a href="#阅读代码时的问题-记录" class="headerlink" title="阅读代码时的问题 记录"></a>阅读代码时的问题 记录</h4><h1 id=""><a href="#" class="headerlink" title=""></a></h1><h3 id="zsh的使用-–服务器心得"><a href="#zsh的使用-–服务器心得" class="headerlink" title="zsh的使用 –服务器心得"></a>zsh的使用 –服务器心得</h3><h3 id="PyTorch里面的torch-nn-Parameter-详解"><a href="#PyTorch里面的torch-nn-Parameter-详解" class="headerlink" title="PyTorch里面的torch.nn.Parameter()详解"></a>PyTorch里面的torch.nn.Parameter()详解</h3><p><a href="https://cloud.tencent.com/developer/article/1608348" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1608348</a></p>
<h3 id="论文阅读的思维导图"><a href="#论文阅读的思维导图" class="headerlink" title="论文阅读的思维导图"></a>论文阅读的思维导图</h3><p>conda 安装新版本python之后，会覆盖之前的版本</p>
<h1 id="LINUX-杀死、暂停、继续、后台运行进程"><a href="#LINUX-杀死、暂停、继续、后台运行进程" class="headerlink" title="LINUX 杀死、暂停、继续、后台运行进程"></a>LINUX 杀死、暂停、继续、后台运行进程</h1><p>ctrl + z</p>
<p>可以将一个正在前台执行的命令放到后台，并且暂停</p>
<p>若想恢复到前台，则</p>
<ol>
<li>jobs  #查看当前有多少在后台运行的命令 会有序号 job号</li>
<li>fg 〔<em>job</em>号〕  将后台中的命令调至前台继续运行  如： fg %1</li>
</ol>
<p><a href="https://blog.csdn.net/QQ1910084514/article/details/80390671" target="_blank" rel="noopener">https://blog.csdn.net/QQ1910084514/article/details/80390671</a></p>
]]></content>
  </entry>
  <entry>
    <title>2020-07-28-pytorch安装</title>
    <url>/2020/07/28/2020-07-28-pytorch%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[<h3 id="Anaconda安装配置"><a href="#Anaconda安装配置" class="headerlink" title="Anaconda安装配置"></a>Anaconda安装配置</h3><p>由于墙的问题，用conda安装Pytorch过程中会连接失败，这是因为Anaconda.org的服务器在国外。在这里可以用清华TUNA镜像源，包含Anaconda仓库的镜像，将其加入conda的配置，配置如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#添加Anaconda的TUNA镜像</span></span><br><span class="line"></span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span><br><span class="line"></span><br><span class="line"><span class="comment">#TUNA的help中镜像地址加有引号，需要去掉</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#设置搜索时显示通道地址</span></span><br><span class="line"></span><br><span class="line">conda config --set show_channel_urls yes</span><br></pre></td></tr></table></figure>

<p>执行完上述命令后，会生成~/.condarc文件，记录着对conda的配置，直接手动创建、编辑该文件是相同的效果。</p>
<h3 id="Pytorch安装"><a href="#Pytorch安装" class="headerlink" title="Pytorch安装"></a>Pytorch安装</h3><p>在这里的安装，我采用conda安装：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conda install pytorch torchvision -c soumith</span><br></pre></td></tr></table></figure>



<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>进入python模式下，看能否导入torch成功：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">python</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>pytorch</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-07-27-linux的session解析</title>
    <url>/2020/07/27/2020-07-27-linux%E7%9A%84session%E8%A7%A3%E6%9E%90/</url>
    <content><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>我们使用ssh连接服务器时，ssh的窗口突然断开了连接，那么在服务器上跑的程序就也跟着断掉了，之前所有跑的数据也将丢失，这样将会浪费我们大量的时间。</p>
<h3 id="为什么ssh一旦断开我们的进程也将会被杀掉？"><a href="#为什么ssh一旦断开我们的进程也将会被杀掉？" class="headerlink" title="为什么ssh一旦断开我们的进程也将会被杀掉？"></a>为什么ssh一旦断开我们的进程也将会被杀掉？</h3><p>元凶：SIGHUP 信号</p>
<p>让我们来看看为什么关掉窗口/断开连接会使得正在运行的程序死掉。</p>
<p>在Linux/Unix中，有这样几个概念：</p>
<p>进程组（process group）：一个或多个进程的集合，每一个进程组有唯一一个进程组ID，即进程组长进程的ID。</p>
<p>会话期（session）：一个或多个进程组的集合，有唯一一个会话期首进程（session leader）。会话期ID为首进程的ID。</p>
<p>会话期可以有一个单独的控制终端（controlling terminal）。与控制终端连接的会话期首进程叫做控制进程（controlling process）。当前与终端交互的进程称为前台进程组。其余进程组称为后台进程组。</p>
<p>根据POSIX.1定义：</p>
<p>挂断信号（SIGHUP）默认的动作是终止程序。</p>
<p>当终端接口检测到网络连接断开，将挂断信号发送给控制进程（会话期首进程）。</p>
<p>如果会话期首进程终止，则该信号发送到该会话期前台进程组。</p>
<p>一个进程退出导致一个孤儿进程组中产生时，如果任意一个孤儿进程组进程处于STOP状态，发送SIGHUP和SIGCONT信号到该进程组中所有进程。</p>
<p>因此当网络断开或终端窗口关闭后，控制进程收到SIGHUP信号退出，会导致该会话期内其他进程退出。</p>
<p><strong>这里我认为我们的进程被杀掉也就是因为ssh与服务器之间的通信断掉了，这个通信断掉之后linux程序就默认将该连接下的所有进程都杀掉</strong></p>
<h3 id="session-是什么？"><a href="#session-是什么？" class="headerlink" title="session 是什么？"></a>session 是什么？</h3><p>我们常见的 Linux session 一般是指 shell session。Shell session 是终端中当前的状态，在终端中只能有一个 session。<code>当我们打开一个新的终端时，总会创建一个新的 shell session。</code></p>
<p>就进程间的关系来说，session 由一个或多个进程组组成。一般情况下，来自单个登录的所有进程都属于同一个 session。我们可以通过下图来理解进程、进程组和 session 之间的关系：</p>
<p><img src="https://img2018.cnblogs.com/blog/952033/202001/952033-20200103182042686-2100862807.png" alt="img"></p>
<p><code>会话是由会话中的第一个进程创建的，一般情况下是打开终端时创建的 shell 进程。</code>该进程也叫 session 的领头进程。Session 中领头进程的 PID 也就是 session 的 SID。我们可以通过下面的命令查看 SID：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">$ ps -o pid,ppid,pgid,sid,tty,comm</span><br></pre></td></tr></table></figure>

<p><img src="https://img2018.cnblogs.com/blog/952033/202001/952033-20200103182117962-99639442.png" alt="img"></p>
<p>Session 中的每个进程组被称为一个 job，有一个 job 会成为 session 的前台 job(foreground)，其它的 job 则是后台 job(background)。每个 session 连接一个控制终端(control terminal)，控制终端中的输入被发送给前台 job，从前台 job 产生的输出也被发送到控制终端上。同时由控制终端产生的信号，比如 ctrl + z 等都会传递给前台 job。</p>
<p>一般情况下 session 和终端是一对一的关系，当我们打开多个终端窗口时，实际上就创建了多个 session。</p>
<p><code>Session 的意义在于多个工作(job)在一个终端中运行，其中的一个为前台 job，它直接接收该终端的输入并把结果输出到该终端。其它的 job 则在后台运行。</code></p>
<h3 id="session-的诞生与消亡"><a href="#session-的诞生与消亡" class="headerlink" title="session 的诞生与消亡"></a>session 的诞生与消亡</h3><p>通常，新的 session 由系统登录程序创建，session 中的领头进程是运行用户登录 shell 的进程。<code>新创建的每个进程都会属于一个进程组，当创建一个进程时，它和父进程在同一个进程组、session 中。</code></p>
<p>将进程放入不同 session 的惟一方法是使用 setsid 函数使其成为新 session 的领头进程。这还会将 session 领头进程放入一个新的进程组中。</p>
<p><code>当 session 中的所有进程都结束时 session 也就消亡了</code>。如下两种：</p>
<p>1.实际使用中比如网络断开了，session 肯定是要消亡的。</p>
<p>2.正常的消亡，比如让 session 的领头进程退出。</p>
<p>一般情况下 session 的领头进程是 shell 进程，如果它处于前台，我们可以使用 <code>exit 命令或者是 ctrl + d</code> 让它退出。或者我们可以直接通过 kill 命令杀死 session 的领头进程。这里面的原理是：当系统检测到挂断(hangup)条件时，内核中的驱动会将 SIGHUP 信号发送到整个 session。通常情况下，这会杀死 session 中的所有进程。</p>
<p>session 与终端的关系<br>如果 session 关联的是伪终端，这个伪终端本身就是随着 session 的建立而创建的，session 结束，那么这个伪终端也会被销毁。<br>如果 session 关联的是 tty1-6，tty 则不会被销毁。因为该终端设备是在系统初始化的时候创建的，并不是依赖该会话建立的，所以当 session 退出，tty 仍然存在。只是 init 系统在 session 结束后，会重启 getty 来监听这个 tty。</p>
<h3 id="nohup"><a href="#nohup" class="headerlink" title="nohup"></a>nohup</h3><p><code>如果我们在 session 中执行了 nohup 等类似的命令，当 session 消亡时，相关的进程并不会随着 session 结束，原因是这些进程不再受 SIGHUP 信号的影响。</code>比如我们执行下面的命令：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">$ nohup sleep <span class="number">1000</span> &gt;/dev/null <span class="number">2</span>&gt;&amp;<span class="number">1</span> &amp;</span><br></pre></td></tr></table></figure>

<p><img src="https://img2018.cnblogs.com/blog/952033/202001/952033-20200103182343352-1366915632.png" alt="img"></p>
<p>此时 sleep 进程的 sid 和其它进程是相同的，还可以通过 pstree 命令看到进程间的父子关系：</p>
<p><img src="https://img2018.cnblogs.com/blog/952033/202001/952033-20200103182417115-817556079.png" alt="img"></p>
<p><code>如果我们退出当前 session 的领头进程(bash)，sleep 进程并不会退出，这样我们就可以放心的等待该进程运行结果了。</code><br>nohup 并不改变进程的 sid，同时也说明在这种情况中，虽然 session 的领头进程退出了，但是 session 依然没有被销毁(至少 sid 还在被引用)。重新建立连接，通过下面的命令查看 sleep 进程的信息，发现进程的 sid 依然是 7837：</p>
<p><img src="https://img2018.cnblogs.com/blog/952033/202001/952033-20200103182448160-376880623.png" alt="img"></p>
<p>但是<code>此时的 sleep 已经被系统的 1 号进程 systemd 收养了</code>：</p>
<p><img src="https://img2018.cnblogs.com/blog/952033/202001/952033-20200103182521953-1574746082.png" alt="img"></p>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><blockquote>
<p><a href="https://www.cnblogs.com/sparkdev/p/12146305.html" target="_blank" rel="noopener">https://www.cnblogs.com/sparkdev/p/12146305.html</a></p>
</blockquote>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-07-26-picgo上传失败原因</title>
    <url>/2020/07/26/2020-07-26-picgo%E4%B8%8A%E4%BC%A0%E5%A4%B1%E8%B4%A5%E5%8E%9F%E5%9B%A0/</url>
    <content><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>在使用后PicGo上传图片至github时总是会显示上传失败，以下是我的解决方案经验，我的情况是可以上传图片，但是偶尔会失败</p>
<h3 id="case-1-检查服务及端口配置"><a href="#case-1-检查服务及端口配置" class="headerlink" title="case 1 检查服务及端口配置"></a>case 1 检查服务及端口配置</h3><p>择相应的选项“设置server”，以下操作</p>
<p><img src="https://i.loli.net/2020/07/27/lf7Bw2jDUqgIxMk.png" alt=""></p>
<p>我们可以选择将开关先关闭，然后打开，确定后再重启软件，一般会成功。</p>
<p>或者修改端口号： 如修改为36688</p>
<p>记住，如果不行，那就直接关闭软件，然后等2分钟后，再打开picgo软件就可以上传成功了。</p>
<h3 id="case-2-查看日志"><a href="#case-2-查看日志" class="headerlink" title="case 2 查看日志"></a>case 2 查看日志</h3><p>找到“设置日志文件”，然后打开日志文件，检查相应的日志，了解上传失败的原因。</p>
<p><img src="https://i.loli.net/2020/07/27/421CU9GzgKkauxt.png" alt=""></p>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://www.shopee6.com/web/web-tutorial/picgo-github-fail.html" target="_blank" rel="noopener">https://www.shopee6.com/web/web-tutorial/picgo-github-fail.html</a></p>
]]></content>
      <categories>
        <category>杂</category>
      </categories>
      <tags>
        <tag>杂</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-07-26-tensorflow函数解析</title>
    <url>/2020/07/26/2020-07-26-tensorflow%E5%87%BD%E6%95%B0%E8%A7%A3%E6%9E%90/</url>
    <content><![CDATA[<p>tf.manul</p>
]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-07-26-numpy函数解析</title>
    <url>/2020/07/26/2020-07-26-numpy%E5%87%BD%E6%95%B0%E8%A7%A3%E6%9E%90/</url>
    <content><![CDATA[<h3 id="newaxis用法"><a href="#newaxis用法" class="headerlink" title="newaxis用法"></a>newaxis用法</h3><p>newaxis表示增加一个新的坐标轴</p>
<ul>
<li>x[:, np.newaxis] ，放在后面，会给列上增加维度</li>
<li>x[np.newaxis, :] ，放在前面，会给行上增加维度</li>
</ul>
<p><strong>用途：</strong> 通常用它将一维的数据转换成一个矩阵，与代码后面的权重矩阵进行相乘。</p>
<h4 id="第一个程序"><a href="#第一个程序" class="headerlink" title="第一个程序"></a>第一个程序</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">a &#x3D; np.array([1,2,3])</span><br><span class="line">print (a.shape,&#39;\n&#39;,a)</span><br></pre></td></tr></table></figure>


<p>结果为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(3,)</span><br><span class="line">[1 2 3]</span><br></pre></td></tr></table></figure>



<h4 id="第二个程序"><a href="#第二个程序" class="headerlink" title="第二个程序"></a>第二个程序</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a &#x3D; np.array([1,2,3])[:,np.newaxis]</span><br><span class="line">print (a.shape,&#39;\n&#39;,a)</span><br></pre></td></tr></table></figure>

<p>结果为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(3, 1)</span><br><span class="line">[[1]</span><br><span class="line">[2]</span><br><span class="line">[3]]</span><br></pre></td></tr></table></figure>

<p>和第一个程序相比，a的shape为（3，）现在为（3，1）变为二维数组了，之前为[1,2,3]，现在变为</p>
<p>[[1]<br>[2]<br>[3]]</p>
<h4 id="第三个程序"><a href="#第三个程序" class="headerlink" title="第三个程序"></a>第三个程序</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a &#x3D; np.array([1,2,3])[np.newaxis,:]</span><br><span class="line">print (a.shape,&#39;\n&#39;,a)</span><br></pre></td></tr></table></figure>


<p>输出结果为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(1, 3)</span><br><span class="line">[[1 2 3]]</span><br></pre></td></tr></table></figure>



<p>输出结果为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(1, 3)</span><br><span class="line">[[1 2 3]]</span><br></pre></td></tr></table></figure>



<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>np.newaxis的作用就是在原来的数组上增加一个维度。[np.newaxis,:]这个地方np.newaxis放的位置有关，第二个程序放在[:,]的后面，相当于在原来的后面增加一个维度，所以变为(3,1)，而第三个则放在前面，则为(1,3)。<code>加到哪一维，那一维就为1</code></p>
<h3 id="concatenate用法"><a href="#concatenate用法" class="headerlink" title="concatenate用法"></a>concatenate用法</h3><p>-用于进行数组拼接</p>
<p>函数定义：</p>
<p><code>numpy.concatenate</code>((a1, a2, …), axis=0, out=None)</p>
<ul>
<li>axis=0: 合并行</li>
<li>axis=1: 合并列</li>
</ul>
<p>例子如下：</p>
<figure class="highlight csharp"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; a=np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line"> &gt;&gt;&gt; b=np.array([[<span class="number">11</span>,<span class="number">21</span>,<span class="number">31</span>],[<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]])</span><br><span class="line"> <span class="meta"># 合并行</span></span><br><span class="line"> &gt;&gt;&gt; np.concatenate((a,b,c),axis=<span class="number">0</span>)  <span class="meta"># 默认情况下，axis=0可以不写</span></span><br><span class="line"> array([[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">  [<span class="meta"> 4,  5,  6</span>],</span><br><span class="line">  [<span class="meta">11, 21, 31</span>],</span><br><span class="line">  [<span class="meta"> 7,  8,  9</span>]])</span><br><span class="line"> <span class="meta"># 合并列</span></span><br><span class="line"> &gt;&gt;&gt; np.concatenate((a,b),axis=<span class="number">1</span>) </span><br><span class="line">  array([[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>, <span class="number">11</span>, <span class="number">21</span>, <span class="number">31</span>],</span><br><span class="line">  [<span class="meta"> 4,  5,  6,  7,  8,  9</span>]])</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-07-26-python切片疑惑解析</title>
    <url>/2020/07/26/2020-07-26-python%E5%88%87%E7%89%87%E7%96%91%E6%83%91%E8%A7%A3%E6%9E%90/</url>
    <content><![CDATA[<p>Python中对于数组和列表进行切片操作是很频繁的，我主要简单总结一下常用集中索引化方式</p>
<h3 id="一维：-n-、-m-、-1-、-1"><a href="#一维：-n-、-m-、-1-、-1" class="headerlink" title="一维： [ : n]、[m : ] 、[-1]、[::-1]"></a>一维： [ : n]、[m : ] 、[-1]、[::-1]</h3><p><strong>[m : ]</strong> ：代表列表中的第m项到最后一项 （从0开始）</p>
<p><strong>[ : n]</strong> ：代表列表中的第0项到第n-1项  （含左不含右）</p>
<p><strong>[-1]：</strong>取最后一个元素</p>
<p><strong>[::-1]</strong>：取从后向前（相反）的元素 （倒序）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">X = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>] )</span><br><span class="line">print(X.shape)</span><br><span class="line">print(<span class="string">'\n'</span>)</span><br><span class="line">print(X[<span class="number">3</span>:])</span><br><span class="line">print(X[:<span class="number">7</span>])</span><br><span class="line">print(X[::<span class="number">-1</span>][:<span class="number">3</span>]) <span class="comment"># 在进行了[::-1]之后得到倒序数组，再取[：3]</span></span><br></pre></td></tr></table></figure>



<p>结果如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(<span class="number">8</span>,)</span><br><span class="line">[<span class="number">4</span> <span class="number">5</span> <span class="number">6</span> <span class="number">7</span> <span class="number">8</span>]</span><br><span class="line">[<span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span> <span class="number">7</span>]</span><br><span class="line">[<span class="number">8</span>,<span class="number">7</span>,<span class="number">6</span>]</span><br></pre></td></tr></table></figure>



<h3 id="二维：-X-0-、X-1-、-X-m-n"><a href="#二维：-X-0-、X-1-、-X-m-n" class="headerlink" title="二维： X[:,0] 、X[:,1] 、 X[:, m:n]"></a>二维： X[:,0] 、X[:,1] 、 X[:, m:n]</h3><p>X[:,0]是numpy中数组的一种写法，表示对一个二维数组，取该二维数组第一维中的所有数据，第二维中取第0个数据，直观来说</p>
<p>X[:,0]：取第二维（所有行）的第0个数据,就是<code>第0列</code></p>
<p>X[:,1] ：取第二维（所有行）的第1个数据，就是<code>第1列</code></p>
<p>X[:, m:n]，即取所有数据的<code>第m到n-1列数据，含左不含右</code></p>
<p>示例如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">X = np.array([[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>],[<span class="number">9</span>,<span class="number">10</span>,<span class="number">11</span>],[<span class="number">12</span>,<span class="number">13</span>,<span class="number">14</span>],[<span class="number">15</span>,<span class="number">16</span>,<span class="number">17</span>],[<span class="number">18</span>,<span class="number">19</span>,<span class="number">20</span>]])</span><br><span class="line">print(X.shape)</span><br><span class="line">print(<span class="string">'\n'</span>)</span><br><span class="line"><span class="keyword">print</span> (X[:,<span class="number">1</span>:<span class="number">3</span>])</span><br></pre></td></tr></table></figure>

<p>结果如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(<span class="number">7</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">[[ <span class="number">1</span>  <span class="number">2</span>]</span><br><span class="line"> [ <span class="number">4</span>  <span class="number">5</span>]</span><br><span class="line"> [ <span class="number">7</span>  <span class="number">8</span>]</span><br><span class="line"> [<span class="number">10</span> <span class="number">11</span>]</span><br><span class="line"> [<span class="number">13</span> <span class="number">14</span>]</span><br><span class="line"> [<span class="number">16</span> <span class="number">17</span>]</span><br><span class="line"> [<span class="number">19</span> <span class="number">20</span>]]</span><br></pre></td></tr></table></figure>



<h3 id="三维-X-0-、X-1-、X-m-n"><a href="#三维-X-0-、X-1-、X-m-n" class="headerlink" title="三维 X[:,:,0]、X[:,:,1]、X[:,:,m:n]"></a>三维 X[:,:,0]、X[:,:,1]、X[:,:,m:n]</h3><p>类比于二维，原理相同</p>
<p>X[:,:,0]：取第三维矩阵中第0列的所有数据</p>
<p>X[:,:,1]：取第三维矩阵中第1列的所有数据</p>
<p>X[:,:,m:n]：取第三维矩阵中第m列到第n-1列的所有数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">注：shape（<span class="number">9</span>,<span class="number">5</span>,<span class="number">2</span>）指的是最外层有<span class="number">9</span>个括号，每个括号里嵌套<span class="number">5</span>个括号，在<span class="number">5</span>个括号里又每个有<span class="number">2</span>个元素</span><br><span class="line"></span><br><span class="line">判断的时候先先出数组的shape，根据shape进行判断</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># !usr/bin/env python</span></span><br><span class="line"><span class="comment"># encoding:utf-8</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">simple_test</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    简单的小实验</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    data_list = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>], [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>], [<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>], [<span class="number">6</span>, <span class="number">7</span>, <span class="number">9</span>], [<span class="number">0</span>, <span class="number">4</span>, <span class="number">7</span>], [<span class="number">4</span>, <span class="number">6</span>, <span class="number">0</span>],[<span class="number">2</span>, <span class="number">9</span>, <span class="number">1</span>], [<span class="number">5</span>, <span class="number">8</span>, <span class="number">7</span>], [<span class="number">9</span>, <span class="number">7</span>, <span class="number">8</span>], [<span class="number">3</span>, <span class="number">7</span>, <span class="number">9</span>]]</span><br><span class="line">    <span class="comment"># data_list.toarray()</span></span><br><span class="line">    data_list = np.array(data_list)</span><br><span class="line">    print(<span class="string">'X[:,0]结果输出为：'</span>)</span><br><span class="line">    print( data_list[:, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    print(  <span class="string">'X[:,1]结果输出为：'</span>)</span><br><span class="line">    print( data_list[:, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    print(<span class="string">'X[:,m:n]结果输出为：'</span>)</span><br><span class="line"></span><br><span class="line">    print( data_list[:, <span class="number">0</span>:<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    data_list = [[[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">7</span>, <span class="number">9</span>], [<span class="number">4</span>, <span class="number">0</span>]], [[<span class="number">1</span>, <span class="number">4</span>], [<span class="number">1</span>, <span class="number">5</span>], [<span class="number">3</span>, <span class="number">6</span>], [<span class="number">8</span>, <span class="number">9</span>], [<span class="number">5</span>, <span class="number">0</span>]],</span><br><span class="line">                 [[<span class="number">8</span>, <span class="number">2</span>], [<span class="number">1</span>, <span class="number">8</span>], [<span class="number">3</span>, <span class="number">5</span>], [<span class="number">7</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">6</span>]],</span><br><span class="line">                 [[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">5</span>], [<span class="number">7</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]], [[<span class="number">9</span>, <span class="number">2</span>], [<span class="number">1</span>, <span class="number">3</span>], [<span class="number">3</span>, <span class="number">5</span>], [<span class="number">7</span>, <span class="number">67</span>], [<span class="number">4</span>, <span class="number">4</span>]],</span><br><span class="line">                 [[<span class="number">8</span>, <span class="number">2</span>], [<span class="number">1</span>, <span class="number">9</span>], [<span class="number">3</span>, <span class="number">43</span>], [<span class="number">7</span>, <span class="number">3</span>], [<span class="number">43</span>, <span class="number">0</span>]],</span><br><span class="line">                 [[<span class="number">1</span>, <span class="number">22</span>], [<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">42</span>], [<span class="number">7</span>, <span class="number">29</span>], [<span class="number">4</span>, <span class="number">20</span>]], [[<span class="number">1</span>, <span class="number">5</span>], [<span class="number">1</span>, <span class="number">20</span>], [<span class="number">3</span>, <span class="number">24</span>], [<span class="number">17</span>, <span class="number">9</span>], [<span class="number">4</span>, <span class="number">10</span>]],</span><br><span class="line">                 [[<span class="number">11</span>, <span class="number">2</span>], [<span class="number">1</span>, <span class="number">110</span>], [<span class="number">3</span>, <span class="number">14</span>], [<span class="number">7</span>, <span class="number">4</span>], [<span class="number">4</span>, <span class="number">2</span>]]]</span><br><span class="line">    data_list = np.array(data_list)</span><br><span class="line">    print(data_list.shape)</span><br><span class="line">    print(<span class="string">'X[:,:,0]结果输出为：'</span>)</span><br><span class="line"></span><br><span class="line">    print( data_list[:, :, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    print(<span class="string">'X[:,:,1]结果输出为：'</span>)</span><br><span class="line"></span><br><span class="line">    print(data_list[:, :, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    print(<span class="string">'X[:,:,m:n]结果输出为：'</span>)</span><br><span class="line"></span><br><span class="line">    print( data_list[:, :, <span class="number">0</span>:<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    simple_test()</span><br></pre></td></tr></table></figure>

<p>部分结如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X[:,:,<span class="number">0</span>]结果输出为：</span><br><span class="line">[[ <span class="number">1</span>  <span class="number">1</span>  <span class="number">3</span>  <span class="number">7</span>  <span class="number">4</span>]</span><br><span class="line"> [ <span class="number">1</span>  <span class="number">1</span>  <span class="number">3</span>  <span class="number">8</span>  <span class="number">5</span>]</span><br><span class="line"> [ <span class="number">8</span>  <span class="number">1</span>  <span class="number">3</span>  <span class="number">7</span>  <span class="number">4</span>]</span><br><span class="line"> [ <span class="number">1</span>  <span class="number">1</span>  <span class="number">3</span>  <span class="number">7</span>  <span class="number">7</span>]</span><br><span class="line"> [ <span class="number">9</span>  <span class="number">1</span>  <span class="number">3</span>  <span class="number">7</span>  <span class="number">4</span>]</span><br><span class="line"> [ <span class="number">8</span>  <span class="number">1</span>  <span class="number">3</span>  <span class="number">7</span> <span class="number">43</span>]</span><br><span class="line"> [ <span class="number">1</span>  <span class="number">1</span>  <span class="number">3</span>  <span class="number">7</span>  <span class="number">4</span>]</span><br><span class="line"> [ <span class="number">1</span>  <span class="number">1</span>  <span class="number">3</span> <span class="number">17</span>  <span class="number">4</span>]</span><br><span class="line"> [<span class="number">11</span>  <span class="number">1</span>  <span class="number">3</span>  <span class="number">7</span>  <span class="number">4</span>]]</span><br><span class="line">X[:,:,<span class="number">1</span>]结果输出为：</span><br><span class="line">[[  <span class="number">2</span>   <span class="number">0</span>   <span class="number">4</span>   <span class="number">9</span>   <span class="number">0</span>]</span><br><span class="line"> [  <span class="number">4</span>   <span class="number">5</span>   <span class="number">6</span>   <span class="number">9</span>   <span class="number">0</span>]</span><br><span class="line"> [  <span class="number">2</span>   <span class="number">8</span>   <span class="number">5</span>   <span class="number">3</span>   <span class="number">6</span>]</span><br><span class="line"> [  <span class="number">1</span>   <span class="number">2</span>   <span class="number">5</span>   <span class="number">6</span>   <span class="number">8</span>]</span><br><span class="line"> [  <span class="number">2</span>   <span class="number">3</span>   <span class="number">5</span>  <span class="number">67</span>   <span class="number">4</span>]</span><br><span class="line"> [  <span class="number">2</span>   <span class="number">9</span>  <span class="number">43</span>   <span class="number">3</span>   <span class="number">0</span>]</span><br><span class="line"> [ <span class="number">22</span>   <span class="number">2</span>  <span class="number">42</span>  <span class="number">29</span>  <span class="number">20</span>]</span><br><span class="line"> [  <span class="number">5</span>  <span class="number">20</span>  <span class="number">24</span>   <span class="number">9</span>  <span class="number">10</span>]</span><br><span class="line"> [  <span class="number">2</span> <span class="number">110</span>  <span class="number">14</span>   <span class="number">4</span>   <span class="number">2</span>]]</span><br></pre></td></tr></table></figure>



<h3 id="start：end：step"><a href="#start：end：step" class="headerlink" title="[start：end：step]"></a>[start：end：step]</h3><p>start:开始索引；end:结束索引；step:步长（步长为正时，从左到右索引，正序取值；步长为负时，从右到左索引，倒序取值）</p>
<p>[::2]  步长为2</p>
<p>[3:7:2] 第3个元素开始，第6个元素结束，步长为2</p>
<p>参考</p>
<p><a href="https://blog.csdn.net/Together_CZ/article/details/79593952" target="_blank" rel="noopener">https://blog.csdn.net/Together_CZ/article/details/79593952</a></p>
]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-07-25-transformer代码</title>
    <url>/2020/07/25/2020-07-25-transformer%E4%BB%A3%E7%A0%81/</url>
    <content><![CDATA[<h1 id="TensorFlow-2-0-教程-Transformer"><a href="#TensorFlow-2-0-教程-Transformer" class="headerlink" title="TensorFlow 2.0 教程-Transformer"></a>TensorFlow 2.0 教程-Transformer</h1><p>这里我们将实现一个Transformer模型，将葡萄牙语翻译为英语。Transformer的核心思想是self-attention–通过关注序列不同位置的内容获取句子的表示。</p>
<p>Transformer的一些优点：</p>
<ul>
<li>不受限于数据的时间/空间关系</li>
<li>可以并行计算</li>
<li>远距离token的相互影响不需要通过很长的时间步或很深的卷积层</li>
<li>可以学习远程依赖</li>
</ul>
<p>Transformer的缺点：</p>
<ul>
<li>对于时间序列，输出需要根据整个历史，而不是当前状态和输入，可能造成效率较低</li>
<li>如果想要获取时间空间信息，需要额外的位置编码</li>
</ul>
<p>In [1]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from __future__ import absolute_import, division, print_function, unicode_literals</span><br><span class="line"># 安装tfds pip install tfds-nightly&#x3D;&#x3D;1.0.2.dev201904090105</span><br><span class="line">import tensorflow_datasets as tfds</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import tensorflow.keras.layers as layers</span><br><span class="line"></span><br><span class="line">import time</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">print(tf.__version__)</span><br><span class="line">2.0.0-alpha0</span><br></pre></td></tr></table></figure>

<h2 id="1-数据输入pipeline"><a href="#1-数据输入pipeline" class="headerlink" title="1.数据输入pipeline"></a>1.数据输入pipeline</h2><p>我们将使用到Portugese-English翻译数据集。</p>
<p>该数据集包含大约50000个训练样例，1100个验证示例和2000个测试示例。</p>
<p>tfds.load :加载数据集.1.下载数据并将其另存为tfrecord文件.   2.加载tfrecord并创建tf.data.Dataset.<br>metadata: 元数据 用于描述数据的数据，比如数码照片的EXIF信息，它就是一种用来描述数码图片的元数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">examples, metadata &#x3D; tfds.load(&#39;ted_hrlr_translate&#x2F;pt_to_en&#39;, with_info&#x3D;True,</span><br><span class="line">                              as_supervised&#x3D;True)</span><br></pre></td></tr></table></figure>



<p>将数据转化为subwords格式</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">train_examples, val_examples &#x3D; examples[&#39;train&#39;], examples[&#39;validation&#39;]</span><br></pre></td></tr></table></figure>



<p>把单词用英语和葡萄牙语分别进行编码</p>
<p> 2**13： 2的3次方大小的词汇表大小</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tokenizer_en &#x3D; tfds.features.text.SubwordTextEncoder.build_from_corpus(</span><br><span class="line">(en.numpy() for pt, en in train_examples), target_vocab_size&#x3D;2**13)</span><br><span class="line">tokenizer_pt &#x3D; tfds.features.text.SubwordTextEncoder.build_from_corpus(</span><br><span class="line">(pt.numpy() for pt, en in train_examples), target_vocab_size&#x3D;2**13)</span><br></pre></td></tr></table></figure>



<p>token转化测试</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sample_str &#x3D; &#39;hello world, tensorflow 2&#39;</span><br><span class="line">tokenized_str &#x3D; tokenizer_en.encode(sample_str)</span><br><span class="line">print(tokenized_str)</span><br><span class="line">original_str &#x3D; tokenizer_en.decode(tokenized_str)</span><br><span class="line">print(original_str)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[3222, 439, 150, 7345, 1378, 2824, 2370, 7881]</span><br><span class="line">hello world, tensorflow 2</span><br></pre></td></tr></table></figure>



<p>添加start、end的token表示</p>
<p>lang1：用于葡萄牙语翻译的开始</p>
<p>lang2：用于英语翻译完成的结束</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def encode(lang1, lang2):</span><br><span class="line">    lang1 &#x3D; [tokenizer_pt.vocab_size] + tokenizer_pt.encode(</span><br><span class="line">        lang1.numpy()) + [tokenizer_pt.vocab_size+1]</span><br><span class="line">    lang2 &#x3D; [tokenizer_en.vocab_size] + tokenizer_en.encode(</span><br><span class="line">        lang2.numpy()) + [tokenizer_en.vocab_size+1]</span><br><span class="line">    return lang1, lang2</span><br></pre></td></tr></table></figure>



<p>过滤长度超过40的数据</p>
<p>tf.logical_and ： 与运算</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">MAX_LENGTH&#x3D;40</span><br><span class="line">def filter_long_sent(x, y, max_length&#x3D;MAX_LENGTH):</span><br><span class="line">    return tf.logical_and(tf.size(x) &lt;&#x3D; max_length,</span><br><span class="line">                         tf.size(y) &lt;&#x3D; max_length)</span><br></pre></td></tr></table></figure>

<p>将python运算，转换为tensorflow运算节点。</p>
<p>这是一个可以把 TensorFlow 和 Python 原生代码无缝衔接起来的函数，有了它，你就可以在 TensorFlow 里面自由的实现你想要的功能，而不用考虑 TensorFlow 有没有实现它的 API</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def tf_encode(pt, en):</span><br><span class="line">    return tf.py_function(encode, [pt, en], [tf.int64, tf.int64])</span><br></pre></td></tr></table></figure>

<h3 id="构造数据集"><a href="#构造数据集" class="headerlink" title="构造数据集"></a>构造数据集</h3><p>In [9]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">BUFFER_SIZE &#x3D; 20000</span><br><span class="line">BATCH_SIZE &#x3D; 64</span><br><span class="line"></span><br><span class="line"># 使用.map()运行相关图操作</span><br><span class="line">train_dataset &#x3D; train_examples.map(tf_encode)</span><br><span class="line"># 过滤过长的数据</span><br><span class="line">train_dataset &#x3D; train_dataset.filter(filter_long_sent) # 这个函数为什么不加（） ？？？</span><br><span class="line"># 使用缓存数据加速读入</span><br><span class="line">train_dataset &#x3D; train_dataset.cache()</span><br><span class="line"># 打乱并获取批数据</span><br><span class="line">train_dataset &#x3D; train_dataset.padded_batch(</span><br><span class="line">BATCH_SIZE, padded_shapes&#x3D;([40], [40]))  # 填充为最大长度-90</span><br><span class="line"># 设置预取数据</span><br><span class="line">train_dataset &#x3D; train_dataset.prefetch(tf.data.experimental.AUTOTUNE)</span><br><span class="line"></span><br><span class="line"># 验证集数据</span><br><span class="line">val_dataset &#x3D; val_examples.map(tf_encode)</span><br><span class="line">val_dataset &#x3D; val_dataset.filter(filter_long_sent).padded_batch(</span><br><span class="line">BATCH_SIZE, padded_shapes&#x3D;([40], [40]))</span><br></pre></td></tr></table></figure>

<p>In [10]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">de_batch, en_batch &#x3D; next(iter(train_dataset))</span><br><span class="line">de_batch, en_batch</span><br></pre></td></tr></table></figure>

<p>Out[10]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(&lt;tf.Tensor: id&#x3D;311363, shape&#x3D;(64, 40), dtype&#x3D;int64, numpy&#x3D;</span><br><span class="line"> array([[8214,  116,   84, ...,    0,    0,    0],</span><br><span class="line">        [8214,    7,  261, ...,    0,    0,    0],</span><br><span class="line">        [8214,  155,   39, ...,    0,    0,    0],</span><br><span class="line">        ...,</span><br><span class="line">        [8214,  639,  590, ...,    0,    0,    0],</span><br><span class="line">        [8214,  204, 3441, ...,    0,    0,    0],</span><br><span class="line">        [8214,   27,   13, ...,    0,    0,    0]])&gt;,</span><br><span class="line"> &lt;tf.Tensor: id&#x3D;311364, shape&#x3D;(64, 40), dtype&#x3D;int64, numpy&#x3D;</span><br><span class="line"> array([[8087,   83,  145, ...,    0,    0,    0],</span><br><span class="line">        [8087, 4670, 1783, ...,    0,    0,    0],</span><br><span class="line">        [8087,  169,   56, ...,    0,    0,    0],</span><br><span class="line">        ...,</span><br><span class="line">        [8087,  174,   79, ...,    0,    0,    0],</span><br><span class="line">        [8087,   11,   16, ...,    0,    0,    0],</span><br><span class="line">        [8087,    4,   12, ...,    0,    0,    0]])&gt;)</span><br></pre></td></tr></table></figure>

<h2 id="2-位置嵌入"><a href="#2-位置嵌入" class="headerlink" title="2.位置嵌入"></a>2.位置嵌入</h2><p>将位置编码矢量添加得到词嵌入，相同位置的词嵌入将会更接近，但并不能直接编码相对位置</p>
<p>基于角度的位置编码方法如下：</p>
<p><img src="https://render.githubusercontent.com/render/math?math=%5CLarge%7BPE_%7B%28pos%2C%202i%29%7D%20%3D%20sin%28pos%20%2F%2010000%5E%7B2i%20%2F%20d_%7Bmodel%7D%7D%29%7D%20%24%24%24%24%5CLarge%7BPE_%7B%28pos%2C%202i%2B1%29%7D%20%3D%20cos%28pos%20%2F%2010000%5E%7B2i%20%2F%20d_%7Bmodel%7D%7D%29%7D&mode=display" alt="$$\Large{PE_{(pos, 2i)} = sin(pos / 10000^{2i / d_{model}})} $$$$\Large{PE_{(pos, 2i+1)} = cos(pos / 10000^{2i / d_{model}})} $$"></p>
<p>得到角度</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def get_angles(pos, i, d_model):</span><br><span class="line">    # 这里的i等价与上面公式中的2i和2i+1</span><br><span class="line">    angle_rates &#x3D; 1 &#x2F; np.power(10000, (2*(i &#x2F;&#x2F; 2))&#x2F; np.float32(d_model))</span><br><span class="line">    return pos * angle_rates</span><br></pre></td></tr></table></figure>

<p>In [12]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def positional_encoding(position, d_model):</span><br><span class="line">	#numpy.arange(start, stop, step, dtype &#x3D; None) np.arange(position) &#x3D; [0,...,position-1]</span><br><span class="line">    angle_rads &#x3D; get_angles(np.arange(position)[:, np.newaxis],   # shape(position,1)</span><br><span class="line">                           np.arange(d_model)[np.newaxis,:],  # shape(1,d_model)</span><br><span class="line">                           d_model)</span><br><span class="line">    # 第2i项使用sin</span><br><span class="line">    sines &#x3D; np.sin(angle_rads[:, 0::2])  #从0开始，步长为2</span><br><span class="line">    # 第2i+1项使用cos</span><br><span class="line">    cones &#x3D; np.cos(angle_rads[:, 1::2]) #从1开始，步长为2</span><br><span class="line">    pos_encoding &#x3D; np.concatenate([sines, cones], axis&#x3D;-1)</span><br><span class="line">    pos_encoding &#x3D; pos_encoding[np.newaxis, ...]</span><br><span class="line">    </span><br><span class="line">    return tf.cast(pos_encoding, dtype&#x3D;tf.float32)</span><br></pre></td></tr></table></figure>



<p>获得位置嵌入编码 </p>
<p>position： 一句话中某个token的位置（0-50）， 如果一句话不够50个单词，那么就需要padding</p>
<p>depth ： 对每一个token进行编码 d_model：512维 </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pos_encoding &#x3D; positional_encoding(50, 512)</span><br><span class="line">print(pos_encoding.shape)</span><br><span class="line"></span><br><span class="line">plt.pcolormesh(pos_encoding[0], cmap&#x3D;&#39;RdBu&#39;)</span><br><span class="line">plt.xlabel(&#39;Depth&#39;)</span><br><span class="line">plt.xlim((0, 512))</span><br><span class="line">plt.ylabel(&#39;Position&#39;)</span><br><span class="line">plt.colorbar()</span><br><span class="line">plt.show() # 在这里左右边分别为原来2i 和 2i+1的特征</span><br><span class="line">(1, 50, 512)</span><br></pre></td></tr></table></figure>

<p><img src="data:image/png;base64,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%0A" alt="img"></p>
<h2 id="3-掩码"><a href="#3-掩码" class="headerlink" title="3.掩码"></a>3.掩码</h2><p>为了避免输入中padding的token对句子语义的影响，需要将<code>padding位mark掉，原来为0的padding项的mark输出为1</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def create_padding_mark(seq):</span><br><span class="line">    # 获取为0的padding项</span><br><span class="line">    seq &#x3D; tf.cast(tf.math.equal(seq, 0), tf.float32) #将seq的编码逐个与0比较，相等为1不等为0</span><br><span class="line">    </span><br><span class="line">    # 扩充维度以便用于attention矩阵</span><br><span class="line">    return seq[:, np.newaxis, np.newaxis, :] # (batch_size,1,1,seq_len)</span><br><span class="line"></span><br><span class="line"># mark 测试</span><br><span class="line">create_padding_mark([[1,2,0,0,3],[3,4,5,0,0]])</span><br></pre></td></tr></table></figure>



<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;tf.Tensor: id&#x3D;311819, shape&#x3D;(2, 1, 1, 5), dtype&#x3D;float32, numpy&#x3D;</span><br><span class="line">array([[[[0., 0., 1., 1., 0.]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">       [[[0., 0., 0., 1., 1.]]]], dtype&#x3D;float32)&gt;</span><br></pre></td></tr></table></figure>

<p>look-ahead mask 用于对未预测的token进行掩码 这意味着要预测第三个单词，只会使用第一个和第二个单词。 要预测第四个单词，仅使用第一个，第二个和第三个单词，依此类推。 ？？？？？？</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def create_look_ahead_mark(size):</span><br><span class="line">    # 1 - 对角线和取下三角的全部对角线（-1-&gt;全部）</span><br><span class="line">    # 这样就可以构造出每个时刻未预测token的掩码</span><br><span class="line">    mark &#x3D; 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)</span><br><span class="line">    return mark  # (seq_len, seq_len)</span><br></pre></td></tr></table></figure>

<p>In [16]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># x &#x3D; tf.random.uniform((1,3))</span><br><span class="line">temp &#x3D; create_look_ahead_mark(3)</span><br><span class="line">print(temp)</span><br><span class="line">tf.Tensor(</span><br><span class="line">[[0. 1. 1.]</span><br><span class="line"> [0. 0. 1.]</span><br><span class="line"> [0. 0. 0.]], shape&#x3D;(3, 3), dtype&#x3D;float32)</span><br></pre></td></tr></table></figure>

<h2 id="4-Scaled-dot-product-attention"><a href="#4-Scaled-dot-product-attention" class="headerlink" title="4.Scaled dot product attention"></a>4.Scaled dot product attention</h2><p><img src="https://camo.githubusercontent.com/22371733ed7ffa9065a60e80d3b83b10d832072a/68747470733a2f2f7777772e74656e736f72666c6f772e6f72672f696d616765732f7475746f7269616c732f7472616e73666f726d65722f7363616c65645f617474656e74696f6e2e706e67" alt="img">进行attention计算的时候有3个输入 Q (query), K (key), V (value)。计算公式如下：<img src="https://render.githubusercontent.com/render/math?math=%5CLarge%7BAttention%28Q%2C%20K%2C%20V%29%20%3D%20softmax_k%28%5Cfrac%7BQK%5ET%7D%7B%5Csqrt%7Bd_k%7D%7D%29%20V%7D&mode=display" alt="$$\Large{Attention(Q, K, V) = softmax_k(\frac{QK^T}{\sqrt{d_k}}) V} $$"></p>
<p>点积注意力通过深度d_k的平方根进行缩放,因为较大的深度会使点积变大，由于使用softmax，会使梯度变小。 例如，考虑Q和K的均值为0且方差为1.它们的矩阵乘法的均值为0，方差为dk。我们使用dk的根用于缩放（而不是任何其他数字），因为Q和K的matmul应该具有0的均值和1的方差。</p>
<p>在这里我们将<code>被掩码的token乘以-1e9(表示负无穷)</code>,这样softmax之后就为0,不对其他token产生影响。</p>
<p>In [17]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def scaled_dot_product_attention(q, k, v, mask):</span><br><span class="line">    # query key 相乘获取匹配关系</span><br><span class="line">    matmul_qk &#x3D; tf.matmul(q, k, transpose_b&#x3D;True)  # transpose_b&#x3D;True表示b项在运算前要进行转置运算</span><br><span class="line">    </span><br><span class="line">    # 使用dk进行缩放</span><br><span class="line">    dk &#x3D; tf.cast(tf.shape(k)[-1], tf.float32)</span><br><span class="line">    scaled_attention_logits &#x3D; matmul_qk &#x2F; tf.math.sqrt(dk)</span><br><span class="line">    </span><br><span class="line">    # 掩码</span><br><span class="line">    if mask is not None:</span><br><span class="line">        scaled_attention_logits +&#x3D; (mask * -1e9)</span><br><span class="line">        </span><br><span class="line">    # 通过softmax获取attention权重</span><br><span class="line">    attention_weights &#x3D; tf.nn.softmax(scaled_attention_logits, axis&#x3D;-1)</span><br><span class="line">    </span><br><span class="line">    # attention 乘上value</span><br><span class="line">    output &#x3D; tf.matmul(attention_weights, v) # （.., seq_len_v, depth）</span><br><span class="line">    </span><br><span class="line">    return output, attention_weights</span><br></pre></td></tr></table></figure>

<p>使用attention获取需要关注的语义</p>
<p>In [18]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def print_out(q, k, v):</span><br><span class="line">    temp_out, temp_att &#x3D; scaled_dot_product_attention(</span><br><span class="line">    q, k, v, None)</span><br><span class="line">    print(&#39;attention weight:&#39;)</span><br><span class="line">    print(temp_att)</span><br><span class="line">    print(&#39;output:&#39;)</span><br><span class="line">    print(temp_out)</span><br></pre></td></tr></table></figure>

<p>attention测试</p>
<p>In [19]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 显示为numpy类型</span><br><span class="line">np.set_printoptions(suppress&#x3D;True)</span><br><span class="line"></span><br><span class="line">temp_k &#x3D; tf.constant([[10,0,0],</span><br><span class="line">                      [0,10,0],</span><br><span class="line">                      [0,0,10],</span><br><span class="line">                      [0,0,10]], dtype&#x3D;tf.float32)  # (4, 3)</span><br><span class="line"></span><br><span class="line">temp_v &#x3D; tf.constant([[   1,0],</span><br><span class="line">                      [  10,0],</span><br><span class="line">                      [ 100,5],</span><br><span class="line">                      [1000,6]], dtype&#x3D;tf.float32)  # (4, 3)</span><br><span class="line"># 关注第2个key, 返回对应的value</span><br><span class="line">temp_q &#x3D; tf.constant([[0,10,0]], dtype&#x3D;tf.float32)</span><br><span class="line">print_out(temp_q, temp_k, temp_v)</span><br><span class="line">attention weight:</span><br><span class="line">tf.Tensor([[0. 1. 0. 0.]], shape&#x3D;(1, 4), dtype&#x3D;float32)</span><br><span class="line">output:</span><br><span class="line">tf.Tensor([[10.  0.]], shape&#x3D;(1, 2), dtype&#x3D;float32)</span><br></pre></td></tr></table></figure>

<p>In [20]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 关注重复的key(第3、4个), 返回对应的value（平均）</span><br><span class="line">temp_q &#x3D; tf.constant([[0,0,10]], dtype&#x3D;tf.float32)</span><br><span class="line">print_out(temp_q, temp_k, temp_v)</span><br><span class="line">attention weight:</span><br><span class="line">tf.Tensor([[0.  0.  0.5 0.5]], shape&#x3D;(1, 4), dtype&#x3D;float32)</span><br><span class="line">output:</span><br><span class="line">tf.Tensor([[550.    5.5]], shape&#x3D;(1, 2), dtype&#x3D;float32)</span><br></pre></td></tr></table></figure>

<p>In [21]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 关注第1、2个key, 返回对应的value（平均）</span><br><span class="line">temp_q &#x3D; tf.constant([[10,10,0]], dtype&#x3D;tf.float32)</span><br><span class="line">print_out(temp_q, temp_k, temp_v)</span><br><span class="line">attention weight:</span><br><span class="line">tf.Tensor([[0.5 0.5 0.  0. ]], shape&#x3D;(1, 4), dtype&#x3D;float32)</span><br><span class="line">output:</span><br><span class="line">tf.Tensor([[5.5 0. ]], shape&#x3D;(1, 2), dtype&#x3D;float32)</span><br></pre></td></tr></table></figure>

<p>In [22]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 依次放入每个query</span><br><span class="line">temp_q &#x3D; tf.constant([[0, 0, 10], [0, 10, 0], [10, 10, 0]], dtype&#x3D;tf.float32)  # (3, 3)</span><br><span class="line">print_out(temp_q, temp_k, temp_v)</span><br><span class="line">attention weight:</span><br><span class="line">tf.Tensor(</span><br><span class="line">[[0.  0.  0.5 0.5]</span><br><span class="line"> [0.  1.  0.  0. ]</span><br><span class="line"> [0.5 0.5 0.  0. ]], shape&#x3D;(3, 4), dtype&#x3D;float32)</span><br><span class="line">output:</span><br><span class="line">tf.Tensor(</span><br><span class="line">[[550.    5.5]</span><br><span class="line"> [ 10.    0. ]</span><br><span class="line"> [  5.5   0. ]], shape&#x3D;(3, 2), dtype&#x3D;float32)</span><br></pre></td></tr></table></figure>

<h2 id="5-Mutil-Head-Attention"><a href="#5-Mutil-Head-Attention" class="headerlink" title="5.Mutil-Head Attention"></a>5.Mutil-Head Attention</h2><p><img src="https://camo.githubusercontent.com/5464abc4535579fdb88afb7117617f6f23ff50af/68747470733a2f2f7777772e74656e736f72666c6f772e6f72672f696d616765732f7475746f7269616c732f7472616e73666f726d65722f6d756c74695f686561645f617474656e74696f6e2e706e67" alt="img"></p>
<p>mutil-head attention包含3部分：</p>
<ul>
<li>线性层与分头</li>
<li>缩放点积注意力</li>
<li>头连接</li>
<li>末尾线性层</li>
</ul>
<p>每个多头注意块有三个输入; Q（查询），K（密钥），V（值）。 它们<code>通过第一层线性层并分成多个头</code>。</p>
<p>注意:点积注意力时需要使用mask， 多头输出需要使用tf.transpose调整各维度。</p>
<p>Q，K和V不是一个单独的注意头，而是分成多个头，因为<code>它允许模型共同参与来自不同表征空间的不同信息。 在拆分之后，每个头部具有降低的维度，总计算成本与具有全维度的单个头部注意力相同。</code></p>
<p>python 中<code>assert断言</code>是声明其布尔值必须为真的判定，如果发生异常就说明表达式为假。可以理解assert断言语句为<strong>raise-if-not</strong>，用来测试表示式，其返回值为假，则会抛出AssertError并且包含错误信息。如果它为真，就不做任何事</p>
<p>“//“不管两者出现任何数，都以整除结果为准，不对小数部分进行处理，直接抛弃，也就是整除法</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 构造mutil head attention层</span><br><span class="line">class MutilHeadAttention(tf.keras.layers.Layer):</span><br><span class="line">    def __init__(self, d_model, num_heads):</span><br><span class="line">        super(MutilHeadAttention, self).__init__()</span><br><span class="line">        self.num_heads &#x3D; num_heads</span><br><span class="line">        self.d_model &#x3D; d_model</span><br><span class="line">        </span><br><span class="line">        # d_model 必须可以正确分为各个头</span><br><span class="line">        assert d_model % num_heads &#x3D;&#x3D; 0</span><br><span class="line">        # 分头后的维度</span><br><span class="line">        self.depth &#x3D; d_model &#x2F;&#x2F; num_heads</span><br><span class="line">        </span><br><span class="line">        self.wq &#x3D; tf.keras.layers.Dense(d_model)</span><br><span class="line">        self.wk &#x3D; tf.keras.layers.Dense(d_model)</span><br><span class="line">        self.wv &#x3D; tf.keras.layers.Dense(d_model)</span><br><span class="line">        </span><br><span class="line">        self.dense &#x3D; tf.keras.layers.Dense(d_model)</span><br><span class="line">        </span><br><span class="line">    def split_heads(self, x, batch_size):</span><br><span class="line">        # 分头, 将头个数的维度 放到 seq_len 前面</span><br><span class="line">        x &#x3D; tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))</span><br><span class="line">        return tf.transpose(x, perm&#x3D;[0, 2, 1, 3])</span><br><span class="line">    </span><br><span class="line">    def call(self, v, k, q, mask):</span><br><span class="line">        batch_size &#x3D; tf.shape(q)[0]</span><br><span class="line">        </span><br><span class="line">        # 分头前的前向网络，获取q、k、v语义</span><br><span class="line">        q &#x3D; self.wq(q)  # (batch_size, seq_len, d_model)</span><br><span class="line">        k &#x3D; self.wk(k)</span><br><span class="line">        v &#x3D; self.wv(v)</span><br><span class="line">        </span><br><span class="line">        # 分头</span><br><span class="line">        q &#x3D; self.split_heads(q, batch_size) # (batch_size, num_heads, seq_len_q, depth)</span><br><span class="line">        k &#x3D; self.split_heads(k, batch_size)</span><br><span class="line">        v &#x3D; self.split_heads(v, batch_size)</span><br><span class="line">        # scaled_attention.shape &#x3D;&#x3D; (batch_size, num_heads, seq_len_v, depth)</span><br><span class="line">        # attention_weights.shape &#x3D;&#x3D; (batch_size, num_heads, seq_len_q, seq_len_k)</span><br><span class="line">        </span><br><span class="line">        # 通过缩放点积注意力层</span><br><span class="line">        scaled_attention, attention_weights &#x3D; scaled_dot_product_attention(</span><br><span class="line">        q, k, v, mask)</span><br><span class="line">        # 把多头维度后移</span><br><span class="line">        scaled_attention &#x3D; tf.transpose(scaled_attention, [0, 2, 1, 3]) # (batch_size, seq_len_v, num_heads, depth)</span><br><span class="line"></span><br><span class="line">        # 合并多头</span><br><span class="line">        concat_attention &#x3D; tf.reshape(scaled_attention, </span><br><span class="line">                                      (batch_size, -1, self.d_model))</span><br><span class="line">        </span><br><span class="line">        # 全连接重塑</span><br><span class="line">        output &#x3D; self.dense(concat_attention)</span><br><span class="line">        return output, attention_weights</span><br></pre></td></tr></table></figure>

<p>测试多头attention</p>
<p>In [24]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">temp_mha &#x3D; MutilHeadAttention(d_model&#x3D;512, num_heads&#x3D;8)</span><br><span class="line">y &#x3D; tf.random.uniform((1, 60, 512))</span><br><span class="line">output, att &#x3D; temp_mha(y, k&#x3D;y, q&#x3D;y, mask&#x3D;None)</span><br><span class="line">print(output.shape, att.shape)</span><br><span class="line">(1, 60, 512) (1, 8, 60, 60)</span><br></pre></td></tr></table></figure>

<p>point wise前向网络</p>
<p>In [25]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def point_wise_feed_forward_network(d_model, diff):</span><br><span class="line">    return tf.keras.Sequential([</span><br><span class="line">        tf.keras.layers.Dense(diff, activation&#x3D;&#39;relu&#39;),</span><br><span class="line">        tf.keras.layers.Dense(d_model)</span><br><span class="line">    ])</span><br></pre></td></tr></table></figure>

<p>In [26]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sample_fnn &#x3D; point_wise_feed_forward_network(512, 2048)</span><br><span class="line">sample_fnn(tf.random.uniform((64, 50, 512))).shape</span><br></pre></td></tr></table></figure>

<p>Out[26]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">TensorShape([64, 50, 512])</span><br></pre></td></tr></table></figure>

<h2 id="6-编码器和解码器"><a href="#6-编码器和解码器" class="headerlink" title="6.编码器和解码器"></a>6.编码器和解码器</h2><p><img src="https://camo.githubusercontent.com/ce5d9e0b508ffa8c84c943258c677b7576f35757/68747470733a2f2f7777772e74656e736f72666c6f772e6f72672f696d616765732f7475746f7269616c732f7472616e73666f726d65722f7472616e73666f726d65722e706e67" alt="img"></p>
<ul>
<li>通过N个编码器层，为序列中的每个字/令牌生成输出。</li>
<li>解码器连接编码器的输出和它自己的输入（自我注意）以预测下一个字。</li>
</ul>
<h3 id="编码层"><a href="#编码层" class="headerlink" title="编码层"></a>编码层</h3><p>每个编码层包含以下子层</p>
<ul>
<li>Multi-head attention（带掩码）</li>
<li>Point wise feed forward networks</li>
</ul>
<p>每个子层中都有残差连接，并最后通过一个正则化层。残差连接有助于避免深度网络中的梯度消失问题。 每个子层输出是LayerNorm(x + Sublayer(x))，规范化是在d_model维的向量上。Transformer一共有n个编码层。</p>
<p>In [27]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class LayerNormalization(tf.keras.layers.Layer):</span><br><span class="line">    def __init__(self, epsilon&#x3D;1e-6, **kwargs):</span><br><span class="line">        self.eps &#x3D; epsilon</span><br><span class="line">        super(LayerNormalization, self).__init__(**kwargs)</span><br><span class="line">    def build(self, input_shape):</span><br><span class="line">        self.gamma &#x3D; self.add_weight(name&#x3D;&#39;gamma&#39;, shape&#x3D;input_shape[-1:],</span><br><span class="line">                                     initializer&#x3D;tf.ones_initializer(), trainable&#x3D;True)</span><br><span class="line">        self.beta &#x3D; self.add_weight(name&#x3D;&#39;beta&#39;, shape&#x3D;input_shape[-1:],</span><br><span class="line">                                    initializer&#x3D;tf.zeros_initializer(), trainable&#x3D;True)</span><br><span class="line">        super(LayerNormalization, self).build(input_shape)</span><br><span class="line">    def call(self, x):</span><br><span class="line">        mean &#x3D; tf.keras.backend.mean(x, axis&#x3D;-1, keepdims&#x3D;True)</span><br><span class="line">        std &#x3D; tf.keras.backend.std(x, axis&#x3D;-1, keepdims&#x3D;True)</span><br><span class="line">        return self.gamma * (x - mean) &#x2F; (std + self.eps) + self.beta</span><br><span class="line">    def compute_output_shape(self, input_shape):</span><br><span class="line">        return input_shape</span><br></pre></td></tr></table></figure>

<p>In [28]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class EncoderLayer(tf.keras.layers.Layer):</span><br><span class="line">    def __init__(self, d_model, n_heads, ddf, dropout_rate&#x3D;0.1):</span><br><span class="line">        super(EncoderLayer, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.mha &#x3D; MutilHeadAttention(d_model, n_heads)</span><br><span class="line">        self.ffn &#x3D; point_wise_feed_forward_network(d_model, ddf)</span><br><span class="line">        </span><br><span class="line">        self.layernorm1 &#x3D; LayerNormalization(epsilon&#x3D;1e-6)</span><br><span class="line">        self.layernorm2 &#x3D; LayerNormalization(epsilon&#x3D;1e-6)</span><br><span class="line">        </span><br><span class="line">        self.dropout1 &#x3D; tf.keras.layers.Dropout(dropout_rate)</span><br><span class="line">        self.dropout2 &#x3D; tf.keras.layers.Dropout(dropout_rate)</span><br><span class="line">        </span><br><span class="line">    def call(self, inputs, training, mask):</span><br><span class="line">        # 多头注意力网络</span><br><span class="line">        att_output, _ &#x3D; self.mha(inputs, inputs, inputs, mask)</span><br><span class="line">        att_output &#x3D; self.dropout1(att_output, training&#x3D;training)</span><br><span class="line">        out1 &#x3D; self.layernorm1(inputs + att_output)  # (batch_size, input_seq_len, d_model)</span><br><span class="line">        # 前向网络</span><br><span class="line">        ffn_output &#x3D; self.ffn(out1)</span><br><span class="line">        ffn_output &#x3D; self.dropout2(ffn_output, training&#x3D;training)</span><br><span class="line">        out2 &#x3D; self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)</span><br><span class="line">        return out2</span><br></pre></td></tr></table></figure>

<p>encoder层测试</p>
<p>In [29]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sample_encoder_layer &#x3D; EncoderLayer(512, 8, 2048)</span><br><span class="line">sample_encoder_layer_output &#x3D; sample_encoder_layer(</span><br><span class="line">tf.random.uniform((64, 43, 512)), False, None)</span><br><span class="line"></span><br><span class="line">sample_encoder_layer_output.shape</span><br></pre></td></tr></table></figure>

<p>Out[29]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">TensorShape([64, 43, 512])</span><br></pre></td></tr></table></figure>

<h3 id="解码层"><a href="#解码层" class="headerlink" title="解码层"></a>解码层</h3><p>每个编码层包含以下子层：</p>
<ul>
<li>Masked muti-head attention（带padding掩码和look-ahead掩码）</li>
<li>Muti-head attention（带padding掩码）value和key来自encoder输出，query来自Masked muti-head attention层输出</li>
<li>Point wise feed forward network</li>
</ul>
<p>每个子层中都有残差连接，并最后通过一个正则化层。残差连接有助于避免深度网络中的梯度消失问题。</p>
<p>每个子层输出是LayerNorm(x + Sublayer(x))，规范化是在d_model维的向量上。Transformer一共有n个解码层。</p>
<p>当Q从解码器的第一个注意块接收输出，并且K接收编码器输出时，注意权重表示基于编码器输出给予解码器输入的重要性。 换句话说，解码器通过查看编码器输出并自我关注其自己的输出来预测下一个字。</p>
<p>ps：因为padding在后面所以look-ahead掩码同时掩padding</p>
<p>In [30]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class DecoderLayer(tf.keras.layers.Layer):</span><br><span class="line">    def __init__(self, d_model, num_heads, dff, drop_rate&#x3D;0.1):</span><br><span class="line">        super(DecoderLayer, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.mha1 &#x3D; MutilHeadAttention(d_model, num_heads)</span><br><span class="line">        self.mha2 &#x3D; MutilHeadAttention(d_model, num_heads)</span><br><span class="line">        </span><br><span class="line">        self.ffn &#x3D; point_wise_feed_forward_network(d_model, dff)</span><br><span class="line">        </span><br><span class="line">        self.layernorm1 &#x3D; LayerNormalization(epsilon&#x3D;1e-6)</span><br><span class="line">        self.layernorm2 &#x3D; LayerNormalization(epsilon&#x3D;1e-6)</span><br><span class="line">        self.layernorm3 &#x3D; LayerNormalization(epsilon&#x3D;1e-6)</span><br><span class="line">        </span><br><span class="line">        self.dropout1 &#x3D; layers.Dropout(drop_rate)</span><br><span class="line">        self.dropout2 &#x3D; layers.Dropout(drop_rate)</span><br><span class="line">        self.dropout3 &#x3D; layers.Dropout(drop_rate)</span><br><span class="line">        </span><br><span class="line">    def call(self,inputs, encode_out, training, </span><br><span class="line">             look_ahead_mask, padding_mask):</span><br><span class="line">        # masked muti-head attention</span><br><span class="line">        att1, att_weight1 &#x3D; self.mha1(inputs, inputs, inputs,look_ahead_mask)</span><br><span class="line">        att1 &#x3D; self.dropout1(att1, training&#x3D;training)</span><br><span class="line">        out1 &#x3D; self.layernorm1(inputs + att1)</span><br><span class="line">        # muti-head attention</span><br><span class="line">        att2, att_weight2 &#x3D; self.mha2(encode_out, encode_out, inputs, padding_mask)</span><br><span class="line">        att2 &#x3D; self.dropout2(att2, training&#x3D;training)</span><br><span class="line">        out2 &#x3D; self.layernorm2(out1 + att2)</span><br><span class="line">        </span><br><span class="line">        ffn_out &#x3D; self.ffn(out2)</span><br><span class="line">        ffn_out &#x3D; self.dropout3(ffn_out, training&#x3D;training)</span><br><span class="line">        out3 &#x3D; self.layernorm3(out2 + ffn_out)</span><br><span class="line">        </span><br><span class="line">        return out3, att_weight1, att_weight2</span><br></pre></td></tr></table></figure>

<p>测试解码层</p>
<p>In [31]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sample_decoder_layer &#x3D; DecoderLayer(512, 8, 2048)</span><br><span class="line"></span><br><span class="line">sample_decoder_layer_output, _, _ &#x3D; sample_decoder_layer(</span><br><span class="line">tf.random.uniform((64, 50, 512)), sample_encoder_layer_output,</span><br><span class="line">    False, None, None)</span><br><span class="line">sample_decoder_layer_output.shape</span><br></pre></td></tr></table></figure>

<p>Out[31]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">TensorShape([64, 50, 512])</span><br></pre></td></tr></table></figure>

<h3 id="编码器"><a href="#编码器" class="headerlink" title="编码器"></a>编码器</h3><p>编码器包含：</p>
<ul>
<li>Input Embedding</li>
<li>Positional Embedding</li>
<li>N个编码层</li>
</ul>
<p>In [32]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Encoder(layers.Layer):</span><br><span class="line">    def __init__(self, n_layers, d_model, n_heads, ddf,</span><br><span class="line">                input_vocab_size, max_seq_len, drop_rate&#x3D;0.1):</span><br><span class="line">        super(Encoder, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.n_layers &#x3D; n_layers</span><br><span class="line">        self.d_model &#x3D; d_model</span><br><span class="line">        </span><br><span class="line">        self.embedding &#x3D; layers.Embedding(input_vocab_size, d_model)</span><br><span class="line">        self.pos_embedding &#x3D; positional_encoding(max_seq_len, d_model)</span><br><span class="line">        </span><br><span class="line">        self.encode_layer &#x3D; [EncoderLayer(d_model, n_heads, ddf, drop_rate)</span><br><span class="line">                            for _ in range(n_layers)]</span><br><span class="line">        </span><br><span class="line">        self.dropout &#x3D; layers.Dropout(drop_rate)</span><br><span class="line">    def call(self, inputs, training, mark):</span><br><span class="line">        </span><br><span class="line">        seq_len &#x3D; inputs.shape[1]</span><br><span class="line">        word_emb &#x3D; self.embedding(inputs)</span><br><span class="line">        word_emb *&#x3D; tf.math.sqrt(tf.cast(self.d_model, tf.float32))</span><br><span class="line">        emb &#x3D; word_emb + self.pos_embedding[:,:seq_len,:]</span><br><span class="line">        x &#x3D; self.dropout(emb, training&#x3D;training)</span><br><span class="line">        for i in range(self.n_layers):</span><br><span class="line">            x &#x3D; self.encode_layer[i](x, training, mark)</span><br><span class="line">        </span><br><span class="line">        return x</span><br></pre></td></tr></table></figure>

<p>编码器测试</p>
<p>In [33]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sample_encoder &#x3D; Encoder(2, 512, 8, 1024, 5000, 200)</span><br><span class="line">sample_encoder_output &#x3D; sample_encoder(tf.random.uniform((64, 120)),</span><br><span class="line">                                      False, None)</span><br><span class="line">sample_encoder_output.shape</span><br></pre></td></tr></table></figure>

<p>Out[33]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">TensorShape([64, 120, 512])</span><br></pre></td></tr></table></figure>

<h3 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h3><p>解码器包含以下部分：1、输出嵌入；2、位置编码；3、n个解码层</p>
<p>输出嵌入和位置编码叠加后输入解码器，解码器最后的输出送给一个全连接</p>
<p>In [34]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># import pdb</span><br><span class="line"># pdb.set_trace()</span><br><span class="line">class Decoder(layers.Layer):</span><br><span class="line">    def __init__(self, n_layers, d_model, n_heads, ddf,</span><br><span class="line">                target_vocab_size, max_seq_len, drop_rate&#x3D;0.1):</span><br><span class="line">        super(Decoder, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.d_model &#x3D; d_model</span><br><span class="line">        self.n_layers &#x3D; n_layers</span><br><span class="line">        </span><br><span class="line">        self.embedding &#x3D; layers.Embedding(target_vocab_size, d_model)</span><br><span class="line">        self.pos_embedding &#x3D; positional_encoding(max_seq_len, d_model)</span><br><span class="line">        </span><br><span class="line">        self.decoder_layers&#x3D; [DecoderLayer(d_model, n_heads, ddf, drop_rate)</span><br><span class="line">                             for _ in range(n_layers)]</span><br><span class="line">        </span><br><span class="line">        self.dropout &#x3D; layers.Dropout(drop_rate)</span><br><span class="line">        </span><br><span class="line">    def call(self, inputs, encoder_out,training,</span><br><span class="line">             look_ahead_mark, padding_mark):</span><br><span class="line">    </span><br><span class="line">        seq_len &#x3D; tf.shape(inputs)[1]</span><br><span class="line">        attention_weights &#x3D; &#123;&#125;</span><br><span class="line">        h &#x3D; self.embedding(inputs)</span><br><span class="line">        h *&#x3D; tf.math.sqrt(tf.cast(self.d_model, tf.float32))</span><br><span class="line">        h +&#x3D; self.pos_embedding[:,:seq_len,:]</span><br><span class="line">        </span><br><span class="line">        h &#x3D; self.dropout(h, training&#x3D;training)</span><br><span class="line">#         print(&#39;--------------------\n&#39;,h, h.shape)</span><br><span class="line">        # 叠加解码层</span><br><span class="line">        for i in range(self.n_layers):</span><br><span class="line">            h, att_w1, att_w2 &#x3D; self.decoder_layers[i](h, encoder_out,</span><br><span class="line">                                                   training, look_ahead_mark,</span><br><span class="line">                                                   padding_mark)</span><br><span class="line">            attention_weights[&#39;decoder_layer&#123;&#125;_att_w1&#39;.format(i+1)] &#x3D; att_w1</span><br><span class="line">            attention_weights[&#39;decoder_layer&#123;&#125;_att_w2&#39;.format(i+1)] &#x3D; att_w2</span><br><span class="line">        </span><br><span class="line">        return h, attention_weights</span><br></pre></td></tr></table></figure>

<p>解码器测试</p>
<p>In [35]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sample_decoder &#x3D; Decoder(2, 512,8,1024,5000, 200)</span><br><span class="line">sample_decoder_output, attn &#x3D; sample_decoder(tf.random.uniform((64, 100)),</span><br><span class="line">                                            sample_encoder_output, False,</span><br><span class="line">                                            None, None)</span><br><span class="line">sample_decoder_output.shape, attn[&#39;decoder_layer1_att_w1&#39;].shape</span><br></pre></td></tr></table></figure>

<p>Out[35]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(TensorShape([64, 100, 512]), TensorShape([64, 8, 100, 100]))</span><br></pre></td></tr></table></figure>

<h3 id="创建Transformer"><a href="#创建Transformer" class="headerlink" title="创建Transformer"></a>创建Transformer</h3><p>Transformer包含编码器、解码器和最后的线性层，解码层的输出经过线性层后得到Transformer的输出</p>
<p>In [36]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Transformer(tf.keras.Model):</span><br><span class="line">    def __init__(self, n_layers, d_model, n_heads, diff,</span><br><span class="line">                input_vocab_size, target_vocab_size,</span><br><span class="line">                max_seq_len, drop_rate&#x3D;0.1):</span><br><span class="line">        super(Transformer, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.encoder &#x3D; Encoder(n_layers, d_model, n_heads,diff,</span><br><span class="line">                              input_vocab_size, max_seq_len, drop_rate)</span><br><span class="line">        </span><br><span class="line">        self.decoder &#x3D; Decoder(n_layers, d_model, n_heads, diff,</span><br><span class="line">                              target_vocab_size, max_seq_len, drop_rate)</span><br><span class="line">        </span><br><span class="line">        self.final_layer &#x3D; tf.keras.layers.Dense(target_vocab_size)</span><br><span class="line">    def call(self, inputs, targets, training, encode_padding_mask, </span><br><span class="line">            look_ahead_mask, decode_padding_mask):</span><br><span class="line">        </span><br><span class="line">        encode_out &#x3D; self.encoder(inputs, training, encode_padding_mask)</span><br><span class="line">        print(encode_out.shape)</span><br><span class="line">        decode_out, att_weights &#x3D; self.decoder(targets, encode_out, training, </span><br><span class="line">                                               look_ahead_mask, decode_padding_mask)</span><br><span class="line">        print(decode_out.shape)</span><br><span class="line">        final_out &#x3D; self.final_layer(decode_out)</span><br><span class="line">        </span><br><span class="line">        return final_out, att_weights</span><br></pre></td></tr></table></figure>

<p>Transformer测试</p>
<p>In [37]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sample_transformer &#x3D; Transformer(</span><br><span class="line">n_layers&#x3D;2, d_model&#x3D;512, n_heads&#x3D;8, diff&#x3D;1024,</span><br><span class="line">input_vocab_size&#x3D;8500, target_vocab_size&#x3D;8000, max_seq_len&#x3D;120</span><br><span class="line">)</span><br><span class="line">temp_input &#x3D; tf.random.uniform((64, 62))</span><br><span class="line">temp_target &#x3D; tf.random.uniform((64, 26))</span><br><span class="line">fn_out, _ &#x3D; sample_transformer(temp_input, temp_target, training&#x3D;False,</span><br><span class="line">                              encode_padding_mask&#x3D;None,</span><br><span class="line">                               look_ahead_mask&#x3D;None,</span><br><span class="line">                               decode_padding_mask&#x3D;None,</span><br><span class="line">                              )</span><br><span class="line">fn_out.shape</span><br><span class="line">(64, 62, 512)</span><br><span class="line">(64, 26, 512)</span><br></pre></td></tr></table></figure>

<p>Out[37]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">TensorShape([64, 26, 8000])</span><br></pre></td></tr></table></figure>

<h2 id="7-实验设置"><a href="#7-实验设置" class="headerlink" title="7.实验设置"></a>7.实验设置</h2><h3 id="设置超参"><a href="#设置超参" class="headerlink" title="设置超参"></a>设置超参</h3><p>In [38]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">num_layers &#x3D; 4</span><br><span class="line">d_model &#x3D; 128</span><br><span class="line">dff &#x3D; 512</span><br><span class="line">num_heads &#x3D; 8</span><br><span class="line"></span><br><span class="line">input_vocab_size &#x3D; tokenizer_pt.vocab_size + 2</span><br><span class="line">target_vocab_size &#x3D; tokenizer_en.vocab_size + 2</span><br><span class="line">max_seq_len &#x3D; 40</span><br><span class="line">dropout_rate &#x3D; 0.1</span><br></pre></td></tr></table></figure>

<h3 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h3><p>带自定义学习率调整的Adam优化器<img src="https://render.githubusercontent.com/render/math?math=%5CLarge%7Blrate%20%3D%20d_%7Bmodel%7D%5E%7B-0.5%7D%20%2A%20min%28step%7B%5C_%7Dnum%5E%7B-0.5%7D%2C%20step%7B%5C_%7Dnum%20%2A%20warmup%7B%5C_%7Dsteps%5E%7B-1.5%7D%29%7D&mode=display" alt="$$\Large{lrate = d_{model}^{-0.5} * min(step{\_}num^{-0.5}, step{\_}num * warmup{\_}steps^{-1.5})}$$"></p>
<p>In [39]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):</span><br><span class="line">    def __init__(self, d_model, warmup_steps&#x3D;4000):</span><br><span class="line">        super(CustomSchedule, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.d_model &#x3D; tf.cast(d_model, tf.float32)</span><br><span class="line">        self.warmup_steps &#x3D; warmup_steps</span><br><span class="line">    </span><br><span class="line">    def __call__(self, step):</span><br><span class="line">        arg1 &#x3D; tf.math.rsqrt(step)</span><br><span class="line">        arg2 &#x3D; step * (self.warmup_steps ** -1.5)</span><br><span class="line">        </span><br><span class="line">        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)</span><br></pre></td></tr></table></figure>

<p>In [40]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">learing_rate &#x3D; CustomSchedule(d_model)</span><br><span class="line">optimizer &#x3D; tf.keras.optimizers.Adam(learing_rate, beta_1&#x3D;0.9, </span><br><span class="line">                                    beta_2&#x3D;0.98, epsilon&#x3D;1e-9)</span><br></pre></td></tr></table></figure>

<p>In [41]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 测试</span><br><span class="line">temp_learing_rate &#x3D; CustomSchedule(d_model)</span><br><span class="line">plt.plot(temp_learing_rate(tf.range(40000, dtype&#x3D;tf.float32)))</span><br><span class="line">plt.xlabel(&#39;learning rate&#39;)</span><br><span class="line">plt.ylabel(&#39;train step&#39;)</span><br></pre></td></tr></table></figure>

<p>Out[41]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Text(0, 0.5, &#39;train step&#39;)</span><br></pre></td></tr></table></figure>

<p><img src="data:image/png;base64,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%0A" alt="img"></p>
<h3 id="损失和指标"><a href="#损失和指标" class="headerlink" title="损失和指标"></a>损失和指标</h3><p>由于目标序列是填充的，因此在计算损耗时应用填充掩码很重要。 padding的掩码为0，没padding的掩码为1</p>
<p>In [42]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">loss_object &#x3D; tf.keras.losses.SparseCategoricalCrossentropy(from_logits&#x3D;True,</span><br><span class="line">                                                           reduction&#x3D;&#39;none&#39;)</span><br><span class="line"></span><br><span class="line">def loss_fun(y_ture, y_pred):</span><br><span class="line">    mask &#x3D; tf.math.logical_not(tf.math.equal(y_ture, 0))  # 为0掩码标1</span><br><span class="line">    loss_ &#x3D; loss_object(y_ture, y_pred)</span><br><span class="line">    </span><br><span class="line">    mask &#x3D; tf.cast(mask, dtype&#x3D;loss_.dtype)</span><br><span class="line">    loss_ *&#x3D; mask</span><br><span class="line">    return tf.reduce_mean(loss_)</span><br></pre></td></tr></table></figure>

<p>In [43]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">train_loss &#x3D; tf.keras.metrics.Mean(name&#x3D;&#39;train_loss&#39;)</span><br><span class="line">train_accuracy &#x3D; tf.keras.metrics.SparseCategoricalAccuracy(name&#x3D;&#39;train_accuracy&#39;)</span><br></pre></td></tr></table></figure>

<h2 id="8、训练和保持模型"><a href="#8、训练和保持模型" class="headerlink" title="8、训练和保持模型"></a>8、训练和保持模型</h2><p>In [44]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">transformer &#x3D; Transformer(num_layers, d_model, num_heads, dff,</span><br><span class="line">                          input_vocab_size, target_vocab_size,</span><br><span class="line">                          max_seq_len, dropout_rate)</span><br></pre></td></tr></table></figure>

<p>In [45]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 构建掩码</span><br><span class="line">def create_mask(inputs,targets):</span><br><span class="line">    encode_padding_mask &#x3D; create_padding_mark(inputs)</span><br><span class="line">    # 这个掩码用于掩输入解码层第二层的编码层输出</span><br><span class="line">    decode_padding_mask &#x3D; create_padding_mark(inputs)</span><br><span class="line">    </span><br><span class="line">    # look_ahead 掩码， 掩掉未预测的词</span><br><span class="line">    look_ahead_mask &#x3D; create_look_ahead_mark(tf.shape(targets)[1])</span><br><span class="line">    # 解码层第一层得到padding掩码</span><br><span class="line">    decode_targets_padding_mask &#x3D; create_padding_mark(targets)</span><br><span class="line">    </span><br><span class="line">    # 合并解码层第一层掩码</span><br><span class="line">    combine_mask &#x3D; tf.maximum(decode_targets_padding_mask, look_ahead_mask)</span><br><span class="line">    </span><br><span class="line">    return encode_padding_mask, combine_mask, decode_padding_mask</span><br></pre></td></tr></table></figure>

<p>创建checkpoint管理器</p>
<p>In [46]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">checkpoint_path &#x3D; &#39;.&#x2F;checkpoint&#x2F;train&#39;</span><br><span class="line">ckpt &#x3D; tf.train.Checkpoint(transformer&#x3D;transformer,</span><br><span class="line">                          optimizer&#x3D;optimizer)</span><br><span class="line"># ckpt管理器</span><br><span class="line">ckpt_manager &#x3D; tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep&#x3D;3)</span><br><span class="line"></span><br><span class="line">if ckpt_manager.latest_checkpoint:</span><br><span class="line">    ckpt.restore(ckpt_manager.latest_checkpoint)</span><br><span class="line">    print(&#39;last checkpoit restore&#39;)</span><br></pre></td></tr></table></figure>

<p>target分为target_input和target real. target_input是传给解码器的输入，target_real是其左移一个位置的结果，每个target_input位置对应下一个预测的标签</p>
<p>如句子=“SOS A丛林中的狮子正在睡觉EOS”</p>
<p>target_input =“SOS丛林中的狮子正在睡觉”</p>
<p>target_real =“丛林中的狮子正在睡觉EOS”</p>
<p>transformer是个自动回归模型：它一次预测一个部分，并使用其到目前为止的输出，决定下一步做什么。</p>
<p>在训练期间使用teacher-forcing，即无论模型当前输出什么都强制将正确输出传给下一步。</p>
<p>而预测时则根据前一个的输出预测下一个词</p>
<p>为防止模型在预期输出处达到峰值，模型使用look-ahead mask</p>
<p>In [47]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">@tf.function</span><br><span class="line">def train_step(inputs, targets):</span><br><span class="line">    tar_inp &#x3D; targets[:,:-1]</span><br><span class="line">    tar_real &#x3D; targets[:,1:]</span><br><span class="line">    # 构造掩码</span><br><span class="line">    encode_padding_mask, combined_mask, decode_padding_mask &#x3D; create_mask(inputs, tar_inp)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    with tf.GradientTape() as tape:</span><br><span class="line">        predictions, _ &#x3D; transformer(inputs, tar_inp,</span><br><span class="line">                                    True,</span><br><span class="line">                                    encode_padding_mask,</span><br><span class="line">                                    combined_mask,</span><br><span class="line">                                    decode_padding_mask)</span><br><span class="line">        loss &#x3D; loss_fun(tar_real, predictions)</span><br><span class="line">    # 求梯度</span><br><span class="line">    gradients &#x3D; tape.gradient(loss, transformer.trainable_variables)</span><br><span class="line">    # 反向传播</span><br><span class="line">    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))</span><br><span class="line">    </span><br><span class="line">    # 记录loss和准确率</span><br><span class="line">    train_loss(loss)</span><br><span class="line">    train_accuracy(tar_real, predictions)</span><br></pre></td></tr></table></figure>

<p>葡萄牙语用作输入语言，英语是目标语言。</p>
<p>In [48]:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">EPOCHS &#x3D; 20</span><br><span class="line">for epoch in range(EPOCHS):</span><br><span class="line">    start &#x3D; time.time()</span><br><span class="line">    </span><br><span class="line">    # 重置记录项</span><br><span class="line">    train_loss.reset_states()</span><br><span class="line">    train_accuracy.reset_states()</span><br><span class="line">    </span><br><span class="line">    # inputs 葡萄牙语， targets英语</span><br><span class="line">    </span><br><span class="line">    for batch, (inputs, targets) in enumerate(train_dataset):</span><br><span class="line">        # 训练</span><br><span class="line">        train_step(inputs, targets)</span><br><span class="line">        </span><br><span class="line">        if batch % 500 &#x3D;&#x3D; 0:</span><br><span class="line">            print(&#39;epoch &#123;&#125;, batch &#123;&#125;, loss:&#123;:.4f&#125;, acc:&#123;:.4f&#125;&#39;.format(</span><br><span class="line">            epoch+1, batch, train_loss.result(), train_accuracy.result()</span><br><span class="line">            ))</span><br><span class="line">            </span><br><span class="line">    if (epoch + 1) % 2 &#x3D;&#x3D; 0:</span><br><span class="line">        ckpt_save_path &#x3D; ckpt_manager.save()</span><br><span class="line">        print(&#39;epoch &#123;&#125;, save model at &#123;&#125;&#39;.format(</span><br><span class="line">        epoch+1, ckpt_save_path</span><br><span class="line">        ))</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    print(&#39;epoch &#123;&#125;, loss:&#123;:.4f&#125;, acc:&#123;:.4f&#125;&#39;.format(</span><br><span class="line">    epoch+1, train_loss.result(), train_accuracy.result()</span><br><span class="line">    ))</span><br><span class="line">    </span><br><span class="line">    print(&#39;time in 1 epoch:&#123;&#125; secs\n&#39;.format(time.time()-start))</span><br><span class="line">(64, 40, 128)</span><br><span class="line">(64, 39, 128)</span><br><span class="line">(64, 40, 128)</span><br><span class="line">(64, 39, 128)</span><br><span class="line">epoch 1, batch 0, loss:4.0259, acc:0.0000</span><br><span class="line">epoch 1, batch 500, loss:3.4436, acc:0.0340</span><br><span class="line">(31, 40, 128)</span><br><span class="line">(31, 39, 128)</span><br><span class="line">epoch 1, loss:3.2112, acc:0.0481</span><br><span class="line">time in 1 epoch:467.3876633644104 secs</span><br><span class="line"></span><br><span class="line">epoch 2, batch 0, loss:2.4443, acc:0.0982</span><br><span class="line">epoch 2, batch 500, loss:2.3006, acc:0.1139</span><br><span class="line">epoch 2, save model at .&#x2F;checkpoint&#x2F;train&#x2F;ckpt-1</span><br><span class="line">epoch 2, loss:2.2473, acc:0.1184</span><br><span class="line">time in 1 epoch:429.6356120109558 secs</span><br><span class="line"></span><br><span class="line">epoch 3, batch 0, loss:2.0709, acc:0.1306</span><br><span class="line">epoch 3, batch 500, loss:2.0279, acc:0.1412</span><br><span class="line">epoch 3, loss:1.9927, acc:0.1443</span><br><span class="line">time in 1 epoch:426.3838963508606 secs</span><br><span class="line"></span><br><span class="line">epoch 4, batch 0, loss:1.8720, acc:0.1571</span><br><span class="line">epoch 4, batch 500, loss:1.8020, acc:0.1678</span><br><span class="line">epoch 4, save model at .&#x2F;checkpoint&#x2F;train&#x2F;ckpt-2</span><br><span class="line">epoch 4, loss:1.7664, acc:0.1714</span><br><span class="line">time in 1 epoch:387.37333059310913 secs</span><br><span class="line"></span><br><span class="line">epoch 5, batch 0, loss:1.6616, acc:0.1807</span><br><span class="line">epoch 5, batch 500, loss:1.5908, acc:0.1936</span><br><span class="line">epoch 5, loss:1.5610, acc:0.1961</span><br><span class="line">time in 1 epoch:389.60524225234985 secs</span><br><span class="line"></span><br><span class="line">epoch 6, batch 0, loss:1.4435, acc:0.2087</span><br><span class="line">epoch 6, batch 500, loss:1.4117, acc:0.2127</span><br><span class="line">epoch 6, save model at .&#x2F;checkpoint&#x2F;train&#x2F;ckpt-3</span><br><span class="line">epoch 6, loss:1.3852, acc:0.2147</span><br><span class="line">time in 1 epoch:438.03212571144104 secs</span><br><span class="line"></span><br><span class="line">epoch 7, batch 0, loss:1.3070, acc:0.2183</span><br><span class="line">epoch 7, batch 500, loss:1.2383, acc:0.2320</span><br><span class="line">epoch 7, loss:1.2111, acc:0.2346</span><br><span class="line">time in 1 epoch:378.90994358062744 secs</span><br><span class="line"></span><br><span class="line">epoch 8, batch 0, loss:1.1545, acc:0.2332</span><br><span class="line">epoch 8, batch 500, loss:1.0854, acc:0.2508</span><br><span class="line">epoch 8, save model at .&#x2F;checkpoint&#x2F;train&#x2F;ckpt-4</span><br><span class="line">epoch 8, loss:1.0658, acc:0.2525</span><br><span class="line">time in 1 epoch:377.7305886745453 secs</span><br><span class="line"></span><br><span class="line">epoch 9, batch 0, loss:1.0109, acc:0.2532</span><br><span class="line">epoch 9, batch 500, loss:0.9780, acc:0.2645</span><br><span class="line">epoch 9, loss:0.9624, acc:0.2656</span><br><span class="line">time in 1 epoch:378.3708670139313 secs</span><br><span class="line"></span><br><span class="line">epoch 10, batch 0, loss:0.9245, acc:0.2576</span><br><span class="line">epoch 10, batch 500, loss:0.8940, acc:0.2749</span><br><span class="line">epoch 10, save model at .&#x2F;checkpoint&#x2F;train&#x2F;ckpt-5</span><br><span class="line">epoch 10, loss:0.8820, acc:0.2757</span><br><span class="line">time in 1 epoch:378.5437033176422 secs</span><br><span class="line"></span><br><span class="line">epoch 11, batch 0, loss:0.8609, acc:0.2728</span><br><span class="line">epoch 11, batch 500, loss:0.8305, acc:0.2833</span><br><span class="line">epoch 11, loss:0.8222, acc:0.2835</span><br><span class="line">time in 1 epoch:378.56798672676086 secs</span><br><span class="line"></span><br><span class="line">epoch 12, batch 0, loss:0.8031, acc:0.2821</span><br><span class="line">epoch 12, batch 500, loss:0.7770, acc:0.2905</span><br><span class="line">epoch 12, save model at .&#x2F;checkpoint&#x2F;train&#x2F;ckpt-6</span><br><span class="line">epoch 12, loss:0.7700, acc:0.2906</span><br><span class="line">time in 1 epoch:378.8077425956726 secs</span><br><span class="line"></span><br><span class="line">epoch 13, batch 0, loss:0.7457, acc:0.2857</span><br><span class="line">epoch 13, batch 500, loss:0.7311, acc:0.2971</span><br><span class="line">epoch 13, loss:0.7257, acc:0.2969</span><br><span class="line">time in 1 epoch:378.34697437286377 secs</span><br><span class="line"></span><br><span class="line">epoch 14, batch 0, loss:0.7159, acc:0.2925</span><br><span class="line">epoch 14, batch 500, loss:0.6931, acc:0.3026</span><br><span class="line">epoch 14, save model at .&#x2F;checkpoint&#x2F;train&#x2F;ckpt-7</span><br><span class="line">epoch 14, loss:0.6874, acc:0.3025</span><br><span class="line">time in 1 epoch:379.3904767036438 secs</span><br><span class="line"></span><br><span class="line">epoch 15, batch 0, loss:0.6885, acc:0.2905</span><br><span class="line">epoch 15, batch 500, loss:0.6594, acc:0.3072</span><br><span class="line">epoch 15, loss:0.6546, acc:0.3070</span><br><span class="line">time in 1 epoch:377.10075068473816 secs</span><br><span class="line"></span><br><span class="line">epoch 16, batch 0, loss:0.6465, acc:0.2961</span><br><span class="line">epoch 16, batch 500, loss:0.6306, acc:0.3117</span><br><span class="line">epoch 16, save model at .&#x2F;checkpoint&#x2F;train&#x2F;ckpt-8</span><br><span class="line">epoch 16, loss:0.6257, acc:0.3115</span><br><span class="line">time in 1 epoch:379.0886535644531 secs</span><br><span class="line"></span><br><span class="line">epoch 17, batch 0, loss:0.6033, acc:0.3021</span><br><span class="line">epoch 17, batch 500, loss:0.6023, acc:0.3162</span><br><span class="line">epoch 17, loss:0.5984, acc:0.3159</span><br><span class="line">time in 1 epoch:377.6911520957947 secs</span><br><span class="line"></span><br><span class="line">epoch 18, batch 0, loss:0.5469, acc:0.3225</span><br><span class="line">epoch 18, batch 500, loss:0.5791, acc:0.3195</span><br><span class="line">epoch 18, save model at .&#x2F;checkpoint&#x2F;train&#x2F;ckpt-9</span><br><span class="line">epoch 18, loss:0.5755, acc:0.3191</span><br><span class="line">time in 1 epoch:378.4746241569519 secs</span><br><span class="line"></span><br><span class="line">epoch 19, batch 0, loss:0.5287, acc:0.3209</span><br><span class="line">epoch 19, batch 500, loss:0.5575, acc:0.3229</span><br><span class="line">epoch 19, loss:0.5546, acc:0.3224</span><br><span class="line">time in 1 epoch:378.284138917923 secs</span><br><span class="line"></span><br><span class="line">epoch 20, batch 0, loss:0.5182, acc:0.3193</span><br><span class="line">epoch 20, batch 500, loss:0.5374, acc:0.3263</span><br><span class="line">epoch 20, save model at .&#x2F;checkpoint&#x2F;train&#x2F;ckpt-10</span><br><span class="line">epoch 20, loss:0.5344, acc:0.3257</span><br><span class="line">time in 1 epoch:377.9467544555664 secs</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>transformer</category>
      </categories>
      <tags>
        <tag>transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-07-23-深度学习杂记</title>
    <url>/2020/07/23/2020-07-23-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h3 id="logits"><a href="#logits" class="headerlink" title="logits"></a>logits</h3><p>在tensorflow代码中，经常会出现logits，logits表示的含义就是在模型的最后一层输出后，进入到Softmax函数之前得到的n维向量。是feature的抽象。 </p>
<p>logits是未归一化的概率， 一般也就是 softmax层的输入。所以logits和lables的shape一样</p>
<p>而softmax是在一个n分类问题中，输入一个n维的logits向量，输出一个n维概率向量，其物理意义是logits代表的物体属于各类的概率。是对logits进行归一化。</p>
<img src="https://i.loli.net/2020/07/23/DT72yXZg4Om1xLM.png" alt="image-20200723160820328" style="zoom:50%;" />

<p>输入softmax的Logits中最大的一维会成为输出中同样最大的一维。例如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">4分类问题的</span><br><span class="line">Logits &#x3D; [1, 5, 1, 0.2]</span><br><span class="line">输入softmax后</span><br><span class="line">得到的 one_hot_pred &#x3D; [0.017 0.958 0.017 0.008]</span><br></pre></td></tr></table></figure>





<h3 id="beam-search"><a href="#beam-search" class="headerlink" title="beam search"></a>beam search</h3><p>在sequence2sequence模型中，beam search的方法只用在测试的情况，因为在训练过程中，每一个decoder的输出是有正确答案的，也就不需要beam search去加大输出的准确率。</p>
<p><strong>我们需要翻译中文“我是中国人”—&gt;英文“I am Chinese”</strong></p>
<p>假设我们的词表大小只有三个单词就是I am Chinese。那么如果我们的beam size为2的话，我们现在来解释,</p>
<p>如下图所示，我们在decoder的过程中，有了beam search方法后，在第一次的输出，我们选取概率最大的”I”和”am”两个单词，而不是只挑选一个概率最大的单词。</p>
<p><img src="https://i.loli.net/2020/07/24/32grlE4uLGpQPCq.png" alt=""></p>
<p>然后接下来我们要做的就是，把“I”单词作为下一个decoder的输入算一遍得到y2的输出概率分布，把“am”单词作为下一个decoder的输入算一遍也得到y2的输出概率分布。</p>
<p>比如将“I”单词作为下一个decoder的输入算一遍得到y2的输出概率分布如下：</p>
<p><img src="https://i.loli.net/2020/07/24/s6OErw5yT41uLPa.png" alt="image-20200724095714528"></p>
<p>比如将“am”单词作为下一个decoder的输入算一遍得到y2的输出概率分布如下：</p>
<p><img src="https://i.loli.net/2020/07/24/Ng1ZyrxehmpUIoC.png" alt="image-20200724095747262"></p>
<p>那么此时我们由于我们的beam size为2，也就是我们只能保留概率最大的两个序列，此时我们可以计算所有的序列概率：</p>
<p>“I I” = 0.4<em>0.3 “I am” = 0.4</em>0.6 “I Chinese” = 0.4*0.1</p>
<p>“am I” = 0.5<em>0.3 “am am” = 0.5</em>0.3 “am Chinese” = 0.5*0.4</p>
<p>我们很容易得出俩个最大概率的序列为 “I am”和“am Chinese”，然后后面会不断重复这个过程，直到遇到结束符为止。</p>
<p><strong>最终输出2个得分最高的序列。</strong></p>
<p><strong>这就是seq2seq中的beam search算法过程</strong></p>
<h3 id="TPU-tensorflow"><a href="#TPU-tensorflow" class="headerlink" title="TPU  -tensorflow"></a>TPU  -tensorflow</h3><p>在神经网络学习过程中，需要进行矩阵运算，包括大量的加法和乘法，所以关键点是我们该如何快速执行大型矩阵运算，同时还需要更小的能耗。</p>
<h4 id="与CPU和GPU的对比"><a href="#与CPU和GPU的对比" class="headerlink" title="与CPU和GPU的对比"></a>与CPU和GPU的对比</h4><p>CPU：CPU 非常灵活，硬件无法一直了解下一个计算是什么，直到它读取了软件的下一个指令。</p>
<p>​    缺点:每一个 CPU 的算术逻辑单元（ALU，控制乘法器和加法器的组件）都只能一个接一个地执行它们，每一次都需要访问内存，限制了总体吞吐量，并需要大量的能耗。</p>
<p>GPU：在单个处理器中使用成千上万个 ALU。现代 GPU 通常在单个处理器中拥有 2500-5000 个 ALU，意味着你可以同时执行数千次乘法和加法运算。在<code>并行化</code>的应用中很好，比如神经网络的矩阵乘法。</p>
<p>​    缺点：因为 GPU 在其 ALU 上执行更多的并行计算，它也会成比例地耗费更多的能量来访问内存，同时也因为复杂的线路而增加 GPU 的物理空间占用。</p>
<h4 id="TPU的工作特点"><a href="#TPU的工作特点" class="headerlink" title="TPU的工作特点"></a>TPU的工作特点</h4><p>TPU 不能运行文本处理软件、控制火箭引擎或执行银行业务，但它们可以为神经网络处理大量的乘法和加法运算，同时 TPU 的速度非常快、能耗非常小且物理空间占用也更小。常用于<code>加速神经网络</code></p>
<p>TPU 可以在神经网络运算上达到高计算吞吐量，同时能耗和物理空间都很小。</p>
<h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4><blockquote>
<p> TPU 加速深度学习    <a href="https://www.ednchina.com/news/201809041331.html" target="_blank" rel="noopener">https://www.ednchina.com/news/201809041331.html</a> </p>
<p>PPT 解释了 TPU 的特性与定义    tpudemo.com</p>
</blockquote>
]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-07-22-如何有效读论文-思维导图</title>
    <url>/2020/07/22/2020-07-22-%E5%A6%82%E4%BD%95%E6%9C%89%E6%95%88%E8%AF%BB%E8%AE%BA%E6%96%87-%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>论文</tag>
      </tags>
  </entry>
  <entry>
    <title>2020-07-22-每周论文分享</title>
    <url>/2020/07/22/2020-07-22-%E6%AF%8F%E5%91%A8%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>论文</tag>
      </tags>
  </entry>
  <entry>
    <title>linux &amp; vim 常用操作-初级</title>
    <url>/2020/07/22/2020-07-22-%20linux&amp;vim%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>跑实验需要用服务器，远程服务器我用的是XShell + FileZilla进行连接和传输文件，然而在操控linux服务器中需要用到一些常见的linux命令，之前学过《鸟哥linux》，但是一直没有总结过，现在总结下。</p>
<h2 id="linux-常用命令"><a href="#linux-常用命令" class="headerlink" title="linux 常用命令"></a>linux 常用命令</h2><h3 id="ls-命令"><a href="#ls-命令" class="headerlink" title="ls 命令"></a>ls 命令</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ls  -a   <span class="comment"># 列出目录所有文件，包含以.开始的隐藏文件</span></span><br><span class="line"></span><br><span class="line">ls ~  <span class="comment">#进入主目录</span></span><br><span class="line"></span><br><span class="line">ls E: <span class="comment"># 在win中进入E盘</span></span><br></pre></td></tr></table></figure>



<h3 id="cd-目录名"><a href="#cd-目录名" class="headerlink" title="cd [目录名]"></a>cd [目录名]</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cd ..   <span class="comment">#返回上一级目录</span></span><br><span class="line"></span><br><span class="line">cd . /temp  <span class="comment"># 当前目录下的temp文件夹</span></span><br></pre></td></tr></table></figure>



<h3 id="pwd-命令"><a href="#pwd-命令" class="headerlink" title="pwd 命令"></a>pwd 命令</h3><p>查看当前工作目录路径</p>
<h3 id="mkdir-命令"><a href="#mkdir-命令" class="headerlink" title="mkdir 命令"></a>mkdir 命令</h3><p>用于创建文件夹</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mkdir temp  <span class="comment">#当前工作目录下创建名为 temp的文件夹</span></span><br><span class="line"></span><br><span class="line">mkdir -p /tmp/test/t1/t   <span class="comment"># 在 tmp 目录下创建路径为 test/t1/t 的目录，若不存在，则创建</span></span><br></pre></td></tr></table></figure>





<h3 id="rm-命令"><a href="#rm-命令" class="headerlink" title="rm 命令"></a>rm 命令</h3><p>删除一个目录中的一个或多个文件或目录</p>
<p>如果没有使用 -r 选项，则 rm 不会删除目录。如果使用 rm 来删除文件，通常仍可以将该文件恢复原状。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rm  -i  *.log <span class="comment"># 删除全部.log文件， 删除前逐一询问</span></span><br><span class="line"></span><br><span class="line">rm a.txt <span class="comment"># 删除文件a.txt</span></span><br><span class="line"></span><br><span class="line">rm -rf test <span class="comment"># 删除test文件夹 </span></span><br><span class="line"></span><br><span class="line">​	 <span class="comment">#-r 就是向下递归，不管有多少级目录，一并删除</span></span><br><span class="line"></span><br><span class="line">​	<span class="comment">#-f 就是直接强行删除，不作任何提示的意思</span></span><br></pre></td></tr></table></figure>



<h3 id="vim-命令"><a href="#vim-命令" class="headerlink" title="vim 命令"></a>vim 命令</h3><p>编辑文件</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">vim a.txt  <span class="comment"># 创建a.txt文件并进入编辑状态 。 如果a.txt 已经存在，则直接进入编辑状态</span></span><br><span class="line"></span><br><span class="line"><span class="number">1.</span> 按下i键，下端显示 –INSERT–。可以进行插入，输入文本 </span><br><span class="line"></span><br><span class="line"><span class="number">2.</span> 输入了之后 按Esc键退出编辑状态</span><br><span class="line"></span><br><span class="line"><span class="number">3.</span> 键入 :wq!保存退出  <span class="comment"># 注意是在英文输入状态下进行的操作</span></span><br><span class="line"></span><br><span class="line">    :w 在编辑的过程中保存文件,相当于word中的ctrl+s    </span><br><span class="line"></span><br><span class="line">   :wq 保存文件并退出</span><br></pre></td></tr></table></figure>



<h3 id="less-命令"><a href="#less-命令" class="headerlink" title="less  命令"></a>less  命令</h3><p>有时实验结果较长，在终端前面的输出会无法显示，此命令可以在终端显示上屏无法查看的内容，可以<strong>分页显示</strong>。 </p>
<p>用法： 在执行命令后面加上|less即可，它可以用<strong><code>PageUp</code></strong>和<strong><code>PageDown</code></strong>按键上下翻页，也可以用<strong><code>上下方向键</code></strong>一点点查看。退出按<strong><code>q</code></strong>。</p>
<p>一般显示的结果过长，都可在命令行后加上<strong>|less</strong>来分页显示</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">python main.py | less</span><br></pre></td></tr></table></figure>

<p>less也可直接查看文件</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">less test.txt</span><br></pre></td></tr></table></figure>



<h3 id="重定向"><a href="#重定向" class="headerlink" title="重定向"></a>重定向</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">python main.py &gt;test.txt  <span class="number">2</span>&gt;&amp;<span class="number">1</span> </span><br><span class="line"></span><br><span class="line"><span class="comment">#表示将标准输出（STDOUT）重定向到test.txt文件</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#2&gt;&amp;1 ：把标准输出和标准错误一起重定向到一个文件中。1是标准输出的文件描述符，2是标准错误的文件描述符</span></span><br></pre></td></tr></table></figure>



<h3 id="cat-命令"><a href="#cat-命令" class="headerlink" title="cat 命令"></a>cat 命令</h3><p>可以直接查看a.txt 文件</p>
<h3 id="cp-命令"><a href="#cp-命令" class="headerlink" title="cp 命令"></a>cp 命令</h3><p>复制文件/文件夹</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cp [option] Source Directory</span><br></pre></td></tr></table></figure>

<p>1.如果要复制的源目录中还存在子目录，此时使用选项R递归地复制子目录。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cp -r test file/  <span class="comment">#将目录test复制到目录file中</span></span><br></pre></td></tr></table></figure>

<p>2..复制并重命名文件</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cp /etc/samba/smb.conf  smb.backup  <span class="comment">#将/etc/samba/smb.conf备份到当前目录中，并将文件重命名 smb.backup</span></span><br></pre></td></tr></table></figure>



<h3 id="swp文件"><a href="#swp文件" class="headerlink" title=".swp文件"></a>.swp文件</h3><p>非正常关闭vi/vim编辑器时（如不小心按下<code>ctrl+Z</code>强制退出）会生成一个.swp文件。 </p>
<h4 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h4><ul>
<li>使用vim -r a.txt 来恢复文件</li>
<li>然后在提示中删除.swp文件即可，以后就不会有提示了</li>
</ul>
<h3 id="Shell-Bash-退出码"><a href="#Shell-Bash-退出码" class="headerlink" title="Shell Bash 退出码"></a>Shell Bash 退出码</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">exit <span class="number">0</span> :也就是说调用环境就认为你的这个程序执行正确</span><br><span class="line">exit <span class="number">1</span> :一般是出错定义这个<span class="number">1</span>，也可以是其他数字，很多系统程序这个错误编号是有约定的含义的。 </span><br><span class="line">但不为<span class="number">0</span> 就表示程序运行出错。 </span><br><span class="line">exit <span class="number">127</span>: command <span class="keyword">not</span> found <span class="comment">#指令输入错误</span></span><br></pre></td></tr></table></figure>



<h3 id="ps-ef-grep-查找进程"><a href="#ps-ef-grep-查找进程" class="headerlink" title="ps -ef | grep 查找进程"></a>ps -ef | grep 查找进程</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ps -ef | grep main.py  <span class="comment">#其中main.py是要查找的关键字</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ps命令将某个进程显示出来</span><br><span class="line"></span><br><span class="line">grep命令是查找</span><br><span class="line"></span><br><span class="line">中间的|是管道命令 是指ps命令与grep同时执行</span><br><span class="line"></span><br><span class="line">字段含义如下：</span><br><span class="line">UID    PID    PPID    C   STIME   TTY    TIME     CMD</span><br><span class="line"></span><br><span class="line">zzw   <span class="number">14124</span>  <span class="number">13991</span>   <span class="number">0</span>   <span class="number">00</span>:<span class="number">38</span>   pts/<span class="number">0</span>   <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span>  grep --color=auto dae</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">UID   ：程序被该 UID 所拥有</span><br><span class="line"></span><br><span class="line">PID   ：就是这个程序的 ID </span><br><span class="line"></span><br><span class="line">PPID  ：则是其上级父程序的ID</span><br><span class="line"></span><br><span class="line">C     ：CPU使用的资源百分比</span><br><span class="line"></span><br><span class="line">STIME ：系统启动时间</span><br><span class="line"></span><br><span class="line">TTY   ：登入者的终端机位置</span><br><span class="line"></span><br><span class="line">TIME  ：使用掉的CPU时间。</span><br><span class="line"></span><br><span class="line">CMD  ：所下达的是什么指令</span><br></pre></td></tr></table></figure>



<p>这里是两个shell命令通过管道进行了结合，第一个ps能够列出当前系统所有活跃的进程，然后通过grep 关键字查找就能找到带有关键字的进程。<code>找到PID</code>（PID是输出的第二列那个数字）再杀掉。</p>
<h3 id="服务器常用操作"><a href="#服务器常用操作" class="headerlink" title="服务器常用操作"></a>服务器常用操作</h3><p>具体操作参考《服务器心得》</p>
]]></content>
  </entry>
  <entry>
    <title>Anaconda管理虚拟环境</title>
    <url>/2020/07/21/2020-07-21-Anaconda%E7%AE%A1%E7%90%86%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/</url>
    <content><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>之前一直用的是pycharm上的虚拟环境管理，后来觉得不利于管理。于是开始使用anaconda。</p>
<p>anaconda可以跨平台使用，我安装window版的anaconda，使用它管理虚拟环境。</p>
<h2 id="下载并安装"><a href="#下载并安装" class="headerlink" title="下载并安装"></a>下载并安装</h2><p>到<a href="https://www.anaconda.com/download/" target="_blank" rel="noopener">官网</a>根据自己情况下载好anaconda版本，安装完毕，可通过以下两种方式进入anacon操作环境</p>
<ul>
<li>1.可以增加到path环境变量，打开cmd/powershell，即可操作</li>
<li>2.打开Anaconda Prompt即可操作（推荐）</li>
</ul>
<p>在命令行输入<code>conda --version</code> ，显示出版本号，即安装成功</p>
<h2 id="anaconda管理虚拟环境"><a href="#anaconda管理虚拟环境" class="headerlink" title="anaconda管理虚拟环境"></a>anaconda管理虚拟环境</h2><p>anaconda自带的是base环境，命令行前的（base）说明当前的虚拟环境是base环境 ，我们创建并管理的虚拟环境会放在<code>C:\Users\Administrator\anaconda3\envs</code> 里，</p>
<h3 id="1-创建新的虚拟环境"><a href="#1-创建新的虚拟环境" class="headerlink" title="1.创建新的虚拟环境"></a>1.创建新的虚拟环境</h3><p>为自己的项目配置一个单独的虚拟环境</p>
<p>创建一个名字叫做python36的虚拟环境， 同时指定python的版本，如果本机内没有安装这个版本的python，就会自动下载安装。 后面<code>python==3.6</code>一般可以不添加</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conda create -n python36  python==<span class="number">3.6</span></span><br></pre></td></tr></table></figure>



<h3 id="2-激活虚拟环境"><a href="#2-激活虚拟环境" class="headerlink" title="2. 激活虚拟环境"></a>2. 激活虚拟环境</h3><p>激活进入python36的虚拟环境。如果activate后什么参数都不加，就会进入anaconda自带的base环境</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conda activate python36</span><br></pre></td></tr></table></figure>



<h3 id="3-退出虚拟环境"><a href="#3-退出虚拟环境" class="headerlink" title="3.退出虚拟环境"></a>3.退出虚拟环境</h3><p>在激活新环境的时候要先退出目前的环境至base环境，然后才能activate 新环境，不然代码会bug</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conda deactivate</span><br></pre></td></tr></table></figure>



<h3 id="4-查看所有的虚拟环境"><a href="#4-查看所有的虚拟环境" class="headerlink" title="4.查看所有的虚拟环境"></a>4.查看所有的虚拟环境</h3><p>如果忘记了虚拟环境名称，可以如下命令</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conda env list</span><br></pre></td></tr></table></figure>

<p><img src="https://i.loli.net/2020/07/22/gnUbt2eR7TZvjfy.png" alt="image-20200722093423681"></p>
<p>可以看到目前的虚拟环境状况，共6个虚拟环境。其中<code>*</code>表示当前操作的虚拟环境。</p>
<h3 id="5-安装第三方包"><a href="#5-安装第三方包" class="headerlink" title="5.安装第三方包"></a>5.安装第三方包</h3><p>现在pyhton36的虚拟环境除了python自带的一些官方包之外是没有其他包</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conda install requests <span class="comment">#安装requests包</span></span><br><span class="line">conda install keras==<span class="number">2.2</span><span class="number">.0</span> <span class="comment">#安装指定版本的keras</span></span><br></pre></td></tr></table></figure>

<p>安装完成之后我们输入python进入解释器并import requests包, 好使的.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">anaconda search -t conda tensorflow <span class="comment"># 帮助找tensorflow可安装的包。 找到合适的资源（win64的版本包）  ，按照指示操作即可</span></span><br></pre></td></tr></table></figure>



<h3 id="6-卸载第三方包"><a href="#6-卸载第三方包" class="headerlink" title="6.卸载第三方包"></a>6.卸载第三方包</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conda remove requests</span><br></pre></td></tr></table></figure>



<h3 id="7-查看环境的包信息"><a href="#7-查看环境的包信息" class="headerlink" title="7.查看环境的包信息"></a>7.查看环境的包信息</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conda list <span class="comment"># 查看当前环境的包信息</span></span><br><span class="line">conda list -n python36 <span class="comment">#查看指定环境的包信息</span></span><br></pre></td></tr></table></figure>



<h3 id="8-卸载环境"><a href="#8-卸载环境" class="headerlink" title="8.卸载环境"></a>8.卸载环境</h3><p>卸载test虚拟环境</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conda remove -n test --all</span><br></pre></td></tr></table></figure>



<h3 id="9-conda查看tensorflow和keras的版本"><a href="#9-conda查看tensorflow和keras的版本" class="headerlink" title="9.conda查看tensorflow和keras的版本"></a>9.conda查看tensorflow和keras的版本</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="number">1.</span>进入python解释器</span><br><span class="line"><span class="number">2.</span><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf </span><br><span class="line"><span class="number">3.</span>tf.__version__ <span class="comment">#注意是两个下划线</span></span><br><span class="line"><span class="number">4.</span> exit（） <span class="comment">#退出python操作环境  或者 ctrl + Z</span></span><br><span class="line">(keras 同理）</span><br></pre></td></tr></table></figure>



<h3 id="10-环境包的克隆"><a href="#10-环境包的克隆" class="headerlink" title="10.环境包的克隆"></a>10.环境包的克隆</h3><p>创建一个新的虚拟环境test， 将环境python36信息克隆到test中</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conda create -n   test --clone  python36</span><br></pre></td></tr></table></figure>



<h3 id="11-导入导出环境"><a href="#11-导入导出环境" class="headerlink" title="11.导入导出环境"></a>11.导入导出环境</h3><p>切换到了要导出的环境之后，使用命令将当前环境导出</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conda env export &gt; environment.yml</span><br></pre></td></tr></table></figure>

<p>使用命令建立（导入）新的环境</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conda env create -f environment.yml</span><br></pre></td></tr></table></figure>

<p>注：<br>不过由于不同的操作平台（例如从windows迁移到linux），迁移的时候会报错，找不到安装的包，因为不同平台的包的格式是不一样的，，每个版本号后面的一串字符就类似于手机的序列号，就是指示用于不同环境下的。目前我没有办法进行有效迁移</p>
<p><img src="https://i.loli.net/2020/07/22/MEFRgdAxiuzyhDZ.png" alt="image-20200722102049506"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">补充：anaconda所谓的创建虚拟环境其实就是安装了一个真实的python环境, </span><br><span class="line">只不过我们可以通过activate,conda等命令去随意的切换我们当前的python环境, </span><br><span class="line">用不同版本的解释器和不同的包环境去运行python脚本.</span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">补充：conda、anaconda概念的差别</span><br><span class="line"></span><br><span class="line"><span class="number">1.</span> conda可以理解为一个工具，也是一个可执行命令，其核心功能是包管理与环境管理。</span><br><span class="line">包管理与pip的使用类似，环境管理则允许用户方便地安装不同版本的python并快速切换。</span><br><span class="line"></span><br><span class="line"><span class="number">2.</span> Anaconda是一个打包的集合，里面预装好了conda、某个版本的python、众多packages、</span><br><span class="line">科学计算工具等，也称为Python的一种发行版。</span><br></pre></td></tr></table></figure>



<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>Anaconda详细安装及使用教程：  <a href="https://blog.csdn.net/ITLearnHall/article/details/81708148" target="_blank" rel="noopener">https://blog.csdn.net/ITLearnHall/article/details/81708148</a></p>
<p>Anaconda虚拟环境跨平台迁移：   <a href="https://blog.csdn.net/lixufeng1028/article/details/80669525" target="_blank" rel="noopener">https://blog.csdn.net/lixufeng1028/article/details/80669525</a></p>
<p>不同tensorflow、keras、python的版本对应：  <a href="https://docs.floydhub.com/guides/environments/" target="_blank" rel="noopener">https://docs.floydhub.com/guides/environments/</a></p>
]]></content>
  </entry>
  <entry>
    <title>Transformer原理</title>
    <url>/2020/07/20/2020-07-20-%20Transformer%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<p>本文转自国外一篇讲解transformer的优秀博客<a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">《The Illustrated Transformer》</a>，阅读完之后对于transformer的理解深入了很多，正文如下：</p>
<p>In the <a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" target="_blank" rel="noopener">previous post, we looked at Attention</a> – a ubiquitous method in modern deep learning models. <code>Attention is a concept that helped improve the performance of neural machine translation applications</code>. In this post, we will look at <strong>The Transformer</strong> – a model that uses attention to boost the speed with which these models can be trained. The Transformers outperforms the Google Neural Machine Translation model in specific tasks. <code>The biggest benefit, however, comes from how The Transformer lends itself to parallelization.</code> It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their <a href="https://cloud.google.com/tpu/" target="_blank" rel="noopener">Cloud TPU</a> offering. So let’s try to break the model apart and look at how it functions.</p>
<p>The Transformer was proposed in the paper <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention is All You Need</a>. A TensorFlow implementation of it is available as a part of the <a href="https://github.com/tensorflow/tensor2tensor" target="_blank" rel="noopener">Tensor2Tensor</a> package. Harvard’s NLP group created a <a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">guide annotating the paper with PyTorch implementation</a>. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.</p>
<h2 id="A-High-Level-Look"><a href="#A-High-Level-Look" class="headerlink" title="A High-Level Look"></a>A High-Level Look</h2><p>Let’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.</p>
<p><img src="https://i.loli.net/2020/07/24/MJXH3BE5vi4wFao.png" alt="img"></p>
<p>Popping open that Optimus Prime goodness, we see an encoding component, a decoding component, and connections between them.</p>
<p><img src="https://i.loli.net/2020/07/26/nvqhT56NEYVf9J7.png" alt="img"></p>
<p><code>The encoding component is a stack of encoders</code> (the paper stacks six of them on top of each other – there’s nothing magical about the number six, one can definitely experiment with other arrangements). <code>The decoding component is a stack of decoders of the same number.</code></p>
<p><img src="https://i.loli.net/2020/07/25/2zSYCOeodvq1mMV.png" alt="img"></p>
<p>The encoders are all identical in structure (yet they do not share weights). Each one is broken down into two sub-layers:</p>
<p><img src="https://i.loli.net/2020/07/25/5Aqnfeo9iPQs1Oh.png" alt="img"></p>
<p><code>The encoder’s inputs first flow through a self-attention layer –</code> a layer that helps the encoder look at other words in the input sentence as it encodes a specific word. We’ll look closer at self-attention later in the post.</p>
<p><code>The outputs of the self-attention layer are fed to a feed-forward neural network</code>. The exact same feed-forward network is independently applied to each position.</p>
<p><code>The decoder has both those layers, but between them is an attention layer that helps the decoder focus on relevant parts of the input sentence</code> (similar what attention does in <a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" target="_blank" rel="noopener">seq2seq models</a>).</p>
<p><img src="https://i.loli.net/2020/07/25/Kmc68dxUuqfQRk4.png" alt="img"></p>
<h2 id="Bringing-The-Tensors-Into-The-Picture"><a href="#Bringing-The-Tensors-Into-The-Picture" class="headerlink" title="Bringing The Tensors Into The Picture"></a>Bringing The Tensors Into The Picture</h2><p>Now that we’ve seen the major components of the model, let’s start to look at the various vectors/tensors and how they flow between these components to turn the input of a trained model into an output.</p>
<p>As is the case in NLP applications in general, <code>we begin by turning each input word into a vector using an</code> <a href="https://medium.com/deeper-learning/glossary-of-deep-learning-word-embedding-f90c3cec34ca" target="_blank" rel="noopener">embedding algorithm</a>.</p>
<p><img src="https://i.loli.net/2020/07/25/jkPRN5E2cDTfL4p.png" alt="img"><br><code>Each word is embedded into a vector of size 512</code>. We’ll represent those vectors with these simple boxes.</p>
<p>The embedding only happens in the bottom-most encoder. <code>The abstraction that is common to all the encoders is that they receive a list of vectors each of the size 512 – In the bottom encoder that would be the word embeddings, but in other encoders, it would be the output of the encoder that’s directly below.</code> The size of this list is hyperparameter we can set – basically it would be the length of the longest sentence in our training dataset.</p>
<p>After embedding the words in our input sequence, each of them flows through each of the two layers of the encoder.</p>
<p><img src="https://i.loli.net/2020/07/25/Aj4sHXynq1lOa2x.png" alt="img"></p>
<p>Here we begin to see <code>one key property of the Transformer, which is that the word in each position flows through its own path in the encoder</code>. <code>There are dependencies between these paths</code> in the self-attention layer. The feed-forward layer does not have those dependencies, however, and <code>thus the various paths can be executed in parallel while flowing through the feed-forward layer.</code></p>
<p>Next, we’ll switch up the example to a shorter sentence and we’ll look at what happens in each sub-layer of the encoder.</p>
<h2 id="Now-We’re-Encoding"><a href="#Now-We’re-Encoding" class="headerlink" title="Now We’re Encoding!"></a>Now We’re Encoding!</h2><p>As we’ve mentioned already, an encoder receives a list of vectors as input. It processes this list by passing these vectors into a ‘self-attention’ layer, then into a feed-forward neural network, then sends out the output upwards to the next encoder.</p>
<p><img src="https://i.loli.net/2020/07/25/e34cngFjsyxNvSq.png" alt="img"><br>The word at each position passes through a self-attention process. Then, they each pass through a feed-forward neural network – the exact same network with each vector flowing through it separately.</p>
<h2 id="Self-Attention-at-a-High-Level"><a href="#Self-Attention-at-a-High-Level" class="headerlink" title="Self-Attention at a High Level"></a>Self-Attention at a High Level</h2><p>Don’t be fooled by me throwing around the word “self-attention” like it’s a concept everyone should be familiar with. I had personally never came across the concept until reading the Attention is All You Need paper. Let us distill how it works.</p>
<p>Say the following sentence is an input sentence we want to translate:</p>
<p>”<code>The animal didn&#39;t cross the street because it was too tired</code>”</p>
<p>What does “it” in this sentence refer to? Is it referring to the street or to the animal? It’s a simple question to a human, but not as simple to an algorithm.</p>
<p><code>When the model is processing the word “it”, self-attention allows it to associate “it” with “animal”.</code></p>
<p><code>As the model processes each word (each position in the input sequence), self attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word.</code></p>
<p>If you’re familiar with RNNs, think of how maintaining a hidden state allows an RNN to incorporate its representation of previous words/vectors it has processed with the current one it’s processing. Self-attention is the method the Transformer uses to bake the “understanding” of other relevant words into the one we’re currently processing.</p>
<p><img src="https://i.loli.net/2020/07/25/VB6jDHY9PuLrShx.png" alt="img"><br><code>As we are encoding the word &quot;it&quot; in encoder #5 (the top encoder in the stack), part of the attention mechanism was focusing on &quot;The Animal&quot;, and baked a part of its representation into the encoding of &quot;it&quot;.</code></p>
<p>Be sure to check out the <a href="https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb" target="_blank" rel="noopener">Tensor2Tensor notebook</a> where you can load a Transformer model, and examine it using this interactive visualization.</p>
<h2 id="Self-Attention-in-Detail"><a href="#Self-Attention-in-Detail" class="headerlink" title="Self-Attention in Detail"></a>Self-Attention in Detail</h2><p>Let’s first look at <code>how to calculate self-attention using vectors</code>, then proceed to look at how it’s actually implemented – using matrices.</p>
<p>The <strong>first step</strong> in calculating self-attention is to <code>create three vectors from each of the encoder’s input vectors</code> (in this case, the embedding of each word). So <code>for each word, we create a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that we trained during the training process.</code></p>
<p>Notice that these new vectors are smaller in dimension than the embedding vector. <code>Their dimensionality is 64</code>, while the embedding and encoder input/output vectors have dimensionality of 512. <code>They don’t HAVE to be smaller, this is an architecture choice to make the computation of multiheaded attention (mostly) constant.</code></p>
<p><img src="https://i.loli.net/2020/07/25/aCuE9FmWZyOn2Xs.png" alt="img"><br>Multiplying x1 by the WQ weight matrix produces q1, the “query” vector associated with that word. We end up creating a “query”, a “key”, and a “value” projection of each word in the input sentence.</p>
<p>What are the “query”, “key”, and “value” vectors?</p>
<p>They’re abstractions that are useful for calculating and thinking about attention. Once you proceed with reading how attention is calculated below, you’ll know pretty much all you need to know about the role each of these vectors plays.</p>
<p><code>The **second step** in calculating self-attention is to calculate a score</code>. Say we’re calculating the self-attention for the first word in this example, “Thinking”. We need to score each word of the input sentence against this word. <code>The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position.</code></p>
<p>The score is calculated by taking the dot product of the query vector with the key vector of the respective word we’re scoring. So if we’re processing the self-attention for the word in position #1, the first score would be the dot product of q1 and k1. The second score would be the dot product of q1 and k2.</p>
<p><img src="https://i.loli.net/2020/07/25/xgtf9K4cJkRpbV2.png" alt="img"></p>
<p><code>The **third and forth steps** are to divide the scores by 8</code> (the square root of the dimension of the <code>key vectors</code> used in the paper – 64. This leads to having more stable gradients. There could be other possible values here, but this is the default), <code>then pass the result through a softmax operation. Softmax normalizes the scores so they’re all positive and add up to 1.</code></p>
<p>-</p>
<p><img src="https://i.loli.net/2020/07/25/u7HQAF28ci3oPpq.png" alt="img"></p>
<p>This softmax score determines how much each word will be expressed at this position. <code>Clearly the word at this position will have the highest softmax score, but sometimes it’s useful to attend to another word that is relevant to the current word.</code></p>
<p><code>The **fifth step** is to multiply each value vector by the softmax score</code> (in preparation to sum them up). The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers like 0.001, for example).</p>
<p><code>The **sixth step** is to sum up the weighted value vectors.</code> This produces the output of the self-attention layer at this position (for the first word).</p>
<p><img src="https://i.loli.net/2020/07/25/SNa4Km1EvVpyXIr.png" alt="img"></p>
<p>That concludes the self-attention calculation. <code>The resulting vector is one we can send along to the feed-forward neural network</code>. In the actual implementation, however, <code>this calculation is done in matrix form for faster processing. So let’s look at that now that we’ve seen the intuition of the calculation on the word level.</code></p>
<h2 id="Matrix-Calculation-of-Self-Attention"><a href="#Matrix-Calculation-of-Self-Attention" class="headerlink" title="Matrix Calculation of Self-Attention"></a>Matrix Calculation of Self-Attention</h2><p><strong>The first step</strong> is to calculate the Query, Key, and Value matrices. We do that by packing our embeddings into a matrix X, and multiplying it by the weight matrices we’ve trained (WQ, WK, WV).</p>
<p><img src="https://i.loli.net/2020/07/25/jAwgJ62muho8nvI.png" alt="img"><br>Every row in the X matrix corresponds to a word in the input sentence. We again see the difference in size of the embedding vector (512, or 4 boxes in the figure), and the q/k/v vectors (64, or 3 boxes in the figure)</p>
<p><strong>Finally</strong>, since we’re dealing with matrices, we can condense steps two through six in one formula to calculate the outputs of the self-attention layer.</p>
<p><img src="https://i.loli.net/2020/07/25/PCcj7Rlpn9XHaMF.png" alt="img"><br>The self-attention calculation in matrix form</p>
<h2 id="The-Beast-With-Many-Heads"><a href="#The-Beast-With-Many-Heads" class="headerlink" title="The Beast With Many Heads"></a>The Beast With Many Heads</h2><p><code>The paper further refined the self-attention layer by adding a mechanism called “multi-headed” attention. This improves the performance of the attention layer in two ways:</code></p>
<ol>
<li><code>It expands the model’s ability to focus on different positions</code>. Yes, <code>in the example above, z1 contains a little bit of every other encoding, but it could be dominated by the the actual word itself.</code> It would be useful if we’re translating a sentence like “The animal didn’t cross the street because it was too tired”, we would want to know which word “it” refers to.</li>
<li><code>It gives the attention layer multiple “representation subspaces</code>”. As we’ll see next, <code>with multi-headed attention we have not only one, but multiple sets of Query/Key/Value weight matrices</code> (the Transformer uses eight attention heads, so we end up with eight sets for each encoder/decoder). <code>Each of these sets is randomly initialized. Then, after training, each set is used to project the input embeddings (or vectors from lower encoders/decoders) into a different representation subspace.</code> </li>
</ol>
<p><img src="https://i.loli.net/2020/07/25/7UB4w8oijMvCfyq.png" alt="img"><br>With multi-headed attention, <code>we maintain separate Q/K/V weight matrices for each head resulting in different Q/K/V matrices.</code> As we did before, we multiply X by the WQ/WK/WV matrices to produce Q/K/V matrices.</p>
<p>If we do the same self-attention calculation <code>we outlined above, just eight different times with different weight matrices, we end up with eight different Z matrices</code></p>
<p><img src="https://i.loli.net/2020/07/25/p9WTsRgd2ye7F8I.png" alt="img"></p>
<p><code>This leaves us with a bit of a challenge.</code> The feed-forward layer is not expecting eight matrices – <code>it’s expecting a single matrix (a vector for each word). So we need a way to condense these eight down into a single matrix.</code></p>
<p>How do we do that? <code>We concat the matrices then multiple them by an additional weights matrix WO.</code></p>
<p><img src="https://i.loli.net/2020/07/25/i7bU1pIfDqgZuAx.png" alt="img"></p>
<p>That’s pretty much all there is to multi-headed self-attention. It’s quite a handful of matrices, I realize. Let me try to put them all in one visual so we can look at them in one place</p>
<p><img src="https://i.loli.net/2020/07/25/YIVPms5cNDj2Mpq.png" alt="img"></p>
<p>Now that we have touched upon attention heads, let’s revisit our example from before to see <code>where the different attention heads are focusing as we encode the word “it” in our example sentence:</code></p>
<p><img src="https://i.loli.net/2020/07/25/nXZ4OMkluRFjwJH.png" alt="img"><br>As we encode the word “it”, <code>one attention head is focusing most on &quot;the animal&quot;, while another is focusing on &quot;tired&quot; -- in a sense, the model&#39;s representation of the word &quot;it&quot; bakes in some of the representation of both &quot;animal&quot; and &quot;tired&quot;.</code></p>
<p>If we add all the attention heads to the picture, however, things can be harder to interpret:</p>
<p><img src="https://i.loli.net/2020/07/25/jk2hUsJGM6qxRwD.png" alt="img"></p>
<h2 id="Representing-The-Order-of-The-Sequence-Using-Positional-Encoding"><a href="#Representing-The-Order-of-The-Sequence-Using-Positional-Encoding" class="headerlink" title="Representing The Order of The Sequence Using Positional Encoding"></a>Representing The Order of The Sequence Using <code>Positional Encoding</code></h2><p>One thing that’s missing from the model as we have described it so far is a way to account for the order of the words in the input sequence.</p>
<p>To address this, the transformer adds a vector to each input embedding. <code>These vectors follow a specific pattern that the model learns, which helps it determine the position of each word, or the distance between different words in the sequence.</code> The intuition here is that adding these values to the embeddings provides meaningful distances between the embedding vectors once they’re projected into Q/K/V vectors and during dot-product attention.</p>
<p><img src="https://i.loli.net/2020/07/25/ypGiMmaYjeIo5ct.png" alt="img"><br>To give the model a sense of the order of the words, we add positional encoding vectors – the values of which follow a specific pattern.</p>
<p>If we assumed the <code>embedding has a dimensionality of 4,</code> the actual positional encodings would look like this:</p>
<p><img src="https://i.loli.net/2020/07/25/u3Kn2rIF68DWmtX.png" alt="img"><br>A real example of positional encoding with a toy embedding size of 4</p>
<p>What might this pattern look like?</p>
<p>In the following figure, <code>each row corresponds the a positional encoding of a vector.</code> So the <code>first row would be the vector we’d add to the embedding of the first word in an input sequence</code>. <code>Each row contains 512 values – each with a value between 1 and -1. We’ve color-coded them so the pattern is visible.</code></p>
<p><img src="https://i.loli.net/2020/07/25/XTrgBuPAKbU58lL.png" alt="img"><br>A real example of <code>positional encoding for 20 words (rows) with an embedding size of 512 (columns)</code>. You can see that it appears split in half down the center. <code>That&#39;s because the values of the left half are generated by one function (which uses sine), and the right half is generated by another function (which uses cosine).</code> They’re then concatenated to form each of the positional encoding vectors.</p>
<p>The formula for positional encoding is described in the paper (section 3.5). You can see the code for generating positional encodings in <a href="https://github.com/tensorflow/tensor2tensor/blob/23bd23b9830059fbc349381b70d9429b5c40a139/tensor2tensor/layers/common_attention.py" target="_blank" rel="noopener"><code>get_timing_signal_1d()</code></a>. <code>This is not the only possible method for positional encoding. It, however, gives the advantage of being able to scale to unseen lengths of sequences</code> (e.g. if our trained model is asked to translate a sentence longer than any of those in our training set).</p>
<p><strong>July 2020 Update:</strong> The positional encoding shown above is from the Tranformer2Transformer implementation of the Transformer. The method shown in the paper is slightly different in that it doesn’t directly concatenate, but interweaves the two signals. The following figure shows what that looks like. <a href="https://github.com/jalammar/jalammar.github.io/blob/master/notebookes/transformer/transformer_positional_encoding_graph.ipynb" target="_blank" rel="noopener">Here’s the code to generate it</a>:</p>
<p><img src="https://i.loli.net/2020/07/25/9V4sImeBjPaXQpE.png" alt="img"></p>
<h2 id="The-Residuals"><a href="#The-Residuals" class="headerlink" title="The Residuals"></a>The <code>Residuals</code></h2><p>One detail in the architecture of the encoder that we need to mention before moving on, is that each sub-layer (self-attention, ffnn) in each encoder has a residual connection around it, and is followed by a <a href="https://arxiv.org/abs/1607.06450" target="_blank" rel="noopener">layer-normalization</a> step.</p>
<p><img src="https://i.loli.net/2020/07/25/uteZNE9q7wxIo6Y.png" alt="img"></p>
<p>If we’re to visualize the vectors and the layer-norm operation associated with self attention, it would look like this:</p>
<p><img src="https://i.loli.net/2020/07/25/2GZuRSainMsYfHQ.png" alt="img"></p>
<p>This goes for the sub-layers of the decoder as well. If we’re to think of a Transformer of 2 stacked encoders and decoders, it would look something like this:</p>
<p><img src="https://i.loli.net/2020/07/25/9RHz2i8nZhyMPdt.png" alt="img"></p>
<h2 id="The-Decoder-Side"><a href="#The-Decoder-Side" class="headerlink" title="The Decoder Side"></a>The Decoder Side</h2><p>Now that we’ve covered most of the concepts on the encoder side, we basically know how the components of decoders work as well. But let’s take a look at how they work together.</p>
<p>The encoder start by processing the input sequence. <code>The output of the top encoder is then transformed into a set of attention vectors K and V. These are to be used by each decoder in its “encoder-decoder attention” layer which helps the decoder focus on appropriate places in the input sequence:</code></p>
<p><img src="https://i.loli.net/2020/07/26/2MZ3SophkRgD9eJ.gif" alt="MQfaxEpcKV8AGDv"></p>
<p>After finishing the encoding phase, we begin the decoding phase. <code>Each step in the decoding phase outputs an element from the output sequence (the English translation sentence in this case).</code></p>
<p>The following steps repeat the process until a <code>special symbol</code> is reached indicating the transformer decoder has completed its output. <code>The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did</code>. And just like we did with the encoder inputs, <code>we embed and add positional encoding</code> to those decoder inputs to indicate the position of each word.</p>
<p><img src="https://jalammar.github.io/images/t/transformer_decoding_2.gif" alt="img"></p>
<p><img src="https://i.loli.net/2020/07/26/lwq4T3gIaEZHdu5.png" alt="image-20200726205210899"></p>
<p>The self attention layers in the decoder operate in a slightly different way than the one in the encoder:</p>
<p><code>In the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence</code>. This is done by <code>masking future positions (</code>setting them to <code>-inf</code>) before the softmax step in the self-attention calculation.</p>
<p><code>The “Encoder-Decoder Attention” layer works just like multiheaded self-attention</code>, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack.</p>
<h2 id="The-Final-Linear-and-Softmax-Layer"><a href="#The-Final-Linear-and-Softmax-Layer" class="headerlink" title="The Final Linear and Softmax Layer"></a>The Final Linear and Softmax Layer</h2><p><code>The decoder stack outputs a vector of floats</code>. How do we turn that into a word? That’s the job of the final Linear layer which is followed by a Softmax Layer.</p>
<p><code>The Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector.</code></p>
<p><code>Let’s assume that our model knows 10,000 unique English words (our model’s “output vocabulary”) that it’s learned from its training dataset. This would make the logits vector 10,000 cells wide – each cell corresponding to the score of a unique word</code>. That is how we interpret the output of the model followed by the Linear layer.</p>
<p><code>The softmax layer then turns those scores into probabilities</code> (all positive, all add up to 1.0). <code>The cell with the highest probability is chosen</code>, and the <code>word associated with it is produced as the output for this time step.</code></p>
<p><img src="https://i.loli.net/2020/07/25/w62DtS8vVYpkQqg.png" alt="img"><br>This figure starts from the bottom with the vector produced as the output of the decoder stack. It is then turned into an output word.</p>
<h2 id="Recap-Of-Training"><a href="#Recap-Of-Training" class="headerlink" title="Recap Of Training"></a>Recap Of Training</h2><p>Now that we’ve covered the entire forward-pass process through a trained Transformer, it would be useful to glance at the intuition of training the model.</p>
<p><code>During training, an untrained model would go through the exact same forward pass. But since we are training it on a labeled training dataset, we can compare its output with the actual correct output.</code></p>
<p>To visualize this, let’s assume our output vocabulary only contains six words(“a”, “am”, “i”, “thanks”, “student”, and “<eos>” (short for ‘end of sentence’)).</p>
<p><img src="https://i.loli.net/2020/07/25/BgKxbL57uJGiZ4E.png" alt="img"><br><code>The output vocabulary of our model is created in the preprocessing phase before we even begin training.</code></p>
<p>Once we define our output vocabulary, we can use a vector of the same width to indicate each word in our vocabulary. This also known as <code>one-hot encoding</code>. So for example, we can indicate the word “am” using the following vector:</p>
<p><img src="https://i.loli.net/2020/07/25/ot8s4Gl6LNC3pdk.png" alt="img"><br>Example: one-hot encoding of our output vocabulary</p>
<p>Following this recap, let’s discuss the model’s loss function – the metric we are optimizing during the training phase to lead up to a trained and hopefully amazingly accurate model.</p>
<h2 id="The-Loss-Function"><a href="#The-Loss-Function" class="headerlink" title="The Loss Function"></a>The <code>Loss Function</code></h2><p>Say we are training our model. Say it’s our first step in the training phase, and we’re training it on a simple example – translating “merci” into “thanks”.</p>
<p>What this means, is that we want the output to be a probability distribution indicating the word “thanks”. But since this model is not yet trained, that’s unlikely to happen just yet.</p>
<p><img src="https://i.loli.net/2020/07/25/qcKFVhUSmiNIkTJ.png" alt="img"><br><code>Since the model&#39;s parameters (weights) are all initialized randomly, the (untrained) model produces a probability distribution with arbitrary values for each cell/word.</code> We can compare it with the actual output, then tweak all the model’s weights using <code>backpropagation</code> to make the output closer to the desired output.</p>
<p>How do you compare two probability distributions? We simply subtract one from the other. For more details, look at <a href="https://colah.github.io/posts/2015-09-Visual-Information/" target="_blank" rel="noopener">cross-entropy</a> and <a href="https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained" target="_blank" rel="noopener">Kullback–Leibler divergence</a>.</p>
<p>But note that this is an oversimplified example. More realistically, we’ll use a sentence longer than one word. For example – input: “je suis étudiant” and expected output: “i am a student”. What this really means, is that we want our model to successively output probability distributions where:</p>
<ul>
<li><code>Each probability distribution is represented by a vector of width vocab_size</code> (6 in our toy example, but more realistically a number like 30,000 or 50,000)</li>
<li>The first probability distribution has the highest probability at the cell associated with the word “i”</li>
<li>The second probability distribution has the highest probability at the cell associated with the word “am”</li>
<li>And so on, until the fifth output distribution indicates ‘<code>&lt;end of sentence&gt;</code>’ symbol, which also has a cell associated with it from the 10,000 element vocabulary.</li>
</ul>
<p><img src="https://i.loli.net/2020/07/25/VKGpQ8MqYTnrROs.png" alt="img"><br>The targeted probability distributions we’ll train our model against in the training example for one sample sentence.</p>
<p>After training the model for enough time on a large enough dataset, we would <code>hope the produced probability distributions would look like this:</code></p>
<p><img src="https://jalammar.github.io/images/t/output_trained_model_probability_distributions.png" alt="img"><br>Hopefully upon training, the model would output the right translation we expect. Of course it’s no real indication if this phrase was part of the training dataset (see: <a href="https://www.youtube.com/watch?v=TIgfjmp-4BA" target="_blank" rel="noopener">cross validation</a>). Notice that every position gets a little bit of probability even if it’s unlikely to be the output of that time step – that’s a very useful property of softmax which helps the training process.</p>
<p>Now, <code>because the model produces the outputs one at a time, we can assume that the model is selecting the word with the highest probability from that probability distribution and throwing away the rest.</code> That’s one way to do it (called <code>greedy decoding</code>). Another way to do it would be to hold on to, say, the top two words (say, ‘I’ and ‘a’ for example), then in the next step, run the model twice: once assuming the first output position was the word ‘I’, and another time assuming the first output position was the word ‘a’, and whichever version produced less error considering both positions #1 and #2 is kept. We repeat this for positions #2 and #3…etc. This method is called “<code>beam search</code>”, where in our example, beam_size was two (meaning that at all times, two partial hypotheses (unfinished translations) are kept in memory), and top_beams is also two (meaning we’ll return two translations). These are both hyperparameters that you can experiment with.</p>
<h2 id="Go-Forth-And-Transform"><a href="#Go-Forth-And-Transform" class="headerlink" title="Go Forth And Transform"></a>Go Forth And Transform</h2><p>I hope you’ve found this a useful place to start to break the ice with the major concepts of the Transformer. If you want to go deeper, I’d suggest these next steps:</p>
<ul>
<li>Read the <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need</a> paper, the Transformer blog post (<a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html" target="_blank" rel="noopener">Transformer: A Novel Neural Network Architecture for Language Understanding</a>), and the <a href="https://ai.googleblog.com/2017/06/accelerating-deep-learning-research.html" target="_blank" rel="noopener">Tensor2Tensor announcement</a>.</li>
<li>Watch <a href="https://www.youtube.com/watch?v=rBCqOTEfxvg" target="_blank" rel="noopener">Łukasz Kaiser’s talk</a> walking through the model and its details</li>
<li>Play with the <a href="https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb" target="_blank" rel="noopener">Jupyter Notebook provided as part of the Tensor2Tensor repo</a></li>
<li>Explore the <a href="https://github.com/tensorflow/tensor2tensor" target="_blank" rel="noopener">Tensor2Tensor repo</a>.</li>
</ul>
]]></content>
      <categories>
        <category>DeepLearning</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
      </tags>
  </entry>
</search>
